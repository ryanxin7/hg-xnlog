[{"categories":["Github"],"content":"使用 GitHub Actions 来自动构建 Docker 镜像并将其上传到 Docker Registry 时，需要以下步骤进行设置： 工作流会在每次将代码推送到 main 分支时执行。它首先检出代码，然后设置 Docker Buildx 环境，接着登录到指定的 Docker Registry，最后构建并推送 Docker 镜像。 创建 Dockerfile：在你的 GitHub 仓库中创建一个名为 Dockerfile 的文件，用于定义镜像的构建过程和内容。 设置 Secrets：在仓库的设置中，添加三个 Secrets，分别是你的 Docker Registry 用户名、密码或访问令牌，以及 Docker Registry 的地址。 创建 Workflow 文件：在 .github/workflows/ 目录下创建一个 .yml 文件（例如：docker-build.yml），在这个文件中定义工作流程的步骤。 Workflow 配置：在 Workflow 文件中，配置工作流程的触发条件，比如当代码被推送到特定分支时触发。然后，定义构建步骤，包括： 检出代码 设置 Docker Buildx 环境（用于构建多平台镜像） 登录到 Docker Registry，使用之前设置的 Secrets 使用 Docker 构建和推送镜像到 Registry，可以指定标签等信息。 ","date":"2023-08-25","objectID":"/posts/blog/push-docker-images-github-registry/:0:0","tags":["Docker","Hugo"],"title":"使用Github Action 构建Docker镜像并上传Registry","uri":"/posts/blog/push-docker-images-github-registry/"},{"categories":["Github"],"content":"创建拥有上传权限的token ","date":"2023-08-25","objectID":"/posts/blog/push-docker-images-github-registry/:1:0","tags":["Docker","Hugo"],"title":"使用Github Action 构建Docker镜像并上传Registry","uri":"/posts/blog/push-docker-images-github-registry/"},{"categories":["Github"],"content":"测试登录Registry 测试登录 root@harbor01[14:16:02]~ #:docker login --username ryanxin7 --password ghp_xxxxxxxx ghcr.io WARNING! Using --password via the CLI is insecure. Use --password-stdin. WARNING! Your password will be stored unencrypted in /root/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded","date":"2023-08-25","objectID":"/posts/blog/push-docker-images-github-registry/:2:0","tags":["Docker","Hugo"],"title":"使用Github Action 构建Docker镜像并上传Registry","uri":"/posts/blog/push-docker-images-github-registry/"},{"categories":["Github"],"content":"构建镜像测试上传到Regisry root@harbor01[14:22:43]/dockerfile/xn-blog #:docker image build -t ghcr.io/xnlog:latest ./ DEPRECATED: The legacy builder is deprecated and will be removed in a future release. Install the buildx component to build images with BuildKit: https://docs.docker.com/go/buildx/ Sending build context to Docker daemon 19.04MB Step 1/4 : FROM nginx:latest ---\u003e 89da1fb6dcb9 Step 2/4 : COPY public/ /usr/share/nginx/html ---\u003e Using cache ---\u003e 342e46da94ee Step 3/4 : COPY default.conf /etc/nginx/conf.d/default.conf ---\u003e Using cache ---\u003e 56c7d4347a26 Step 4/4 : EXPOSE 8848 ---\u003e Using cache ---\u003e 35e2e284b708 Successfully built 35e2e284b708 Successfully tagged ghcr.io/xnlog:latest root@racknerd-20e7f5:~# docker pull ghcr.io/ryanxin7/xnlog:latest latest: Pulling from ryanxin7/xnlog 52d2b7f179e3: Pull complete fd9f026c6310: Pull complete 055fa98b4363: Pull complete 96576293dd29: Pull complete a7c4092be904: Pull complete e3b6889c8954: Pull complete da761d9a302b: Pull complete e8c074410147: Pull complete 4d2b965ac974: Pull complete Digest: sha256:3bcffe2f09e7584d9b05da90af16c43b195c377ce645dbc013f8b9ba70ce83de Status: Downloaded newer image for ghcr.io/ryanxin7/xnlog:latest ghcr.io/ryanxin7/xnlog:latest","date":"2023-08-25","objectID":"/posts/blog/push-docker-images-github-registry/:3:0","tags":["Docker","Hugo"],"title":"使用Github Action 构建Docker镜像并上传Registry","uri":"/posts/blog/push-docker-images-github-registry/"},{"categories":["Github"],"content":"在packages中查看镜像 ","date":"2023-08-25","objectID":"/posts/blog/push-docker-images-github-registry/:4:0","tags":["Docker","Hugo"],"title":"使用Github Action 构建Docker镜像并上传Registry","uri":"/posts/blog/push-docker-images-github-registry/"},{"categories":["Github"],"content":"配置action secret ","date":"2023-08-25","objectID":"/posts/blog/push-docker-images-github-registry/:5:0","tags":["Docker","Hugo"],"title":"使用Github Action 构建Docker镜像并上传Registry","uri":"/posts/blog/push-docker-images-github-registry/"},{"categories":["Github"],"content":"创建workflow文件 mkdir workflow name: Docker Image CI for GHCR on: push jobs: build_and_publish: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - name: Build and push the image run: | docker login --username ryanxin7 --password ${{ secrets.DOCKERPACKAING }} ghcr.io docker build docker/. --tag ghcr.io/ryanxin7/xnlog:latest docker push ghcr.io/ryanxin7/xnlog:latest ","date":"2023-08-25","objectID":"/posts/blog/push-docker-images-github-registry/:6:0","tags":["Docker","Hugo"],"title":"使用Github Action 构建Docker镜像并上传Registry","uri":"/posts/blog/push-docker-images-github-registry/"},{"categories":["Github"],"content":"测试提交代码 xx9z@xin MINGW64 /c/xnblog/xnlog (main) $ git add . xx9z@xin MINGW64 /c/xnblog/xnlog (main) $ git commit -m \"blog update\" [main a5724b1] blog update 5 files changed, 477 insertions(+) create mode 100644 content/posts/Blog/push-docker-images-github-registry.md rename \"content/posts/\\344\\275\\277\\347\\224\\250Algolia\\345\\256\\236\\347\\216\\260Hugo\\346\\234\\254\\345\\234\\260\\346\\231\\272\\350\\203\\275\\346\\220\\234\\347\\264\\242.md\" =\u003e \"content/posts/Blog/\\344\\275\\277\\347\\224\\250Algolia\\345\\256\\236\\347\\216\\260Hugo\\346\\234\\254\\345\\234\\260\\346\\231\\272\\350\\203\\275\\346\\220\\234\\347\\264\\242.md\" (100%) create mode 100644 content/posts/kubernetes/k8s-replace-NFS-storage.md create mode 100644 \"content/posts/kubernetes/k8s\\345\\274\\272\\345\\210\\266\\345\\210\\240\\351\\231\\244pod\u0026pv\u0026pvc\\345\\222\\214ns\u0026namespace\\346\\226\\271\\346\\263\\225.md\" create mode 100644 content/posts/kubernetes/redis-on-k8scluster.md xx9z@xin MINGW64 /c/xnblog/xnlog (main) $ git push origin main Enumerating objects: 14, done. Counting objects: 100% (14/14), done. Delta compression using up to 16 threads Compressing objects: 100% (9/9), done. Writing objects: 100% (10/10), 5.57 KiB | 2.78 MiB/s, done. Total 10 (delta 2), reused 0 (delta 0), pack-reused 0 remote: Resolving deltas: 100% (2/2), completed with 2 local objects. To github.com:ryanxin7/hg-xnlog.git 12d766d..a5724b1 main -\u003e main ","date":"2023-08-25","objectID":"/posts/blog/push-docker-images-github-registry/:7:0","tags":["Docker","Hugo"],"title":"使用Github Action 构建Docker镜像并上传Registry","uri":"/posts/blog/push-docker-images-github-registry/"},{"categories":["Hugo"],"content":"1.简介 Algolia是一家提供搜索即服务的技术公司，帮助开发者为他们的应用程序或网站构建高速、精准的搜索功能。 免费的计划每个月可以查询10000次，对于个人站点也是够用了。 ","date":"2023-08-15","objectID":"/posts/blog/%E4%BD%BF%E7%94%A8algolia%E5%AE%9E%E7%8E%B0hugo%E6%9C%AC%E5%9C%B0%E6%99%BA%E8%83%BD%E6%90%9C%E7%B4%A2/:1:0","tags":["个人网站","Hugo"],"title":"使用Algolia实现Hugo本地智能搜索","uri":"/posts/blog/%E4%BD%BF%E7%94%A8algolia%E5%AE%9E%E7%8E%B0hugo%E6%9C%AC%E5%9C%B0%E6%99%BA%E8%83%BD%E6%90%9C%E7%B4%A2/"},{"categories":["Hugo"],"content":"2. 配置Algolia Algolia的配置步骤通常包括以下几个主要方面：创建账户、导入数据、设置索引、集成到应用程序中以及调整搜索体验。 ","date":"2023-08-15","objectID":"/posts/blog/%E4%BD%BF%E7%94%A8algolia%E5%AE%9E%E7%8E%B0hugo%E6%9C%AC%E5%9C%B0%E6%99%BA%E8%83%BD%E6%90%9C%E7%B4%A2/:2:0","tags":["个人网站","Hugo"],"title":"使用Algolia实现Hugo本地智能搜索","uri":"/posts/blog/%E4%BD%BF%E7%94%A8algolia%E5%AE%9E%E7%8E%B0hugo%E6%9C%AC%E5%9C%B0%E6%99%BA%E8%83%BD%E6%90%9C%E7%B4%A2/"},{"categories":["Hugo"],"content":"2.1 创建账号 访问Algolia的官方网站（https://www.algolia.com/），注册一个账户并登录。Google和Github账号可以直接登录。 ","date":"2023-08-15","objectID":"/posts/blog/%E4%BD%BF%E7%94%A8algolia%E5%AE%9E%E7%8E%B0hugo%E6%9C%AC%E5%9C%B0%E6%99%BA%E8%83%BD%E6%90%9C%E7%B4%A2/:2:1","tags":["个人网站","Hugo"],"title":"使用Algolia实现Hugo本地智能搜索","uri":"/posts/blog/%E4%BD%BF%E7%94%A8algolia%E5%AE%9E%E7%8E%B0hugo%E6%9C%AC%E5%9C%B0%E6%99%BA%E8%83%BD%E6%90%9C%E7%B4%A2/"},{"categories":["Hugo"],"content":"2.2 创建应用程序 选择免费套餐 选择位置 ","date":"2023-08-15","objectID":"/posts/blog/%E4%BD%BF%E7%94%A8algolia%E5%AE%9E%E7%8E%B0hugo%E6%9C%AC%E5%9C%B0%E6%99%BA%E8%83%BD%E6%90%9C%E7%B4%A2/:2:2","tags":["个人网站","Hugo"],"title":"使用Algolia实现Hugo本地智能搜索","uri":"/posts/blog/%E4%BD%BF%E7%94%A8algolia%E5%AE%9E%E7%8E%B0hugo%E6%9C%AC%E5%9C%B0%E6%99%BA%E8%83%BD%E6%90%9C%E7%B4%A2/"},{"categories":["Hugo"],"content":"2.3 创建索引 在Algolia控制台中，创建一个新的索引。索引是存储数据的容器，用于执行搜索操作。 ","date":"2023-08-15","objectID":"/posts/blog/%E4%BD%BF%E7%94%A8algolia%E5%AE%9E%E7%8E%B0hugo%E6%9C%AC%E5%9C%B0%E6%99%BA%E8%83%BD%E6%90%9C%E7%B4%A2/:2:3","tags":["个人网站","Hugo"],"title":"使用Algolia实现Hugo本地智能搜索","uri":"/posts/blog/%E4%BD%BF%E7%94%A8algolia%E5%AE%9E%E7%8E%B0hugo%E6%9C%AC%E5%9C%B0%E6%99%BA%E8%83%BD%E6%90%9C%E7%B4%A2/"},{"categories":["Hugo"],"content":"2.4 导入数据 您可以使用Algolia的API、SDK或工具来导入数据，确保数据字段与搜索需求匹配。 ","date":"2023-08-15","objectID":"/posts/blog/%E4%BD%BF%E7%94%A8algolia%E5%AE%9E%E7%8E%B0hugo%E6%9C%AC%E5%9C%B0%E6%99%BA%E8%83%BD%E6%90%9C%E7%B4%A2/:2:4","tags":["个人网站","Hugo"],"title":"使用Algolia实现Hugo本地智能搜索","uri":"/posts/blog/%E4%BD%BF%E7%94%A8algolia%E5%AE%9E%E7%8E%B0hugo%E6%9C%AC%E5%9C%B0%E6%99%BA%E8%83%BD%E6%90%9C%E7%B4%A2/"},{"categories":["Hugo"],"content":"2.5 配置搜索属性 定义索引字段，为每个索引定义需要搜索和显示的字段。也可以设置字段的搜索权重和过滤条件。 ","date":"2023-08-15","objectID":"/posts/blog/%E4%BD%BF%E7%94%A8algolia%E5%AE%9E%E7%8E%B0hugo%E6%9C%AC%E5%9C%B0%E6%99%BA%E8%83%BD%E6%90%9C%E7%B4%A2/:2:5","tags":["个人网站","Hugo"],"title":"使用Algolia实现Hugo本地智能搜索","uri":"/posts/blog/%E4%BD%BF%E7%94%A8algolia%E5%AE%9E%E7%8E%B0hugo%E6%9C%AC%E5%9C%B0%E6%99%BA%E8%83%BD%E6%90%9C%E7%B4%A2/"},{"categories":["Hugo"],"content":"2.6 配置排名规则 调整搜索结果的排名规则，以确保最相关的结果显示在顶部 ","date":"2023-08-15","objectID":"/posts/blog/%E4%BD%BF%E7%94%A8algolia%E5%AE%9E%E7%8E%B0hugo%E6%9C%AC%E5%9C%B0%E6%99%BA%E8%83%BD%E6%90%9C%E7%B4%A2/:2:6","tags":["个人网站","Hugo"],"title":"使用Algolia实现Hugo本地智能搜索","uri":"/posts/blog/%E4%BD%BF%E7%94%A8algolia%E5%AE%9E%E7%8E%B0hugo%E6%9C%AC%E5%9C%B0%E6%99%BA%E8%83%BD%E6%90%9C%E7%B4%A2/"},{"categories":["Hugo"],"content":"2.6 测试搜索 通过Algolia UI界面，测试搜索效果。 ","date":"2023-08-15","objectID":"/posts/blog/%E4%BD%BF%E7%94%A8algolia%E5%AE%9E%E7%8E%B0hugo%E6%9C%AC%E5%9C%B0%E6%99%BA%E8%83%BD%E6%90%9C%E7%B4%A2/:2:7","tags":["个人网站","Hugo"],"title":"使用Algolia实现Hugo本地智能搜索","uri":"/posts/blog/%E4%BD%BF%E7%94%A8algolia%E5%AE%9E%E7%8E%B0hugo%E6%9C%AC%E5%9C%B0%E6%99%BA%E8%83%BD%E6%90%9C%E7%B4%A2/"},{"categories":["Hugo"],"content":"3.配置hugo主题使用Algolia搜索 ","date":"2023-08-15","objectID":"/posts/blog/%E4%BD%BF%E7%94%A8algolia%E5%AE%9E%E7%8E%B0hugo%E6%9C%AC%E5%9C%B0%E6%99%BA%E8%83%BD%E6%90%9C%E7%B4%A2/:3:0","tags":["个人网站","Hugo"],"title":"使用Algolia实现Hugo本地智能搜索","uri":"/posts/blog/%E4%BD%BF%E7%94%A8algolia%E5%AE%9E%E7%8E%B0hugo%E6%9C%AC%E5%9C%B0%E6%99%BA%E8%83%BD%E6%90%9C%E7%B4%A2/"},{"categories":["Hugo"],"content":"3.1 获取Algolia Key 点击右上角头像—\u003esetting—\u003e API Keys ","date":"2023-08-15","objectID":"/posts/blog/%E4%BD%BF%E7%94%A8algolia%E5%AE%9E%E7%8E%B0hugo%E6%9C%AC%E5%9C%B0%E6%99%BA%E8%83%BD%E6%90%9C%E7%B4%A2/:3:1","tags":["个人网站","Hugo"],"title":"使用Algolia实现Hugo本地智能搜索","uri":"/posts/blog/%E4%BD%BF%E7%94%A8algolia%E5%AE%9E%E7%8E%B0hugo%E6%9C%AC%E5%9C%B0%E6%99%BA%E8%83%BD%E6%90%9C%E7%B4%A2/"},{"categories":["Hugo"],"content":"3.2 配置主题相关参数 为了生成搜索功能所需要的 index.json, 请在你的 网站配置 中添加 JSON 输出文件类型到 outputs 部分的 home 字段中。 在 Hugo 中，可以通过修改网站配置文件（通常是 config.toml、config.yaml 或 config.json）来指定不同部分的输出格式。这样，可以在生成网站页面时，为不同的页面部分选择不同的输出格式，包括 JSON。 打开网站配置文件。 寻找名为 outputs 的部分，如果没有则创建它。 在 outputs 部分中，可以为不同的页面部分指定输出格式。 # Options to make hugo output files home = [\"HTML\", \"RSS\", \"JSON\", \"BaiduUrls\"] page = [\"HTML\", \"MarkDown\"] section = [\"HTML\", \"RSS\"] taxonomy = [\"HTML\", \"RSS\"] taxonomyTerm = [\"HTML\"]保存网站配置文件 在运行 Hugo 构建命令（例如 hugo 或 hugo build）以生成网站时，Hugo 将生成 index.json 文件作为上述部分的输出。 outputFormats 在 Algolia 中，outputFormats 是用于控制返回搜索结果的格式的设置选项之一。 通过调整 outputFormats，可以决定搜索结果以何种格式返回应用程序。 [MarkDown] mediaType = \"text/markdown\" isPlainText = true isHTML = false # Options to make output baidu_urls.txt file [BaiduUrls] baseName = \"baidu_urls\" mediaType = \"text/plain\" isPlainText = true isHTML = false这段代码的作用是为 Hugo 网站生成 Algolia 搜索所需的 JSON 索引文件，以便在搜索时快速检索和展示内容。 {{- if .Site.Params.search -}} {{- $index := slice -}} {{- $pages := where .Site.RegularPages \"Params.password\" \"eq\" nil -}} {{- if .Site.Params.page.hiddenFromSearch -}} {{- $pages = where $pages \"Params.hiddenfromsearch\" false -}} {{- else -}} {{- $pages = where $pages \"Params.hiddenfromsearch\" \"!=\" true -}} {{- end -}} {{- range $pages -}} {{- $uri := .RelPermalink -}} {{- if $.Site.Params.search.absoluteURL -}} {{- $uri = .Permalink -}} {{- end -}} {{- $meta := dict \"uri\" $uri \"title\" .Title \"tags\" .Params.tags \"categories\" .Params.categories -}} {{- $meta = $.Site.Params.dateFormat | default \"2006-01-02\" | .PublishDate.Format | dict \"date\" | merge $meta -}} {{- with .Description -}} {{- $index = $index | append (dict \"content\" . \"objectID\" $uri | merge $meta) -}} {{- end -}} {{- $params := .Params | merge $.Site.Params.page -}} {{/* Extended Markdown syntax */}} {{- $content := dict \"Content\" .Content \"Ruby\" $params.ruby \"Fraction\" $params.fraction \"Fontawesome\" $params.fontawesome | partial \"function/content.html\" -}} {{/* Remove line number for code */}} {{- $content = $content | replaceRE `\u003cspan class=\"lnt?\"\u003e *\\d*\\n?\u003c/span\u003e` \"\" -}} {{- range $i, $contenti := split $content \"\u003ch2 id=\" -}} {{- if gt $i 0 -}} {{- $contenti = printf \"\u003ch2 id=%v\" $contenti -}} {{- end -}} {{- range $j, $contentj := split $contenti \"\u003ch3 id=\" -}} {{- if gt $j 0 -}} {{- $contentj = printf \"\u003ch3 id=%v\" $contentj -}} {{- end -}} {{/* Plainify, unescape and remove (\\n, \\t) */}} {{- $contentj = $contentj | plainify | htmlUnescape | replaceRE `[\\n\\t ]+` \" \" -}} {{- if gt $.Site.Params.search.contentLength 0 -}} {{- $contentj = substr $contentj 0 $.Site.Params.search.contentLength -}} {{- end -}} {{- if $contentj | and (ne $contentj \" \") -}} {{- $one := printf \"%v:%v:%v\" $uri $i $j | dict \"content\" $contentj \"objectID\" | merge $meta -}} {{- $index = $index | append $one -}} {{- end -}} {{- end -}} {{- end -}} {{- end -}} {{- $index | jsonify | safeJS -}} {{- end -}}","date":"2023-08-15","objectID":"/posts/blog/%E4%BD%BF%E7%94%A8algolia%E5%AE%9E%E7%8E%B0hugo%E6%9C%AC%E5%9C%B0%E6%99%BA%E8%83%BD%E6%90%9C%E7%B4%A2/:3:2","tags":["个人网站","Hugo"],"title":"使用Algolia实现Hugo本地智能搜索","uri":"/posts/blog/%E4%BD%BF%E7%94%A8algolia%E5%AE%9E%E7%8E%B0hugo%E6%9C%AC%E5%9C%B0%E6%99%BA%E8%83%BD%E6%90%9C%E7%B4%A2/"},{"categories":["Hugo"],"content":"3.3 配置hugo主题应用Algolia # 搜索配置 [params.search] enable = true # 搜索引擎的类型 [\"lunr\", \"algolia\", \"fuse\"] type = \"algolia\" # 文章内容最长索引长度 contentLength = 4000 # 搜索框的占位提示语 placeholder = \"\" # 最大结果数目 maxResultLength = 10 # 结果内容片段长度 snippetLength = 50 # 搜索结果中高亮部分的 HTML 标签 highlightTag = \"em\" # 是否在搜索索引中使用基于 baseURL 的绝对路径 absoluteURL = false [params.search.algolia] index = \"xx-log\" appID = \"SFSFN4DBN1\" searchKey = \"bd48328538sdb2f38b20753c17c60ba92f\"","date":"2023-08-15","objectID":"/posts/blog/%E4%BD%BF%E7%94%A8algolia%E5%AE%9E%E7%8E%B0hugo%E6%9C%AC%E5%9C%B0%E6%99%BA%E8%83%BD%E6%90%9C%E7%B4%A2/:3:3","tags":["个人网站","Hugo"],"title":"使用Algolia实现Hugo本地智能搜索","uri":"/posts/blog/%E4%BD%BF%E7%94%A8algolia%E5%AE%9E%E7%8E%B0hugo%E6%9C%AC%E5%9C%B0%E6%99%BA%E8%83%BD%E6%90%9C%E7%B4%A2/"},{"categories":["Hugo"],"content":"4.测试效果 ","date":"2023-08-15","objectID":"/posts/blog/%E4%BD%BF%E7%94%A8algolia%E5%AE%9E%E7%8E%B0hugo%E6%9C%AC%E5%9C%B0%E6%99%BA%E8%83%BD%E6%90%9C%E7%B4%A2/:4:0","tags":["个人网站","Hugo"],"title":"使用Algolia实现Hugo本地智能搜索","uri":"/posts/blog/%E4%BD%BF%E7%94%A8algolia%E5%AE%9E%E7%8E%B0hugo%E6%9C%AC%E5%9C%B0%E6%99%BA%E8%83%BD%E6%90%9C%E7%B4%A2/"},{"categories":null,"content":"1. 开通 Github Aciton 上传代码一般已Github仓库为主，但Jenkins由于网络原因经常无法拉取Github上的代码，于是考虑将Github仓库自动同步到Gitee上，拉取国内仓库代码进行自动部署。 ","date":"2023-06-12","objectID":"/posts/cicd/github-synctogitee/:1:0","tags":["CI/CD","持续集成"],"title":"Github Actions 自动同步到 Gitee","uri":"/posts/cicd/github-synctogitee/"},{"categories":null,"content":"1.1 在Github仓库下开通Actions的功能 点击Actions选项卡→ 点击右下角Create a new workflow，命名为SyncToGitee.yml即可 ","date":"2023-06-12","objectID":"/posts/cicd/github-synctogitee/:1:1","tags":["CI/CD","持续集成"],"title":"Github Actions 自动同步到 Gitee","uri":"/posts/cicd/github-synctogitee/"},{"categories":null,"content":"1.2 编写workflow的yml代码 可以复制如下代码到自己yml中，需要更改的地方，在代码中已经标出。 name: SyncToGitee on: push: branches: - main jobs: repo-sync: runs-on: ubuntu-latest steps: - name: Checkout source codes uses: actions/checkout@v3 - name: Mirror the Github organization repos to Gitee. uses: Yikun/hub-mirror-action@master with: src: 'github/ryanxin7' # 这里改为自己github账号名称，如github/ryanxin7 dst: 'gitee/ryanxin' # 这里改为gitee上账号名称，如gitee/ryanxin dst_key: ${{ secrets.GITEE_PRIVATE_KEY }} # 这是本地生成的私钥，Github拿着私钥调用Gitee公钥 dst_token: ${{ secrets.GITEE_TOKEN }} # 这是gitee上生成的token，下面会讲 force_update: true static_list: \"xxlog\" # 同步的仓库名称，这里为xxlog，意思是会自动同步该仓库到gitee下同名仓库 debug: true","date":"2023-06-12","objectID":"/posts/cicd/github-synctogitee/:1:2","tags":["CI/CD","持续集成"],"title":"Github Actions 自动同步到 Gitee","uri":"/posts/cicd/github-synctogitee/"},{"categories":null,"content":"2.配置公钥私钥和Gitee Token ","date":"2023-06-12","objectID":"/posts/cicd/github-synctogitee/:2:0","tags":["CI/CD","持续集成"],"title":"Github Actions 自动同步到 Gitee","uri":"/posts/cicd/github-synctogitee/"},{"categories":null,"content":"2.1 配置Gitee私钥 配置公钥和私钥：公钥是Gitee这里拿着，私钥是Github拿着。因为是Github这里要同步到Gitee. 生成私钥和公钥：ssh-kengen -t ed25529 -C xxxx@xxx.com ，具体可参见：生成/添加SSH公钥 生成完之后，会在指定目录下有两个文件：id_ed25519和id_ed25519.public，前者是私钥，后者是公钥 将id_ed25519用记事本打开，复制里面内容，粘贴到Github个人仓库下的secret中。 步骤：点击仓库首页选项卡setting，会看到如下图，点击新建New repository secret： 输入Name为GITEE_PRIVATE_KEY, Value为复制id_ed25519的私钥内容 ","date":"2023-06-12","objectID":"/posts/cicd/github-synctogitee/:2:1","tags":["CI/CD","持续集成"],"title":"Github Actions 自动同步到 Gitee","uri":"/posts/cicd/github-synctogitee/"},{"categories":null,"content":"2.2 配置Gitee 公钥 输入标题为GITEE_PUB_KEY, Value为复制id_ed25519.pub的私钥内容 ","date":"2023-06-12","objectID":"/posts/cicd/github-synctogitee/:2:2","tags":["CI/CD","持续集成"],"title":"Github Actions 自动同步到 Gitee","uri":"/posts/cicd/github-synctogitee/"},{"categories":null,"content":"2.3 配置私人令牌 打开Gitee个人账号的设置页面 → 点击安全设置下的私人令牌 → 右上角生成新令牌，如下图所示： 需要添加以下权限： 点击提交之后，会得到类似下图所示的私人令牌，将其复制，并配置到Github的secret界面，类似上一步的私钥那样。 配置到Github的secret界面 最终Github这里配置的Actions secrets如下： ","date":"2023-06-12","objectID":"/posts/cicd/github-synctogitee/:2:3","tags":["CI/CD","持续集成"],"title":"Github Actions 自动同步到 Gitee","uri":"/posts/cicd/github-synctogitee/"},{"categories":null,"content":"3.查看同步状态 成功同步 ","date":"2023-06-12","objectID":"/posts/cicd/github-synctogitee/:2:4","tags":["CI/CD","持续集成"],"title":"Github Actions 自动同步到 Gitee","uri":"/posts/cicd/github-synctogitee/"},{"categories":null,"content":"Jenkins 安装与基础配置 ","date":"2023-06-12","objectID":"/posts/cicd/jenkins-install/:1:0","tags":["CI/CD"],"title":"Jenkins 安装与基础配置","uri":"/posts/cicd/jenkins-install/"},{"categories":null,"content":"配置java环境 Jdk下载：https://www.oracle.com/java/technologies/downloads/ 版本jdk要求: tar -xf #创建软连接 root@etcd02[11:05:51]/apps/Jenkins #:ln -sv /apps/jdk1.8.0_371/ /usr/local/jdk '/usr/local/jdk' -\u003e '/apps/Jenkins/jdk1.8.0_371/' root@etcd02[11:07:53]/apps/Jenkins #:ln -sv /apps/jdk1.8.0_371/bin/java /usr/bin/java '/usr/bin/java' -\u003e '/apps/Jenkins/jdk1.8.0_371/bin/java'root@server:/apps# ln -sv /apps/jdk-17.0.6/ /usr/local/jdk '/usr/local/jdk' -\u003e '/apps/jdk-17.0.6/' root@server:/apps# ln -sv /apps/jdk-17.0.6/bin/java /usr/bin/java '/usr/bin/java' -\u003e '/apps/jdk-17.0.6/bin/java' apt-get install fontconfig","date":"2023-06-12","objectID":"/posts/cicd/jenkins-install/:1:1","tags":["CI/CD"],"title":"Jenkins 安装与基础配置","uri":"/posts/cicd/jenkins-install/"},{"categories":null,"content":"配置环境变量 vim /etc/profile.d/jdk-bin-path.sh export JAVA_HOME=/usr/local/jdk export PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$PATH export CLASSPATH=.$CLASSPATH:$JAVA_HOME/lib:$JAVA_HOME/jre/lib:$JAVA_HOME/lib/tools.jar source /etc/profile.d/jdk-bin-path.shroot@etcd02[13:55:57]/etc/profile.d #:java -version java version \"1.8.0_371\" Java(TM) SE Runtime Environment (build 1.8.0_371-b11) Java HotSpot(TM) 64-Bit Server VM (build 25.371-b11, mixed mode)","date":"2023-06-12","objectID":"/posts/cicd/jenkins-install/:1:2","tags":["CI/CD"],"title":"Jenkins 安装与基础配置","uri":"/posts/cicd/jenkins-install/"},{"categories":null,"content":"安装Jenkins Ubuntu 安装包下载： https://mirrors.tuna.tsinghua.edu.cn/jenkins/debian-stable/ 安装安装依赖 apt install net-tools dpkg -i jenkins_2.361.4_all.deb 获取密码 设置清华源 该url是国内的清华大学的镜像地址（建议使用清华大学的镜像服务器，修改后刷新页面即可. https://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.json find / -name *.UpdateCenter.xml /var/lib/jenkins/hudson.model.UpdateCenter.xml vim /var/lib/jenkins/hudson.model.UpdateCenter.xml \u003c?xml version='1.1' encoding='UTF-8'?\u003e \u003csites\u003e \u003csite\u003e \u003cid\u003edefault\u003c/id\u003e \u003curl\u003ehttps://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.json\u003c/url\u003e \u003c/site\u003e \u003c/sites\u003e下载插件 ","date":"2023-06-12","objectID":"/posts/cicd/jenkins-install/:1:3","tags":["CI/CD"],"title":"Jenkins 安装与基础配置","uri":"/posts/cicd/jenkins-install/"},{"categories":null,"content":"1.安装插件 ","date":"2023-06-12","objectID":"/posts/cicd/jenkins-autodeploy/:1:0","tags":["CI/CD","持续集成"],"title":"Jenkins实践-自动构建并发布前端项目","uri":"/posts/cicd/jenkins-autodeploy/"},{"categories":null,"content":"1.1 安装NodeJS插件 点击系统管理,然后点击插件管理,在可选插件里面搜索NodeJS插件,然后安装 ","date":"2023-06-12","objectID":"/posts/cicd/jenkins-autodeploy/:1:1","tags":["CI/CD","持续集成"],"title":"Jenkins实践-自动构建并发布前端项目","uri":"/posts/cicd/jenkins-autodeploy/"},{"categories":null,"content":"1.2 安装连接SSH的插件 Publish Over SSH用于连接远程服务器 ","date":"2023-06-12","objectID":"/posts/cicd/jenkins-autodeploy/:1:2","tags":["CI/CD","持续集成"],"title":"Jenkins实践-自动构建并发布前端项目","uri":"/posts/cicd/jenkins-autodeploy/"},{"categories":null,"content":"1.3 安装把应用发布到远程服务器的插件 **Deploy to container **插件用于把打包的应用发布到远程服务 ","date":"2023-06-12","objectID":"/posts/cicd/jenkins-autodeploy/:1:3","tags":["CI/CD","持续集成"],"title":"Jenkins实践-自动构建并发布前端项目","uri":"/posts/cicd/jenkins-autodeploy/"},{"categories":null,"content":"2. 配置git和NodeJS环境 ","date":"2023-06-12","objectID":"/posts/cicd/jenkins-autodeploy/:2:0","tags":["CI/CD","持续集成"],"title":"Jenkins实践-自动构建并发布前端项目","uri":"/posts/cicd/jenkins-autodeploy/"},{"categories":null,"content":"2.1 安装配置git #安装git root@server:~# apt install git root@server:~# whereis git #查看git的执行文件位置, 默认是在 /usr/bin/git whereis git git: /usr/bin/git /usr/share/man/man1/git.1.gz ","date":"2023-06-12","objectID":"/posts/cicd/jenkins-autodeploy/:2:1","tags":["CI/CD","持续集成"],"title":"Jenkins实践-自动构建并发布前端项目","uri":"/posts/cicd/jenkins-autodeploy/"},{"categories":null,"content":"2.2 安装配置NodeJS NodeJs 下载地址：https://nodejs.org/dist/ cd /apps tar -zxvf node-v16.18.1-linux-x64.tar.gz #创建软连接 ln -sv node-v16.18.1-linux-x64/ /usr/local/node填写本地node路径 ","date":"2023-06-12","objectID":"/posts/cicd/jenkins-autodeploy/:2:2","tags":["CI/CD","持续集成"],"title":"Jenkins实践-自动构建并发布前端项目","uri":"/posts/cicd/jenkins-autodeploy/"},{"categories":null,"content":"3. 新建项目部署信息 ","date":"2023-06-12","objectID":"/posts/cicd/jenkins-autodeploy/:3:0","tags":["CI/CD","持续集成"],"title":"Jenkins实践-自动构建并发布前端项目","uri":"/posts/cicd/jenkins-autodeploy/"},{"categories":null,"content":"3.1 源码管理 填写项目仓库地址 配置免密公钥认证 ","date":"2023-06-12","objectID":"/posts/cicd/jenkins-autodeploy/:3:1","tags":["CI/CD","持续集成"],"title":"Jenkins实践-自动构建并发布前端项目","uri":"/posts/cicd/jenkins-autodeploy/"},{"categories":null,"content":"3.2 构建触发器 定时每五分钟检查一次代码仓库有没有新的提交，如果有新的提交就自动构建项目并发布到目标前端服务器。 ","date":"2023-06-12","objectID":"/posts/cicd/jenkins-autodeploy/:3:2","tags":["CI/CD","持续集成"],"title":"Jenkins实践-自动构建并发布前端项目","uri":"/posts/cicd/jenkins-autodeploy/"},{"categories":null,"content":"3.3 构建环境 3.4 执行Shell命令 npm config get registry npm install --legacy-peer-deps npm run docs:build cd src/.vuepress/dist export DIST_NAME=\"dist-v\"$(date +\"%Y%m%d%H%M%S\")\"\" tar -zcf $WORKSPACE/deployment/$DIST_NAME.tar.gz ./* \\cp $WORKSPACE/deployment/$DIST_NAME.tar.gz $WORKSPACE/deployment/dist-latest.tar.gz rm -rf $WORKSPACE/src/.vuepress/dist","date":"2023-06-12","objectID":"/posts/cicd/jenkins-autodeploy/:3:3","tags":["CI/CD","持续集成"],"title":"Jenkins实践-自动构建并发布前端项目","uri":"/posts/cicd/jenkins-autodeploy/"},{"categories":null,"content":"3.5 构建后操作 用到SSH Publishers 插件，将项目代码文件推送到目标主机。 SSH Publishers 配置 系统管理—\u003e 系统配置 —\u003e Publish over SSH Passphrase: 公钥密码 Name:目标服务器名称 Hostname：目标服务器IP地址 Username: 目标主机用户名 Remote Directory：目标主机存放目录 ","date":"2023-06-12","objectID":"/posts/cicd/jenkins-autodeploy/:3:4","tags":["CI/CD","持续集成"],"title":"Jenkins实践-自动构建并发布前端项目","uri":"/posts/cicd/jenkins-autodeploy/"},{"categories":null,"content":"4.测试项目自动发布 ","date":"2023-06-12","objectID":"/posts/cicd/jenkins-autodeploy/:4:0","tags":["CI/CD","持续集成"],"title":"Jenkins实践-自动构建并发布前端项目","uri":"/posts/cicd/jenkins-autodeploy/"},{"categories":null,"content":"4.1 测试手动构建发布 立即构建 控制台输出查看任务进度 构建成功 前端服务器目录下验证 ","date":"2023-06-12","objectID":"/posts/cicd/jenkins-autodeploy/:4:1","tags":["CI/CD","持续集成"],"title":"Jenkins实践-自动构建并发布前端项目","uri":"/posts/cicd/jenkins-autodeploy/"},{"categories":null,"content":"4.2 测试自动构建发布 代码更新后自动构建并发布 ","date":"2023-06-12","objectID":"/posts/cicd/jenkins-autodeploy/:4:2","tags":["CI/CD","持续集成"],"title":"Jenkins实践-自动构建并发布前端项目","uri":"/posts/cicd/jenkins-autodeploy/"},{"categories":["Kubernetes"],"content":"前言 Kubernetes二进制部署相对于使用自动化工具（如kubeadm）而言，涉及更多的手动步骤和配置过程。然而，这种部署方式在一些情境下具有显著的优势。通过手动下载、配置和启动Kubernetes的各个组件，用户能够获得更高程度的定制性和精细控制权，以便根据特定需求进行调整。这种灵活性使用户能够选择所需的Kubernetes版本、特定的组件配置，以及自定义的网络和存储方案。 同时，通过深入参与每个部署步骤，用户可以更加深入地理解Kubernetes的内部工作机制和组件之间的关系。这种深入了解对于排查问题、优化性能以及实现定制的集群架构至关重要。此外，二进制部署还允许用户在没有网络连接的环境中进行部署，这在某些限制性网络环境下非常有用。对于特殊场景，如需要定制化的认证、授权和网络设置，手动部署能够更好地满足需求。 然而，需要注意的是，Kubernetes的二进制部署需要更多的时间、技术知识和资源投入。相较于自动化工具，它可能增加了出错的风险，需要更多的监控和维护工作。因此，在选择部署方式时，应该根据自身的技能水平、时间成本和项目需求来选择适合自己的部署方式。 k8s-实战案例_v1.21.x-部署.pdf ","date":"2023-04-28","objectID":"/posts/kubernetes/kubernetes-1/:1:0","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署","uri":"/posts/kubernetes/kubernetes-1/"},{"categories":["Kubernetes"],"content":"1.基础环境配置 ","date":"2023-04-28","objectID":"/posts/kubernetes/kubernetes-1/:2:0","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署","uri":"/posts/kubernetes/kubernetes-1/"},{"categories":["Kubernetes"],"content":"1.1 时间同步 ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime cat /etc/default/locale LANG=en_US.UTF-8 LC_TIME=en_DK.UTF-8 */5 * * * * /usr/sbin/ntpdate time1.aliyun.com \u0026\u003e /dev/null \u0026\u0026 hwclock -w/usr/sbin/ntpdate ","date":"2023-04-28","objectID":"/posts/kubernetes/kubernetes-1/:2:1","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署","uri":"/posts/kubernetes/kubernetes-1/"},{"categories":["Kubernetes"],"content":"1.2 安裝docker root@master01:/home/ceamg# cd /apps/docker/ root@master01:/apps/docker# tar xvf docker-19.03.15-binary-install.tar.gz root@master01:/apps/docker# ll total 153128 drwxr-xr-x 2 root root 4096 Apr 11 2021 ./ drwxr-xr-x 3 root root 4096 Jan 2 03:52 ../ -rw-r--r-- 1 root root 647 Apr 11 2021 containerd.service -rw-r--r-- 1 root root 78156440 Jan 2 03:57 docker-19.03.15-binary-install.tar.gz -rw-r--r-- 1 root root 62436240 Feb 5 2021 docker-19.03.15.tgz -rwxr-xr-x 1 root root 16168192 Jun 24 2019 docker-compose-Linux-x86_64_1.24.1* -rwxr-xr-x 1 root root 2708 Apr 11 2021 docker-install.sh* -rw-r--r-- 1 root root 1683 Apr 11 2021 docker.service -rw-r--r-- 1 root root 197 Apr 11 2021 docker.socket -rw-r--r-- 1 root root 454 Apr 11 2021 limits.conf -rw-r--r-- 1 root root 257 Apr 11 2021 sysctl.conf#!/bin/bash DIR=`pwd` PACKAGE_NAME=\"docker-19.03.15.tgz\" DOCKER_FILE=${DIR}/${PACKAGE_NAME} centos_install_docker(){ grep \"Kernel\" /etc/issue \u0026\u003e /dev/null if [ $? -eq 0 ];then /bin/echo \"当前系统是`cat /etc/redhat-release`,即将开始系统初始化、配置docker-compose与安装docker\" \u0026\u0026 sleep 1 systemctl stop firewalld \u0026\u0026 systemctl disable firewalld \u0026\u0026 echo \"防火墙已关闭\" \u0026\u0026 sleep 1 systemctl stop NetworkManager \u0026\u0026 systemctl disable NetworkManager \u0026\u0026 echo \"NetworkManager\" \u0026\u0026 sleep 1 sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux \u0026\u0026 setenforce 0 \u0026\u0026 echo \"selinux 已关闭\" \u0026\u0026 sleep 1 \\cp ${DIR}/limits.conf /etc/security/limits.conf \\cp ${DIR}/sysctl.conf /etc/sysctl.conf /bin/tar xvf ${DOCKER_FILE} \\cp docker/* /usr/bin \\cp containerd.service /lib/systemd/system/containerd.service \\cp docker.service /lib/systemd/system/docker.service \\cp docker.socket /lib/systemd/system/docker.socket \\cp ${DIR}/docker-compose-Linux-x86_64_1.24.1 /usr/bin/docker-compose groupadd docker \u0026\u0026 useradd docker -g docker id -u magedu \u0026\u003e /dev/null if [ $? -ne 0 ];then useradd magedu usermod magedu -G docker fi systemctl enable containerd.service \u0026\u0026 systemctl restart containerd.service systemctl enable docker.service \u0026\u0026 systemctl restart docker.service systemctl enable docker.socket \u0026\u0026 systemctl restart docker.socket fi } ubuntu_install_docker(){ grep \"Ubuntu\" /etc/issue \u0026\u003e /dev/null if [ $? -eq 0 ];then /bin/echo \"当前系统是`cat /etc/issue`,即将开始系统初始化、配置docker-compose与安装docker\" \u0026\u0026 sleep 1 \\cp ${DIR}/limits.conf /etc/security/limits.conf \\cp ${DIR}/sysctl.conf /etc/sysctl.conf /bin/tar xvf ${DOCKER_FILE} \\cp docker/* /usr/bin \\cp containerd.service /lib/systemd/system/containerd.service \\cp docker.service /lib/systemd/system/docker.service \\cp docker.socket /lib/systemd/system/docker.socket \\cp ${DIR}/docker-compose-Linux-x86_64_1.24.1 /usr/bin/docker-compose ulimit -n 1000000 /bin/su -c - ceamg \"ulimit -n 1000000\" /bin/echo \"docker 安装完成!\" \u0026\u0026 sleep 1 id -u magedu \u0026\u003e /dev/null if [ $? -ne 0 ];then groupadd -r docker useradd -r -m -g docker docker fi systemctl enable containerd.service \u0026\u0026 systemctl restart containerd.service systemctl enable docker.service \u0026\u0026 systemctl restart docker.service systemctl enable docker.socket \u0026\u0026 systemctl restart docker.socket fi } main(){ centos_install_docker ubuntu_install_docker } mainsudo tee /etc/docker/daemon.json \u003c\u003c-'EOF' { \"registry-mirrors\": [\"https://lc2kkql3.mirror.aliyuncs.com\"], \"storage-driver\": \"overlay\", \"data-root\": \"/data/docker\" } EOF sudo systemctl daemon-reload sudo systemctl restart dockerroot@master01:~# cat /etc/sysctl.conf net.ipv4.ip_forward=1 vm.max_map_count=262144 kernel.pid_max=4194303 fs.file-max=1000000 net.ipv4.tcp_max_tw_buckets=6000 net.netfilter.nf_conntrack_max=2097152 net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 vm.swappiness=0root@master01:/apps/docker# bash docker-install.sh 当前系统是Ubuntu 20.04.3 LTS \\n \\l,即将开始系统初始化、配置docker-compose与安装docker docker/ docker/dockerd docker/docker-proxy docker/containerd-shim docker/docker-init docker/docker docker/runc docker/ctr docker/containerd su: user jack does not exist docker 安装完成! Created symlink /etc/systemd/system/multi-user.target.wants/co","date":"2023-04-28","objectID":"/posts/kubernetes/kubernetes-1/:2:2","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署","uri":"/posts/kubernetes/kubernetes-1/"},{"categories":["Kubernetes"],"content":"1.3 安装ansible #部署节点安装ansible root@master01:~# apt install python3-pip git root@master01:~# pip3 install ansible -i https://mirrors.aliyun.com/pypi/simple/ root@master01:~# ansible --version ansible [core 2.13.7] config file = None configured module search path = ['/root/.ansible/plugins/modules', '/usr/share/ansible/plugins/modules'] ansible python module location = /usr/local/lib/python3.8/dist-packages/ansible ansible collection location = /root/.ansible/collections:/usr/share/ansible/collections executable location = /usr/local/bin/ansible python version = 3.8.10 (default, Nov 14 2022, 12:59:47) [GCC 9.4.0] jinja version = 3.1.2 libyaml = True ","date":"2023-04-28","objectID":"/posts/kubernetes/kubernetes-1/:2:3","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署","uri":"/posts/kubernetes/kubernetes-1/"},{"categories":["Kubernetes"],"content":"1.4 配置集群免秘钥登录 #⽣成密钥对 root@k8s-master1:~# ssh-keygen #安装sshpass命令⽤于同步公钥到各k8s服务器 # apt-get install sshpass #分发公钥脚本： root@k8s-master1:~# cat scp-key.sh #!/bin/bash #⽬标主机列表 IP=\" 10.1.0.32 10.1.0.33 10.1.0.34 10.1.0.35 10.1.0.38 \" for node in ${IP};do sshpass -p ceamg.com ssh-copy-id ${node} -o StrictHostKeyChecking=no if [ $? -eq 0 ];then echo \"${node} 秘钥copy完成\" else echo \"${node} 秘钥copy失败\" fi done ","date":"2023-04-28","objectID":"/posts/kubernetes/kubernetes-1/:2:4","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署","uri":"/posts/kubernetes/kubernetes-1/"},{"categories":["Kubernetes"],"content":"1.5 部署节点下载kubeasz部署项⽬及组件 使⽤ **master01 **作为部署节点GitHub - easzlab/kubeasz: 使用Ansible脚本安装K8S集群，介绍组件交互原理，方便直接，不受国内网络环境影响 root@k8s-master1:~# export release=3.3.1 root@k8s-master1:~# curl -C- -fLO --retry 3 https://github.com/easzlab/kubeasz/releases/download/${release}/ezdownroot@master01:~# chmod a+x ezdown root@master01:~# ./ezdown -D 2023-01-02 13:28:24 INFO Action begin: download_all 2023-01-02 13:28:24 INFO downloading docker binaries, version 19.03.15 --2023-01-02 13:28:24-- https://mirrors.tuna.tsinghua.edu.cn/docker-ce/linux/static/stable/x86_64/docker-19.03.15.tgz Resolving mirrors.tuna.tsinghua.edu.cn (mirrors.tuna.tsinghua.edu.cn)... 101.6.15.130, 2402:f000:1:400::2 Connecting to mirrors.tuna.tsinghua.edu.cn (mirrors.tuna.tsinghua.edu.cn)|101.6.15.130|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 62436240 (60M) [application/octet-stream] Saving to: ‘docker-19.03.15.tgz’ docker-19.03.15.tgz 100%[========================================================================================================================\u003e] 59.54M 11.2MB/s in 5.5s 2023-01-02 13:28:29 (10.9 MB/s) - ‘docker-19.03.15.tgz’ saved [62436240/62436240] 2023-01-02 13:28:31 WARN docker is already running. 2023-01-02 13:28:31 INFO downloading kubeasz: 3.3.1 2023-01-02 13:28:31 DEBUG run a temporary container Unable to find image 'easzlab/kubeasz:3.3.1' locally 3.3.1: Pulling from easzlab/kubeasz Status: Image is up to date for easzlab/kubeasz:3.3.1 docker.io/easzlab/kubeasz:3.3.1 2023-01-02 13:41:44 INFO Action successed: download_all root@master01:~# cd /etc/kubeasz/ root@master01:/etc/kubeasz/down# ll total 1136932 drwxr-xr-x 2 root root 4096 Jan 2 13:41 ./ drwxrwxr-x 12 root root 4096 Jan 2 13:32 ../ -rw------- 1 root root 383673856 Jan 2 13:35 calico_v3.19.4.tar -rw------- 1 root root 48941568 Jan 2 13:36 coredns_1.9.3.tar -rw------- 1 root root 246784000 Jan 2 13:39 dashboard_v2.5.1.tar -rw-r--r-- 1 root root 62436240 Feb 1 2021 docker-19.03.15.tgz -rw------- 1 root root 106171392 Jan 2 13:37 k8s-dns-node-cache_1.21.1.tar -rw------- 1 root root 179129856 Jan 2 13:41 kubeasz_3.3.1.tar -rw------- 1 root root 43832320 Jan 2 13:40 metrics-scraper_v1.0.8.tar -rw------- 1 root root 65683968 Jan 2 13:41 metrics-server_v0.5.2.tar -rw------- 1 root root 721408 Jan 2 13:41 pause_3.7.tar -rw------- 1 root root 26815488 Jan 2 13:32 registry-2.tar上述脚本运行成功后，所有文件（kubeasz代码、二进制、离线镜像）均已整理好放入目录/etc/kubeasz ","date":"2023-04-28","objectID":"/posts/kubernetes/kubernetes-1/:2:5","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署","uri":"/posts/kubernetes/kubernetes-1/"},{"categories":["Kubernetes"],"content":"2.部署 harbor 镜像仓库 ","date":"2023-04-28","objectID":"/posts/kubernetes/kubernetes-1/:3:0","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署","uri":"/posts/kubernetes/kubernetes-1/"},{"categories":["Kubernetes"],"content":"2.1 创建自签ssl证书 #本地解析 root@harbor01:~# echo \"10.1.0.38 harbor.ceamg.com \u003e\u003e /etc/hosts\" mkdir -p /data/cert cd /data/cert #创建ca和harbor证书请求 openssl genrsa -out ca.key 4096 openssl req -x509 -new -nodes -sha512 -days 7300 -subj \"/C=CN/ST=Beijing/L=Beijing/O=example/OU=Personal/CN=harbor.ceamg.com\" -key ca.key -out ca.crt openssl genrsa -out harbor.ceamg.com.key 4096 openssl req -sha512 -new -subj \"/C=CN/ST=Beijing/L=Beijing/O=example/OU=Personal/CN=harbor.ceamg.com\" -key harbor.ceamg.com.key -out harbor.ceamg.com.csr #创建v3文件 cat \u003e v3.ext \u003c\u003c-EOF authorityKeyIdentifier=keyid,issuer basicConstraints=CA:FALSE keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment extendedKeyUsage = serverAuth subjectAltName = @alt_names [alt_names] DNS.1=harbor.ceamg.com DNS.2=harbor DNS.3=ks-allinone EOF #使用v3文件签发harbor证书 openssl x509 -req -sha512 -days 7300 -extfile v3.ext -CA ca.crt -CAkey ca.key -CAcreateserial -in harbor.ceamg.com.csr -out harbor.ceamg.com.crt #转换成cert openssl x509 -inform PEM -in harbor.ceamg.com.crt -out harbor.ceamg.com.cert #添加根证书让系统信任证书 root@harbor01:/data/cert# cp harbor.ceamg.com.crt /usr/local/share/ca-certificates/ root@harbor01:/data/cert# update-ca-certificates Updating certificates in /etc/ssl/certs... rehash: warning: skipping ca-certificates.crt,it does not contain exactly one certificate or CRL 1 added, 0 removed; done. Running hooks in /etc/ca-certificates/update.d... done. #update-ca-certificates命令将PEM格式的根证书内容附加到/etc/ssl/certs/ca-certificates.crt ，而/etc/ssl/certs/ca-certificates.crt 包含了系统自带的各种可信根证书. ","date":"2023-04-28","objectID":"/posts/kubernetes/kubernetes-1/:3:1","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署","uri":"/posts/kubernetes/kubernetes-1/"},{"categories":["Kubernetes"],"content":"2.2 修改harbor配置 root@harbor01:/apps/harbor/harbor# cp harbor.yml.tmpl harbor.ymlroot@harbor01:/apps/harbor/harbor# grep -v \"#\" harbor.yml | grep -v \"^$\" hostname: harbor.ceamg.com http: port: 80 https: port: 443 certificate: /data/cert/harbor.ceamg.com.crt private_key: /apps/harbor/certs/harbor.ceamg.com.key harbor_admin_password: ceamg.com database: password: root123 max_idle_conns: 100 max_open_conns: 900 conn_max_lifetime: 5m conn_max_idle_time: 0 data_volume: /data trivy: ignore_unfixed: false skip_update: false offline_scan: false security_check: vuln insecure: false jobservice: max_job_workers: 10 notification: webhook_job_max_retry: 10 chart: absolute_url: disabled log: level: info local: rotate_count: 50 rotate_size: 200M location: /var/log/harbor _version: 2.7.0 proxy: http_proxy: https_proxy: no_proxy: components: - core - jobservice - trivy upload_purging: enabled: true age: 168h interval: 24h dryrun: false cache: enabled: false expire_hours: 24 ","date":"2023-04-28","objectID":"/posts/kubernetes/kubernetes-1/:3:2","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署","uri":"/posts/kubernetes/kubernetes-1/"},{"categories":["Kubernetes"],"content":"2.3 安装harbor Harbor – Reconfigure Harbor and Manage the Harbor Lifecycle 有扫描–with-trivy ,有认证–with-notary，有helm charts 模块退出–with-chartmuseum 其中–with-clair已弃用 #更新配置文件 root@harbor01:/apps/harbor/harbor# ./prepare root@harbor01:/apps/harbor/harbor# ./install.sh --with-notary --with-trivy --with-chartmuseum [Step 0]: checking if docker is installed ... Note: docker version: 19.03.15 [Step 1]: checking docker-compose is installed ... Note: docker-compose version: 1.24.1 [Step 2]: loading Harbor images ... 8991ee7e1c66: Loading layer [==================================================\u003e] 37.72MB/37.72MB caef0c5d2fe0: Loading layer [==================================================\u003e] 43.84MB/43.84MB d0ae0913849c: Loading layer [==================================================\u003e] 66.03MB/66.03MB d6c3137fc4e6: Loading layer [==================================================\u003e] 18.2MB/18.2MB db156fb6962c: Loading layer [==================================================\u003e] 65.54kB/65.54kB 578a990cf79f: Loading layer [==================================================\u003e] 2.56kB/2.56kB 9415b3c8b317: Loading layer [==================================================\u003e] 1.536kB/1.536kB bdb2dfba8b17: Loading layer [==================================================\u003e] 12.29kB/12.29kB 6a1b6c491cd2: Loading layer [==================================================\u003e] 2.613MB/2.613MB c35c2488b48b: Loading layer [==================================================\u003e] 407kB/407kB Loaded image: goharbor/prepare:v2.7.0 Loaded image: goharbor/harbor-db:v2.7.0 Loaded image: goharbor/harbor-core:v2.7.0 Loaded image: goharbor/harbor-log:v2.7.0 Loaded image: goharbor/harbor-exporter:v2.7.0 Loaded image: goharbor/nginx-photon:v2.7.0 Loaded image: goharbor/chartmuseum-photon:v2.7.0 Loaded image: goharbor/harbor-portal:v2.7.0 Loaded image: goharbor/harbor-jobservice:v2.7.0 Loaded image: goharbor/harbor-registryctl:v2.7.0 Loaded image: goharbor/registry-photon:v2.7.0 Loaded image: goharbor/notary-server-photon:v2.7.0 Loaded image: goharbor/redis-photon:v2.7.0 Loaded image: goharbor/notary-signer-photon:v2.7.0 Loaded image: goharbor/trivy-adapter-photon:v2.7.0 [Step 3]: preparing environment ... [Step 4]: preparing harbor configs ... prepare base dir is set to /apps/harbor/harbor Generated configuration file: /config/portal/nginx.conf Generated configuration file: /config/log/logrotate.conf Generated configuration file: /config/log/rsyslog_docker.conf Generated configuration file: /config/nginx/nginx.conf Generated configuration file: /config/core/env Generated configuration file: /config/core/app.conf Generated configuration file: /config/registry/config.yml Generated configuration file: /config/registryctl/env Generated configuration file: /config/registryctl/config.yml Generated configuration file: /config/db/env Generated configuration file: /config/jobservice/env Generated configuration file: /config/jobservice/config.yml Generated and saved secret to file: /data/secret/keys/secretkey Successfully called func: create_root_cert Generated configuration file: /config/trivy-adapter/env Generated configuration file: /compose_location/docker-compose.yml Clean up the input dir Note: stopping existing Harbor instance ... Removing network harbor_harbor WARNING: Network harbor_harbor not found. [Step 5]: starting Harbor ... Creating network \"harbor_harbor\" with the default driver Creating harbor-log ... done Creating redis ... done Creating harbor-portal ... done Creating registry ... done Creating harbor-db ... done Creating registryctl ... done Creating trivy-adapter ... done Creating harbor-core ... done Creating harbor-jobservice ... done Creating nginx ... done ✔ ----Harbor has been installed and started successfully.---- ","date":"2023-04-28","objectID":"/posts/kubernetes/kubernetes-1/:3:3","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署","uri":"/posts/kubernetes/kubernetes-1/"},{"categories":["Kubernetes"],"content":"2.4 harbor 调试 #关闭 harbor $~ sudo docker-compose down -v #更新配置 vim /apps/harbor.yml prepare #重新生成配置文件,增加上其他chart功能等 sudo prepare --with-notary --with-trivy --with-chartmuseum #启动 harbor $~ sudo docker-compose up -d ","date":"2023-04-28","objectID":"/posts/kubernetes/kubernetes-1/:3:4","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署","uri":"/posts/kubernetes/kubernetes-1/"},{"categories":["Kubernetes"],"content":"3.创建集群配置实例 ","date":"2023-04-28","objectID":"/posts/kubernetes/kubernetes-1/:4:0","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署","uri":"/posts/kubernetes/kubernetes-1/"},{"categories":["Kubernetes"],"content":"3.1 ⽣成k8s集群 hosts⽂件 root@master01:/etc/kubeasz# ./ezctl new k8s-01 2023-01-03 04:52:33 DEBUG generate custom cluster files in /etc/kubeasz/clusters/k8s-01 2023-01-03 04:52:33 DEBUG set versions 2023-01-03 04:52:33 DEBUG cluster k8s-01: files successfully created. 2023-01-03 04:52:33 INFO next steps 1: to config '/etc/kubeasz/clusters/k8s-01/hosts' 2023-01-03 04:52:33 INFO next steps 2: to config '/etc/kubeasz/clusters/k8s-01/config.yml'# 'NEW_INSTALL': 'true' to install a harbor server; 'false' to integrate with existed one [harbor] 10.1.0.38 NEW_INSTALL=false # [optional] loadbalance for accessing k8s from outside [ex_lb] #192.168.1.6 LB_ROLE=backup EX_APISERVER_VIP=192.168.1.250 EX_APISERVER_PORT=8443 #192.168.1.7 LB_ROLE=master EX_APISERVER_VIP=192.168.1.250 EX_APISERVER_PORT=8443 # [optional] ntp server for the cluster [chrony] #192.168.1.1 [all:vars] # --------- Main Variables --------------- # Secure port for apiservers SECURE_PORT=\"6443\" # Cluster container-runtime supported: docker, containerd # if k8s version \u003e= 1.24, docker is not supported CONTAINER_RUNTIME=\"containerd\" # Network plugins supported: calico, flannel, kube-router, cilium, kube-ovn CLUSTER_NETWORK=\"calico\" # Service proxy mode of kube-proxy: 'iptables' or 'ipvs' PROXY_MODE=\"ipvs\" # K8S Service CIDR, not overlap with node(host) networking SERVICE_CIDR=\"10.10.0.0/16\" # Cluster CIDR (Pod CIDR), not overlap with node(host) networking CLUSTER_CIDR=\"10.20.0.0/16\" # NodePort Range NODE_PORT_RANGE=\"30000-65535\" # Cluster DNS Domain CLUSTER_DNS_DOMAIN=\"ceamg.local\" # -------- Additional Variables (don't change the default value right now) --- # Binaries Directory bin_dir=\"/usr/local/bin\" # Deploy Directory (kubeasz workspace) base_dir=\"/etc/kubeasz\" # Directory for a specific cluster cluster_dir=\"{{ base_dir }}/clusters/k8s-01\" # CA and other components cert/key Directory ca_dir=\"/etc/kubernetes/ssl\" ","date":"2023-04-28","objectID":"/posts/kubernetes/kubernetes-1/:4:1","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署","uri":"/posts/kubernetes/kubernetes-1/"},{"categories":["Kubernetes"],"content":"3.1 ⽣成k8s集群 config ⽂件 root@master01:/etc/kubeasz# vim /etc/kubeasz/clusters/k8s-01/config.yml ############################ # prepare ############################ # 可选离线安装系统软件包 (offline|online) INSTALL_SOURCE: \"online\" # 可选进行系统安全加固 github.com/dev-sec/ansible-collection-hardening OS_HARDEN: false ############################ # role:deploy ############################ # default: ca will expire in 100 years # default: certs issued by the ca will expire in 50 years CA_EXPIRY: \"876000h\" CERT_EXPIRY: \"438000h\" # kubeconfig 配置参数 CLUSTER_NAME: \"cluster1\" CONTEXT_NAME: \"context-{{ CLUSTER_NAME }}\" # k8s version K8S_VER: \"1.24.2\" ############################ # role:etcd ############################ # 设置不同的wal目录，可以避免磁盘io竞争，提高性能 ETCD_DATA_DIR: \"/var/lib/etcd\" ETCD_WAL_DIR: \"\" ############################ # role:runtime [containerd,docker] ############################ # ------------------------------------------- containerd # [.]启用容器仓库镜像 ENABLE_MIRROR_REGISTRY: true # [containerd]基础容器镜像 SANDBOX_IMAGE: \"easzlab.io.local:5000/easzlab/pause:3.7\" # [containerd]容器持久化存储目录 CONTAINERD_STORAGE_DIR: \"/var/lib/containerd\" # ------------------------------------------- docker # [docker]容器存储目录 DOCKER_STORAGE_DIR: \"/var/lib/docker\" # [docker]开启Restful API ENABLE_REMOTE_API: false # [docker]信任的HTTP仓库 INSECURE_REG: '[\"http://easzlab.io.local:5000\"]' ############################ # role:kube-master ############################ # k8s 集群 master 节点证书配置，可以添加多个ip和域名（比如增加公网ip和域名） MASTER_CERT_HOSTS: - \"10.1.1.1\" - \"k8s.easzlab.io\" #- \"www.test.com\" # node 节点上 pod 网段掩码长度（决定每个节点最多能分配的pod ip地址） # 如果flannel 使用 --kube-subnet-mgr 参数，那么它将读取该设置为每个节点分配pod网段 # https://github.com/coreos/flannel/issues/847 NODE_CIDR_LEN: 24 ############################ # role:kube-node ############################ # Kubelet 根目录 KUBELET_ROOT_DIR: \"/var/lib/kubelet\" # node节点最大pod 数 MAX_PODS: 300 # 配置为kube组件（kubelet,kube-proxy,dockerd等）预留的资源量 # 数值设置详见templates/kubelet-config.yaml.j2 KUBE_RESERVED_ENABLED: \"yes\" # k8s 官方不建议草率开启 system-reserved, 除非你基于长期监控，了解系统的资源占用状况； # 并且随着系统运行时间，需要适当增加资源预留，数值设置详见templates/kubelet-config.yaml.j2 # 系统预留设置基于 4c/8g 虚机，最小化安装系统服务，如果使用高性能物理机可以适当增加预留 # 另外，集群安装时候apiserver等资源占用会短时较大，建议至少预留1g内存 SYS_RESERVED_ENABLED: \"no\" ############################ # role:network [flannel,calico,cilium,kube-ovn,kube-router] ############################ # ------------------------------------------- flannel # [flannel]设置flannel 后端\"host-gw\",\"vxlan\"等 FLANNEL_BACKEND: \"vxlan\" DIRECT_ROUTING: false # [flannel] flanneld_image: \"quay.io/coreos/flannel:v0.10.0-amd64\" flannelVer: \"v0.15.1\" flanneld_image: \"easzlab.io.local:5000/easzlab/flannel:{{ flannelVer }}\" # ------------------------------------------- calico # [calico]设置 CALICO_IPV4POOL_IPIP=“off”,可以提高网络性能，条件限制详见 docs/setup/calico.md CALICO_IPV4POOL_IPIP: \"Always\" # [calico]设置 calico-node使用的host IP，bgp邻居通过该地址建立，可手工指定也可以自动发现 IP_AUTODETECTION_METHOD: \"can-reach={{ groups['kube_master'][0] }}\" # [calico]设置calico 网络 backend: brid, vxlan, none CALICO_NETWORKING_BACKEND: \"brid\" # [calico]设置calico 是否使用route reflectors # 如果集群规模超过50个节点，建议启用该特性 CALICO_RR_ENABLED: false # CALICO_RR_NODES 配置route reflectors的节点，如果未设置默认使用集群master节点 # CALICO_RR_NODES: [\"192.168.1.1\", \"192.168.1.2\"] CALICO_RR_NODES: [] # [calico]更新支持calico 版本: [v3.3.x] [v3.4.x] [v3.8.x] [v3.15.x] calico_ver: \"v3.19.4\" # [calico]calico 主版本 calico_ver_main: \"{{ calico_ver.split('.')[0] }}.{{ calico_ver.split('.')[1] }}\" # ------------------------------------------- cilium # [cilium]镜像版本 cilium_ver: \"1.11.6\" cilium_connectivity_check: true cilium_hubble_enabled: false cilium_hubble_ui_enabled: false # ------------------------------------------- kube-ovn # [kube-ovn]选择 OVN DB and OVN Control Plane 节点，默认为第一个master节点 OVN_DB_NODE: \"{{ groups['kube_master'][0] }}\" # [kube-ovn]离线镜像tar包 kube_ovn_ver: \"v1.5.3\" # ------------------------------------------- kube-router # [kube-router]公有云上存在限制，一般需要始终开启 ipinip；自有环境可以设置为 \"subnet\" OVERLAY_TYPE: \"full\" # [kube-router]NetworkPolicy 支持开关 FIREWALL_ENABLE: true # [k","date":"2023-04-28","objectID":"/posts/kubernetes/kubernetes-1/:4:2","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署","uri":"/posts/kubernetes/kubernetes-1/"},{"categories":["Kubernetes"],"content":"4.步骤1-基础环境初始化 root@master01:/etc/kubeasz# ./ezctl help setup Usage: ezctl setup \u003ccluster\u003e \u003cstep\u003e available steps: 01 prepare to prepare CA/certs \u0026 kubeconfig \u0026 other system settings 02 etcd to setup the etcd cluster 03 container-runtime to setup the container runtime(docker or containerd) 04 kube-master to setup the master nodes 05 kube-node to setup the worker nodes 06 network to setup the network plugin 07 cluster-addon to setup other useful plugins 90 all to run 01~07 all at once 10 ex-lb to install external loadbalance for accessing k8s from outside 11 harbor to install a new harbor server or to integrate with an existed one examples: ./ezctl setup test-k8s 01 (or ./ezctl setup test-k8s prepare) ./ezctl setup test-k8s 02 (or ./ezctl setup test-k8s etcd) ./ezctl setup test-k8s all ./ezctl setup test-k8s 04 -t restart_master vim playbooks/01.prepare.yml #系统基础初始化主机配置 root@master01:/etc/kubeasz# ./ezctl setup k8s-01 01 #准备CA和基础系统设置 ","date":"2023-04-28","objectID":"/posts/kubernetes/kubernetes-1/:5:0","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署","uri":"/posts/kubernetes/kubernetes-1/"},{"categories":["Kubernetes"],"content":"5.步骤2-部署etcd集群 可更改启动脚本路径及版本等⾃定义配置 root@master01:/etc/kubeasz# ./ezctl setup k8s-01 02 ansible-playbook -i clusters/k8s-01/hosts -e @clusters/k8s-01/config.yml playbooks/02.etcd.yml 2023-01-03 13:39:13 INFO cluster:k8s-01 setup step:02 begins in 5s, press any key to abort健康检查 export NODE_IPS=\"10.1.0.34 10.1.0.35\" root@etcd01:~# for ip in ${NODE_IPS}; do ETCDCTL_API=3 /usr/local/bin/etcdctl --endpoints=https://${ip}:2379 --cacert=/etc/kubernetes/ssl/ca.pem --cert=/etc/kubernetes/ssl/etcd.pem --key=/etc/kubernetes/ssl/etcd-key.pem endpoint health; done https://10.1.0.34:2379 is healthy: successfully committed proposal: took = 14.95631ms https://10.1.0.35:2379 is healthy: successfully committed proposal: took = 15.037491ms 注：以上返回信息表示etcd集群运⾏正常，否则异常！部署containerd 同步docker证书脚本： #!/bin/bash #⽬标主机列表 IP=\" 10.1.0.32 10.1.0.33 10.1.0.34 10.1.0.35 \" for node in ${IP};do sshpass -p ceamg.com ssh-copy-id ${node} -o StrictHostKeyChecking=no if [ $? -eq 0 ];then echo \"${node} 秘钥copy完成\" echo \"${node} 秘钥copy完成,准备环境初始化.....\" ssh ${node} \"mkdir /etc/containerd/certs.d/harbor.ceamg.com -p\" echo \"Harbor 证书创建成功!\" scp /etc/containerd/certs.d/harbor.ceamg.com/harbor.ceamg.com.cert /etc/containerd/certs.d/harbor.ceamg.com/harbor.ceamg.com.crt /etc/containerd/certs.d/harbor.ceamg.com/harbor.ceamg.com.key /etc/containerd/certs.d/harbor.ceamg.com/ca.crt ${node}:/etc/containerd/certs.d/harbor.ceamg.com/ echo \"Harbor 证书拷贝成功!\" ssh ${node} \"echo \"10.1.0.38 harbor.ceamg.com\" \u003e\u003e /etc/hosts\" echo \"host 解析添加完成\" #scp -r /root/.docker ${node}:/root/ #echo \"Harbor 认证件拷完成!\" else echo \"${node} 秘钥copy失败\" fi done #执⾏脚本进⾏证书分发 root@k8s-master1:/etc/kubeasz# bash /root/scp-key.sh ","date":"2023-04-28","objectID":"/posts/kubernetes/kubernetes-1/:6:0","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署","uri":"/posts/kubernetes/kubernetes-1/"},{"categories":["Kubernetes"],"content":"6.步骤3-部署运行时环境 项目根据k8s版本提供不同的默认容器运行时： k8s 版本 \u003c 1.24 时，支持docker containerd 可选 k8s 版本 \u003e= 1.24 时，仅支持 containerd ","date":"2023-04-28","objectID":"/posts/kubernetes/kubernetes-1/:7:0","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署","uri":"/posts/kubernetes/kubernetes-1/"},{"categories":["Kubernetes"],"content":"6.1 kubeasz 集成安装 containerd 注意：k8s 1.24以后，项目已经设置默认容器运行时为 containerd，无需手动修改 ./ezctl setup k8s-01 05 ","date":"2023-04-28","objectID":"/posts/kubernetes/kubernetes-1/:7:1","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署","uri":"/posts/kubernetes/kubernetes-1/"},{"categories":["Kubernetes"],"content":"6.2 配置containerd 对接私有harbor仓库 修改role模板文件 vim roles/containerd/templates/config.toml.j2主要修改如下： [plugins.\"io.containerd.grpc.v1.cri\".registry] [plugins.\"io.containerd.grpc.v1.cri\".registry.auths] [plugins.\"io.containerd.grpc.v1.cri\".registry.configs] [plugins.\"io.containerd.grpc.v1.cri\".registry.configs.\"https://harbor.ceamg.com\"] username = \"admin\" password = \"ceamg.com\" [plugins.\"io.containerd.grpc.v1.cri\".registry.configs.\"easzlab.io.local:5000\".tls] insecure_skip_verify = true [plugins.\"io.containerd.grpc.v1.cri\".registry.configs.\"harbor.ceamg.com\".tls] insecure_skip_verify = true ca_file = \"/etc/containerd/certs.d/harbor.ceamg.com/ca.crt\" cert_file = \"/etc/containerd/certs.d/harbor.ceamg.com/harbor.ceamg.com.cert\" key_file = \"/etc/containerd/certs.d/harbor.ceamg.com/harbor.ceamg.com.key\" [plugins.\"io.containerd.grpc.v1.cri\".registry.headers] [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors] [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors.\"easzlab.io.local:5000\"] endpoint = [\"http://easzlab.io.local:5000\"] [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors.\"harbor.ceamg.com\"] endpoint = [\"https://harbor.ceamg.com\"] [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors.\"docker.io\"] endpoint = [\"https://lc2kkql3.mirror.aliyuncs.com\",\"https://docker.mirrors.ustc.edu.cn\", \"http://hub-mirror.c.163.com\"] [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors.\"gcr.io\"] endpoint = [\"https://gcr.mirrors.ustc.edu.cn\"] [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors.\"k8s.gcr.io\"] endpoint = [\"https://gcr.mirrors.ustc.edu.cn/google-containers/\"] [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors.\"quay.io\"] endpoint = [\"https://quay.mirrors.ustc.edu.cn\"] [plugins.\"io.containerd.grpc.v1.cri\".registry.auths.\"https://harbor.ceamg.com\"] [plugins.\"io.containerd.grpc.v1.cri\".x509_key_pair_streaming] tls_cert_file = \"\" tls_key_file = \"\" ","date":"2023-04-28","objectID":"/posts/kubernetes/kubernetes-1/:7:2","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署","uri":"/posts/kubernetes/kubernetes-1/"},{"categories":["Kubernetes"],"content":"6.3 containerd 使用证书对接harbor实现上传下载 6.3.1 使用脚本同步证书到客户端 #!/bin/bash #目标主机列表 IP=\" 10.1.0.32 10.1.0.33 10.1.0.34 10.1.0.35 \" for node in ${IP};do sshpass -p ceamg.com ssh-copy-id ${node} -o StrictHostKeyChecking=no if [ $? -eq 0 ];then echo \"${node} 秘钥copy完成\" echo \"${node} 秘钥copy完成,准备环境初始化.....\" ssh ${node} \"mkdir /etc/containerd/certs.d/harbor.ceamg.com -p\" echo \"Harbor 证书创建成功!\" scp /etc/containerd/certs.d/harbor.ceamg.com/harbor.ceamg.com.cert /etc/containerd/certs.d/harbor.ceamg.com/harbor.ceamg.com.crt /etc/containerd/certs.d/harbor.ceamg.com/harbor.ceamg.com.key /etc/containerd/certs.d/harbor.ceamg.com/ca.crt ${node}:/etc/containerd/certs.d/harbor.ceamg.com/ echo \"Harbor 证书拷贝成功!\" ssh ${node} \"echo \"10.1.0.38 harbor.ceamg.com\" \u003e\u003e /etc/hosts\" echo \"host 解析添加完成\" #scp -r /root/.docker ${node}:/root/ #echo \"Harbor 认证件拷完成!\" else echo \"${node} 秘钥copy失败\" fi done 6.3.2 测试containerd 客户端使用证书登录harbor 推送镜像 nerdctl.pdf root@master01:/etc/containerd/certs.d/harbor.ceamg.com# ls ca.crt harbor.ceamg.com.cert harbor.ceamg.com.crt harbor.ceamg.com.key root@master01:/etc/containerd/certs.d/harbor.ceamg.com# nerdctl login harbor.ceamg.com WARNING: Your password will be stored unencrypted in /root/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded root@master01:/etc/containerd/certs.d/harbor.ceamg.com# nerdctl images REPOSITORY TAG IMAGE ID CREATED PLATFORM SIZE BLOB SIZE nginx latest 0047b729188a 4 hours ago linux/amd64 149.4 MiB 54.2 MiB harbor.ceamg.com/library/nginx latest 0047b729188a 3 hours ago linux/amd64 149.4 MiB 54.2 MiB root@master01:/etc/containerd/certs.d/harbor.ceamg.com# nerdctl push harbor.ceamg.com/library/nginx INFO[0000] pushing as a reduced-platform image (application/vnd.docker.distribution.manifest.list.v2+json, sha256:3f727bfae5cee62f35f014637b350dbc1d0b416bdd1717b61c5ce5b036771aa0) index-sha256:3f727bfae5cee62f35f014637b350dbc1d0b416bdd1717b61c5ce5b036771aa0: done |++++++++++++++++++++++++++++++++++++++| manifest-sha256:9a821cadb1b13cb782ec66445325045b2213459008a41c72d8d87cde94b33c8c: done |++++++++++++++++++++++++++++++++++++++| config-sha256:1403e55ab369cd1c8039c34e6b4d47ca40bbde39c371254c7cba14756f472f52: done |++++++++++++++++++++++++++++++++++++++| elapsed: 1.1 s total: 9.3 Ki (8.5 KiB/s) ","date":"2023-04-28","objectID":"/posts/kubernetes/kubernetes-1/:7:3","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署","uri":"/posts/kubernetes/kubernetes-1/"},{"categories":["Kubernetes"],"content":"7.步骤4-部署master cat playbooks/04.kube-master.yml - hosts: kube_master roles: - kube-lb # 四层负载均衡，监听在127.0.0.1:6443，转发到真实master节点apiserver服务 - kube-master # - kube-node # 因为网络、监控等daemonset组件，master节点也推荐安装kubelet和kube-proxy服务 ... root@master01:/etc/kubeasz# ./ezctl setup k8s-01 04 ansible-playbook -i clusters/k8s-01/hosts -e @clusters/k8s-01/config.yml playbooks/04.kube-master.yml 2023-01-03 14:07:04 INFO cluster:k8s-01 setup step:04 begins in 5s, press any key to abort: 验证 master 集群 # 查看进程状态 systemctl status kube-apiserver systemctl status kube-controller-manager systemctl status kube-scheduler # 查看进程运行日志 journalctl -u kube-apiserver journalctl -u kube-controller-manager journalctl -u kube-scheduler执行 kubectl get componentstatus 可以看到 root@master01:/etc/kubeasz# kubectl get componentstatus Warning: v1 ComponentStatus is deprecated in v1.19+ NAME STATUS MESSAGE ERROR scheduler Healthy ok controller-manager Healthy ok etcd-1 Healthy {\"health\":\"true\",\"reason\":\"\"} etcd-0 Healthy {\"health\":\"true\",\"reason\":\"\"} ","date":"2023-04-28","objectID":"/posts/kubernetes/kubernetes-1/:8:0","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署","uri":"/posts/kubernetes/kubernetes-1/"},{"categories":["Kubernetes"],"content":"8.步骤5-部署node节点 kube_node 是集群中运行工作负载的节点，前置条件需要先部署好kube_master节点，它需要部署如下组件： cat playbooks/05.kube-node.yml - hosts: kube_node roles: - { role: kube-lb, when: \"inventory_hostname not in groups['kube_master']\" } - { role: kube-node, when: \"inventory_hostname not in groups['kube_master']\" } kube-lb：由nginx裁剪编译的四层负载均衡，用于将请求转发到主节点的 apiserver服务 kubelet：kube_node上最主要的组件 kube-proxy： 发布应用服务与负载均衡 root@master01:/etc/kubeasz# ./ezctl setup k8s-01 05 ansible-playbook -i clusters/k8s-01/hosts -e @clusters/k8s-01/config.yml playbooks/05.kube-node.yml 2023-01-04 09:06:25 INFO cluster:k8s-01 setup step:05 begins in 5s, press any key to abort: 验证 node 状态 systemctl status kubelet # 查看状态 systemctl status kube-proxy journalctl -u kubelet # 查看日志 journalctl -u kube-proxy 运行 kubectl get node 可以看到类似 root@worker01:/etc/containerd/certs.d/harbor.ceamg.com# kubectl get nodes NAME STATUS ROLES AGE VERSION 10.1.0.31 Ready,SchedulingDisabled master 21h v1.24.2 10.1.0.32 Ready node 21h v1.24.2 10.1.0.33 Ready node 21h v1.24.2 ","date":"2023-04-28","objectID":"/posts/kubernetes/kubernetes-1/:9:0","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署","uri":"/posts/kubernetes/kubernetes-1/"},{"categories":["Kubernetes"],"content":"9.步骤6-部署网络组件 首先回顾下K8S网络设计原则，在配置集群网络插件或者实践K8S 应用/服务部署请牢记这些原则： 1.每个Pod都拥有一个独立IP地址，Pod内所有容器共享一个网络命名空间 2.集群内所有Pod都在一个直接连通的扁平网络中，可通过IP直接访问 所有容器之间无需NAT就可以直接互相访问 所有Node和所有容器之间无需NAT就可以直接互相访问 容器自己看到的IP跟其他容器看到的一样 3.Service cluster IP只可在集群内部访问，外部请求需要通过NodePort、LoadBalance或者Ingress来访问 calico 是k8s社区最流行的网络插件之一，也是k8s-conformance test 默认使用的网络插件，功能丰富，支持network policy；是当前kubeasz项目的默认网络插件。 如果需要安装calico，请在clusters/xxxx/hosts文件中设置变量 CLUSTER_NETWORK=\"calico\" ","date":"2023-04-28","objectID":"/posts/kubernetes/kubernetes-1/:10:0","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署","uri":"/posts/kubernetes/kubernetes-1/"},{"categories":["Kubernetes"],"content":"9.1 使⽤calico⽹络组件 vim clusters/k8s-01/config.yml # ------------------------------------------- calico # [calico]设置 CALICO_IPV4POOL_IPIP=“off”,可以提高网络性能，条件限制详见 docs/setup/calico.md CALICO_IPV4POOL_IPIP: \"Always\" # [calico]设置 calico-node使用的host IP，bgp邻居通过该地址建立，可手工指定也可以自动发现 IP_AUTODETECTION_METHOD: \"can-reach={{ groups['kube_master'][0] }}\" # [calico]设置calico 网络 backend: brid, vxlan, none CALICO_NETWORKING_BACKEND: \"brid\" # [calico]设置calico 是否使用route reflectors # 如果集群规模超过50个节点，建议启用该特性 CALICO_RR_ENABLED: false # CALICO_RR_NODES 配置route reflectors的节点，如果未设置默认使用集群master节点 # CALICO_RR_NODES: [\"192.168.1.1\", \"192.168.1.2\"] CALICO_RR_NODES: [] # [calico]更新支持calico 版本: [v3.3.x] [v3.4.x] [v3.8.x] [v3.15.x] calico_ver: \"v3.19.4\" # [calico]calico 主版本 calico_ver_main: \"{{ calico_ver.split('.')[0] }}.{{ calico_ver.split('.')[1] }}\"./ezctl setup k8s-01 06 ","date":"2023-04-28","objectID":"/posts/kubernetes/kubernetes-1/:10:1","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署","uri":"/posts/kubernetes/kubernetes-1/"},{"categories":["Kubernetes"],"content":"9.2 验证calico网络 执行calico安装成功后可以验证如下：(需要等待镜像下载完成，有时候即便上一步已经配置了docker国内加速，还是可能比较慢，请确认以下容器运行起来以后，再执行后续验证步骤) ","date":"2023-04-28","objectID":"/posts/kubernetes/kubernetes-1/:10:2","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署","uri":"/posts/kubernetes/kubernetes-1/"},{"categories":["Kubernetes"],"content":"9.3 查看所有calico节点状态 root@master01:/etc/kubeasz# kubectl get pod -A -o wide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES kube-system calico-kube-controllers-5c8bb696bb-hf2cp 1/1 Running 0 6m10s 10.1.0.33 10.1.0.33 \u003cnone\u003e \u003cnone\u003e kube-system calico-node-6nlt6 1/1 Running 0 6m10s 10.1.0.32 10.1.0.32 \u003cnone\u003e \u003cnone\u003e kube-system calico-node-fd6rj 1/1 Running 0 6m10s 10.1.0.33 10.1.0.33 \u003cnone\u003e \u003cnone\u003e kube-system calico-node-lhgh4 1/1 Running 0 6m10s 10.1.0.31 10.1.0.31 \u003cnone\u003e \u003cnone\u003eroot@master01:/etc/kubeasz# calicoctl node status Calico process is running. IPv4 BGP status +--------------+-------------------+-------+----------+-------------+ | PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO | +--------------+-------------------+-------+----------+-------------+ | 10.1.0.32 | node-to-node mesh | up | 04:16:44 | Established | | 10.1.0.33 | node-to-node mesh | up | 04:16:43 | Established | +--------------+-------------------+-------+----------+-------------+ ","date":"2023-04-28","objectID":"/posts/kubernetes/kubernetes-1/:10:3","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署","uri":"/posts/kubernetes/kubernetes-1/"},{"categories":["Kubernetes"],"content":"9.4 创建容器测试网络通信 root@master01:/etc/kubeasz# kubectl run net-test1 --image=harbor.ceamg.com/library/alpine sleep 360000 pod/net-test1 created root@master01:/etc/kubeasz# kubectl run net-test2 --image=harbor.ceamg.com/library/alpine sleep 360000 pod/net-test2 created root@master01:/etc/kubeasz# kubectl run net-test3 --image=harbor.ceamg.com/library/alpine sleep 360000 pod/net-test3 created root@master01:/etc/kubeasz# kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES net-test1 1/1 Running 0 19s 10.20.5.3 10.1.0.32 \u003cnone\u003e \u003cnone\u003e net-test2 1/1 Running 0 15s 10.20.30.67 10.1.0.33 \u003cnone\u003e \u003cnone\u003e net-test3 1/1 Running 0 12s 10.20.30.68 10.1.0.33 \u003cnone\u003e \u003cnone\u003e test 1/1 Running 0 16m 10.20.5.1 10.1.0.32 \u003cnone\u003e \u003cnone\u003eroot@master01:/etc/kubeasz# kubectl exec -it net-test1 -- sh / # ping 10.20.30.67 PING 10.20.30.67 (10.20.30.67): 56 data bytes 64 bytes from 10.20.30.67: seq=0 ttl=62 time=0.481 ms ^C --- 10.20.30.67 ping statistics --- 1 packets transmitted, 1 packets received, 0% packet loss round-trip min/avg/max = 0.481/0.481/0.481 ms / # ping 10.20.30.68 PING 10.20.30.68 (10.20.30.68): 56 data bytes 64 bytes from 10.20.30.68: seq=0 ttl=62 time=0.631 ms 64 bytes from 10.20.30.68: seq=1 ttl=62 time=1.360 ms 64 bytes from 10.20.30.68: seq=2 ttl=62 time=0.420 ms ^C --- 10.20.30.68 ping statistics --- 3 packets transmitted, 3 packets received, 0% packet loss round-trip min/avg/max = 0.420/0.803/1.360 ms / # ping 223.5.5.5 PING 223.5.5.5 (223.5.5.5): 56 data bytes 64 bytes from 223.5.5.5: seq=0 ttl=114 time=7.597 ms 64 bytes from 223.5.5.5: seq=1 ttl=114 time=7.072 ms 64 bytes from 223.5.5.5: seq=2 ttl=114 time=7.583 ms ^C --- 223.5.5.5 ping statistics --- 3 packets transmitted, 3 packets received, 0% packet loss round-trip min/avg/max = 7.072/7.417/7.597 ms ","date":"2023-04-28","objectID":"/posts/kubernetes/kubernetes-1/:10:4","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署","uri":"/posts/kubernetes/kubernetes-1/"},{"categories":["Kubernetes"],"content":"10.步骤7-安装集群插件-coredns DNS 是 k8s 集群首要部署的组件，它为集群中的其他 pods 提供域名解析服务；主要可以解析 集群服务名 SVC 和 Pod hostname；目前建议部署 coredns。 ","date":"2023-04-28","objectID":"/posts/kubernetes/kubernetes-1/:11:0","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署","uri":"/posts/kubernetes/kubernetes-1/"},{"categories":["Kubernetes"],"content":"10.1 下载二进制包 kubernetes/CHANGELOG-1.24.md at master · kubernetes/kubernetes root@master01:/usr/local/src# ll total 489740 drwxr-xr-x 2 root root 4096 Jan 4 13:09 ./ drwxr-xr-x 13 root root 4096 Jan 1 13:20 ../ -rw-r--r-- 1 root root 30495559 Jan 4 13:09 kubernetes-client-linux-amd64.tar.gz -rw-r--r-- 1 root root 123361203 Jan 4 13:09 kubernetes-node-linux-amd64.tar.gz -rw-r--r-- 1 root root 347075448 Jan 4 13:09 kubernetes-server-linux-amd64.tar.gz -rw-r--r-- 1 root root 532769 Jan 4 13:09 kubernetes.tar.gz #解压后 root@master01:/usr/local/src/kubernetes# ll total 36996 drwxr-xr-x 10 root root 4096 Dec 8 18:31 ./ drwxr-xr-x 3 root root 4096 Jan 4 13:11 ../ drwxr-xr-x 2 root root 4096 Dec 8 18:26 addons/ drwxr-xr-x 3 root root 4096 Dec 8 18:31 client/ drwxr-xr-x 9 root root 4096 Dec 8 18:31 cluster/ drwxr-xr-x 2 root root 4096 Dec 8 18:31 docs/ drwxr-xr-x 3 root root 4096 Dec 8 18:31 hack/ -rw-r--r-- 1 root root 37826576 Dec 8 18:26 kubernetes-src.tar.gz drwxr-xr-x 4 root root 4096 Dec 8 18:31 LICENSES/ drwxr-xr-x 3 root root 4096 Dec 8 18:25 node/ -rw-r--r-- 1 root root 4443 Dec 8 18:31 README.md drwxr-xr-x 3 root root 4096 Dec 8 18:31 server/ -rw-r--r-- 1 root root 8 Dec 8 18:31 version #插件目录 root@master01:/usr/local/src/kubernetes/cluster/addons# ll total 80 drwxr-xr-x 18 root root 4096 Dec 8 18:31 ./ drwxr-xr-x 9 root root 4096 Dec 8 18:31 ../ drwxr-xr-x 2 root root 4096 Dec 8 18:31 addon-manager/ drwxr-xr-x 3 root root 4096 Dec 8 18:31 calico-policy-controller/ drwxr-xr-x 3 root root 4096 Dec 8 18:31 cluster-loadbalancing/ drwxr-xr-x 3 root root 4096 Dec 8 18:31 device-plugins/ drwxr-xr-x 5 root root 4096 Dec 8 18:31 dns/ drwxr-xr-x 2 root root 4096 Dec 8 18:31 dns-horizontal-autoscaler/ drwxr-xr-x 4 root root 4096 Dec 8 18:31 fluentd-gcp/ drwxr-xr-x 3 root root 4096 Dec 8 18:31 ip-masq-agent/ drwxr-xr-x 2 root root 4096 Dec 8 18:31 kube-proxy/ drwxr-xr-x 3 root root 4096 Dec 8 18:31 metadata-agent/ drwxr-xr-x 3 root root 4096 Dec 8 18:31 metadata-proxy/ drwxr-xr-x 2 root root 4096 Dec 8 18:31 metrics-server/ drwxr-xr-x 5 root root 4096 Dec 8 18:31 node-problem-detector/ -rw-r--r-- 1 root root 104 Dec 8 18:31 OWNERS drwxr-xr-x 8 root root 4096 Dec 8 18:31 rbac/ -rw-r--r-- 1 root root 1655 Dec 8 18:31 README.md drwxr-xr-x 8 root root 4096 Dec 8 18:31 storage-class/ drwxr-xr-x 4 root root 4096 Dec 8 18:31 volumesnapshots/ root@master01:/usr/local/src/kubernetes/cluster/addons/dns/coredns# ll total 44 drwxr-xr-x 2 root root 4096 Dec 8 18:31 ./ drwxr-xr-x 5 root root 4096 Dec 8 18:31 ../ -rw-r--r-- 1 root root 5060 Dec 8 18:31 coredns.yaml.base -rw-r--r-- 1 root root 5110 Dec 8 18:31 coredns.yaml.in -rw-r--r-- 1 root root 5112 Dec 8 18:31 coredns.yaml.sed -rw-r--r-- 1 root root 1075 Dec 8 18:31 Makefile -rw-r--r-- 1 root root 344 Dec 8 18:31 transforms2salt.sed -rw-r--r-- 1 root root 287 Dec 8 18:31 transforms2sed.sed cp coredns.yaml.base /root/ mv /root/coredns.yaml.base /root/coredns-ceamg.yaml vim /root/coredns-ceamg.yaml ","date":"2023-04-28","objectID":"/posts/kubernetes/kubernetes-1/:11:1","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署","uri":"/posts/kubernetes/kubernetes-1/"},{"categories":["Kubernetes"],"content":"10.2 修改配置文件 主要配置参数： error: #错误⽇志输出到stdout。 health： #CoreDNS的运⾏状况报告为http://localhost:8080/health. cache： #启⽤coredns缓存。 reload：#配置⾃动重新加载配置⽂件，如果修改了ConfigMap的配置，会在两分钟后⽣效. loadbalance：#⼀个域名有多个记录会被轮询解析。 cache 30 #缓存时间 kubernetes：#CoreDNS将根据指定的service domain名称在Kubernetes SVC中进⾏域名解析。 forward： #不是Kubernetes集群域内的域名查询都进⾏转发指定的服务器（/etc/resolv.conf） prometheus：#CoreDNS的指标数据可以配置Prometheus 访问http://coredns svc:9153/metrics 进⾏收集。 ready：#当coredns 服务启动完成后会进⾏在状态监测，会有个URL 路径为/ready返回200状态码，否则返回报错。kubernetes __DNS__DOMAIN_是 clusters/k8s-01/hosts 中填写的内容CLUSTER_DNS_DOMAIN # Cluster DNS Domain CLUSTER_DNS_DOMAIN=\"ceamg.local\"212 clusterIP: __DNS__SERVER__是clusters/k8s-01/hosts 中填写的内容SERVICE_CIDR 第二个IP 也就是 10.10.0.2 # K8S Service CIDR, not overlap with node(host) networking SERVICE_CIDR=\"10.10.0.0/16 / # cat /etc/resolv.conf search default.svc.ceamg.local svc.ceamg.local ceamg.local nameserver 10.10.0.2修改如下行内容： 77 kubernetes ceamg.local in-addr.arpa ip6.arpa { 83 forward . 192.168.0.15 { 142 image: harbor.ceamg.com/baseimages/coredns:v1.8.6 145 limits: 146 memory: 2048Mi 147 requests: 148 cpu: 1000m 149 memory: 1024Mi 212 clusterIP: 10.10.0.2 209 spec: 210 type: NodePort 211 selector: 212 k8s-app: kube-dns 213 clusterIP: 10.10.0.2 214 ports: 215 - name: dns 216 port: 53 217 protocol: UDP 218 - name: dns-tcp 219 port: 53 220 protocol: TCP 221 - name: metrics 222 port: 9153 223 protocol: TCP 224 targetPort: 9153 225 nodePort: 30009 查看资源格式： kubectl explain ","date":"2023-04-28","objectID":"/posts/kubernetes/kubernetes-1/:11:2","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署","uri":"/posts/kubernetes/kubernetes-1/"},{"categories":["Kubernetes"],"content":"10.3 下载镜像并推送到harbor root@master01:/usr/local/src/kubernetes/cluster/addons/dns/coredns# nerdctl pull registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:v1.8.6 root@master01:/usr/local/src/kubernetes/cluster/addons/dns/coredns# nerdctl tag registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:v1.8.6 harbor.ceamg.com/baseimages/coredns:v1.8.6 root@master01:/usr/local/src/kubernetes/cluster/addons/dns/coredns# nerdctl push harbor.ceamg.com/baseimages/coredns:v1.8.6 INFO[0000] pushing as a reduced-platform image (application/vnd.docker.distribution.manifest.list.v2+json, sha256:53011ff05d62cd740ae785a98f646ace63374073b0e564a35d4cea008f040940) ","date":"2023-04-28","objectID":"/posts/kubernetes/kubernetes-1/:11:3","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署","uri":"/posts/kubernetes/kubernetes-1/"},{"categories":["Kubernetes"],"content":"10.4 安装coredns root@master01:/usr/local/src/kubernetes/cluster/addons/dns/coredns# kubectl apply -f /root/coredns-ceamg.yaml serviceaccount/coredns created clusterrole.rbac.authorization.k8s.io/system:coredns created clusterrolebinding.rbac.authorization.k8s.io/system:coredns created configmap/coredns created deployment.apps/coredns created service/kube-dns created root@master01:~# kubectl get pod -A NAMESPACE NAME READY STATUS RESTARTS AGE default net-test1 1/1 Running 0 25m default net-test2 1/1 Running 0 25m default net-test3 1/1 Running 0 25m default net-test4 1/1 Running 0 49m kube-system calico-kube-controllers-5c8bb696bb-hf2cp 1/1 Running 1 (57m ago) 151m kube-system calico-node-6nlt6 1/1 Running 0 151m kube-system calico-node-fd6rj 1/1 Running 0 151m kube-system calico-node-lhgh4 1/1 Running 0 151m kube-system coredns-6c496b89f6-hd8vf 1/1 Running 0 3s ","date":"2023-04-28","objectID":"/posts/kubernetes/kubernetes-1/:11:4","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署","uri":"/posts/kubernetes/kubernetes-1/"},{"categories":["Kubernetes"],"content":"10.5 启动容器测试域名解析 root@master01:~# kubectl exec -it net-test1 error: you must specify at least one command for the container root@master01:~# kubectl exec -it net-test1 -- sh / # / # ping www.baidu.com PING www.baidu.com (110.242.68.3): 56 data bytes 64 bytes from 110.242.68.3: seq=0 ttl=49 time=9.778 ms --- www.baidu.com ping statistics --- 1 packets transmitted, 1 packets received, 0% packet loss round-trip min/avg/max = 9.778/9.778/9.778 ms / # / # cat /etc/resolv.conf search default.svc.ceamg.local svc.ceamg.local ceamg.local nameserver 10.10.0.2 options ndots:5 ","date":"2023-04-28","objectID":"/posts/kubernetes/kubernetes-1/:11:5","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署","uri":"/posts/kubernetes/kubernetes-1/"},{"categories":["Kubernetes"],"content":"10.6 测试 prometheus 监控项 http://10.1.0.31:30009/metrics ","date":"2023-04-28","objectID":"/posts/kubernetes/kubernetes-1/:11:6","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署","uri":"/posts/kubernetes/kubernetes-1/"},{"categories":["Kubernetes"],"content":"11. 步骤8-安装集群插件-dashboard https://github.com/kubernetes/dashboard ","date":"2023-04-28","objectID":"/posts/kubernetes/kubernetes-1/:12:0","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署","uri":"/posts/kubernetes/kubernetes-1/"},{"categories":["Kubernetes"],"content":"11.1 下载对应kubernetes版本的dashboard Compatibility Kubernetes version 1.21 1.22 1.23 1.24 Compatibility ? ? ? ✓ ✓ Fully supported version range. ? Due to breaking changes between Kubernetes API versions, some features might not work correctly in the Dashboard. # Copyright 2017 The Kubernetes Authors. # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. apiVersion: v1 kind: Namespace metadata: name: kubernetes-dashboard --- apiVersion: v1 kind: ServiceAccount metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboard --- kind: Service apiVersion: v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboard spec: ports: - port: 443 targetPort: 8443 selector: k8s-app: kubernetes-dashboard --- apiVersion: v1 kind: Secret metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-certs namespace: kubernetes-dashboard type: Opaque --- apiVersion: v1 kind: Secret metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-csrf namespace: kubernetes-dashboard type: Opaque data: csrf: \"\" --- apiVersion: v1 kind: Secret metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-key-holder namespace: kubernetes-dashboard type: Opaque --- kind: ConfigMap apiVersion: v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-settings namespace: kubernetes-dashboard --- kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboard rules: # Allow Dashboard to get, update and delete Dashboard exclusive secrets. - apiGroups: [\"\"] resources: [\"secrets\"] resourceNames: [\"kubernetes-dashboard-key-holder\", \"kubernetes-dashboard-certs\", \"kubernetes-dashboard-csrf\"] verbs: [\"get\", \"update\", \"delete\"] # Allow Dashboard to get and update 'kubernetes-dashboard-settings' config map. - apiGroups: [\"\"] resources: [\"configmaps\"] resourceNames: [\"kubernetes-dashboard-settings\"] verbs: [\"get\", \"update\"] # Allow Dashboard to get metrics. - apiGroups: [\"\"] resources: [\"services\"] resourceNames: [\"heapster\", \"dashboard-metrics-scraper\"] verbs: [\"proxy\"] - apiGroups: [\"\"] resources: [\"services/proxy\"] resourceNames: [\"heapster\", \"http:heapster:\", \"https:heapster:\", \"dashboard-metrics-scraper\", \"http:dashboard-metrics-scraper\"] verbs: [\"get\"] --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard rules: # Allow Metrics Scraper to get metrics from the Metrics server - apiGroups: [\"metrics.k8s.io\"] resources: [\"pods\", \"nodes\"] verbs: [\"get\", \"list\", \"watch\"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboard roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: kubernetes-dashboard subjects: - kind: ServiceAccount name: kubernetes-dashboard namespace: kubernetes-dashboard --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: kubernetes-dashboard roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: kubernetes-dashboard subjects: - kind: ServiceAccount name: kubernetes-dashboard namespace: kubernetes-dashboard --- kind: Deployment apiVersion: apps/v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboard spec: replicas: 1 revisionHis","date":"2023-04-28","objectID":"/posts/kubernetes/kubernetes-1/:12:1","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署","uri":"/posts/kubernetes/kubernetes-1/"},{"categories":["Kubernetes"],"content":"11.2 修改service暴露方式 32 kind: Service 33 apiVersion: v1 34 metadata: 35 labels: 36 k8s-app: kubernetes-dashboard 37 name: kubernetes-dashboard 38 namespace: kubernetes-dashboard 39 spec: 40 type: NodePort 41 ports: 42 - port: 443 43 targetPort: 8443 44 nodePort: 30010 45 selector: 46 k8s-app: kubernetes-dashboard ","date":"2023-04-28","objectID":"/posts/kubernetes/kubernetes-1/:12:2","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署","uri":"/posts/kubernetes/kubernetes-1/"},{"categories":["Kubernetes"],"content":"11.3 下载镜像推送到harbor root@master01:~# cat k8s-dashboard-ceamg.yml | grep image image: kubernetesui/dashboard:v2.6.1 imagePullPolicy: Always image: kubernetesui/metrics-scraper:v1.0.8 root@master01:~#nerdctl pull kubernetesui/dashboard:v2.6.1 root@master01:~# nerdctl pull kubernetesui/metrics-scraper:v1.0.8 root@master01:~# nerdctl tag kubernetesui/dashboard:v2.6.1 harbor.ceamg.com/baseimages/dashboard:v2.6.1 root@master01:~# nerdctl push harbor.ceamg.com/baseimages/dashboard:v2.6.1 INFO[0000] pushing as a reduced-platform image (application/vnd.docker.distribution.manifest.list.v2+json, sha256:f12df071f8bad3e1965b5246095bd3f78df0eb76ceabcc0878d42849d33e4a10) index-sha256:f12df071f8bad3e1965b5246095bd3f78df0eb76ceabcc0878d42849d33e4a10: done |++++++++++++++++++++++++++++++++++++++| manifest-sha256:d95e1adbe846450bf9451f9c95ab33865115909cf3962960af5983bb916cf320: done |++++++++++++++++++++++++++++++++++++++| config-sha256:783e2b6d87ed93a9f9fee34e84c2b029b7a9572b2f41f761437e58af9c26827f: done |++++++++++++++++++++++++++++++++++++++| elapsed: 3.2 s total: 2.5 Ki (814.0 B/s) root@master01:~# root@master01:~# nerdctl tag kubernetesui/metrics-scraper:v1.0.8 harbor.ceamg.com/baseimages/metrics-scraper:v1.0.8 root@master01:~# nerdctl push harbor.ceamg.com/baseimages/metrics-scraper:v1.0.8 INFO[0000] pushing as a reduced-platform image (application/vnd.docker.distribution.manifest.list.v2+json, sha256:9fdef455b4f9a8ee315a0aa3bd71787cfd929e759da3b4d7e65aaa56510d747b) index-sha256:9fdef455b4f9a8ee315a0aa3bd71787cfd929e759da3b4d7e65aaa56510d747b: done |++++++++++++++++++++++++++++++++++++++| manifest-sha256:43227e8286fd379ee0415a5e2156a9439c4056807e3caa38e1dd413b0644807a: done |++++++++++++++++++++++++++++++++++++++| config-sha256:115053965e86b2df4d78af78d7951b8644839d20a03820c6df59a261103315f7: done |++++++++++++++++++++++++++++++++++++++| elapsed: 0.8 s total: 2.2 Ki (2.7 KiB/s) ","date":"2023-04-28","objectID":"/posts/kubernetes/kubernetes-1/:12:3","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署","uri":"/posts/kubernetes/kubernetes-1/"},{"categories":["Kubernetes"],"content":"11.4 修改镜像地址 195 image: harbor.ceamg.com/baseimages/dashboard:v2.6.1 280 image: harbor.ceamg.com/baseimages/metrics-scraper:v1.0.8 ","date":"2023-04-28","objectID":"/posts/kubernetes/kubernetes-1/:12:4","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署","uri":"/posts/kubernetes/kubernetes-1/"},{"categories":["Kubernetes"],"content":"11.5 安装dashboard组件 kubectl apply -f k8s-dashboard-ceamg.yml ","date":"2023-04-28","objectID":"/posts/kubernetes/kubernetes-1/:12:5","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署","uri":"/posts/kubernetes/kubernetes-1/"},{"categories":["Kubernetes"],"content":"11.6 查看组件运行状态 root@master01:~# kubectl get pod -A NAMESPACE NAME READY STATUS RESTARTS AGE default net-test1 1/1 Running 0 99m default net-test2 1/1 Running 0 99m default net-test3 1/1 Running 0 99m default net-test4 1/1 Running 0 123m kube-system calico-kube-controllers-5c8bb696bb-hf2cp 1/1 Running 1 (131m ago) 3h45m kube-system calico-node-6nlt6 1/1 Running 0 3h45m kube-system calico-node-fd6rj 1/1 Running 0 3h45m kube-system calico-node-lhgh4 1/1 Running 0 3h45m kube-system coredns-6c496b89f6-hd8vf 1/1 Running 0 73m kubernetes-dashboard dashboard-metrics-scraper-8b9c56ffb-tjjc4 1/1 Running 0 14s kubernetes-dashboard kubernetes-dashboard-6f9f585c48-vv2pz 1/1 Running 0 14s ","date":"2023-04-28","objectID":"/posts/kubernetes/kubernetes-1/:12:6","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署","uri":"/posts/kubernetes/kubernetes-1/"},{"categories":["Kubernetes"],"content":"11.7 获取登陆 token apiVersion: v1 kind: ServiceAccount metadata: name: admin-user namespace: kubernetes-dashboard --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin-user namespace: kubernetes-dashboard root@master01:~# kubectl apply -f admin-user.yml serviceaccount/admin-user created clusterrolebinding.rbac.authorization.k8s.io/admin-user created:::warning 注意：v1.24.0 更新之后进行创建 ServiceAccount 不会自动生成 Secret 需要对其手动创建 ::: # 创建token root@master01:~# kubectl -n kubernetes-dashboard create token admin-user --duration 604800s eyJhbGciOiJSUzI1NiIsImtpZCI6ImptTldRRDRZZVVSdXRhaU1RNUtyQmJUSmVTbW55VThqNHhLU1l6U3B4R28ifQ.eyJhdWQiOlsiYXBpIiwiaXN0aW8tY2EiXSwiZXhwIjoxNjczNDI1MDI3LCJpYXQiOjE2NzI4MjAyMjcsImlzcyI6Imh0dHBzOi8va3ViZXJuZXRlcy5kZWZhdWx0LnN2YyIsImt1YmVybmV0ZXMuaW8iOnsibmFtZXNwYWNlIjoia3ViZXJuZXRlcy1kYXNoYm9hcmQiLCJzZXJ2aWNlYWNjb3VudCI6eyJuYW1lIjoiYWRtaW4tdXNlciIsInVpZCI6IjdhNTAzN2E4LTQ2MGEtNGM3YS05NWQ5LTNjM2JkNGQ0YTUyZSJ9fSwibmJmIjoxNjcyODIwMjI3LCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZXJuZXRlcy1kYXNoYm9hcmQ6YWRtaW4tdXNlciJ9.ciX6c6hUe8NqPHWp7GteAecZ75L50sKL0l0jk6hETJM9xUVkE-knhm-wQWogOCq1vJMWtg_qeYqsyxfFAMbXdnGgXUXH3tuLVe0NcSHfGVa0BBfjUqODODoAcKdEWJJqdTO_QCfzHTTGkBDZoPgqjALBFzMVh_anlUdeSehRtTh6y2L0dsMRbWuEp1YI8phXumRGIbsrRDOenCycfyPh2AUEChMhD_uYS85z2tDVbno-1y4sSoSiPPn-awUEAxo-ly7zIOUz_b6ZiMhM6nGTuJ-7Jyxq4A8f2pj-iyXA_ve3g1Y4AaInd1aaZhCQ_82rOpmHP0Idyzg_lqEneltBaw方式二手动创建secrit root@master01:/zookeeper# kubectl apply -f secrit secret/admin-user created apiVersion: v1 kind: Secret type: kubernetes.io/service-account-token metadata: name: admin-user namespace: kubernetes-dashboard annotations: kubernetes.io/service-account.name: \"admin-user\" root@master01:/zookeeper# kubectl -n kubernetes-dashboard describe sa admin-user Name: admin-user Namespace: kubernetes-dashboard Labels: \u003cnone\u003e Annotations: \u003cnone\u003e Image pull secrets: \u003cnone\u003e Mountable secrets: \u003cnone\u003e Tokens: admin-user Events: \u003cnone\u003e root@master01:/zookeeper# kubectl -n kubernetes-dashboard describe secrets admin-user Name: admin-user Namespace: kubernetes-dashboard Labels: \u003cnone\u003e Annotations: kubernetes.io/service-account.name: admin-user kubernetes.io/service-account.uid: 7a5037a8-460a-4c7a-95d9-3c3bd4d4a52e Type: kubernetes.io/service-account-token Data ==== namespace: 20 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6ImptTldRRDRZZVVSdXRhaU1RNUtyQmJUSmVTbW55VThqNHhLU1l6U3B4R28ifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiI3YTUwMzdhOC00NjBhLTRjN2EtOTVkOS0zYzNiZDRkNGE1MmUiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZXJuZXRlcy1kYXNoYm9hcmQ6YWRtaW4tdXNlciJ9.YIZ1UepKs7WzebxKMOVIPkmz0KLkIyV59S7D0x4sBpefqseX6lSfV_YbhDjQv0dm6ne9HJ85dHzF1-qmSJEO_EW3m-aNOfmem7jkqr8XDUIgHceeKZimauTodKvApsWWD_Flsk7r2nin-MoNkOJ5mi6g5Pu3iQuKhQINl3G9Wwch5c-5FV0l-RBWR1rw9rVby6fh1jfkAhMWGL7lWKJeAA6fE2dTJVSJ-ZhW_bzwPTTDKNhIlpRsyKEnFXwWmK9Jqoxq8y5H0iJIhbvkYCxwUG2Gjjfi6jIWhJvWo20_kTq5Cy-7BNXafBI5D6VKmFwHFyOLBQcvkntN2IpVRNcfbA ca.crt: 1302 byteshttps://blog.csdn.net/qq_41619571/article/details/127217339 ","date":"2023-04-28","objectID":"/posts/kubernetes/kubernetes-1/:12:7","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署","uri":"/posts/kubernetes/kubernetes-1/"},{"categories":["Kubernetes"],"content":"11.8 登录测试 ","date":"2023-04-28","objectID":"/posts/kubernetes/kubernetes-1/:12:8","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署","uri":"/posts/kubernetes/kubernetes-1/"},{"categories":["Kubernetes"],"content":"12. 集群管理 集群管理主要是添加master、添加node、删除master与删除node等节点管理及监控 ","date":"2023-04-28","objectID":"/posts/kubernetes/kubernetes-1/:13:0","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署","uri":"/posts/kubernetes/kubernetes-1/"},{"categories":["Kubernetes"],"content":"12.1 添加node节点 ./ezctl add-node k8s-01 10.1.0.39 ","date":"2023-04-28","objectID":"/posts/kubernetes/kubernetes-1/:13:1","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署","uri":"/posts/kubernetes/kubernetes-1/"},{"categories":["Kubernetes"],"content":"12.2 添加master 节点 root@master01:/etc/kubeasz# ./ezctl add-master k8s-01 10.1.0.30master 节点添加后会向 node节点 /etc/kube-lb/conf/kube-lb.conf 中添加反向代理节点 user root; worker_processes 1; error_log /etc/kube-lb/logs/error.log warn; events { worker_connections 3000; } stream { upstream backend { server 10.1.0.30:6443 max_fails=2 fail_timeout=3s; server 10.1.0.31:6443 max_fails=2 fail_timeout=3s; } server { listen 127.0.0.1:6443; proxy_connect_timeout 1s; proxy_pass backend; } } ","date":"2023-04-28","objectID":"/posts/kubernetes/kubernetes-1/:13:2","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署","uri":"/posts/kubernetes/kubernetes-1/"},{"categories":["Kubernetes"],"content":"12.3 验证当前节点 root@master02:~# kubectl get nodes NAME STATUS ROLES AGE VERSION 10.1.0.30 Ready,SchedulingDisabled master 15m v1.24.2 10.1.0.31 Ready,SchedulingDisabled master 44h v1.24.2 10.1.0.32 Ready node 44h v1.24.2 10.1.0.33 Ready node 44h v1.24.2 root@master02:~# calicoctl node status Calico process is running. IPv4 BGP status +--------------+-------------------+-------+----------+-------------+ | PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO | +--------------+-------------------+-------+----------+-------------+ | 10.1.0.31 | node-to-node mesh | up | 02:47:43 | Established | | 10.1.0.32 | node-to-node mesh | up | 02:47:12 | Established | | 10.1.0.33 | node-to-node mesh | up | 02:47:31 | Established | +--------------+-------------------+-------+----------+-------------+ IPv6 BGP status No IPv6 peers found. ","date":"2023-04-28","objectID":"/posts/kubernetes/kubernetes-1/:13:3","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署","uri":"/posts/kubernetes/kubernetes-1/"},{"categories":["Kubernetes"],"content":"13. 集群升级 先升级master 保证集群中至少有一个master节点可用 ，在node节点nginx反向代理中注释掉要升级的master节点。 ","date":"2023-04-28","objectID":"/posts/kubernetes/kubernetes-1/:14:0","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署","uri":"/posts/kubernetes/kubernetes-1/"},{"categories":["Kubernetes"],"content":"13.1 升级master节点 在各个node节点反向代理配置中注释掉要升级的master节点 vim /etc/kube-lb/conf/kube-lb.conf user root; worker_processes 1; error_log /etc/kube-lb/logs/error.log warn; events { worker_connections 3000; } stream { upstream backend { #server 10.1.0.30:6443 max_fails=2 fail_timeout=3s; server 10.1.0.31:6443 max_fails=2 fail_timeout=3s; } server { listen 127.0.0.1:6443; proxy_connect_timeout 1s; proxy_pass backend; } } #重启服务 root@worker01:~# systemctl restart kube-lb.servicenode节点升级需要停服务，需要关闭kubelet 和 kube-proxy服务替换二进制文件 kube-apiserver kube-controller-manager kubectl kubelet kube-proxy kube-scheduler 去github找到想要升级的版本下载二进制文件： https://github.com/kubernetes/kubernetes/releases root@master01:/usr/local/src# ll total 489744 drwxr-xr-x 3 root root 4096 Jan 4 13:11 ./ drwxr-xr-x 13 root root 4096 Jan 1 13:20 ../ drwxr-xr-x 10 root root 4096 Dec 8 18:31 kubernetes/ -rw-r--r-- 1 root root 30495559 Jan 4 13:09 kubernetes-client-linux-amd64.tar.gz -rw-r--r-- 1 root root 123361203 Jan 4 13:09 kubernetes-node-linux-amd64.tar.gz -rw-r--r-- 1 root root 347075448 Jan 4 13:09 kubernetes-server-linux-amd64.tar.gz -rw-r--r-- 1 root root 532769 Jan 4 13:09 kubernetes.tar.gz二进制文件在/server/bin目录下面 root@master01:/usr/local/src/kubernetes/server/bin# ll -ls total 1090008 4 drwxr-xr-x 2 root root 4096 Dec 8 18:26 ./ 4 drwxr-xr-x 3 root root 4096 Dec 8 18:31 ../ 54176 -rwxr-xr-x 1 root root 55476224 Dec 8 18:26 apiextensions-apiserver* 43380 -rwxr-xr-x 1 root root 44421120 Dec 8 18:26 kubeadm* 48408 -rwxr-xr-x 1 root root 49569792 Dec 8 18:26 kube-aggregator* 123032 -rwxr-xr-x 1 root root 125980672 Dec 8 18:26 kube-apiserver* 4 -rw-r--r-- 1 root root 8 Dec 8 18:25 kube-apiserver.docker_tag 128092 -rw------- 1 root root 131165184 Dec 8 18:25 kube-apiserver.tar 112896 -rwxr-xr-x 1 root root 115605504 Dec 8 18:26 kube-controller-manager* 4 -rw-r--r-- 1 root root 8 Dec 8 18:25 kube-controller-manager.docker_tag 117960 -rw------- 1 root root 120790016 Dec 8 18:25 kube-controller-manager.tar 44680 -rwxr-xr-x 1 root root 45752320 Dec 8 18:26 kubectl* 53796 -rwxr-xr-x 1 root root 55085992 Dec 8 18:26 kubectl-convert* 113376 -rwxr-xr-x 1 root root 116095704 Dec 8 18:26 kubelet* 1452 -rwxr-xr-x 1 root root 1486848 Dec 8 18:26 kube-log-runner* 40820 -rwxr-xr-x 1 root root 41799680 Dec 8 18:26 kube-proxy* 4 -rw-r--r-- 1 root root 8 Dec 8 18:25 kube-proxy.docker_tag 109280 -rw------- 1 root root 111901184 Dec 8 18:25 kube-proxy.tar 46096 -rwxr-xr-x 1 root root 47202304 Dec 8 18:26 kube-scheduler* 4 -rw-r--r-- 1 root root 8 Dec 8 18:25 kube-scheduler.docker_tag 51160 -rw------- 1 root root 52386816 Dec 8 18:25 kube-scheduler.tar 1380 -rwxr-xr-x 1 root root 1413120 Dec 8 18:26 mounter*root@master01:/usr/local/src/kubernetes/server/bin# ./kube-apiserver --version Kubernetes v1.24.9 #当前版本 root@master01:/usr/local/src/kubernetes/server/bin# /etc/kubeasz/bin/kube-apiserver --version Kubernetes v1.24.2停止服务 systemctl stop kube-proxy kube-controller-manager kubelet kube-scheduler kube-apiserver替换二进制文件 root@master01:/usr/local/src/kubernetes/server/bin# scp kube-apiserver kube-controller-manager kubelet kube-scheduler kube-proxy 10.1.0.30:/usr/local/bin kube-apiserver 100% 120MB 129.5MB/s 00:00 kube-controller-manager 100% 110MB 128.8MB/s 00:00 kubelet 100% 111MB 137.1MB/s 00:00 kube-scheduler 100% 45MB 128.5MB/s 00:00 kube-proxy 100% 40MB 132.0MB/s 00:00启动服务 systemctl start kube-proxy kube-controller-manager kubelet kube-scheduler kube-apiserver验证版本 root@master02:~# kubectl get nodes NAME STATUS ROLES AGE VERSION 10.1.0.30 Ready,SchedulingDisabled master 136m v1.24.9 10.1.0.31 Ready,SchedulingDisabled master 46h v1.24.2 10.1.0.32 Ready node 46h v1.24.2 10.1.0.33 Ready node 46h v1.24.2在另外的master节点上重复以上操作 root@master01:/usr/local/src/kubernetes/server/bin# systemctl stop kube-proxy kube-controller-manager kubelet kube-scheduler kube-apiserver root@master01:/usr/local/src/kubernetes/server/bin# \\cp kube-apiserver kube-controller-manager kubelet kube-scheduler kube-proxy /usr/local/bin root@m","date":"2023-04-28","objectID":"/posts/kubernetes/kubernetes-1/:14:1","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署","uri":"/posts/kubernetes/kubernetes-1/"},{"categories":["Kubernetes"],"content":"13.2 升级node节点 node节点只需要替换kubelet 和 kube-proxy 两个 关闭服务 root@worker01:~# systemctl stop kubelet.service kube-proxy.service替换二进制文件 root@master01:/usr/local/src/kubernetes/server/bin# scp kubelet kube-proxy 10.1.0.32:/usr/local/bin kubelet 100% 111MB 134.8MB/s 00:00 kube-proxy 100% 40MB 139.3MB/s 00:00 root@master01:/usr/local/src/kubernetes/server/bin# scp kubelet kube-proxy 10.1.0.33:/usr/local/bin启动服务 root@worker01:~# systemctl start kubelet.service kube-proxy.service验证版本 root@master02:~# kubectl get nodes NAME STATUS ROLES AGE VERSION 10.1.0.30 Ready,SchedulingDisabled master 157m v1.24.9 10.1.0.31 Ready,SchedulingDisabled master 47h v1.24.9 10.1.0.32 Ready node 46h v1.24.9 10.1.0.33 Ready node 46h v1.24.9","date":"2023-04-28","objectID":"/posts/kubernetes/kubernetes-1/:14:2","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署","uri":"/posts/kubernetes/kubernetes-1/"},{"categories":["Kubernetes"],"content":"1.为什么需要容器编排系统？ Docker在管理单个容器时表现出色，对于一些由有限几个或十几个容器构建的应用程序来说，直接在Docker引擎上自主运行、部署和管理是相对容易的。然而，当涉及到企业级应用程序，其中包含数百甚至上千个容器时，仅依赖Docker引擎来进行管理将变得异常复杂，甚至难以实现。 容器编排是一套自动化管理容器应用的流程，包括部署、管理、扩展和联网等各种操作，旨在应对大规模容器集群的挑战。它能够自动化许多任务，例如容器的调度和部署、资源的分配、应用规模的弹性扩缩、在主机故障或资源不足时的容器迁移、负载均衡，以及对容器和主机运行状况的监控等。 对于企业级应用程序，其中容器数量庞大、复杂性高，容器编排成为不可或缺的工具。通过容器编排平台，如Kubernetes，可以实现高度自动化的容器管理和协调，无论是在多台主机上还是跨多个数据中心中。容器编排能够实现复杂的任务调度、负载均衡、故障恢复，同时提供灵活的扩展性和资源管理。 因此，容器编排在处理大规模容器集群时变得至关重要。它提供了一种结构化的方式来管理大量容器，使得企业能够更高效、可靠地运行复杂的应用程序，从而克服了仅仅依赖Docker引擎进行管理所面临的挑战。 2.Kubernetes 集群概述 Kubernetes 是一个跨多主机的容器编排平台，它使用共享网络将多个主机构建成统一的集群，其中master作为控制中心负载整个集群系统，余下的主机运行为worker节点，这些工作节点使用本地和外部资源接收请求并以pod形式运行工作负载。 Master节点\rMaster节点：Master节点是Kubernetes集群的控制中心，负责管理和监控整个集群的状态和操作。它包括以下核心组件： API Server（API服务器）：提供了集群内部的API，用于与Kubernetes进行通信，提交和管理操作请求。 Controller Manager（控制器管理器）：负责监控集群状态，并根据期望状态对集群进行调整和控制。 Scheduler（调度器）：负责将新的Pod分配到Worker节点上，以实现负载均衡和资源利用的最佳化。 etcd（分布式键值存储）：用于保存集群的配置数据和状态信息。 Worker节点\rWorker节点：Worker节点是集群中实际运行容器的地方，它们接收Master节点的指令并执行相关操作。每个Worker节点包括以下组件： Kubelet（节点代理）：负责管理节点上的容器，确保Pod按照预期状态运行。 Kube Proxy（代理）：负责维护网络规则，实现Pod之间和外部的网络通信。 Container Runtime（容器运行时）：负责在节点上运行容器，常用的包括Docker、containerd等。 Pod：Pod是Kubernetes中的最小部署单元，它可以包含一个或多个容器，并共享相同的网络和存储空间。Pod作为部署、扩展和管理的基本单位，可以容纳应用及其依赖，并提供了更好的资源隔离和灵活性。 3.Kubernetes API对象 Kubernetes的RESTful API以资源的形式抽象了多种概念来描述应用程序及其周边组件。这些抽象出的程序和组件被统称为API对象，它们都有特定的类型和属性，每个API对象都使用”名称“作为唯一标识符。 对象类型\r以下是一些常见的Kubernetes API对象类型和它们的一些属性： Pod（Pod）：Pod是最小的可部署单元，可以包含一个或多个容器，它们共享网络和存储资源。Pod通常用于封装紧密耦合的应用组件。 ReplicaSet（副本集）：ReplicaSet确保指定数量的Pod副本在集群中运行。它适用于应用部署和扩展。 Deployment（部署）：Deployment建立在ReplicaSet之上，提供了声明式的方式来管理Pod副本的创建和更新。它用于应用的滚动更新和版本控制。 Service（服务）：Service定义了一组Pod的网络访问方式，为Pod提供了稳定的IP地址和DNS名称。它用于内部或外部网络访问。 Ingress（入口）：Ingress允许从集群外部访问Service，提供了HTTP和HTTPS路由的规则。 ConfigMap（配置映射）：ConfigMap用于将配置数据从应用代码中分离出来，使配置的管理更加灵活。 Secret（密钥）：Secret用于存储敏感信息，如密码、API密钥等，以安全地传递给Pod。 Namespace（命名空间）：Namespace用于将集群分割为多个虚拟集群，每个命名空间中的对象相互隔离，有助于多租户的管理。 StatefulSet（有状态副本集）：类似于ReplicaSet，但用于有状态应用，可以为每个Pod分配稳定的网络标识和存储。 DaemonSet（守护进程集）：DaemonSet确保每个节点上都运行一个Pod副本，适用于在每个节点上运行特定任务的情况。 Job（任务）：Job用于运行一次性的任务，确保任务成功完成，例如批量处理任务。 Kubernetes使用命名空间（Namespace）为资源提供了作用域，并将大多数资源类型划分到命名空间级别。命名空间可以看作是一个虚拟的集群，用于在物理集群内部划分不同的逻辑工作单元，从而将不同的资源隔离开来。这对于多租户环境、多个项目的隔离和资源管理非常有用。 在Kubernetes中，每个资源对象都属于一个特定的命名空间。一些资源默认在\"default\"命名空间中，但也可以创建自定义的命名空间，将资源放置在其中。 通过使用命名空间，Kubernetes可以实现资源的逻辑隔离，从而不同项目、团队或应用可以在同一个集群中运行，而彼此之间不会干扰。同时，命名空间还有助于对资源进行分类、管理和监控。需要注意的是，不是所有的资源都支持命名空间，一些核心资源如Nodes和PersistentVolumes并不属于命名空间。 4.Kubernetes 配置清单 Manifest 在Kubernetes中，应用程序的部署和管理需要通过配置清单（也称为资源清单或资源配置）来指定所需的状态，并将其提交给Kubernetes的API服务器。这些配置清单描述了要创建的资源对象的属性和规格，包括元数据、期望状态以及观察状态等信息。 配置清单是一个包含资源对象定义的文件，通常采用JSON或YAML格式编码。这些清单文件指定了要部署的应用程序、服务、副本集等的特性和配置。 当你想在Kubernetes中运行应用程序，那么就需要创建一个配置清单文件，其中描述了应用程序的属性和规格，比如名称、镜像、端口等。这个清单可以使用JSON或YAML格式编写。然后，需要将这个配置清单文件提交给Kubernetes的API服务器。 Kubernetes的API服务器会接收并存储这个配置清单，然后根据清单中的信息，在集群中创建相应的资源，如Pod、服务等。API服务器会确保描述的期望状态（比如运行3个副本）与观察状态（实际运行的副本数）保持一致。 提交配置清单通常使用HTTP/HTTPS协议与Kubernetes的API服务器通信。API服务器会验证和处理请求，并将清单中定义的资源创建或更新到集群中。Kubernetes API服务器通过HTTP GET请求查询资源对象的状态，通常以JSON格式进行序列化。同时，Kubernetes支持更高效的protobuf格式来减少数据传输和解析开销。 Kubernetes提供了一种声明式的方法来管理应用程序和资源对象，使得部署和管理变得更加可靠、可重复和可自动化。 ","date":"2023-04-24","objectID":"/posts/kubernetes/1.kubernetes%E5%9F%BA%E7%A1%80/:0:0","tags":["k8s进阶训练营"],"title":"Kubernetes 基础","uri":"/posts/kubernetes/1.kubernetes%E5%9F%BA%E7%A1%80/"},{"categories":["HAProxy"],"content":"六、HAProxy https实现 #配置HAProxy支持https协议，支持ssl会话； bind *:443 ssl crt /PATH/TO/SOME_PEM_FILE #crt 后证书文件为PEM格式，且同时包含证书和所有私钥 cat demo.crt demo.key \u003e demo.pem #把80端口的请求重向定443 bind *:80 redirect scheme https if !{ ssl_fc } #向后端传递用户请求的协议和端口（frontend或backend） http_request set-header X-Forwarded-Port %[dst_port] http_request add-header X-Forwared-Proto https if { ssl_fc } ","date":"2023-02-07","objectID":"/posts/haproxy/haproxy-4/:1:0","tags":["负载均衡"],"title":"HAProxy-https实现（四）","uri":"/posts/haproxy/haproxy-4/"},{"categories":["HAProxy"],"content":"6.1 证书制作 #方法1 [root@centos7 ~]mkdir /etc/haproxy/certs/ [root@centos7 ~]cd /etc/haproxy/certs/ [root@centos7 certs]#openssl genrsa -out haproxy.key 2048 [root@centos7 certs]#openssl req -new -x509 -key haproxy.key -out haproxy.crt -subj \"/CN=www.xinblog.org\" #或者用下一条命令实现 [root@centos7 certs]#openssl req -x509 -newkey rsa:2048 -subj \"/CN=www.magedu.org\" -keyout haproxy.key -nodes -days 365 -out haproxy.crt [root@centos7 certs]#cat haproxy.key haproxy.crt \u003e haproxy.pem [root@centos7 certs]#openssl x509 -in haproxy.pem -noout -text #查看证书 ","date":"2023-02-07","objectID":"/posts/haproxy/haproxy-4/:1:1","tags":["负载均衡"],"title":"HAProxy-https实现（四）","uri":"/posts/haproxy/haproxy-4/"},{"categories":["HAProxy"],"content":"6.2 https配置示例 [root@centos7 ~]#cat /etc/haproxy/conf.d/test.cfg frontend magedu_http_port bind 10.0.0.7:80 bind 10.0.0.7:443 ssl crt /etc/haproxy/certs/haproxy.pem redirect scheme https if !{ ssl_fc } # 注意{ }内的空格 http-request set-header X-forwarded-Port %[dst_port] http-request add-header X-forwarded-Proto https if { ssl_fc } mode http balance roundrobin log global option httplog ###################### acl setting ############################### acl mobile_domain hdr_dom(host) -i mobile.magedu.org ###################### acl hosts ################################# default_backend pc_hosts ################### backend hosts ################################# backend mobile_hosts mode http server web1 10.0.0.17:80 check inter 2000 fall 3 rise 5 backend pc_hosts mode http #http-request set-header X-forwarded-Port %[dst_port] 也可加在此处 #http-request add-header X-forwarded-Proto https if { ssl_fc } server web2 10.0.0.27:80 check inter 2000 fall 3 rise 5 [root@centos7 ~]#ss -ntl State Recv-Q Send-Q Local Address:Port Peer Address:Port LISTEN 0 100 127.0.0.1:25 *:* LISTEN 0 128 10.0.0.7:443 *:* LISTEN 0 128 *:9999 *:* LISTEN 0 128 10.0.0.7:80 *:* LISTEN 0 128 *:22 *:* LISTEN 0 128 [::]:22 [::]:* global maxconn 100000 chroot /var/lib/haproxy stats socket /var/lib/haproxy/haproxy.sock mode 600 level admin #uid 99 #gid 99 user haproxy group haproxy daemon #nbproc 4 #cpu-map 1 0 #cpu-map 2 1 #cpu-map 3 2 #cpu-map 4 3 pidfile /var/lib/haproxy/haproxy.pid log 127.0.0.1 local2 info defaults option http-keep-alive maxconn 100000 option forwardfor mode http timeout connect 300000ms timeout client 300000ms timeout server 300000ms listen stats mode http bind 0.0.0.0:9999 stats enable log global stats uri /haproxy-status stats auth haadmin:123456 listen http_80 mode http bind 10.1.0.6:30013 bind 10.1.0.6:443 ssl crt /etc/haproxy/certs/haproxy.pem redirect scheme https if !{ ssl_fc } http-request set-header X-forwarded-Port %[dst_port] http-request add-header X-forwarded-Proto https if { ssl_fc } balance roundrobin log global option forwardfor server web1 10.1.0.31:30013 check inter 2000 fall 3 rise 5 server web2 10.1.0.32:30013 check inter 2000 fall 3 rise 5 ","date":"2023-02-07","objectID":"/posts/haproxy/haproxy-4/:1:2","tags":["负载均衡"],"title":"HAProxy-https实现（四）","uri":"/posts/haproxy/haproxy-4/"},{"categories":["HAProxy"],"content":"五、HAProxy调度算法 HAProxy通过固定参数 **balance ** 指明对后端服务器的调度算法，该参数可以配置在listen或backend选项中。 HAProxy的调度算法分为静态和动态调度算法，但是有些算法可以根据参数在静态和动态算法中相互转换。 官方文档：http://cbonte.github.io/haproxy-dconv/2.6/configuration.html#balance ","date":"2023-02-06","objectID":"/posts/haproxy/haproxy-3/:1:0","tags":["负载均衡"],"title":"HAProxy-调度算法(三)","uri":"/posts/haproxy/haproxy-3/"},{"categories":["HAProxy"],"content":"5.1 静态算法 :::info ** 静态算法**：按照事先定义好的规则轮询公平调度，不关心后端服务器的当前负载、链接数和响应速度等，且无法实时修改权重，只能靠重启HAProxy生效。 ::: 可以利用 socat工具对服务器动态权重和其它状态的调整，Socat 是 Linux 下的一个多功能的网络工具，名字来由是Socket CAT，Socat 的主要特点就是在两个数据流之间建立通道，且支持众多协议和链接方式。如 IP、TCP、 UDP、IPv6、Socket文件等 **利用工具socat 对服务器动态权重调整 ** [root@centos7 ~]#yum -y install socat [root@centos7 ~]#echo \"show info\" | socat stdio /var/lib/haproxy/haproxy.sock Name: HAProxy Version: 2.1.3 Release_date: 2020/02/12 Nbthread: 4 Nbproc: 1 Process_num: 1 Pid: 2279 Uptime: 0d 0h46m07s Uptime_sec: 2767 Memmax_MB: 0 PoolAlloc_MB: 0 PoolUsed_MB: 0 PoolFailed: 0 Ulimit-n: 200041 Maxsock: 200041 Maxconn: 100000 Hard_maxconn: 100000 CurrConns: 0 CumConns: 1 CumReq: 1 MaxSslConns: 0 CurrSslConns: 0 CumSslConns: 0 Maxpipes: 0 PipesUsed: 0 PipesFree: 0 ConnRate: 0 ConnRateLimit: 0 MaxConnRate: 0 SessRate: 0 SessRateLimit: 0 MaxSessRate: 0 SslRate: 0 SslRateLimit: 0 MaxSslRate: 0 SslFrontendKeyRate: 0 SslFrontendMaxKeyRate: 0 SslFrontendSessionReuse_pct: 0 SslBackendKeyRate: 0 SslBackendMaxKeyRate: 0 SslCacheLookups: 0 SslCacheMisses: 0 CompressBpsIn: 0 CompressBpsOut: 0 CompressBpsRateLim: 0 ZlibMemUsage: 0 MaxZlibMemUsage: 0 Tasks: 19 Run_queue: 1 Idle_pct: 100 node: centos7.wangxiaochun.com Stopping: 0 Jobs: 7 Unstoppable Jobs: 0 Listeners: 6 ActivePeers: 0 ConnectedPeers: 0 DroppedLogs: 0 BusyPolling: 0 FailedResolutions: 0 TotalBytesOut: 0 BytesOutRate: 0 DebugCommandsIssued: 0 [root@centos7 ~]#echo \"show servers state\" | socat stdio /var/lib/haproxy/haproxy.sock1 # be_id be_name srv_id srv_name srv_addr srv_op_state srv_admin_state srv_uweight srv_iweight srv_time_since_last_change srv_check_status srv_check_result srv_check_health srv_check_state srv_agent_state bk_f_forced_id srv_f_forced_id srv_fqdn srv_port srvrecord 2 magedu-test-80 1 web1 10.0.0.17 2 0 2 1 812 6 3 7 6 0 0 0 - 80 - 2 magedu-test-80 2 web2 10.0.0.27 2 0 2 3 812 6 3 4 6 0 0 0 - 80 - 4 web_port 1 web1 127.0.0.1 0 0 1 1 810 8 2 0 6 0 0 0 - 8080 - [root@centos7 ~]#echo \"get weight magedu-test-80/web2\" | socat stdio /var/lib/haproxy/haproxy.sock 3 (initial 3) #修改weight，注意只针对单进程有效 [root@centos7 ~]#echo \"set weight magedu-test-80/web2 2\" | socat stdio /var/lib/haproxy/haproxy.sock [root@centos7 ~]#echo \"get weight magedu-test-80/web2\" | socat stdio /var/lib/haproxy/haproxy.sock 2 (initial 3) #将后端服务器禁用，注意只针对单进程有效 [root@centos7 ~]#echo \"disable server magedu-test-80/web2\" | socat stdio /var/lib/haproxy/haproxy.sock #将后端服务器软下线，即weight设为0 [root@centos7 ~]#echo \"set weight magedu-test-80/web1 0\" | socat stdio /var/lib/haproxy/haproxy.sock #将后端服务器禁用，针对多进程 [root@centos7 ~]#vim /etc/haproxy/haproxy.cfg ...... stats socket /var/lib/haproxy/haproxy1.sock mode 600 level admin process 1 stats socket /var/lib/haproxy/haproxy2.sock mode 600 level admin process 2 nbproc 2 ..... [root@centos7 ~]#echo \"disable server magedu-test-80/web2\" | socat stdio /var/lib/haproxy/haproxy1.sock [root@centos7 ~]#echo \"disable server magedu-test-80/web2\" | socat stdio /var/lib/haproxy/haproxy2.sock [root@haproxy ~]#for i in {1..2};do echo \"set weight magedu-test-80/web$i 10\" | socat stdio /var/lib/haproxy/haproxy$i.sock;done #如果静态算法，如:static-rr，可以更改weight为0或1，但不支持动态更改weight为其它值，否则会提示下面信息 [root@centos7 ~]#echo \"set weight magedu-test-80/web1 0\" | socat stdio /var/lib/haproxy/haproxy.sock [root@centos7 ~]#echo \"set weight magedu-test-80/web1 1\" | socat stdio /var/lib/haproxy/haproxy.sock [root@centos7 ~]#echo \"set weight magedu-test-80/web1 2\" | socat stdio /var/lib/haproxy/haproxy.sock Backend is using a static LB algorithm and only accepts weights '0%' and '100%'.5.1.1 static-rr :::info static-rr：基于权重的轮询调度，不支持权重的运行时利用socat进行动态调整及后端服务器慢启动，其后端主机数量没有限制，相当于LVS中的 wrr ::: listen web_host bind 10.0.0.7:80,:8801-8810,10.0.0.7:9001-9010 mode http log global balance static-rr server web1 10.0.0.17:80 weight 1 check inter 3000 fall 2 rise 5 server web2 10.0.0.27:80 weight 2 check inter 3000 fall 2 rise 55.1.2 first :::warning first：根据服务器在列表中的位置，自上而下进行调度，但是其只会当第一台服务器的连接数达到上限，新请求才会分配给下一台服务，因此会忽略服务器的权","date":"2023-02-06","objectID":"/posts/haproxy/haproxy-3/:1:1","tags":["负载均衡"],"title":"HAProxy-调度算法(三)","uri":"/posts/haproxy/haproxy-3/"},{"categories":["HAProxy"],"content":"5.2 动态算法 :::success 动态算法：基于后端服务器状态进行调度适当调整，优先调度至当前负载较低的服务器，且权重可以在haproxy运行时动态调整无需重启。 ::: 5.2.1 roundrobin roundrobin：基于权重的轮询动态调度算法，支持权重的运行时调整，不同于lvs中的rr轮训模式，HAProxy中的roundrobin支持慢启动(新加的服务器会逐渐增加转发数)，其每个后端backend中最多支持4095个real server，支持对real server权重动态调整，roundrobin为默认调度算法。 listen web_host bind 10.0.0.7:80,:8801-8810,10.0.0.7:9001-9010 mode http log global balance roundrobin server web1 10.0.0.17:80 weight 1 check inter 3000 fall 2 rise 5 server web2 10.0.0.27:80 weight 2 check inter 3000 fall 2 rise 5支持动态调整权重: # echo \"get weight web_host/web1\" | socat stdio /var/lib/haproxy/haproxy.sock 1 (initial 1) # echo \"set weight web_host/web1 3\" | socat stdio /var/lib/haproxy/haproxy.sock # echo \"get weight web_host/web1\" | socat stdio /var/lib/haproxy/haproxy.sock 3 (initial 1)5.2.2 leastconn leastconn加权的最少连接的动态，支持权重的运行时调整和慢启动，即当前后端服务器连接最少的优先调度(新客户端连接)，比较适合长连接的场景使用，比如：MySQL等场景。 listen web_host bind 10.0.0.7:80,:8801-8810,10.0.0.7:9001-9010 mode http log global balance leastconn server web1 10.0.0.17:80 weight 1 check inter 3000 fall 2 rise 5 server web2 10.0.0.27:80 weight 1 check inter 3000 fall 2 rise 55.2.3 random 在1.9版本开始增加一个叫做random的负载平衡算法，其基于随机数作为一致性hash的key，随机负载平衡对于大型服务器场或经常添加或删除服务器非常有用，支持weight的动态调整，weight较大的主机有更大概率获取新请求。 listen web_host bind 10.0.0.7:80,:8801-8810,10.0.0.7:9001-9010 mode http log global balance random server web1 10.0.0.17:80 weight 1 check inter 3000 fall 2 rise 5 server web2 10.0.0.27:80 weight 1 check inter 3000 fall 2 rise 5","date":"2023-02-06","objectID":"/posts/haproxy/haproxy-3/:1:2","tags":["负载均衡"],"title":"HAProxy-调度算法(三)","uri":"/posts/haproxy/haproxy-3/"},{"categories":["HAProxy"],"content":"5.3 其他算法 其它算法即可作为静态算法，又可以通过选项成为动态算法 5.3.1 source 源地址hash，基于用户源地址hash并将请求转发到后端服务器，后续同一个源地址请求将被转发至同一个后端web服务器。 此方式当后端服务器数据量发生变化时，会导致很多用户的请求转发至新的后端服务器，默认为静态方式，但是可以通过hash-type支持的选项更改。 这个算法一般是在不插入Cookie的TCP模式下使用，也可给拒绝会话cookie的客户提供最好的会话粘性，适用于session会话保持但不支持cookie和缓存的场景 源地址有两种转发客户端请求到后端服务器的服务器选取计算方式，分别是取模法和一致性hash 5.3.2 map-base取模法 map-based：取模法，对source地址进行hash计算，再基于服务器总权重的取模，最终结果决定将此请求转发至对应的后端服务器。 此方法是静态的，即不支持在线调整权重，不支持慢启动，可实现对后端服务器均衡调度。 缺点是当服务器的总权重发生变化时，即有服务器上线或下线，都会因总权重发生变化而导致调度结果整体改变，hash-type 指定的默认值为此算法 。 所谓取模运算，就是计算两个数相除之后的余数，**10%7=3, 7%4=3 ** map-based算法：基于权重取模，hash(source_ip)%所有后端服务器相加的总权重 listen web_host bind 10.0.0.7:80,:8801-8810,10.0.0.7:9001-9010 mode tcp log global balance source hash-type map-based server web1 10.0.0.17:80 weight 1 check inter 3000 fall 2 rise 3 server web2 10.0.0.27:80 weight 1 check inter 3000 fall 2 rise 3 [root@haproxy ~]#echo \"set weight web_host/10.0.0.27 10\" | socat stdio /var/lib/haproxy/haproxy.sock Backend is using a static LB algorithm and only accepts weights '0%' and '100%'. [root@haproxy ~]#echo \"set weight web_host/10.0.0.27 0\" | socat stdio /var/lib/haproxy/haproxy.sock [root@haproxy conf.d]#echo \"get weight web_host/10.0.0.27\" | socat stdio /var/lib/haproxy/haproxy.sock 0 (initial 1)5.3.3 一致性hash 一致性哈希，当服务器的总权重发生变化时，对调度结果影响是局部的，不会引起大的变动，hash（o）mod n ，该hash算法是动态的，支持使用 socat等工具进行在线权重调整，支持慢启动 。 算法： 1、key1=hash(source_ip)%(2^32) [0---4294967295] 2、keyA=hash(后端服务器虚拟ip)%(2^32) 3、将key1和keyA都放在hash环上，将用户请求调度到离key1最近的keyA对应的后端服务器** hash环偏斜问题 ** 增加虚拟服务器IP数量，比如：一个后端服务器根据权重为1生成1000个虚拟IP，再hash。而后端服务器权重为2则生成2000的虚拟IP，再hash,最终在hash环上生成3000个节点，从而解决hash环偏斜问题hash对象 Hash对象到后端服务器的映射关系： 一致性hash示意图 后端服务器在线与离线的调度方式 一致性hash配置示例 listen web_host bind 10.0.0.7:80,:8801-8810,10.0.0.7:9001-9010 mode tcp log global balance source hash-type consistent server web1 10.0.0.17:80 weight 1 check inter 3000 fall 2 rise 5 server web2 10.0.0.27:80 weight 1 check inter 3000 fall 2 rise 5","date":"2023-02-06","objectID":"/posts/haproxy/haproxy-3/:1:3","tags":["负载均衡"],"title":"HAProxy-调度算法(三)","uri":"/posts/haproxy/haproxy-3/"},{"categories":["HAProxy"],"content":"四、基础配置详解 官方文档：http://cbonte.github.io/haproxy-dconv/2.6/configuration.html HAProxy 的配置文件haproxy.cfg由两大部分组成，分别是global和proxies部分 **global ：全局配置段 ** 进程及安全配置相关的参数 性能调整相关参数 Debug参数** proxies：代理配置段 ** defaults：为frontend, backend, listen提供默认配置 frontend：前端，相当于nginx中的server {} backend：后端，相当于nginx中的upstream {} listen：同时拥有前端和后端配置 ","date":"2023-02-05","objectID":"/posts/haproxy/haproxy-2/:1:0","tags":["负载均衡"],"title":"HAProxy-基础配置详解 （二）","uri":"/posts/haproxy/haproxy-2/"},{"categories":["HAProxy"],"content":"4.1 global配置 4.1.1 global 配置参数说明 官方文档：http://cbonte.github.io/haproxy-dconv/2.6/configuration.html chroot #锁定运行目录 deamon #以守护进程运行 stats socket /var/lib/haproxy/haproxy.sock mode 600 level admin process 1 #链接本机的socket文件，定义权限方便对负载进行动态调整 user, group, uid, gid #运行haproxy的用户身份 nbproc n #多进程模式，开启worker进程数，建议与cpu个数相同，默认为1。开启时就不在支持线程模式，没开启时，一个进程下面有多个线程 #nbthread 1 #指定每个haproxy进程开启的线程数，默认为每个进程一个线程,和nbproc互斥（版本有关） #如果同时启用nbproc和nbthread 会出现以下日志的错误，无法启动服务 Apr 7 14:46:23 haproxy haproxy: [ALERT] 097/144623 (1454) : config : cannot enable multiple processes if multiple threads are configured. Please use either nbproc or nbthread but not both. cpu-map 1 0 #绑定haproxy 进程至指定CPU，将第一个work进程绑定至0号CPU cpu-map 2 1 #绑定haproxy 进程至指定CPU，将第二个work进程绑定至1号CPU maxconn 100000 #每个haproxy进程的最大并发连接数 maxsslconn n #每个haproxy进程ssl最大连接数,用于haproxy配置了证书的场景下 maxconnrate n #每个进程每秒创建的最大连接数量 spread-checks n #后端server状态check随机提前或延迟百分比时间，建议2-5(20%-50%)之间，默认值0 pidfile #指定pid文件路径 log 127.0.0.1 local2 info #定义全局的syslog服务器；日志服务器需要开启UDP协议，最多可以定义两个 4.4.2 多进程和线程 范例：多进程和socket文件 查看CPU核心数量 [root@localhost haproxy]# cat /proc/cpuinfo | grep \"cores\" | uniq cpu cores : 8[root@centos7 ~]#vim /etc/haproxy/haproxy.cfg global maxconn 100000 chroot /apps/haproxy stats socket /var/lib/haproxy/haproxy.sock1 mode 600 level admin process 1 stats socket /var/lib/haproxy/haproxy.sock2 mode 600 level admin process 2 uid 99 gid 99 daemon nbproc 2 [root@centos7 ~]#systemctl restart haproxy [root@centos7 ~]#pstree -p |grep haproxy |-haproxy(2634)-+-haproxy(2637) | `-haproxy(2638) [root@centos7 ~]#ll /var/lib/haproxy/ total 4 -rw-r--r-- 1 root root 5 Mar 31 18:49 haproxy.pid srw------- 1 root root 0 Mar 31 18:49 haproxy.sock1 srw------- 1 root root 0 Mar 31 18:49 haproxy.sock2 4.4.3 配置HAProxy记录日志到指定日志文件中 #在global配置项定义： log 127.0.0.1 local{1-7} info #基于syslog记录日志到指定设备，级别有(err、warning、info、debug) listen web_port bind 127.0.0.1:80 mode http log global #开启当前web_port的日志功能，默认不记录日志 server web1 127.0.0.1:8080 check inter 3000 fall 2 rise 5 # systemctl restart haproxy Rsyslog配置 vim /etc/rsyslog.conf $ModLoad imudp $UDPServerRun 514 # Save boot messages also to boot.log local7.* /var/log/boot.log local5.* /var/log/haproxy.log local0.* /var/log/haproxy.log # systemctl restart rsyslog 验证HAProxy日志 重启syslog服务并访问app页面，然后验证是否生成日志 [root@localhost log]# tail -f haproxy.log Feb 13 11:11:57 localhost haproxy[3127]: Connect from 172.16.32.242:64058 to 10.1.0.6:30013 (web_port/HTTP) Feb 13 11:11:57 localhost haproxy[3127]: Connect from 172.16.32.242:64058 to 10.1.0.6:30013 (web_port/HTTP) Feb 13 11:11:59 localhost haproxy[3127]: Connect from 172.16.32.242:64058 to 10.1.0.6:30013 (web_port/HTTP) Feb 13 11:11:59 localhost haproxy[3127]: Connect from 172.16.32.242:64058 to 10.1.0.6:30013 (web_port/HTTP) Feb 13 11:11:59 localhost haproxy[3127]: Connect from 172.16.32.242:64058 to 10.1.0.6:30013 (web_port/HTTP) Feb 13 11:11:59 localhost haproxy[3127]: Connect from 172.16.32.242:64058 to 10.1.0.6:30013 (web_port/HTTP) Feb 13 11:11:59 localhost haproxy[3127]: Connect from 172.16.32.242:64061 to 10.1.0.6:30013 (web_port/HTTP) Feb 13 11:11:59 localhost haproxy[3127]: Connect from 172.16.32.242:64061 to 10.1.0.6:30013 (web_port/HTTP) Feb 13 11:11:59 localhost haproxy[3127]: Connect from 172.16.32.242:64061 to 10.1.0.6:30013 (web_port/HTTP) Feb 13 11:11:59 localhost haproxy[3127]: Connect from 172.16.32.242:64061 to 10.1.0.6:30013 (web_port/HTTP) ","date":"2023-02-05","objectID":"/posts/haproxy/haproxy-2/:1:1","tags":["负载均衡"],"title":"HAProxy-基础配置详解 （二）","uri":"/posts/haproxy/haproxy-2/"},{"categories":["HAProxy"],"content":"4.2 Proxies配置 官方文档：http://docs.haproxy.org/2.6/configuration.html#4 defaults [\u003cname\u003e] #默认配置项，针对以下的frontend、backend和listen生效，可以多个name也可以没有name frontend \u003cname\u003e #前端servername，类似于Nginx的一个虚拟主机 server和LVS服务集群。 backend \u003cname\u003e #后端服务器组，等于nginx的upstream和LVS中的RS服务器 listen \u003cname\u003e #将frontend和backend合并在一起配置，相对于frontend和backend配置更简洁，生产常用**注意：name字段只能使用大小写字母，数字，‘-’(dash)，’_‘(underscore)，’.’ (dot)和 ‘:’(colon)，并且严格区分大小写 ** 4.2.1 Proxies配置-frontend frontend 配置参数： bind： #指定HAProxy的监听地址，可以是IPV4或IPV6，可以同时监听多个IP或端口，可同时用于listen字段中 #格式： bind [\u003caddress\u003e]:\u003cport_range\u003e [, ...] [param*] #注意：如果需要绑定在非本机的IP，需要开启内核参数：net.ipv4.ip_nonlocal_bind=1范例： listen http_proxy #监听http的多个IP的多个端口和sock文件 bind :80,:443,:8801-8810 bind 10.0.0.1:10080,10.0.0.1:10443 bind /var/run/ssl-frontend.sock user root mode 600 accept-proxy listen http_https_proxy #https监听 bind :80 bind :443 ssl crt /etc/haproxy/site.pem #公钥和私钥公共文件 listen http_https_proxy_explicit #监听ipv6、ipv4和unix sock文件 bind ipv6@:80 bind ipv4@public_ssl:443 ssl crt /etc/haproxy/site.pem bind unix@ssl-frontend.sock user root mode 600 accept-proxy listen external_bind_app1 #监听file descriptor bind \"fd@${FD_APP1}\"** 生产示例：** frontend magedu_web_port #可以采用后面形式命名：业务-服务-端口号 bind :80,:8080 bind 10.0.0.7:10080,:8801-8810,10.0.0.17:9001-9010 mode http|tcp #指定负载协议类型 use_backend \u003cbackend_name\u003e #调用的后端服务器组名称 4.2.2 Proxies配置-backend 定义一组后端服务器，backend服务器将被frontend进行调用。 mode http|tcp #指定负载协议类型,和对应的frontend必须一致 option #配置选项 server #定义后端real server注意：option后面加** httpchk，smtpchk,mysql-check,pgsql-check，ssl-hello-chk**方法，可用于实现更多应用层检测功能。 option 配置 check #对指定real进行健康状态检查，如果不加此设置，默认不开启检查 addr \u003cIP\u003e #可指定的健康状态监测IP，可以是专门的数据网段，减少业务网络的流量 port \u003cnum\u003e #指定的健康状态监测端口 inter \u003cnum\u003e #健康状态检查间隔时间，默认2000 ms fall \u003cnum\u003e #后端服务器从线上转为线下的检查的连续失效次数，默认为3 rise \u003cnum\u003e #后端服务器从下线恢复上线的检查的连续有效次数，默认为2 weight \u003cweight\u003e #默认为1，最大值为256，0表示不参与负载均衡，但仍接受持久连接 backup #将后端服务器标记为备份状态,只在所有非备份主机down机时提供服务，类似Sorry Server disabled #将后端服务器标记为不可用状态，即维护状态，除了持久模式，将不再接受连接 redirect prefix http://www.baidu.com/ #将请求临时(302)重定向至其它URL，只适用于http模式 redir http://www.baidu.com #将请求临时(302)重定向至其它URL，只适用于http模式 maxconn \u003cmaxconn\u003e #当前后端server的最大并发连接数 backlog \u003cbacklog\u003e #当前端服务器的连接数达到上限后的后援队列长度，注意：不支持backend 4.2.3 frontend+backend配置实例 范例1： frontend xin-test-http bind :80,:8080 mode tcp use_backend magedu-test-http-nodes backend magedu-test-http-nodes mode tcp default-server inter 1000 weight 6 server web1 10.0.0.17:80 check weight 2 addr 10.0.0.117 port 8080 server web1 10.0.0.27:80 check范例2： #官网业务访问入口 frontend WEB_PORT_80 bind 10.0.0.7:80 mode http use_backend web_prot_http_nodes backend web_prot_http_nodes mode http option forwardfor server 10.0.0.17 10.0.0.17:8080 check inter 3000 fall 3 rise 5 server 10.0.0.27 10.0.0.27:8080 check inter 3000 fall 3 rise 5 4.2.4 Proxies配置-listen替代frontend+backend 使用listen替换上面的frontend和backend的配置方式，可以简化设置，通常只用于TCP协议的应用 #官网业务访问入口 listen WEB_PORT_80 bind 10.0.0.7:80 mode http option forwardfor server web1 10.0.0.17:8080 check inter 3000 fall 3 rise 5 server web2 10.0.0.27:8080 check inter 3000 fall 3 rise 5 ","date":"2023-02-05","objectID":"/posts/haproxy/haproxy-2/:1:2","tags":["负载均衡"],"title":"HAProxy-基础配置详解 （二）","uri":"/posts/haproxy/haproxy-2/"},{"categories":["HAProxy"],"content":"4.3 使用子配置文件保存配置 当业务众多时，将所有配置都放在一个配置文件中，会造成维护困难。可以考虑按业务分类，将配置信息拆分，放在不同的子配置文件中，从而达到方便维护的目的。 #创建子配置目录 [root@centos7 ~]#mkdir /etc/haproxy/conf.d/ #创建子配置文件，注意：必须为cfg后缀 [root@centos7 ~]#vim /etc/haproxy/conf.d/test.cfg listen WEB_PORT_80 bind 10.0.0.7:80 mode http balance roundrobin server web1 10.0.0.17:80 check inter 3000 fall 2 rise 5 server web2 10.0.0.27:80 check inter 3000 fall 2 rise 5 #添加子配置目录到unit文件中 [root@centos7 ~]#vim /lib/systemd/system/haproxy.service [Unit] Description=HAProxy Load Balancer After=syslog.target network.target [Service] ExecStartPre=/usr/sbin/haproxy -f /etc/haproxy/haproxy.cfg -f /etc/haproxy/conf.d/ -c -q ExecStart=/usr/sbin/haproxy -Ws -f /etc/haproxy/haproxy.cfg -f /etc/haproxy/conf.d/ -p /var/lib/haproxy/haproxy.pid ExecReload=/bin/kill -USR2 $MAINPID [Install] WantedBy=multi-user.target [root@centos7 ~]#systemctl daemon-reload [root@centos7 ~]#systemctl restart haproxy ","date":"2023-02-05","objectID":"/posts/haproxy/haproxy-2/:1:3","tags":["负载均衡"],"title":"HAProxy-基础配置详解 （二）","uri":"/posts/haproxy/haproxy-2/"},{"categories":["docker"],"content":"1.下载Docker二进制包 Docker 下载地址： https://download.docker.com/win/static/stable/x86_64/ https://mirrors.aliyun.com/docker-ce/linux/static/stable/x86_64/ Docker-compos 下载地址： https://github.com/docker/compose/releases https://github.com/docker/compose/releases/download/v2.20.3/docker-compose-linux-x86_64 ","date":"2023-02-04","objectID":"/posts/docker/docker%E4%BA%8C%E8%BF%9B%E5%88%B6%E5%8C%85%E6%96%B9%E5%BC%8F%E5%AE%89%E8%A3%85/:1:0","tags":["docker"],"title":"Docker 二进制方式安装","uri":"/posts/docker/docker%E4%BA%8C%E8%BF%9B%E5%88%B6%E5%8C%85%E6%96%B9%E5%BC%8F%E5%AE%89%E8%A3%85/"},{"categories":["docker"],"content":"2.安装Docker tar xvf docker-24.0.5.zip cp docker/* /usr/bin cp containerd.service /lib/systemd/system/containerd.service cp docker.service /lib/systemd/system/docker.service cp docker.socket /lib/systemd/system/docker.socket cp docker-compose-Linux-x86_64_2.20.3 /usr/bin/docker-compose groupadd docker \u0026\u0026 useradd docker -g docker systemctl enable containerd.service \u0026\u0026 systemctl restart containerd.service systemctl enable docker.service \u0026\u0026 systemctl restart docker.service systemctl enable docker.socket \u0026\u0026 systemctl restart docker.socket ","date":"2023-02-04","objectID":"/posts/docker/docker%E4%BA%8C%E8%BF%9B%E5%88%B6%E5%8C%85%E6%96%B9%E5%BC%8F%E5%AE%89%E8%A3%85/:2:0","tags":["docker"],"title":"Docker 二进制方式安装","uri":"/posts/docker/docker%E4%BA%8C%E8%BF%9B%E5%88%B6%E5%8C%85%E6%96%B9%E5%BC%8F%E5%AE%89%E8%A3%85/"},{"categories":["docker"],"content":"2.1 containerd.service containerd 是 Docker 的核心组件之一，负责管理容器的生命周期、镜像传输以及容器进程的执行,如创建命名空间、控制组、文件系统等。 Docker 在其架构中使用了容器运行时（Container Runtime）来管理容器的生命周期。containerd 实现了 OCI（Open Container Initiative）标准，这是一个开放的行业标准，旨在定义容器和容器运行时的规范。这使得 containerd 能够与其他符合 OCI 标准的工具和库协同工作。 在 Linux 系统中，containerd 以守护进程的形式运行。为了确保 containerd 在系统启动时自动启动，并能够受到 systemd（一个常用的初始化系统和服务管理器）的管理，需要创建并配置一个 containerd.service 单元。 这个服务单元定义了 containerd 守护进程的启动方式、参数以及其他相关设置。 [Unit] Description=containerd container runtime Documentation=https://containerd.io After=network.target local-fs.target [Service] ExecStartPre=-/sbin/modprobe overlay ExecStart=/usr/bin/containerd Type=notify Delegate=yes KillMode=process Restart=always # Having non-zero Limit*s causes performance problems due to accounting overhead # in the kernel. We recommend using cgroups to do container-local accounting. LimitNPROC=infinity LimitCORE=infinity LimitNOFILE=1048576 # Comment TasksMax if your systemd version does not supports it. # Only systemd 226 and above support this version. TasksMax=infinity [Install] WantedBy=multi-user.target","date":"2023-02-04","objectID":"/posts/docker/docker%E4%BA%8C%E8%BF%9B%E5%88%B6%E5%8C%85%E6%96%B9%E5%BC%8F%E5%AE%89%E8%A3%85/:2:1","tags":["docker"],"title":"Docker 二进制方式安装","uri":"/posts/docker/docker%E4%BA%8C%E8%BF%9B%E5%88%B6%E5%8C%85%E6%96%B9%E5%BC%8F%E5%AE%89%E8%A3%85/"},{"categories":["docker"],"content":"2.2 docker.service docker.service 是一个 Systemd 服务单元，用于管理 Docker 守护进程（dockerd）的运行。Systemd 是一个常用的初始化系统和服务管理器，而服务单元则定义了如何启动、停止和管理特定的服务。 在 Docker 的架构中，dockerd 是 Docker 守护进程，负责管理容器的创建、运行、停止等任务。docker.service 的作用是管理 dockerd 进程的生命周期，使得 Docker 守护进程可以在系统启动时自动启动，并在需要时提供管理和监控。 [Unit] Description=Docker Application Container Engine Documentation=https://docs.docker.com BindsTo=containerd.service After=network-online.target firewalld.service containerd.service Wants=network-online.target Requires=docker.socket [Service] Type=notify # the default is not to use systemd for cgroups because the delegate issues still # exists and systemd currently does not support the cgroup feature set required # for containers run by docker ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock ExecReload=/bin/kill -s HUP $MAINPID TimeoutSec=0 RestartSec=2 Restart=always # Note that StartLimit* options were moved from \"Service\" to \"Unit\" in systemd 229. # Both the old, and new location are accepted by systemd 229 and up, so using the old location # to make them work for either version of systemd. StartLimitBurst=3 # Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230. # Both the old, and new name are accepted by systemd 230 and up, so using the old name to make # this option work for either version of systemd. StartLimitInterval=60s # Having non-zero Limit*s causes performance problems due to accounting overhead # in the kernel. We recommend using cgroups to do container-local accounting. LimitNOFILE=infinity LimitNPROC=infinity LimitCORE=infinity # Comment TasksMax if your systemd version does not support it. # Only systemd 226 and above support this option. TasksMax=infinity # set delegate yes so that systemd does not reset the cgroups of docker containers Delegate=yes # kill only the docker process, not all processes in the cgroup KillMode=process [Install] WantedBy=multi-user.target Description 提供了关于服务的简要描述。 Documentation 可以提供指向 Docker 文档的链接。 ExecStart 指定了如何启动 dockerd 进程，这里的 -H fd:// 告诉 Docker 守护进程通过文件描述符进行通信。 Restart 规定了在发生错误时如何重启服务。 StartLimitIntervalSec 和 StartLimitBurst 规定了在一段时间内尝试启动服务的次数限制，以避免过多的重试。 WantedBy=multi-user.target 表示该服务会在多用户模式下启动，即在系统引导后的一般操作状态下。 ","date":"2023-02-04","objectID":"/posts/docker/docker%E4%BA%8C%E8%BF%9B%E5%88%B6%E5%8C%85%E6%96%B9%E5%BC%8F%E5%AE%89%E8%A3%85/:2:2","tags":["docker"],"title":"Docker 二进制方式安装","uri":"/posts/docker/docker%E4%BA%8C%E8%BF%9B%E5%88%B6%E5%8C%85%E6%96%B9%E5%BC%8F%E5%AE%89%E8%A3%85/"},{"categories":["docker"],"content":"2.3 docker.socket docker.socket 是一个 Systemd 套接字（socket）单元，用于与 Docker 守护进程（dockerd）之间的通信。 具体来说，docker.socket 通过监听一个特定的网络端口或者 Unix 域套接字（Unix Domain Socket），等待来自 Docker 客户端的连接请求。一旦有请求连接，docker.socket 就会将请求转发给 dockerd 进程，然后 dockerd 处理这些请求并执行相应的操作，如创建或管理容器。 [Unit] Description=Docker Socket for the API PartOf=docker.service [Socket] ListenStream=/var/run/docker.sock SocketMode=0660 SocketUser=root SocketGroup=docker [Install] WantedBy=sockets.target","date":"2023-02-04","objectID":"/posts/docker/docker%E4%BA%8C%E8%BF%9B%E5%88%B6%E5%8C%85%E6%96%B9%E5%BC%8F%E5%AE%89%E8%A3%85/:2:3","tags":["docker"],"title":"Docker 二进制方式安装","uri":"/posts/docker/docker%E4%BA%8C%E8%BF%9B%E5%88%B6%E5%8C%85%E6%96%B9%E5%BC%8F%E5%AE%89%E8%A3%85/"},{"categories":["HAProxy"],"content":"三、HAProxy安装及基础配置 介绍HAProxy的基础安装及基础配置 ","date":"2023-02-04","objectID":"/posts/haproxy/haproxy-1/:1:0","tags":["负载均衡"],"title":"HAProxy-安装及基础配置（一）","uri":"/posts/haproxy/haproxy-1/"},{"categories":["HAProxy"],"content":"3.1 源码包安装 官方提供了Ubuntu和Debian的包，没有Centos的包 ubuntu 安装 apt-get install --no-install-recommends software-properties-common #--no-install-recommends 参数来避免安装非必须的文件，从而减小镜像的体积 add-apt-repository ppa:vbernat/haproxy-2.6 apt-get install haproxy=2.6.\\*#安装常用软件包 apt-get install --no-install-recommends software-properties-common -y #--no-install-recommends 参数来避免安装非必须的文件，从而减小镜像的体积 #安装源 root@etcd01[11:10:22]~ #:add-apt-repository ppa:vbernat/haproxy-2.6 HAProxy is a free, very fast and reliable solution offering high availability, load balancing, and proxying for TCP and HTTP-based applications. It is particularly suited for web sites crawling under very high loads while needing persistence or Layer7 processing. Supporting tens of thousands of connections is clearly realistic with todays hardware. Its mode of operation makes its integration into existing architectures very easy and riskless, while still offering the possibility not to expose fragile web servers to the Net. This PPA contains packages for HAProxy 2.6. More info: https://launchpad.net/~vbernat/+archive/ubuntu/haproxy-2.6 Press [ENTER] to continue or Ctrl-c to cancel adding it. Get:1 http://ppa.launchpad.net/vbernat/haproxy-2.6/ubuntu focal InRelease [23.8 kB] Hit:2 http://cn.archive.ubuntu.com/ubuntu focal InRelease Hit:3 http://cn.archive.ubuntu.com/ubuntu focal-updates InRelease Hit:4 http://cn.archive.ubuntu.com/ubuntu focal-backports InRelease Hit:5 http://cn.archive.ubuntu.com/ubuntu focal-security InRelease Get:6 http://ppa.launchpad.net/vbernat/haproxy-2.6/ubuntu focal/main amd64 Packages [1,000 B] Get:7 http://ppa.launchpad.net/vbernat/haproxy-2.6/ubuntu focal/main Translation-en [704 B] Fetched 25.5 kB in 2s (14.0 kB/s) Reading package lists... Done #查看可用版本 root@etcd01[11:11:01]~ #:apt-cache madison haproxy haproxy | 2.6.8-1ppa1~focal | http://ppa.launchpad.net/vbernat/haproxy-2.6/ubuntu focal/main amd64 Packages haproxy | 2.0.29-0ubuntu1.1 | http://cn.archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages haproxy | 2.0.29-0ubuntu1.1 | http://cn.archive.ubuntu.com/ubuntu focal-security/main amd64 Packages haproxy | 2.0.13-2 | http://cn.archive.ubuntu.com/ubuntu focal/main amd64 Packages #安装2.6 apt-get install haproxy=2.6.\\* -y #验证haproxy版本 root@etcd01[13:50:48]~ #:haproxy -v HAProxy version 2.6.8-1ppa1~focal 2023/01/24 - https://haproxy.org/ Status: long-term supported branch - will stop receiving fixes around Q2 2027. Known bugs: http://www.haproxy.org/bugs/bugs-2.6.8.html Running on: Linux 5.4.0-135-generic #152-Ubuntu SMP Wed Nov 23 20:19:22 UTC 2022 x86_64 Centos安装 在centos系统上通过yum、编译等多种安装方式。默认yum源默认的base仓库中包含haproxy的安装包文件，但是版本比较旧，是1.5.18的版本，距离当前版本已经有较长时间没有更新，由于版本比较旧所以有很多功能不支持，如果对功能和性能没有要求可以使用此版本，否则推荐使用新版本。 # yum install haproxy -y #验证haproxy版本 # haproxy -v HA-Proxy version 1.5.18 2016/05/10 Copyright 2000-2016 Willy Tarreau \u003cwilly@haproxy.org\u003e ","date":"2023-02-04","objectID":"/posts/haproxy/haproxy-1/:1:1","tags":["负载均衡"],"title":"HAProxy-安装及基础配置（一）","uri":"/posts/haproxy/haproxy-1/"},{"categories":["HAProxy"],"content":"3.2 编译安装HAProxy 编译安装HAProxy 2.0 LTS版本，源码包下载地址：http://www.haproxy.org/download/ 3.2.1 解决lua环境 HAProxy支持基于lua实现功能扩展，lua是一种小巧的脚本语言，于1993年由巴西里约热内卢天主教大学（Pontiﬁcal Catholic University of Rio de Janeiro）里的一个研究小组开发，其设计目的是为了嵌入应用程序中，从而为应用程序提供灵活的扩展和定制功能。 Lua 官网：www.lua.org Lua应用场景 游戏开发 独立应用脚本 Web应用脚本 扩展和数据库插件，如MySQL Proxy 安全系统，如入侵检测系统 Centos 环境：由于centos自带的lua版本比较低并不符合HAProxy要求的lua最低版本(5.3)的要求，因此需要编译安装较新版本的lua环境，然后才能编译安装HAProxy，过程如下： #当前系统版本 [root@centos7 ~]#lua -v Lua 5.1.4 Copyright (C) 1994-2008 Lua.org, PUC-Rio #安装基础命令及编译依赖环境 [root@centos7 ~]# yum install gcc readline-devel [root@centos7 ~]# wget http://www.lua.org/ftp/lua-5.3.5.tar.gz [root@centos7 ~]# tar xvf lua-5.3.5.tar.gz -C /usr/local/src [root@centos7 ~]# cd /usr/local/src/lua-5.3.5 [root@centos7 lua-5.3.5]# make linux test [root@localhost lua-5.3.5]# make linux test cd src \u0026\u0026 make linux make[1]: 进入目录“/usr/local/src/lua-5.3.5/src” make all SYSCFLAGS=\"-DLUA_USE_LINUX\" SYSLIBS=\"-Wl,-E -ldl -lreadline\" make[2]: 进入目录“/usr/local/src/lua-5.3.5/src” gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o lapi.o lapi.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o lcode.o lcode.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o lctype.o lctype.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o ldebug.o ldebug.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o ldo.o ldo.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o ldump.o ldump.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o lfunc.o lfunc.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o lgc.o lgc.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o llex.o llex.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o lmem.o lmem.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o lobject.o lobject.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o lopcodes.o lopcodes.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o lparser.o lparser.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o lstate.o lstate.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o lstring.o lstring.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o ltable.o ltable.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o ltm.o ltm.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o lundump.o lundump.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o lvm.o lvm.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o lzio.o lzio.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o lauxlib.o lauxlib.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o lbaselib.o lbaselib.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o lbitlib.o lbitlib.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o lcorolib.o lcorolib.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o ldblib.o ldblib.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o liolib.o liolib.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o lmathlib.o lmathlib.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o loslib.o loslib.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o lstrlib.o lstrlib.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o ltablib.o ltablib.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o lutf8lib.o lutf8lib.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o loadlib.o loadlib.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o linit.o linit.c ar rcu liblua.a lapi.o lcode.o l","date":"2023-02-04","objectID":"/posts/haproxy/haproxy-1/:1:2","tags":["负载均衡"],"title":"HAProxy-安装及基础配置（一）","uri":"/posts/haproxy/haproxy-1/"},{"categories":["HAProxy"],"content":"3.3 验证haproxy状态 3.3.1 验证监听端口 [root@localhost haproxy]# ss -tnl State Recv-Q Send-Q Local Address:Port Peer Address:Port LISTEN 0 128 127.0.0.1:631 *:* LISTEN 0 100 *:8088 *:* LISTEN 0 3 127.0.0.1:31769 *:* LISTEN 0 100 127.0.0.1:25 *:* LISTEN 0 128 *:1883 *:* LISTEN 0 128 *:30013 *:* LISTEN 0 50 *:8161 *:* LISTEN 0 1 127.0.0.1:8005 *:* LISTEN 0 128 *:5672 *:* LISTEN 0 50 *:43178 *:* LISTEN 0 128 *:6379 *:* LISTEN 0 128 *:61613 *:* LISTEN 0 50 *:61614 *:* LISTEN 0 128 *:9999 *:* 3.3.2 查看haproxy的状态页面 浏览器访问：http://haproxy-server:9999/haproxy-status 3.3.3 测试转发 10.1.0.6:30013 转发到 10.1.0.31:30013 ✅ ","date":"2023-02-04","objectID":"/posts/haproxy/haproxy-1/:1:3","tags":["负载均衡"],"title":"HAProxy-安装及基础配置（一）","uri":"/posts/haproxy/haproxy-1/"},{"categories":null,"content":"# /etc/rsyncd: configuration file for rsync daemon mode # See rsyncd.conf man page for more options. # configuration example: uid = root gid = root use chroot = no max connections = 200 timeout = 6000 ignore errors read only = false list = false auth users = rsync_backup secrets file = /etc/rsync.passwd log file = /var/log/rsyncd.log # pid file = /var/run/rsyncd.pid # exclude = lost+found/ # transfer logging = yes # timeout = 900 # ignore nonreadable = yes # dont compress = *.gz *.tgz *.zip *.z *.Z *.rpm *.deb *.bz2 [nfs-backup] comment = welcome to backup! path = /nfs-server [mysql-data-backup] comment = welcome to backup! path = /nfs-server2/tj-prod-mysql57-data-pvc-d4968732-0cf1-4395-9854-fd4581d2906d [mysql-log-backup] comment = welcome to backup! path = /nfs-server2/tj-prod-mysql57-log-pvc-7f831088-8828-4ab5-8e20-03ab5b048cdd [tree-file-backup] comment = welcome to backup! path = /nfs-server2/tj-prod-app-data-pvc-0770e2e6-70c4-4426-9a43-75e2cab94291 # [ftp] # path = /home/ftp # comment = ftp export areanohup rsync -avzrogpP --append-verify rsync_backup@127.0.0.1::tree-file-backup /nfs-server2/tj-prod-app-data-pvc-51174db7-fd78-4448-b9fb-21b6f8577e58 \u003e /var/log/rsync-log-treefile.log 2\u003e\u00261 \u0026 tail -f /var/log/rsync-log-treefile.log tree-file-server-for-pt/storage/high/2022/02/14/86b7f9c882e744a5b51be2c551f24a4b.jpg 32,827 100% 244.71kB/s 0:00:00 (xfr#138425, ir-chk=149321/287796) tree-file-server-for-pt/storage/high/2022/02/14/86b834f605614e07bccc92feaa9ab687.jpg 35,251 100% 256.90kB/s 0:00:00 (xfr#138426, ir-chk=149320/287796) tree-file-server-for-pt/storage/high/2022/02/14/86b8ac41b8394c529c00994427078f0f.png 1,345 100% 9.80kB/s 0:00:00 (xfr#138427, ir-chk=149319/287796) tree-file-server-for-pt/storage/high/2022/02/14/86b8b2ed44dc45c3ae182eccff7b4499.png 1,722 100% 12.55kB/s 0:00:00 (xfr#138428, ir-chk=149318/287796) tree-file-server-for-pt/storage/high/2022/02/14/86b932ab32e4472b8bb582c160885877.jpg 34,690 100% 250.94kB/s 0:00:00 (xfr#138429, ir-chk=149317/287796) tree-file-server-for-pt/storage/high/2022/02/14/86b9ad6ecdec45c9846aee66df52769f.png 1,978 100% 14.31kB/s 0:00:00 (xfr#138430, ir-chk=149316/287796) tree-file-server-for-pt/storage/high/2022/02/14/86ba0c2fd3a7462597c2c1e688a081d9.jpg 37,435 100% 270.80kB/s 0:00:00 (xfr#138431, ir-chk=149315/287796) tree-file-server-for-pt/storage/high/2022/02/14/86ba59e758674905ba39073f647db788.png 1,184 100% 8.50kB/s 0:00:00 (xfr#138432, ir-chk=149314/287796) tree-file-server-for-pt/storage/high/2022/02/14/86ba8bf9b31e464fa53b5cef03b29d52.jpg 31,866 100% 227.15kB/s 0:00:00 (xfr#138433, ir-chk=149313/287796) tree-file-server-for-pt/storage/high/2022/02/14/86bb4df3f5e8490f8f1eb421706828aa.jpg 33,953 100% 240.27kB/s 0:00:00 (xfr#138434, ir-chk=149312/287796) tree-file-server-for-pt/storage/high/2022/02/14/86bbc7d02a5f4076a6a2545f6cfc729d.jpg 30,784 100% 216.28kB/s 0:00:00 (xfr#138435, ir-chk=149311/287796) tree-file-server-for-pt/storage/high/2022/02/14/86bc058ff7e847dd9c6b6c7a69d551a8.jpg","date":"0001-01-01","objectID":"/posts/kubernetes/k8s-replace-nfs-storage/:0:0","tags":null,"title":"","uri":"/posts/kubernetes/k8s-replace-nfs-storage/"},{"categories":null,"content":"k8s强制删除pod\u0026pv\u0026pvc和ns\u0026namespace方法 注意：以下操作方法十分危险，三思而行！！！ 如果名称空间、pod、pv、pvc全部处于“Terminating”状态时，此时的该名称空间下的所有控制器都已经被删除了，之所以出现pod、pvc、pv、ns无法删除，那是因为kubelet 阻塞，有其他的资源在使用该namespace，比如CRD等，尝试重启kubelet，再删除该namespace 也不好使。 正确的删除方法：删除pod–\u003e 删除pvc —\u003e 删除pv –\u003e 删除名称空间 ","date":"0001-01-01","objectID":"/posts/kubernetes/k8s%E5%BC%BA%E5%88%B6%E5%88%A0%E9%99%A4podpvpvc%E5%92%8Cnsnamespace%E6%96%B9%E6%B3%95/:0:0","tags":null,"title":"","uri":"/posts/kubernetes/k8s%E5%BC%BA%E5%88%B6%E5%88%A0%E9%99%A4podpvpvc%E5%92%8Cnsnamespace%E6%96%B9%E6%B3%95/"},{"categories":null,"content":"一、强制删除pod $ kubectl delete pod \u003cyour-pod-name\u003e -n \u003cname-space\u003e --force --grace-period=0解决方法：加参数 --force --grace-period=0，grace-period表示过渡存活期，默认30s，在删除POD之前允许POD慢慢终止其上的容器进程，从而优雅退出，0表示立即终止POD ","date":"0001-01-01","objectID":"/posts/kubernetes/k8s%E5%BC%BA%E5%88%B6%E5%88%A0%E9%99%A4podpvpvc%E5%92%8Cnsnamespace%E6%96%B9%E6%B3%95/:1:0","tags":null,"title":"","uri":"/posts/kubernetes/k8s%E5%BC%BA%E5%88%B6%E5%88%A0%E9%99%A4podpvpvc%E5%92%8Cnsnamespace%E6%96%B9%E6%B3%95/"},{"categories":null,"content":"二、强制删除pv、pvc 直接删除k8s etcd数据库中的记录 $ kubectl patch pv xxx -p '{\"metadata\":{\"finalizers\":null}}' $ kubectl patch pvc xxx -p '{\"metadata\":{\"finalizers\":null}}'","date":"0001-01-01","objectID":"/posts/kubernetes/k8s%E5%BC%BA%E5%88%B6%E5%88%A0%E9%99%A4podpvpvc%E5%92%8Cnsnamespace%E6%96%B9%E6%B3%95/:2:0","tags":null,"title":"","uri":"/posts/kubernetes/k8s%E5%BC%BA%E5%88%B6%E5%88%A0%E9%99%A4podpvpvc%E5%92%8Cnsnamespace%E6%96%B9%E6%B3%95/"},{"categories":null,"content":"三、强制删除ns 在尝试以下命令强制删除也不好使： $ kubectl delete ns \u003cterminating-namespace\u003e --force --grace-period=0解决方法： 1）运行以下命令以查看处于“Terminating”状态的namespace： $ kubectl get namespaces2）选择一个Terminating namespace，并查看namespace 中的finalizer。运行以下命令： $ kubectl get namespace \u003cterminating-namespace\u003e -o yaml输出信息如下： apiVersion: v1 kind: Namespace metadata: creationTimestamp: \"2019-11-20T15:18:06Z\" deletionTimestamp: \"2020-01-16T02:50:02Z\" name: \u003cterminating-namespace\u003e resourceVersion: \"3249493\" selfLink: /api/v1/namespaces/knative-eventing uid: f300ea38-c8c2-4653-b432-b66103e412db spec: finalizers: - kubernetes status:3）导出json格式到文件 $ kubectl get namespace \u003cterminating-namespace\u003e -o json \u003etmp.json4）编辑tmp.josn，删除finalizers 字段的值 { \"apiVersion\": \"v1\", \"kind\": \"Namespace\", \"metadata\": { \"creationTimestamp\": \"2019-11-20T15:18:06Z\", \"deletionTimestamp\": \"2020-01-16T02:50:02Z\", \"name\": \"\u003cterminating-namespace\u003e\", \"resourceVersion\": \"3249493\", \"selfLink\": \"/api/v1/namespaces/knative-eventing\", \"uid\": \"f300ea38-c8c2-4653-b432-b66103e412db\" }, \"spec\": { #从此行开始删除 \"finalizers\": [] }, # 删到此行 \"status\": { \"phase\": \"Terminating\" } }5）开启proxy $ kubectl proxy执行该命令后，当前终端会被卡住 6）打开新的一个窗口，执行以下命令 $ curl -k -H \"Content-Type: application/json\" -X PUT --data-binary @tmp.json http://127.0.0.1:8001/api/v1/namespaces/\u003cterminating-namespace\u003e/finalize输出信息如下： { \"kind\": \"Namespace\", \"apiVersion\": \"v1\", \"metadata\": { \"name\": \"istio-system\", \"selfLink\": \"/api/v1/namespaces/istio-system/finalize\", \"uid\": \"2e274537-727f-4a8f-ae8c-397473ed619a\", \"resourceVersion\": \"3249492\", \"creationTimestamp\": \"2019-11-20T15:18:06Z\", \"deletionTimestamp\": \"2020-01-16T02:50:02Z\" }, \"spec\": { }, \"status\": { \"phase\": \"Terminating\" } }7）确认处于Terminating 状态的namespace已经被删除 $ kubectl get namespaces如果还有处于Terminating 状态的namespace，重复以上操作，删除即可！ ","date":"0001-01-01","objectID":"/posts/kubernetes/k8s%E5%BC%BA%E5%88%B6%E5%88%A0%E9%99%A4podpvpvc%E5%92%8Cnsnamespace%E6%96%B9%E6%B3%95/:3:0","tags":null,"title":"","uri":"/posts/kubernetes/k8s%E5%BC%BA%E5%88%B6%E5%88%A0%E9%99%A4podpvpvc%E5%92%8Cnsnamespace%E6%96%B9%E6%B3%95/"},{"categories":null,"content":"k8s中部署单节点redis https://gitee.com/zdevops/k8s-yaml/blob/main/redis/single/ ","date":"0001-01-01","objectID":"/posts/kubernetes/redis-on-k8scluster/:0:0","tags":null,"title":"","uri":"/posts/kubernetes/redis-on-k8scluster/"},{"categories":null,"content":"redis-cm.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: redis-config namespace: zdevops data: redis-config: | appendonly yes protected-mode no dir /data port 6379 requirepass redis@abc.com ","date":"0001-01-01","objectID":"/posts/kubernetes/redis-on-k8scluster/:1:0","tags":null,"title":"","uri":"/posts/kubernetes/redis-on-k8scluster/"},{"categories":null,"content":"redis-sts.yaml --- apiVersion: apps/v1 kind: StatefulSet metadata: name: redis namespace: zdevops labels: app: redis spec: serviceName: redis-headless replicas: 1 selector: matchLabels: app: redis template: metadata: labels: app: redis spec: containers: - name: redis image: 'registry.zdevops.com.cn/library/redis:6.2.7' command: - \"redis-server\" args: - \"/etc/redis/redis.conf\" ports: - name: redis-6379 containerPort: 6379 protocol: TCP volumeMounts: - name: config mountPath: /etc/redis - name: data mountPath: /data resources: limits: cpu: '2' memory: 4000Mi requests: cpu: 100m memory: 500Mi volumes: - name: config configMap: name: redis-config items: - key: redis-config path: redis.conf volumeClaimTemplates: - metadata: name: data namespace: zdevops spec: accessModes: [ \"ReadWriteOnce\" ] storageClassName: \"glusterfs\" resources: requests: storage: 5Gi --- apiVersion: v1 kind: Service metadata: name: redis-headless namespace: zdevops labels: app: redis spec: ports: - name: redis-6379 protocol: TCP port: 6379 targetPort: 6379 selector: app: redis clusterIP: None type: ClusterIP","date":"0001-01-01","objectID":"/posts/kubernetes/redis-on-k8scluster/:2:0","tags":null,"title":"","uri":"/posts/kubernetes/redis-on-k8scluster/"}]