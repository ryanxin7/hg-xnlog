[{"categories":["ElasticStack","Ansible"],"content":"由于等保要求，需要收集所有服务器的日志，但是服务器这么多每个都配置工作量太大了，所以使用ansible 批量管理工具，配置并启用各个节点服务器的rsyslog客户端，然后将日志发送到一个集中的日志服务端，达到日志收集的目的。 ansible 安装和配置见Ansible 批量生成并服务器密码 ","date":"2023-10-20","objectID":"/posts/elk/ansible%E6%89%B9%E9%87%8F%E9%85%8D%E7%BD%AErsyslog%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%8F%91%E9%80%81%E6%97%A5%E5%BF%97/:0:0","tags":["日志收集","rsyslog"],"title":"Ansible批量配置rsyslog客户端","uri":"/posts/elk/ansible%E6%89%B9%E9%87%8F%E9%85%8D%E7%BD%AErsyslog%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%8F%91%E9%80%81%E6%97%A5%E5%BF%97/"},{"categories":["ElasticStack","Ansible"],"content":"创建Ansible项目 mkdir -p /ansible-project/log-sync/playbooks mkdir -p /ansible-project/log-sync/inventory","date":"2023-10-20","objectID":"/posts/elk/ansible%E6%89%B9%E9%87%8F%E9%85%8D%E7%BD%AErsyslog%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%8F%91%E9%80%81%E6%97%A5%E5%BF%97/:1:0","tags":["日志收集","rsyslog"],"title":"Ansible批量配置rsyslog客户端","uri":"/posts/elk/ansible%E6%89%B9%E9%87%8F%E9%85%8D%E7%BD%AErsyslog%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%8F%91%E9%80%81%E6%97%A5%E5%BF%97/"},{"categories":["ElasticStack","Ansible"],"content":"创建Hosts文件 vim inventory/hosts [qcbdzb] qcbdzb-server1 ansible_host=192.168.11.2 qcbdzb-server2 ansible_host=192.168.11.3 qcbdzb-server3 ansible_host=192.168.11.4 qcbdzb-server4 ansible_host=192.168.11.5 qcbdzb-server5 ansible_host=192.168.11.6 qcbdzb-server6 ansible_host=192.168.11.7 qcbdzb-server7 ansible_host=192.168.11.8 qcbdzb-server8 ansible_host=192.168.11.9 qcbdzb-server9 ansible_host=10.1.1.104 qcbdzb-server10 ansible_host=10.1.1.105 qcbdzb-server11 ansible_host=10.1.1.106 qcbdzb-server12 ansible_host=10.1.1.102 qcbdzb-server13 ansible_host=10.1.1.103 [qcbqcfb] qcbqcfb-server1 ansible_host=192.168.10.10 qcbqcfb-server2 ansible_host=192.168.10.11 qcbqcfb-server3 ansible_host=10.1.0.2 qcbqcfb-server4 ansible_host=10.1.0.3 qcbqcfb-server5 ansible_host=192.168.10.21 qcbqcfb-server6 ansible_host=192.168.10.2 qcbqcfb-server7 ansible_host=10.1.0.8 qcbqcfb-server8 ansible_host=10.1.0.5 [jtrmt] jtrmt-server1 ansible_host=192.168.10.5 jtrmt-server2 ansible_host=192.168.10.15 jtrmt-server3 ansible_host=192.168.10.13 jtrmt-server4 ansible_host=10.1.0.4 jtrmt-server5 ansible_host=192.168.10.6 jtrmt-server6 ansible_host=192.168.10.8 jtrmt-server7 ansible_host=192.168.10.7 jtrmt-server8 ansible_host=192.168.10.4 jtrmt-server9 ansible_host=192.168.10.1 jtrmt-server10 ansible_host=192.168.10.12 jtrmt-server11 ansible_host=10.1.0.1 jtrmt-server12 ansible_host=192.168.10.17 jtrmt-server13 ansible_host=192.168.10.18 jtrmt-server14 ansible_host=192.168.10.3 jtrmt-server15 ansible_host=192.168.10.9 jtrmt-server16 ansible_host=192.168.10.14 jtrmt-server17 ansible_host=192.168.10.16 [dddl] dddl-server1 ansible_host=192.168.10.20 [webservers] webservers-server1 ansible_host=192.168.10.32 webservers-server2 ansible_host=192.168.10.33 webservers-server3 ansible_host=10.1.0.10 webservers-server4 ansible_host=10.1.0.15 webservers-server5 ansible_host=10.1.0.25 webservers-server6 ansible_host=10.1.0.21 webservers-server7 ansible_host=10.1.0.22 webservers-server8 ansible_host=10.1.0.23 webservers-server9 ansible_host=192.168.10.36 webservers-server10 ansible_host=10.1.1.1 webservers-server11 ansible_host=10.1.1.2 webservers-server12 ansible_host=10.1.1.3 webservers-server13 ansible_host=10.1.0.11 webservers-server14 ansible_host=10.1.0.12 webservers-server15 ansible_host=10.1.0.13 webservers-server16 ansible_host=192.168.10.35 webservers-server17 ansible_host=192.168.10.34 webservers-server18 ansible_host=192.168.10.31","date":"2023-10-20","objectID":"/posts/elk/ansible%E6%89%B9%E9%87%8F%E9%85%8D%E7%BD%AErsyslog%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%8F%91%E9%80%81%E6%97%A5%E5%BF%97/:1:1","tags":["日志收集","rsyslog"],"title":"Ansible批量配置rsyslog客户端","uri":"/posts/elk/ansible%E6%89%B9%E9%87%8F%E9%85%8D%E7%BD%AErsyslog%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%8F%91%E9%80%81%E6%97%A5%E5%BF%97/"},{"categories":["ElasticStack","Ansible"],"content":"创建Playbook Playbook 执行以下步骤： 备份当前的 rsyslog.conf 文件，将新的 rsyslog 配置文件复制到目标服务器，检查新的配置文件是否存在，并在进行更改时重新启动 rsyslog 服务。 vim /ansible-project/log-sync/playbooks/main.yaml --- - name: Sync Servers logs file hosts: qcbdzb,qcbqcfb,jtrmt,dddl,webservers become: yes become_user: root tasks: - name: Backup rsyslog.conf file command: cp /etc/rsyslog.conf /etc/rsyslog.conf.{{ ansible_date_time.date }} - name: Copy rsyslog.conf copy: src: /ansible-project/log-sync/playbooks/rsyslog.conf dest: /etc/rsyslog.conf notify: Start_rsyslog - name: Check rsyslog.conf for changes stat: path: /etc/rsyslog.conf register: rsyslog_conf handlers: - name: Start_rsyslog systemd: name: rsyslog state: restarted when: rsyslog_conf.stat.exists and rsyslog_conf.stat.isreg解析： 备份 rsyslog.conf 文件： name: Backup rsyslog.conf file 是任务的描述。 使用 command 模块运行一个 shell 命令。在这种情况下，它通过复制将当前的 rsyslog.conf 文件创建了一个带有时间戳的备份文件。使用 ansible_date_time.date 变量获取当前日期。 复制 rsyslog.conf： name: Copy rsyslog.conf 是另一个任务描述。 使用 copy 模块将新的 rsyslog 配置文件从 Ansible 控制节点（本地）复制到目标服务器。源文件位于 /ansible-project/log-sync/playbooks/rsyslog.conf，并将其复制到目标服务器的 /etc/rsyslog.conf。复制后，它触发名为 Start_rsyslog 的处理程序。 检查 rsyslog.conf 是否有更改： name: Check rsyslog.conf for changes 描述了任务。 使用 stat 模块来检查新的 rsyslog.conf 文件是否存在于服务器上，以及它是否是一个常规文件（而不是目录或符号链接）。检查的结果存储在名为 rsyslog_conf 的变量中。 handlers ：Start_rsyslog 当 rsyslog_conf 变量指示新的 rsyslog.conf 文件存在且是常规文件时，将触发此处理程序。 它使用 systemd 模块来重新启动 rsyslog 服务，以确保新的配置生效。 ","date":"2023-10-20","objectID":"/posts/elk/ansible%E6%89%B9%E9%87%8F%E9%85%8D%E7%BD%AErsyslog%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%8F%91%E9%80%81%E6%97%A5%E5%BF%97/:1:2","tags":["日志收集","rsyslog"],"title":"Ansible批量配置rsyslog客户端","uri":"/posts/elk/ansible%E6%89%B9%E9%87%8F%E9%85%8D%E7%BD%AErsyslog%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%8F%91%E9%80%81%E6%97%A5%E5%BF%97/"},{"categories":["ElasticStack","Ansible"],"content":"执行过程 root@LogServer03:/ansible-project/log-sync/playbooks# ansible-playbook /ansible-project/log-sync/playbooks/main.yaml -i /ansible-project/log-sync/inventory/hosts PLAY [Sync Servers logs file] ************************************************************************************************************************************ TASK [Gathering Facts] ******************************************************************************************************************************************* ok: [qcbdzb-server3] ok: [qcbdzb-server2] ok: [qcbdzb-server4] ok: [qcbdzb-server5] ok: [qcbdzb-server1] fatal: [qcbdzb-server9]: UNREACHABLE! =\u003e {\"changed\": false, \"msg\": \"Failed to connect to the host via ssh: root@10.1.1.104: Permission denied (publickey,password,keyboard-interactive).\", \"unreachable\": true} fatal: [qcbdzb-server10]: UNREACHABLE! =\u003e {\"changed\": false, \"msg\": \"Failed to connect to the host via ssh: root@10.1.1.105: Permission denied (publickey,password,keyboard-interactive).\", \"unreachable\": true} fatal: [qcbdzb-server11]: UNREACHABLE! =\u003e {\"changed\": false, \"msg\": \"Failed to connect to the host via ssh: root@10.1.1.106: Permission denied (publickey,password,keyboard-interactive).\", \"unreachable\": true} fatal: [qcbdzb-server12]: UNREACHABLE! =\u003e {\"changed\": false, \"msg\": \"Failed to connect to the host via ssh: root@10.1.1.102: Permission denied (publickey,password,keyboard-interactive).\", \"unreachable\": true} fatal: [qcbdzb-server13]: UNREACHABLE! =\u003e {\"changed\": false, \"msg\": \"Failed to connect to the host via ssh: root@10.1.1.103: Permission denied (publickey,password,keyboard-interactive).\", \"unreachable\": true} ok: [qcbdzb-server7] ok: [qcbdzb-server6] ok: [qcbqcfb-server2] ok: [qcbdzb-server8] ok: [qcbqcfb-server4] ok: [qcbqcfb-server3] ok: [qcbqcfb-server6] ok: [qcbqcfb-server5] ok: [jtrmt-server2] ok: [jtrmt-server3] ok: [qcbqcfb-server8] ok: [jtrmt-server4] ok: [jtrmt-server5] ok: [jtrmt-server6] ok: [qcbqcfb-server1] ok: [jtrmt-server9] ok: [qcbqcfb-server7] ok: [jtrmt-server1] ok: [jtrmt-server11] ok: [jtrmt-server7] ok: [jtrmt-server14] ok: [jtrmt-server13] ok: [jtrmt-server10] ok: [jtrmt-server8] ok: [jtrmt-server16] ok: [dddl-server1] ok: [webservers-server1] ok: [webservers-server3] ok: [webservers-server4] ok: [webservers-server5] ok: [webservers-server6] ok: [webservers-server7] ok: [webservers-server8] ok: [webservers-server9] ok: [webservers-server10] ok: [webservers-server11] ok: [webservers-server12] ok: [webservers-server13] ok: [webservers-server14] ok: [webservers-server15] ok: [webservers-server2] ok: [jtrmt-server17] ok: [webservers-server18] ok: [jtrmt-server12] ok: [jtrmt-server15] fatal: [webservers-server16]: FAILED! =\u003e {\"msg\": \"Timeout (12s) waiting for privilege escalation prompt: \"} fatal: [webservers-server17]: FAILED! =\u003e {\"msg\": \"Timeout (12s) waiting for privilege escalation prompt: \"} TASK [Backup rsyslog.conf file] ************************************************************************************************** ******************************** changed: [qcbdzb-server4] changed: [qcbdzb-server3] changed: [qcbdzb-server2] changed: [qcbdzb-server5] changed: [qcbdzb-server1] changed: [qcbdzb-server6] changed: [qcbdzb-server8] changed: [qcbdzb-server7] changed: [qcbqcfb-server2] changed: [qcbqcfb-server1] changed: [qcbqcfb-server4] changed: [qcbqcfb-server3] changed: [qcbqcfb-server6] changed: [qcbqcfb-server7] changed: [qcbqcfb-server8] changed: [jtrmt-server1] changed: [jtrmt-server2] changed: [jtrmt-server5] changed: [jtrmt-server7] changed: [jtrmt-server6] changed: [jtrmt-server4] changed: [jtrmt-server3] changed: [jtrmt-server11] changed: [jtrmt-server9] changed: [jtrmt-server10] changed: [jtrmt-server14] changed: [jtrmt-server13] changed: [jtrmt-server16] changed: [jtrmt-server8] changed: [dddl-server1] changed: [qcbqcfb-server5] changed: [jtrmt-server17] changed: [webservers-server1] changed: [webservers-server3] changed: [webservers-server4] changed: [webservers-server6] changed: [webser","date":"2023-10-20","objectID":"/posts/elk/ansible%E6%89%B9%E9%87%8F%E9%85%8D%E7%BD%AErsyslog%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%8F%91%E9%80%81%E6%97%A5%E5%BF%97/:2:0","tags":["日志收集","rsyslog"],"title":"Ansible批量配置rsyslog客户端","uri":"/posts/elk/ansible%E6%89%B9%E9%87%8F%E9%85%8D%E7%BD%AErsyslog%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%8F%91%E9%80%81%E6%97%A5%E5%BF%97/"},{"categories":["ElasticStack","Ansible"],"content":"查看日志已经收到了 root@LogServer03:/var/log/rsyslog# ls 10.1.0.1 10.1.0.21 10.1.0.8 10.123.0.22 10.124.0.24 10.124.0.31 10.126.0.3 192.168.10.13 192.168.10.21 192.168.10.5 192.168.11.4 10.1.0.10 10.1.0.22 10.1.1.1 10.123.0.27 10.124.0.25 10.124.0.32 10.126.0.4 192.168.10.14 192.168.10.3 192.168.10.6 192.168.11.5 10.1.0.11 10.1.0.23 10.1.1.2 10.123.0.28 10.124.0.26 10.124.0.33 10.126.0.6 192.168.10.15 192.168.10.31 192.168.10.7 192.168.11.6 10.1.0.12 10.1.0.25 10.1.1.3 10.124.0.10 10.124.0.27 10.124.0.34 192.168.10.1 192.168.10.16 192.168.10.32 192.168.10.8 192.168.11.7 10.1.0.13 10.1.0.3 10.123.0.13 10.124.0.11 10.124.0.28 10.124.0.35 192.168.10.10 192.168.10.18 192.168.10.33 192.168.10.9 192.168.11.8 10.1.0.15 10.1.0.4 10.123.0.18 10.124.0.17 10.124.0.29 10.124.0.36 192.168.10.11 192.168.10.2 192.168.10.36 192.168.11.2 192.168.11.9 10.1.0.2 10.1.0.5 10.123.0.2 10.124.0.18 10.124.0.30 10.126.0.1 192.168.10.12 192.168.10.20 192.168.10.4 192.168.11.3","date":"2023-10-20","objectID":"/posts/elk/ansible%E6%89%B9%E9%87%8F%E9%85%8D%E7%BD%AErsyslog%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%8F%91%E9%80%81%E6%97%A5%E5%BF%97/:3:0","tags":["日志收集","rsyslog"],"title":"Ansible批量配置rsyslog客户端","uri":"/posts/elk/ansible%E6%89%B9%E9%87%8F%E9%85%8D%E7%BD%AErsyslog%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%8F%91%E9%80%81%E6%97%A5%E5%BF%97/"},{"categories":["Ansible"],"content":"为了满足三级等保中服务器密码更改周期的要求。通过使用 Ansible 执行密码生成脚本，并将执行结果记录在日志文件中，可以确保密码的自动生成和周期性更改，并在需要时审计密码更改的历史记录。 生成密码的 Bash 脚本： #!/bin/bash #Maintainer： Zhangxinxin #Email: xx9z@outlook.com #Time: 2023.10 password=\"XXXXX$(ifconfig | awk '/ether/{print $2}' | md5sum | cut -b -6)\" host=$(ifconfig | awk -F'[: ]+' '/inet /{print $3}' | sed '2d') user=root timestamp=$(date +\"%Y-%m-%d %H:%M:%S\") if echo \"$user:$password\" | chpasswd; then success_message=\"[$timestamp] 当前主机: $host, 账号: $user, 密码: $password\" echo \"$success_message\" #echo \"$success_message\" | ssh root@192.168.10.109 \"cat \u003e\u003e /var/log/Psitems.log\" else error_message=\"[$timestamp] 密码更改失败\" echo \"$error_message\" exit 1 fi","date":"2023-10-17","objectID":"/posts/cicd/ansibleautopassword/:0:0","tags":["自动化和配置工具"],"title":"Ansible 批量生成并服务器密码","uri":"/posts/cicd/ansibleautopassword/"},{"categories":["Ansible"],"content":"安装Ansible 启用 Ansible PPA 存储库 Ansible 软件包及其依赖项可在 Ubuntu 22.04/20.04 的默认软件包存储库中找到，但这不是最新的 Ansible 版本。因此，要安装最新且稳定的 Ansible，请启用其 PPA 存储库，运行以下命令。 $ sudo apt install -y software-properties-common $ sudo add-apt-repository --yes --update ppa:ansible/ansible更新软件包仓库索引 sudo apt update安装最新版本的 Ansible 在 Ubuntu 20.04 LTS / 22.04 LTS 上安装最新版本的 Ansible，运行 apt 命令 sudo apt install -y ansible安装成功后，执行命令验证 Ansible 的版本 root@LogServer03:/ansible-project/Automaticallychangepass/playbooks# ansible --version ansible [core 2.12.10] config file = /etc/ansible/ansible.cfg configured module search path = ['/root/.ansible/plugins/modules', '/usr/share/ansible/plugins/modules'] ansible python module location = /usr/lib/python3/dist-packages/ansible ansible collection location = /root/.ansible/collections:/usr/share/ansible/collections executable location = /usr/bin/ansible python version = 3.8.10 (default, May 26 2023, 14:05:08) [GCC 9.4.0] jinja version = 2.10.1 libyaml = True","date":"2023-10-17","objectID":"/posts/cicd/ansibleautopassword/:0:1","tags":["自动化和配置工具"],"title":"Ansible 批量生成并服务器密码","uri":"/posts/cicd/ansibleautopassword/"},{"categories":["Ansible"],"content":"设置 SSH 密钥并在被管理节点之间共享 免密脚本 再目标主机列表添加相应主机IP地址 #!/bin/bash #⽬标主机列表 IP=\" 192.168.10.32 192.168.10.33 \" for node in ${IP};do sshpass -p nyqcjt@123 ssh-copy-id ${node} -o StrictHostKeyChecking=no if [ $? -eq 0 ];then echo \"${node} 秘钥copy完成\" else echo \"${node} 秘钥copy失败\" fi done","date":"2023-10-17","objectID":"/posts/cicd/ansibleautopassword/:0:2","tags":["自动化和配置工具"],"title":"Ansible 批量生成并服务器密码","uri":"/posts/cicd/ansibleautopassword/"},{"categories":["Ansible"],"content":"创建 ansible cfg 和 inventory 文件 通常建议每个项目都有单独的 ansible.cfg 和 inventory 文件。 我使用 Automaticallychangepass 作为项目名称。因此，首先通过运行 mkdir 命令创建项目文件夹。 mkdir Automaticallychangepass使用以下 wget 命令下载示例 ansble.cfg 文件到 ~/demo 文件夹 $ cd Automaticallychangepass $ wget https://raw.githubusercontent.com/ansible/ansible/stable-2.9/examples/ansible.cfg编辑 ~/Automaticallychangepass/ansible.cfg 文件, 设置如下参数 vim ~/Automaticallychangepass/ansible.cfg在 default 部分下面 inventory= /ansible-project/Automaticallychangepass/inventory remote_user= root host_key_checking= False在 privilege_escalation 部分下面 become=True become_method=sudo become_user=root become_ask_pass=False现在，让我们创建 ~/demo/ansible.cfg 文件中定义的 inventory 文件 vim /ansible-project/Automaticallychangepass/inventory [qcbdzb] qcbdzb-server1 ansible_host=192.168.11.2 qcbdzb-server2 ansible_host=192.168.11.3 qcbdzb-server3 ansible_host=192.168.11.4 server4 ansible_host=192.168.11.5 server5 ansible_host=192.168.11.6 server6 ansible_host=192.168.11.7 server7 ansible_host=192.168.11.8 server8 ansible_host=192.168.11.9 server9 ansible_host=10.1.1.104 server10 ansible_host=10.1.1.105 server11 ansible_host=10.1.1.106 server12 ansible_host=10.1.1.102 server13 ansible_host=10.1.1.103 [qcbqcfb] server1 ansible_host=192.168.10.10 server2 ansible_host=192.168.10.11 server3 ansible_host=10.1.0.2 server4 ansible_host=10.1.0.3 server5 ansible_host=192.168.10.21 server6 ansible_host=192.168.10.2 server7 ansible_host=10.1.0.8 server8 ansible_host=10.1.0.5 [jtrmt] server1 ansible_host=192.168.10.5 server2 ansible_host=192.168.10.15 server3 ansible_host=192.168.10.13 server4 ansible_host=10.1.0.4 server5 ansible_host=192.168.10.6 server6 ansible_host=192.168.10.8 server7 ansible_host=192.168.10.7 server8 ansible_host=192.168.10.4 server9 ansible_host=192.168.10.1 server10 ansible_host=192.168.10.12 server11 ansible_host=10.1.0.1 server12 ansible_host=192.168.10.17 server13 ansible_host=192.168.10.18 server14 ansible_host=192.168.10.3 server15 ansible_host=192.168.10.9 server16 ansible_host=192.168.10.14 server17 ansible_host=192.168.10.16 [dddl] server1 ansible_host=192.168.10.20 [webservers] server1 ansible_host=192.168.10.32 server2 ansible_host=192.168.10.33 server3 ansible_host=10.1.0.10 server4 ansible_host=10.1.0.15 server5 ansible_host=10.1.0.25 server6 ansible_host=10.1.0.21 server7 ansible_host=10.1.0.22 server8 ansible_host=10.1.0.23 server9 ansible_host=192.168.10.36 server10 ansible_host=10.1.1.1 server11 ansible_host=10.1.1.2 server12 ansible_host=10.1.1.3 server13 ansible_host=10.1.0.11 server14 ansible_host=10.1.0.12 server15 ansible_host=10.1.0.13 server16 ansible_host=192.168.10.35 server17 ansible_host=192.168.10.34 server18 ansible_host=192.168.10.31重新执行 ansible --version 命令，确认是否设置了新的配置文件 ansible 现在正在读取我们项目的 ansible 配置文件。让我们使用 ansible ad-hoc 命令来验证被控节点的连通性 ansible all -m ping ansible webservers -i /ansible-project/Automaticallychangepass/inventory/hosts -m ping","date":"2023-10-17","objectID":"/posts/cicd/ansibleautopassword/:0:3","tags":["自动化和配置工具"],"title":"Ansible 批量生成并服务器密码","uri":"/posts/cicd/ansibleautopassword/"},{"categories":["Ansible"],"content":"创建一个 Ansible 剧本 --- - name: Generate and Sync Password hosts: qcbdzb,qcbqcfb,jtrmt,dddl,webservers become: yes become_user: root tasks: - name: Generate password copy: src: /ansible-project/Automaticallychangepass/playbooks/generate-password.sh dest: /tmp/generate-password.sh - name: Change permissive command: chmod a+x /tmp/generate-password.sh - name: Change user password command: /tmp/generate-password.sh register: script_result changed_when: false - name: Display script output debug: var: script_result.stdout_lines - name: Append to log file lineinfile: path: /var/log/Psitems.log line: \"{{ script_result.stdout_lines }}\" delegate_to: localhost这个Ansible playbook它的工作流程如下： Generate password (生成密码): 首先，Ansible会将脚本文件 generate-password.sh 复制到目标主机上的 /tmp/generate-password.sh。这确保了目标主机上有脚本文件可供执行。 Change permission (更改权限): 接下来，Ansible会使用 chmod 命令更改脚本文件的权限，将其设置为可执行。这确保了脚本可以在目标主机上执行。 Change user password (更改用户密码): 然后，Ansible执行脚本 /tmp/generate-password.sh 以生成密码并将其存储在 script_result 变量中。changed_when: false 表示即使任务未更改系统状态，也将任务标记为成功。 Display script output (显示脚本输出): 此任务使用 debug 模块来显示脚本的标准输出，以便您可以查看脚本生成的密码或其他输出。 Append to log file (追加到日志文件): 最后，使用 lineinfile 模块将 script_result.stdout_lines 内容追加到日志文件 /var/log/Psitems.log 中。这可以帮助记录脚本执行的结果。 ","date":"2023-10-17","objectID":"/posts/cicd/ansibleautopassword/:0:4","tags":["自动化和配置工具"],"title":"Ansible 批量生成并服务器密码","uri":"/posts/cicd/ansibleautopassword/"},{"categories":["Ansible"],"content":"执行剧本 ansible-playbook /ansible-project/Automaticallychangepass/playbooks/main.yaml -i /ansible-project/Automaticallychangepass/inventory/hostssible-project/Automaticallychangepass/inventory/hosts","date":"2023-10-17","objectID":"/posts/cicd/ansibleautopassword/:0:5","tags":["自动化和配置工具"],"title":"Ansible 批量生成并服务器密码","uri":"/posts/cicd/ansibleautopassword/"},{"categories":["Ansible"],"content":"第一次执行的一些记录 root@LogServer03:/ansible-project/Automaticallychangepass/playbooks# ansible-playbook /ansible-project/Automaticallychangepass/playbooks/main.yaml -i /ansible-project/Automaticallychangepass/inventory/hosts PLAY [Generate and Sync Password] ************************************************************************************************************************** TASK [Gathering Facts] ************************************************************************************************************************************* fatal: [qcbdzb-server2]: UNREACHABLE! =\u003e {\"changed\": false, \"msg\": \"Failed to connect to the host via ssh: ssh: connect to host 192.168.11.3 port 22: Connection refused\", \"unreachable\": true} ok: [qcbdzb-server4] ok: [qcbdzb-server3] ok: [qcbdzb-server5] ok: [qcbdzb-server1] ok: [qcbdzb-server6] ok: [qcbdzb-server7] ok: [qcbdzb-server10] ok: [qcbdzb-server9] ok: [qcbdzb-server11] ok: [qcbdzb-server8] ok: [qcbdzb-server12] ok: [qcbdzb-server13] ok: [qcbqcfb-server2] ok: [qcbqcfb-server3] ok: [qcbqcfb-server4] ok: [qcbqcfb-server6] ok: [qcbqcfb-server5] ok: [qcbqcfb-server8] ok: [qcbqcfb-server7] ok: [qcbqcfb-server1] ok: [jtrmt-server5] ok: [jtrmt-server4] ok: [jtrmt-server3] ok: [jtrmt-server1] ok: [jtrmt-server9] ok: [jtrmt-server6] ok: [jtrmt-server11] ok: [jtrmt-server2] fatal: [jtrmt-server12]: FAILED! =\u003e {\"msg\": \"Timeout (12s) waiting for privilege escalation prompt: \"} ok: [jtrmt-server14] ok: [jtrmt-server7] ok: [jtrmt-server8] ok: [jtrmt-server13] ok: [dddl-server1] ok: [jtrmt-server10] ok: [webservers-server1] ok: [webservers-server3] ok: [webservers-server4] ok: [webservers-server5] ok: [webservers-server6] ok: [jtrmt-server16] ok: [webservers-server7] ok: [webservers-server8] ok: [webservers-server10] ok: [webservers-server11] ok: [webservers-server12] ok: [webservers-server13] ok: [jtrmt-server15] ok: [webservers-server14] ok: [webservers-server15] ok: [jtrmt-server17] ok: [webservers-server18] ok: [webservers-server2] ok: [webservers-server9] fatal: [webservers-server17]: FAILED! =\u003e {\"msg\": \"Timeout (12s) waiting for privilege escalation prompt: \"} fatal: [webservers-server16]: FAILED! =\u003e {\"msg\": \"Timeout (12s) waiting for privilege escalation prompt: \"} TASK [Generate password] *********************************************************************************************************************************** ok: [qcbdzb-server3] ok: [qcbdzb-server4] ok: [qcbdzb-server5] ok: [qcbdzb-server1] ok: [qcbdzb-server6] ok: [qcbdzb-server8] ok: [qcbdzb-server10] ok: [qcbdzb-server7] ok: [qcbdzb-server9] ok: [qcbdzb-server11] ok: [qcbdzb-server13] ok: [qcbdzb-server12] changed: [qcbqcfb-server4] changed: [qcbqcfb-server3] changed: [qcbqcfb-server2] changed: [qcbqcfb-server6] changed: [qcbqcfb-server8] changed: [qcbqcfb-server7] changed: [jtrmt-server3] changed: [qcbqcfb-server1] changed: [jtrmt-server4] changed: [jtrmt-server5] changed: [jtrmt-server6] changed: [qcbqcfb-server5] changed: [jtrmt-server9] changed: [jtrmt-server7] changed: [jtrmt-server11] changed: [jtrmt-server2] changed: [jtrmt-server8] changed: [jtrmt-server14] changed: [jtrmt-server1] changed: [jtrmt-server10] ok: [dddl-server1] changed: [webservers-server1] changed: [jtrmt-server13] changed: [webservers-server3] changed: [webservers-server4] changed: [webservers-server5] changed: [webservers-server6] changed: [webservers-server7] changed: [webservers-server8] changed: [jtrmt-server16] changed: [webservers-server10] changed: [webservers-server11] changed: [webservers-server12] changed: [webservers-server13] changed: [webservers-server14] changed: [webservers-server15] changed: [webservers-server18] changed: [jtrmt-server17] changed: [webservers-server2] changed: [webservers-server9] changed: [jtrmt-server15] TASK [Change permissive] *********************************************************************************************************************************** fatal: [qcbdzb-server3]: UNREACHABLE! =\u003e {\"changed\": false, \"msg\": \"Faile","date":"2023-10-17","objectID":"/posts/cicd/ansibleautopassword/:0:6","tags":["自动化和配置工具"],"title":"Ansible 批量生成并服务器密码","uri":"/posts/cicd/ansibleautopassword/"},{"categories":["switch"],"content":" 登录到交换机： 使用SSH、Telnet或串口连接登录到Brocade ICX 6450交换机。 进入特权EXEC模式： 输入以下命令，进入特权EXEC模式，并输入特权级别的密码（如果已设置）： bashCopy code\renable 进入配置模式： Copy code\rconfigure terminal 将端口1到23配置为VLAN 70： 使用以下命令将端口1到23配置为VLAN 70： interface range ethernet 1/1/1 to 1/1/23\runtagged ethernet 1/1/1 to 1/1/23\r#取消配置\runtagged ethernet 1/1/1 to 1/1/23 将端口24配置为Trunk： 使用以下命令将端口24配置为Trunk，并允许VLAN 70的标签通过： interface ethernet 1/1/24\rtagged ethernet 1/1/24\rexit 保存配置： 在您完成所有配置后，确保保存配置以使其永久生效： write memory 退出配置模式： 输入以下命令退出配置模式： exit 验证配置： 您可以使用以下命令来验证端口范围的配置和Trunk端口的配置： show interfaces ethernet 1/1/1 to 1/1/24 status 这些步骤将帮助您将1到23个端口配置为VLAN 70，并将24号端口配置为Trunk以连接到上层设备。 其他配置： # Create interface IP (MODE FIRMWARE SWITCH - SPS08080b) ICX7150-C12 Switch# ip address 192.168.1.27 255.255.255.0\rICX7150-C12 Switch# show ip\rICX7150-C12 Switch# ip default-gateway 192.168.1.1# Create interface vlan IP (MODE FIRMWARE ROUTER - SPR08061b) ICX7150-C12 Switch(config)# vlan 1\rICX7150-C12 Router(config-vlan-1)# router-interface ve1\rICX7150-C12 Router(config-vlan-1)# exit\rICX7150-C12 Switch(config)# interface ve1\rICX7150-C12 Switch(config-vif-1)# ip address 192.168.1.170/24\rICX7150-C12 Switch(config-vif-1)# exit\rICX7150-C12 Switch(config)# ip route 0.0.0.0/0 192.168.158.151\rICX7150-C12 Switch(config)# show ip interface# Disable ip dhcp-client ICX7150-C12 Switch(config)# no ip dhcp-client enable\rICX7150-C12 Switch(config-if-e1000-2/1/1)# no ip dhcp-client enable 配置命令参考来源 ","date":"2023-10-09","objectID":"/posts/notes/2023-10-09-brocade-icx-6450%E4%BA%A4%E6%8D%A2%E6%9C%BA%E7%AE%80%E5%8D%95%E9%85%8D%E7%BD%AE%E5%91%BD%E4%BB%A4/:0:0","tags":["Brocade"],"title":"Brocade ICX 6450交换机简单配置命令","uri":"/posts/notes/2023-10-09-brocade-icx-6450%E4%BA%A4%E6%8D%A2%E6%9C%BA%E7%AE%80%E5%8D%95%E9%85%8D%E7%BD%AE%E5%91%BD%E4%BB%A4/"},{"categories":["Kubernetes"],"content":" 警告\rKubelet 客户端证书在 Kubernetes 集群中扮演着重要的角色，因为它们用于节点与控制平面之间的安全通信和身份验证。当 kubelet 客户端证书在主节点和工作节点之间不一致时，可能会导致以下问题： 认证问题：如果主节点和工作节点上的 kubelet 客户端证书不匹配，工作节点可能无法成功通过 kube-apiserver 连接到集群控制平面。这可能会导致节点在集群中无法注册，或者在节点重新启动后无法重新加入集群。 授权问题：证书不匹配可能导致工作节点无法获得所需的授权来执行其任务。这可能会影响到节点上运行的 Pod 的权限，导致访问受限或拒绝访问控制。 安全性问题：证书不一致可能会降低集群的整体安全性，因为节点身份的验证不再可靠。攻击者可能会利用此漏洞来伪装为不同的节点，试图执行恶意操作或访问敏感数据。 集群稳定性问题：证书问题可能导致节点的异常行为，例如节点的意外停机或无法访问。这可能会导致应用程序中断和集群不稳定 ","date":"2023-09-13","objectID":"/posts/kubernetes/advanced/k8s%E9%9B%86%E7%BE%A4%E8%AF%81%E4%B9%A6%E8%BF%87%E6%9C%9F%E5%A4%84%E7%90%86-node-noready%E7%8A%B6%E6%80%81/:0:0","tags":["Kubernetes"],"title":"K8S集群证书过期处理-Node NoReady状态","uri":"/posts/kubernetes/advanced/k8s%E9%9B%86%E7%BE%A4%E8%AF%81%E4%B9%A6%E8%BF%87%E6%9C%9F%E5%A4%84%E7%90%86-node-noready%E7%8A%B6%E6%80%81/"},{"categories":["Kubernetes"],"content":"1. 排查过程 查看pod状态发现 zz-biz-01节点上的pod全部为 Terminating 状态 查看集群节点状态 zz-biz-01 为NoReady 状态 [root@zz-front-01 ~]# kubectl get node\rNAME STATUS ROLES AGE VERSION\rzz-biz-01 NoReady \u003cnone\u003e 729d v1.21.4\rzz-front-01 Ready control-plane,master 729d v1.21.4查看该节点详细信息: 在Conditions中显示NodeStatusUnknown，说明该节点与master节点通信异常，可能是网络问题也可能是证书认证问题。 [root@zz-front-01 pki]# kubectl describe node zz-biz-01 Name: zz-biz-01 Roles: \u003cnone\u003e Labels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/os=linux kubernetes.io/arch=amd64 kubernetes.io/hostname=zz-biz-01 kubernetes.io/os=linux Annotations: flannel.alpha.coreos.com/backend-data: {\"VNI\":1,\"VtepMAC\":\"9e:4b:af:c8:84:39\"} flannel.alpha.coreos.com/backend-type: vxlan flannel.alpha.coreos.com/kube-subnet-manager: true flannel.alpha.coreos.com/public-ip: 172.21.10.158 kubeadm.alpha.kubernetes.io/cri-socket: /run/containerd/containerd.sock node.alpha.kubernetes.io/ttl: 0 volumes.kubernetes.io/controller-managed-attach-detach: true CreationTimestamp: Mon, 13 Sep 2021 20:43:32 +0800 Taints: node.kubernetes.io/unreachable:NoExecute node.kubernetes.io/unreachable:NoSchedule Unschedulable: false Lease: HolderIdentity: zz-biz-01 AcquireTime: \u003cunset\u003e RenewTime: Wed, 17 May 2023 12:11:16 +0800 Conditions: Type Status LastHeartbeatTime LastTransitionTime Reason Message ---- ------ ----------------- ------------------ ------ ------- NetworkUnavailable False Sat, 22 Apr 2023 13:18:44 +0800 Sat, 22 Apr 2023 13:18:44 +0800 FlannelIsUp Flannel is running on thi s node MemoryPressure Unknown Wed, 17 May 2023 12:10:38 +0800 Tue, 12 Sep 2023 11:07:27 +0800 NodeStatusUnknown Kubelet stopped posting n ode status. DiskPressure Unknown Wed, 17 May 2023 12:10:38 +0800 Tue, 12 Sep 2023 11:07:27 +0800 NodeStatusUnknown Kubelet stopped posting n ode status. PIDPressure Unknown Wed, 17 May 2023 12:10:38 +0800 Tue, 12 Sep 2023 11:07:27 +0800 NodeStatusUnknown Kubelet stopped posting n ode status. Ready Unknown Wed, 17 May 2023 12:10:38 +0800 Tue, 12 Sep 2023 11:07:27 +0800 NodeStatusUnknown Kubelet stopped posting n ode status. Addresses: InternalIP: 172.21.10.158 Hostname: zz-biz-01 Capacity: cpu: 8 ephemeral-storage: 51175Mi hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 16265960Ki pods: 110 Allocatable: cpu: 8 ephemeral-storage: 48294789041 hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 16163560Ki pods: 110 System Info: Machine ID: 9f225d6ba3fa4e24acb9d493fcc2a126 System UUID: 420B4078-BE8A-D926-2521-CFE406B4A8B9 Boot ID: 14618e98-5f60-4483-8afe-dffe3ecb1319 Kernel Version: 3.10.0-1160.42.2.el7.x86_64 OS Image: CentOS Linux 7 (Core) Operating System: linux Architecture: amd64 Container Runtime Version: containerd://1.4.9 Kubelet Version: v1.21.4 Kube-Proxy Version: v1.21.4 PodCIDR: 10.244.1.0/24 PodCIDRs: 10.244.1.0/24 Non-terminated Pods: (2 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits Age --------- ---- ------------ ---------- --------------- ------------- --- kube-system kube-flannel-ds-lk647 100m (1%) 100m (1%) 50Mi (0%) 50Mi (0%) 728d kube-system kube-proxy-vgd4p 0 (0%) 0 (0%) 0 (0%) 0 (0%) 728d Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 100m (1%) 100m (1%) memory 50Mi (0%) 50Mi (0%) ephemeral-storage 0 (0%) 0 (0%) hugepages-1Gi 0 (0%) 0 (0%) hugepages-2Mi 0 (0%) 0 (0%) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Starting 60m kubelet Starting kubelet. Warning InvalidDiskCapacity 60m kubelet invalid capacity 0 on image filesystem Normal NodeAllocatableEnforced 60m kubelet Updated Node Allocatable limit across pods Normal NodeHasSufficientMemory 60m (x5 over 60m) kubelet Node zz-biz-01 status is now: NodeHasSufficientMemory Normal NodeHasNoDiskPressure 60m (x5 over 60m) kubelet Node zz-biz-01 status is now: NodeHasNoDiskPressure Normal NodeHasSufficientPID 60m (x5 over 60m) kubelet Node zz-biz-01 status is now: NodeHasSufficientPID Warning InvalidDiskCapacity 59m kubelet invalid capacity 0","date":"2023-09-13","objectID":"/posts/kubernetes/advanced/k8s%E9%9B%86%E7%BE%A4%E8%AF%81%E4%B9%A6%E8%BF%87%E6%9C%9F%E5%A4%84%E7%90%86-node-noready%E7%8A%B6%E6%80%81/:1:0","tags":["Kubernetes"],"title":"K8S集群证书过期处理-Node NoReady状态","uri":"/posts/kubernetes/advanced/k8s%E9%9B%86%E7%BE%A4%E8%AF%81%E4%B9%A6%E8%BF%87%E6%9C%9F%E5%A4%84%E7%90%86-node-noready%E7%8A%B6%E6%80%81/"},{"categories":["Kubernetes"],"content":"2.处理过程 刷新master证书，过程参考 https://www.xinn.cc/posts/notes/k8s-certificate-missery/ https://www.jianshu.com/p/10269275239d 通过查看/etc/kubernetes/kubelet.conf 发现证书路径/var/lib/kubelet/pki/kubelet-client-current.pem 到/var/lib/kubelet/pki/ 路径下查看证书日期 [root@zz-biz-01 pki]# ls apiserver-kubelet-client.crt kubelet-client-2021-09-13-20-43-21.pem kubelet-client-current.pem kubelet.key apiserver-kubelet-client.key kubelet-client-2022-08-16-09-23-17.pem kubelet.crt kubelet.key.bak bak kubelet-client-2023-04-25-14-58-57.pem kubelet.crt.bak查看证书有效期 openssl x509 -in /var/lib/kubelet/pki/kubelet-client-current.pem -noout -text | grep Not批量清除zz-biz-01节点上全部为 Terminating 状态的Pod kubectl get pods -n kube-system | grep Terminating | awk '{print $1}' | xargs kubectl delete pod -n kube-system --force --grace-period=0 kubectl get pods -n zz-prod | grep Terminating | awk '{print $1}' | xargs kubectl delete pod -n zz-prod --force --grace-period=0查看worker 节点的kubelet日志 证书已经更新了但是worker节点还是出现认证问题： 9月 13 09:21:36 zz-biz-01 systemd[1]: Unit kubelet.service entered failed state. 9月 13 09:21:36 zz-biz-01 systemd[1]: kubelet.service failed. 9月 13 09:21:47 zz-biz-01 systemd[1]: kubelet.service holdoff time over, scheduling restart. 9月 13 09:21:47 zz-biz-01 systemd[1]: Stopped kubelet: The Kubernetes Node Agent. 9月 13 09:21:47 zz-biz-01 systemd[1]: Started kubelet: The Kubernetes Node Agent. 9月 13 09:21:47 zz-biz-01 kubelet[14097]: I0913 09:21:47.127825 14097 server.go:197] \"Warning: For remote container runtime, --pod-infra-contain er-image is ignored in kubelet, which should be set in that remote runtime instead\" 9月 13 09:21:47 zz-biz-01 kubelet[14097]: I0913 09:21:47.155032 14097 server.go:440] \"Kubelet version\" kubeletVersion=\"v1.21.4\" 9月 13 09:21:47 zz-biz-01 kubelet[14097]: I0913 09:21:47.155367 14097 server.go:851] \"Client rotation is on, will bootstrap in background\" 9月 13 09:21:47 zz-biz-01 kubelet[14097]: E0913 09:21:47.158797 14097 bootstrap.go:240] unable to read existing bootstrap client config from /et c/kubernetes/kubelet.conf: invalid configuration: [unable to read client-cert /var/lib/kubelet/pki/kubelet-client-current.pem for default-auth due to open /var/lib/kubelet/pki/kubelet-client-current.pem: no such file or directory, unable to read client-key /var/lib/kubelet/pki/kubelet-client -current.pem for default-auth due to open /var/lib/kubelet/pki/kubelet-client-current.pem: no such file or directory] 9月 13 09:21:47 zz-biz-01 systemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILURE 9月 13 09:21:47 zz-biz-01 kubelet[14097]: E0913 09:21:47.158910 14097 server.go:292] \"Failed to run kubelet\" err=\"failed to run Kubelet: unable to load bootstrap kubeconfig: stat /etc/kubernetes/bootstrap-kubelet.conf: no such file or directory\" 9月 13 09:21:47 zz-biz-01 systemd[1]: Unit kubelet.service entered failed state. 9月 13 09:21:47 zz-biz-01 systemd[1]: kubelet.service failed. 9月 13 09:21:57 zz-biz-01 systemd[1]: kubelet.service holdoff time over, scheduling restart. 9月 13 09:21:57 zz-biz-01 systemd[1]: Stopped kubelet: The Kubernetes Node Agent. 9月 13 09:21:57 zz-biz-01 systemd[1]: Started kubelet: The Kubernetes Node Agent. 9月 13 09:21:57 zz-biz-01 kubelet[14136]: I0913 09:21:57.395574 14136 server.go:197] \"Warning: For remote container runtime, --pod-infra-contain er-image is ignored in kubelet, which should be set in that remote runtime instead\" 9月 13 09:21:57 zz-biz-01 kubelet[14136]: I0913 09:21:57.413917 14136 server.go:440] \"Kubelet version\" kubeletVersion=\"v1.21.4\" 9月 13 09:21:57 zz-biz-01 kubelet[14136]: I0913 09:21:57.414228 14136 server.go:851] \"Client rotation is on, will bootstrap in background\" 9月 13 09:21:57 zz-biz-01 kubelet[14136]: E0913 09:21:57.415375 14136 bootstrap.go:240] unable to read existing bootstrap client config from /et c/kubernetes/kubelet.conf: invalid configuration: [unable to read client-cert /var/lib/kubelet/pki/kubelet-client-current.pem for default-auth due to open /var/lib/kubelet/pki/kubelet-client-current.pe","date":"2023-09-13","objectID":"/posts/kubernetes/advanced/k8s%E9%9B%86%E7%BE%A4%E8%AF%81%E4%B9%A6%E8%BF%87%E6%9C%9F%E5%A4%84%E7%90%86-node-noready%E7%8A%B6%E6%80%81/:2:0","tags":["Kubernetes"],"title":"K8S集群证书过期处理-Node NoReady状态","uri":"/posts/kubernetes/advanced/k8s%E9%9B%86%E7%BE%A4%E8%AF%81%E4%B9%A6%E8%BF%87%E6%9C%9F%E5%A4%84%E7%90%86-node-noready%E7%8A%B6%E6%80%81/"},{"categories":["Kubernetes"],"content":"3.验证集群状态 [root@zz-front-01 ~]# kubectl get node NAME STATUS ROLES AGE VERSION zz-biz-01 Ready \u003cnone\u003e 729d v1.21.4 zz-front-01 Ready control-plane,master 729d v1.21.4 ","date":"2023-09-13","objectID":"/posts/kubernetes/advanced/k8s%E9%9B%86%E7%BE%A4%E8%AF%81%E4%B9%A6%E8%BF%87%E6%9C%9F%E5%A4%84%E7%90%86-node-noready%E7%8A%B6%E6%80%81/:3:0","tags":["Kubernetes"],"title":"K8S集群证书过期处理-Node NoReady状态","uri":"/posts/kubernetes/advanced/k8s%E9%9B%86%E7%BE%A4%E8%AF%81%E4%B9%A6%E8%BF%87%E6%9C%9F%E5%A4%84%E7%90%86-node-noready%E7%8A%B6%E6%80%81/"},{"categories":["Kubernetes"],"content":"用label控制Pod的位置 在 Kubernetes 中，你可以使用节点标签（Node Labels）和节点选择器（Node Selector）来控制将 Pod 调度到特定的节点上。这允许你根据节点的属性和特征，如硬件配置、资源需求、GPU 支持等，将 Pod 定向地调度到特定的节点上。 ","date":"2023-09-06","objectID":"/posts/kubernetes/primary/%E7%94%A8label%E6%8E%A7%E5%88%B6pod%E7%9A%84%E4%BD%8D%E7%BD%AE/:1:0","tags":["k8s知识点"],"title":"使用节点标签（Node Labels）和节点选择器（Node Selector）来控制将 Pod 调度到特定的节点","uri":"/posts/kubernetes/primary/%E7%94%A8label%E6%8E%A7%E5%88%B6pod%E7%9A%84%E4%BD%8D%E7%BD%AE/"},{"categories":["Kubernetes"],"content":"1.为节点添加标签（Node Labels） 首先，你需要为节点添加标签，以标识它们的属性。例如，如果你有一些节点配置了 SSD 磁盘，可以给它们添加一个名为 “ssd=true 的标签。你可以使用以下命令为节点添加标签： kubectl label nodes \u003cnode-name\u003e \u003clabel-key\u003e=\u003clabel-value\u003e其中，\u003cnode-name\u003e 是节点的名称，\u003clabel-key\u003e 是标签的键，\u003clabel-value\u003e 是标签的值。 例如，要为名为 “node-1” 的节点添加 “ssd=true” 标签，可以运行： kubectl label nodes node-1 ssd=true查看标签 kubectl get node --show-labels","date":"2023-09-06","objectID":"/posts/kubernetes/primary/%E7%94%A8label%E6%8E%A7%E5%88%B6pod%E7%9A%84%E4%BD%8D%E7%BD%AE/:1:1","tags":["k8s知识点"],"title":"使用节点标签（Node Labels）和节点选择器（Node Selector）来控制将 Pod 调度到特定的节点","uri":"/posts/kubernetes/primary/%E7%94%A8label%E6%8E%A7%E5%88%B6pod%E7%9A%84%E4%BD%8D%E7%BD%AE/"},{"categories":["Kubernetes"],"content":"2.定义 Pod 的节点选择器（Node Selector）： 在你的 Pod 配置文件中，你可以使用 nodeSelector 字段来定义 Pod 的节点选择器。这个字段告诉 Kubernetes 将 Pod 调度到拥有特定标签的节点上。例如： apiVersion: v1 kind: Pod metadata: name: my-pod spec: nodeSelector: ssd: \"true\" containers: - name: my-container image: my-image上面的示例将 Pod my-pod 调度到拥有标签 ssd=true 的节点上。 ","date":"2023-09-06","objectID":"/posts/kubernetes/primary/%E7%94%A8label%E6%8E%A7%E5%88%B6pod%E7%9A%84%E4%BD%8D%E7%BD%AE/:1:2","tags":["k8s知识点"],"title":"使用节点标签（Node Labels）和节点选择器（Node Selector）来控制将 Pod 调度到特定的节点","uri":"/posts/kubernetes/primary/%E7%94%A8label%E6%8E%A7%E5%88%B6pod%E7%9A%84%E4%BD%8D%E7%BD%AE/"},{"categories":["Kubernetes"],"content":"3.部署 Pod 使用 kubectl apply -f \u003cpod-config-file\u003e.yaml 命令来部署你的 Pod。 ","date":"2023-09-06","objectID":"/posts/kubernetes/primary/%E7%94%A8label%E6%8E%A7%E5%88%B6pod%E7%9A%84%E4%BD%8D%E7%BD%AE/:2:0","tags":["k8s知识点"],"title":"使用节点标签（Node Labels）和节点选择器（Node Selector）来控制将 Pod 调度到特定的节点","uri":"/posts/kubernetes/primary/%E7%94%A8label%E6%8E%A7%E5%88%B6pod%E7%9A%84%E4%BD%8D%E7%BD%AE/"},{"categories":["Kubernetes"],"content":"4.验证调度 使用 kubectl get pods -o wide 命令来查看 Pod 的调度情况，确保它们已经被调度到具有相应标签的节点上。 通过这种方式，你可以实现将 Pod 部署到特定类型的节点上，根据节点的属性和特征来满足你的需求。这是 Kubernetes 中一个非常有用的特性，可以根据不同的硬件配置和需求来优化资源利用和性能。 ","date":"2023-09-06","objectID":"/posts/kubernetes/primary/%E7%94%A8label%E6%8E%A7%E5%88%B6pod%E7%9A%84%E4%BD%8D%E7%BD%AE/:3:0","tags":["k8s知识点"],"title":"使用节点标签（Node Labels）和节点选择器（Node Selector）来控制将 Pod 调度到特定的节点","uri":"/posts/kubernetes/primary/%E7%94%A8label%E6%8E%A7%E5%88%B6pod%E7%9A%84%E4%BD%8D%E7%BD%AE/"},{"categories":["Github"],"content":"使用 GitHub Actions 来自动构建 Docker 镜像并将其上传到 Docker Registry 时，需要以下步骤进行设置： 工作流会在每次将代码推送到 main 分支时执行。它首先检出代码，然后设置 Docker Buildx 环境，接着登录到指定的 Docker Registry，最后构建并推送 Docker 镜像。 创建 Dockerfile：在你的 GitHub 仓库中创建一个名为 Dockerfile 的文件，用于定义镜像的构建过程和内容。 设置 Secrets：在仓库的设置中，添加三个 Secrets，分别是你的 Docker Registry 用户名、密码或访问令牌，以及 Docker Registry 的地址。 创建 Workflow 文件：在 .github/workflows/ 目录下创建一个 .yml 文件（例如：docker-build.yml），在这个文件中定义工作流程的步骤。 Workflow 配置：在 Workflow 文件中，配置工作流程的触发条件，比如当代码被推送到特定分支时触发。然后，定义构建步骤，包括： 检出代码 设置 Docker Buildx 环境（用于构建多平台镜像） 登录到 Docker Registry，使用之前设置的 Secrets 使用 Docker 构建和推送镜像到 Registry，可以指定标签等信息。 ","date":"2023-08-25","objectID":"/posts/blog/push-docker-images-github-registry/:0:0","tags":["Docker","Hugo"],"title":"使用Github Action 构建Docker镜像并上传Registry","uri":"/posts/blog/push-docker-images-github-registry/"},{"categories":["Github"],"content":"创建拥有上传权限的token ","date":"2023-08-25","objectID":"/posts/blog/push-docker-images-github-registry/:1:0","tags":["Docker","Hugo"],"title":"使用Github Action 构建Docker镜像并上传Registry","uri":"/posts/blog/push-docker-images-github-registry/"},{"categories":["Github"],"content":"测试登录Registry 测试登录 root@harbor01[14:16:02]~ #:docker login --username ryanxin7 --password ghp_xxxxxxxx ghcr.io WARNING! Using --password via the CLI is insecure. Use --password-stdin. WARNING! Your password will be stored unencrypted in /root/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded","date":"2023-08-25","objectID":"/posts/blog/push-docker-images-github-registry/:2:0","tags":["Docker","Hugo"],"title":"使用Github Action 构建Docker镜像并上传Registry","uri":"/posts/blog/push-docker-images-github-registry/"},{"categories":["Github"],"content":"构建镜像测试上传到Regisry root@harbor01[14:22:43]/dockerfile/xn-blog #:docker image build -t ghcr.io/xnlog:latest ./ DEPRECATED: The legacy builder is deprecated and will be removed in a future release. Install the buildx component to build images with BuildKit: https://docs.docker.com/go/buildx/ Sending build context to Docker daemon 19.04MB Step 1/4 : FROM nginx:latest ---\u003e 89da1fb6dcb9 Step 2/4 : COPY public/ /usr/share/nginx/html ---\u003e Using cache ---\u003e 342e46da94ee Step 3/4 : COPY default.conf /etc/nginx/conf.d/default.conf ---\u003e Using cache ---\u003e 56c7d4347a26 Step 4/4 : EXPOSE 8848 ---\u003e Using cache ---\u003e 35e2e284b708 Successfully built 35e2e284b708 Successfully tagged ghcr.io/xnlog:latest root@racknerd-20e7f5:~# docker pull ghcr.io/ryanxin7/xnlog:latest latest: Pulling from ryanxin7/xnlog 52d2b7f179e3: Pull complete fd9f026c6310: Pull complete 055fa98b4363: Pull complete 96576293dd29: Pull complete a7c4092be904: Pull complete e3b6889c8954: Pull complete da761d9a302b: Pull complete e8c074410147: Pull complete 4d2b965ac974: Pull complete Digest: sha256:3bcffe2f09e7584d9b05da90af16c43b195c377ce645dbc013f8b9ba70ce83de Status: Downloaded newer image for ghcr.io/ryanxin7/xnlog:latest ghcr.io/ryanxin7/xnlog:latest","date":"2023-08-25","objectID":"/posts/blog/push-docker-images-github-registry/:3:0","tags":["Docker","Hugo"],"title":"使用Github Action 构建Docker镜像并上传Registry","uri":"/posts/blog/push-docker-images-github-registry/"},{"categories":["Github"],"content":"在packages中查看镜像 ","date":"2023-08-25","objectID":"/posts/blog/push-docker-images-github-registry/:4:0","tags":["Docker","Hugo"],"title":"使用Github Action 构建Docker镜像并上传Registry","uri":"/posts/blog/push-docker-images-github-registry/"},{"categories":["Github"],"content":"配置action secret ","date":"2023-08-25","objectID":"/posts/blog/push-docker-images-github-registry/:5:0","tags":["Docker","Hugo"],"title":"使用Github Action 构建Docker镜像并上传Registry","uri":"/posts/blog/push-docker-images-github-registry/"},{"categories":["Github"],"content":"创建workflow文件 mkdir workflow name: Docker Image CI for GHCR on: push jobs: build_and_publish: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - name: Build and push the image run: | docker login --username ryanxin7 --password ${{ secrets.DOCKERPACKAING }} ghcr.io docker build docker/. --tag ghcr.io/ryanxin7/xnlog:latest docker push ghcr.io/ryanxin7/xnlog:latest ","date":"2023-08-25","objectID":"/posts/blog/push-docker-images-github-registry/:6:0","tags":["Docker","Hugo"],"title":"使用Github Action 构建Docker镜像并上传Registry","uri":"/posts/blog/push-docker-images-github-registry/"},{"categories":["Github"],"content":"测试提交代码 xx9z@xin MINGW64 /c/xnblog/xnlog (main) $ git add . xx9z@xin MINGW64 /c/xnblog/xnlog (main) $ git commit -m \"blog update\" [main a5724b1] blog update 5 files changed, 477 insertions(+) create mode 100644 content/posts/Blog/push-docker-images-github-registry.md rename \"content/posts/\\344\\275\\277\\347\\224\\250Algolia\\345\\256\\236\\347\\216\\260Hugo\\346\\234\\254\\345\\234\\260\\346\\231\\272\\350\\203\\275\\346\\220\\234\\347\\264\\242.md\" =\u003e \"content/posts/Blog/\\344\\275\\277\\347\\224\\250Algolia\\345\\256\\236\\347\\216\\260Hugo\\346\\234\\254\\345\\234\\260\\346\\231\\272\\350\\203\\275\\346\\220\\234\\347\\264\\242.md\" (100%) create mode 100644 content/posts/kubernetes/k8s-replace-NFS-storage.md create mode 100644 \"content/posts/kubernetes/k8s\\345\\274\\272\\345\\210\\266\\345\\210\\240\\351\\231\\244pod\u0026pv\u0026pvc\\345\\222\\214ns\u0026namespace\\346\\226\\271\\346\\263\\225.md\" create mode 100644 content/posts/kubernetes/redis-on-k8scluster.md xx9z@xin MINGW64 /c/xnblog/xnlog (main) $ git push origin main Enumerating objects: 14, done. Counting objects: 100% (14/14), done. Delta compression using up to 16 threads Compressing objects: 100% (9/9), done. Writing objects: 100% (10/10), 5.57 KiB | 2.78 MiB/s, done. Total 10 (delta 2), reused 0 (delta 0), pack-reused 0 remote: Resolving deltas: 100% (2/2), completed with 2 local objects. To github.com:ryanxin7/hg-xnlog.git 12d766d..a5724b1 main -\u003e main ","date":"2023-08-25","objectID":"/posts/blog/push-docker-images-github-registry/:7:0","tags":["Docker","Hugo"],"title":"使用Github Action 构建Docker镜像并上传Registry","uri":"/posts/blog/push-docker-images-github-registry/"},{"categories":["Hugo"],"content":"1.简介 Algolia是一家提供搜索即服务的技术公司，帮助开发者为他们的应用程序或网站构建高速、精准的搜索功能。 免费的计划每个月可以查询10000次，对于个人站点也是够用了。 ","date":"2023-08-15","objectID":"/posts/blog/%E4%BD%BF%E7%94%A8algolia%E5%AE%9E%E7%8E%B0hugo%E6%9C%AC%E5%9C%B0%E6%99%BA%E8%83%BD%E6%90%9C%E7%B4%A2/:1:0","tags":["个人网站","Hugo"],"title":"使用Algolia实现Hugo本地智能搜索","uri":"/posts/blog/%E4%BD%BF%E7%94%A8algolia%E5%AE%9E%E7%8E%B0hugo%E6%9C%AC%E5%9C%B0%E6%99%BA%E8%83%BD%E6%90%9C%E7%B4%A2/"},{"categories":["Hugo"],"content":"2. 配置Algolia Algolia的配置步骤通常包括以下几个主要方面：创建账户、导入数据、设置索引、集成到应用程序中以及调整搜索体验。 ","date":"2023-08-15","objectID":"/posts/blog/%E4%BD%BF%E7%94%A8algolia%E5%AE%9E%E7%8E%B0hugo%E6%9C%AC%E5%9C%B0%E6%99%BA%E8%83%BD%E6%90%9C%E7%B4%A2/:2:0","tags":["个人网站","Hugo"],"title":"使用Algolia实现Hugo本地智能搜索","uri":"/posts/blog/%E4%BD%BF%E7%94%A8algolia%E5%AE%9E%E7%8E%B0hugo%E6%9C%AC%E5%9C%B0%E6%99%BA%E8%83%BD%E6%90%9C%E7%B4%A2/"},{"categories":["Hugo"],"content":"2.1 创建账号 访问Algolia的官方网站（https://www.algolia.com/），注册一个账户并登录。Google和Github账号可以直接登录。 ","date":"2023-08-15","objectID":"/posts/blog/%E4%BD%BF%E7%94%A8algolia%E5%AE%9E%E7%8E%B0hugo%E6%9C%AC%E5%9C%B0%E6%99%BA%E8%83%BD%E6%90%9C%E7%B4%A2/:2:1","tags":["个人网站","Hugo"],"title":"使用Algolia实现Hugo本地智能搜索","uri":"/posts/blog/%E4%BD%BF%E7%94%A8algolia%E5%AE%9E%E7%8E%B0hugo%E6%9C%AC%E5%9C%B0%E6%99%BA%E8%83%BD%E6%90%9C%E7%B4%A2/"},{"categories":["Hugo"],"content":"2.2 创建应用程序 选择免费套餐 选择位置 ","date":"2023-08-15","objectID":"/posts/blog/%E4%BD%BF%E7%94%A8algolia%E5%AE%9E%E7%8E%B0hugo%E6%9C%AC%E5%9C%B0%E6%99%BA%E8%83%BD%E6%90%9C%E7%B4%A2/:2:2","tags":["个人网站","Hugo"],"title":"使用Algolia实现Hugo本地智能搜索","uri":"/posts/blog/%E4%BD%BF%E7%94%A8algolia%E5%AE%9E%E7%8E%B0hugo%E6%9C%AC%E5%9C%B0%E6%99%BA%E8%83%BD%E6%90%9C%E7%B4%A2/"},{"categories":["Hugo"],"content":"2.3 创建索引 在Algolia控制台中，创建一个新的索引。索引是存储数据的容器，用于执行搜索操作。 ","date":"2023-08-15","objectID":"/posts/blog/%E4%BD%BF%E7%94%A8algolia%E5%AE%9E%E7%8E%B0hugo%E6%9C%AC%E5%9C%B0%E6%99%BA%E8%83%BD%E6%90%9C%E7%B4%A2/:2:3","tags":["个人网站","Hugo"],"title":"使用Algolia实现Hugo本地智能搜索","uri":"/posts/blog/%E4%BD%BF%E7%94%A8algolia%E5%AE%9E%E7%8E%B0hugo%E6%9C%AC%E5%9C%B0%E6%99%BA%E8%83%BD%E6%90%9C%E7%B4%A2/"},{"categories":["Hugo"],"content":"2.4 导入数据 您可以使用Algolia的API、SDK或工具来导入数据，确保数据字段与搜索需求匹配。 ","date":"2023-08-15","objectID":"/posts/blog/%E4%BD%BF%E7%94%A8algolia%E5%AE%9E%E7%8E%B0hugo%E6%9C%AC%E5%9C%B0%E6%99%BA%E8%83%BD%E6%90%9C%E7%B4%A2/:2:4","tags":["个人网站","Hugo"],"title":"使用Algolia实现Hugo本地智能搜索","uri":"/posts/blog/%E4%BD%BF%E7%94%A8algolia%E5%AE%9E%E7%8E%B0hugo%E6%9C%AC%E5%9C%B0%E6%99%BA%E8%83%BD%E6%90%9C%E7%B4%A2/"},{"categories":["Hugo"],"content":"2.5 配置搜索属性 定义索引字段，为每个索引定义需要搜索和显示的字段。也可以设置字段的搜索权重和过滤条件。 ","date":"2023-08-15","objectID":"/posts/blog/%E4%BD%BF%E7%94%A8algolia%E5%AE%9E%E7%8E%B0hugo%E6%9C%AC%E5%9C%B0%E6%99%BA%E8%83%BD%E6%90%9C%E7%B4%A2/:2:5","tags":["个人网站","Hugo"],"title":"使用Algolia实现Hugo本地智能搜索","uri":"/posts/blog/%E4%BD%BF%E7%94%A8algolia%E5%AE%9E%E7%8E%B0hugo%E6%9C%AC%E5%9C%B0%E6%99%BA%E8%83%BD%E6%90%9C%E7%B4%A2/"},{"categories":["Hugo"],"content":"2.6 配置排名规则 调整搜索结果的排名规则，以确保最相关的结果显示在顶部 ","date":"2023-08-15","objectID":"/posts/blog/%E4%BD%BF%E7%94%A8algolia%E5%AE%9E%E7%8E%B0hugo%E6%9C%AC%E5%9C%B0%E6%99%BA%E8%83%BD%E6%90%9C%E7%B4%A2/:2:6","tags":["个人网站","Hugo"],"title":"使用Algolia实现Hugo本地智能搜索","uri":"/posts/blog/%E4%BD%BF%E7%94%A8algolia%E5%AE%9E%E7%8E%B0hugo%E6%9C%AC%E5%9C%B0%E6%99%BA%E8%83%BD%E6%90%9C%E7%B4%A2/"},{"categories":["Hugo"],"content":"2.6 测试搜索 通过Algolia UI界面，测试搜索效果。 ","date":"2023-08-15","objectID":"/posts/blog/%E4%BD%BF%E7%94%A8algolia%E5%AE%9E%E7%8E%B0hugo%E6%9C%AC%E5%9C%B0%E6%99%BA%E8%83%BD%E6%90%9C%E7%B4%A2/:2:7","tags":["个人网站","Hugo"],"title":"使用Algolia实现Hugo本地智能搜索","uri":"/posts/blog/%E4%BD%BF%E7%94%A8algolia%E5%AE%9E%E7%8E%B0hugo%E6%9C%AC%E5%9C%B0%E6%99%BA%E8%83%BD%E6%90%9C%E7%B4%A2/"},{"categories":["Hugo"],"content":"3.配置hugo主题使用Algolia搜索 ","date":"2023-08-15","objectID":"/posts/blog/%E4%BD%BF%E7%94%A8algolia%E5%AE%9E%E7%8E%B0hugo%E6%9C%AC%E5%9C%B0%E6%99%BA%E8%83%BD%E6%90%9C%E7%B4%A2/:3:0","tags":["个人网站","Hugo"],"title":"使用Algolia实现Hugo本地智能搜索","uri":"/posts/blog/%E4%BD%BF%E7%94%A8algolia%E5%AE%9E%E7%8E%B0hugo%E6%9C%AC%E5%9C%B0%E6%99%BA%E8%83%BD%E6%90%9C%E7%B4%A2/"},{"categories":["Hugo"],"content":"3.1 获取Algolia Key 点击右上角头像—\u003esetting—\u003e API Keys ","date":"2023-08-15","objectID":"/posts/blog/%E4%BD%BF%E7%94%A8algolia%E5%AE%9E%E7%8E%B0hugo%E6%9C%AC%E5%9C%B0%E6%99%BA%E8%83%BD%E6%90%9C%E7%B4%A2/:3:1","tags":["个人网站","Hugo"],"title":"使用Algolia实现Hugo本地智能搜索","uri":"/posts/blog/%E4%BD%BF%E7%94%A8algolia%E5%AE%9E%E7%8E%B0hugo%E6%9C%AC%E5%9C%B0%E6%99%BA%E8%83%BD%E6%90%9C%E7%B4%A2/"},{"categories":["Hugo"],"content":"3.2 配置主题相关参数 为了生成搜索功能所需要的 index.json, 请在你的 网站配置 中添加 JSON 输出文件类型到 outputs 部分的 home 字段中。 在 Hugo 中，可以通过修改网站配置文件（通常是 config.toml、config.yaml 或 config.json）来指定不同部分的输出格式。这样，可以在生成网站页面时，为不同的页面部分选择不同的输出格式，包括 JSON。 打开网站配置文件。 寻找名为 outputs 的部分，如果没有则创建它。 在 outputs 部分中，可以为不同的页面部分指定输出格式。 # Options to make hugo output files home = [\"HTML\", \"RSS\", \"JSON\", \"BaiduUrls\"] page = [\"HTML\", \"MarkDown\"] section = [\"HTML\", \"RSS\"] taxonomy = [\"HTML\", \"RSS\"] taxonomyTerm = [\"HTML\"]保存网站配置文件 在运行 Hugo 构建命令（例如 hugo 或 hugo build）以生成网站时，Hugo 将生成 index.json 文件作为上述部分的输出。 outputFormats 在 Algolia 中，outputFormats 是用于控制返回搜索结果的格式的设置选项之一。 通过调整 outputFormats，可以决定搜索结果以何种格式返回应用程序。 [MarkDown] mediaType = \"text/markdown\" isPlainText = true isHTML = false # Options to make output baidu_urls.txt file [BaiduUrls] baseName = \"baidu_urls\" mediaType = \"text/plain\" isPlainText = true isHTML = false这段代码的作用是为 Hugo 网站生成 Algolia 搜索所需的 JSON 索引文件，以便在搜索时快速检索和展示内容。 {{- if .Site.Params.search -}} {{- $index := slice -}} {{- $pages := where .Site.RegularPages \"Params.password\" \"eq\" nil -}} {{- if .Site.Params.page.hiddenFromSearch -}} {{- $pages = where $pages \"Params.hiddenfromsearch\" false -}} {{- else -}} {{- $pages = where $pages \"Params.hiddenfromsearch\" \"!=\" true -}} {{- end -}} {{- range $pages -}} {{- $uri := .RelPermalink -}} {{- if $.Site.Params.search.absoluteURL -}} {{- $uri = .Permalink -}} {{- end -}} {{- $meta := dict \"uri\" $uri \"title\" .Title \"tags\" .Params.tags \"categories\" .Params.categories -}} {{- $meta = $.Site.Params.dateFormat | default \"2006-01-02\" | .PublishDate.Format | dict \"date\" | merge $meta -}} {{- with .Description -}} {{- $index = $index | append (dict \"content\" . \"objectID\" $uri | merge $meta) -}} {{- end -}} {{- $params := .Params | merge $.Site.Params.page -}} {{/* Extended Markdown syntax */}} {{- $content := dict \"Content\" .Content \"Ruby\" $params.ruby \"Fraction\" $params.fraction \"Fontawesome\" $params.fontawesome | partial \"function/content.html\" -}} {{/* Remove line number for code */}} {{- $content = $content | replaceRE `\u003cspan class=\"lnt?\"\u003e *\\d*\\n?\u003c/span\u003e` \"\" -}} {{- range $i, $contenti := split $content \"\u003ch2 id=\" -}} {{- if gt $i 0 -}} {{- $contenti = printf \"\u003ch2 id=%v\" $contenti -}} {{- end -}} {{- range $j, $contentj := split $contenti \"\u003ch3 id=\" -}} {{- if gt $j 0 -}} {{- $contentj = printf \"\u003ch3 id=%v\" $contentj -}} {{- end -}} {{/* Plainify, unescape and remove (\\n, \\t) */}} {{- $contentj = $contentj | plainify | htmlUnescape | replaceRE `[\\n\\t ]+` \" \" -}} {{- if gt $.Site.Params.search.contentLength 0 -}} {{- $contentj = substr $contentj 0 $.Site.Params.search.contentLength -}} {{- end -}} {{- if $contentj | and (ne $contentj \" \") -}} {{- $one := printf \"%v:%v:%v\" $uri $i $j | dict \"content\" $contentj \"objectID\" | merge $meta -}} {{- $index = $index | append $one -}} {{- end -}} {{- end -}} {{- end -}} {{- end -}} {{- $index | jsonify | safeJS -}} {{- end -}}","date":"2023-08-15","objectID":"/posts/blog/%E4%BD%BF%E7%94%A8algolia%E5%AE%9E%E7%8E%B0hugo%E6%9C%AC%E5%9C%B0%E6%99%BA%E8%83%BD%E6%90%9C%E7%B4%A2/:3:2","tags":["个人网站","Hugo"],"title":"使用Algolia实现Hugo本地智能搜索","uri":"/posts/blog/%E4%BD%BF%E7%94%A8algolia%E5%AE%9E%E7%8E%B0hugo%E6%9C%AC%E5%9C%B0%E6%99%BA%E8%83%BD%E6%90%9C%E7%B4%A2/"},{"categories":["Hugo"],"content":"3.3 配置hugo主题应用Algolia # 搜索配置 [params.search] enable = true # 搜索引擎的类型 [\"lunr\", \"algolia\", \"fuse\"] type = \"algolia\" # 文章内容最长索引长度 contentLength = 4000 # 搜索框的占位提示语 placeholder = \"\" # 最大结果数目 maxResultLength = 10 # 结果内容片段长度 snippetLength = 50 # 搜索结果中高亮部分的 HTML 标签 highlightTag = \"em\" # 是否在搜索索引中使用基于 baseURL 的绝对路径 absoluteURL = false [params.search.algolia] index = \"xx-log\" appID = \"SFSFN4DBN1\" searchKey = \"bd48328538sdb2f38b20753c17c60ba92f\"","date":"2023-08-15","objectID":"/posts/blog/%E4%BD%BF%E7%94%A8algolia%E5%AE%9E%E7%8E%B0hugo%E6%9C%AC%E5%9C%B0%E6%99%BA%E8%83%BD%E6%90%9C%E7%B4%A2/:3:3","tags":["个人网站","Hugo"],"title":"使用Algolia实现Hugo本地智能搜索","uri":"/posts/blog/%E4%BD%BF%E7%94%A8algolia%E5%AE%9E%E7%8E%B0hugo%E6%9C%AC%E5%9C%B0%E6%99%BA%E8%83%BD%E6%90%9C%E7%B4%A2/"},{"categories":["Hugo"],"content":"4.测试效果 ","date":"2023-08-15","objectID":"/posts/blog/%E4%BD%BF%E7%94%A8algolia%E5%AE%9E%E7%8E%B0hugo%E6%9C%AC%E5%9C%B0%E6%99%BA%E8%83%BD%E6%90%9C%E7%B4%A2/:4:0","tags":["个人网站","Hugo"],"title":"使用Algolia实现Hugo本地智能搜索","uri":"/posts/blog/%E4%BD%BF%E7%94%A8algolia%E5%AE%9E%E7%8E%B0hugo%E6%9C%AC%E5%9C%B0%E6%99%BA%E8%83%BD%E6%90%9C%E7%B4%A2/"},{"categories":["Kubernetes"],"content":"k8s强制删除pod\u0026pv\u0026pvc和ns\u0026namespace方法 注意：以下操作方法十分危险，三思而行！！！ 如果名称空间、pod、pv、pvc全部处于“Terminating”状态时，此时的该名称空间下的所有控制器都已经被删除了，之所以出现pod、pvc、pv、ns无法删除，那是因为kubelet 阻塞，有其他的资源在使用该namespace，比如CRD等，尝试重启kubelet，再删除该namespace 也不好使。 正确的删除方法：删除pod–\u003e 删除pvc —\u003e 删除pv –\u003e 删除名称空间 ","date":"2023-08-14","objectID":"/posts/kubernetes/advanced/k8s%E5%BC%BA%E5%88%B6%E5%88%A0%E9%99%A4podpvpvc%E5%92%8Cnsnamespace%E6%96%B9%E6%B3%95/:0:0","tags":["Kubernetes"],"title":"k8s强制删除pod\u0026pv\u0026pvc和ns\u0026namespace方法","uri":"/posts/kubernetes/advanced/k8s%E5%BC%BA%E5%88%B6%E5%88%A0%E9%99%A4podpvpvc%E5%92%8Cnsnamespace%E6%96%B9%E6%B3%95/"},{"categories":["Kubernetes"],"content":"一、强制删除pod $ kubectl delete pod \u003cyour-pod-name\u003e -n \u003cname-space\u003e --force --grace-period=0解决方法：加参数 --force --grace-period=0，grace-period表示过渡存活期，默认30s，在删除POD之前允许POD慢慢终止其上的容器进程，从而优雅退出，0表示立即终止POD ","date":"2023-08-14","objectID":"/posts/kubernetes/advanced/k8s%E5%BC%BA%E5%88%B6%E5%88%A0%E9%99%A4podpvpvc%E5%92%8Cnsnamespace%E6%96%B9%E6%B3%95/:1:0","tags":["Kubernetes"],"title":"k8s强制删除pod\u0026pv\u0026pvc和ns\u0026namespace方法","uri":"/posts/kubernetes/advanced/k8s%E5%BC%BA%E5%88%B6%E5%88%A0%E9%99%A4podpvpvc%E5%92%8Cnsnamespace%E6%96%B9%E6%B3%95/"},{"categories":["Kubernetes"],"content":"二、强制删除pv、pvc 直接删除k8s etcd数据库中的记录 $ kubectl patch pv xxx -p '{\"metadata\":{\"finalizers\":null}}' $ kubectl patch pvc xxx -p '{\"metadata\":{\"finalizers\":null}}'","date":"2023-08-14","objectID":"/posts/kubernetes/advanced/k8s%E5%BC%BA%E5%88%B6%E5%88%A0%E9%99%A4podpvpvc%E5%92%8Cnsnamespace%E6%96%B9%E6%B3%95/:2:0","tags":["Kubernetes"],"title":"k8s强制删除pod\u0026pv\u0026pvc和ns\u0026namespace方法","uri":"/posts/kubernetes/advanced/k8s%E5%BC%BA%E5%88%B6%E5%88%A0%E9%99%A4podpvpvc%E5%92%8Cnsnamespace%E6%96%B9%E6%B3%95/"},{"categories":["Kubernetes"],"content":"三、强制删除ns 在尝试以下命令强制删除也不好使： $ kubectl delete ns \u003cterminating-namespace\u003e --force --grace-period=0解决方法： 1）运行以下命令以查看处于“Terminating”状态的namespace： $ kubectl get namespaces2）选择一个Terminating namespace，并查看namespace 中的finalizer。运行以下命令： $ kubectl get namespace \u003cterminating-namespace\u003e -o yaml输出信息如下： apiVersion: v1 kind: Namespace metadata: creationTimestamp: \"2019-11-20T15:18:06Z\" deletionTimestamp: \"2020-01-16T02:50:02Z\" name: \u003cterminating-namespace\u003e resourceVersion: \"3249493\" selfLink: /api/v1/namespaces/knative-eventing uid: f300ea38-c8c2-4653-b432-b66103e412db spec: finalizers: - kubernetes status:3）导出json格式到文件 $ kubectl get namespace \u003cterminating-namespace\u003e -o json \u003etmp.json4）编辑tmp.josn，删除finalizers 字段的值 { \"apiVersion\": \"v1\", \"kind\": \"Namespace\", \"metadata\": { \"creationTimestamp\": \"2019-11-20T15:18:06Z\", \"deletionTimestamp\": \"2020-01-16T02:50:02Z\", \"name\": \"\u003cterminating-namespace\u003e\", \"resourceVersion\": \"3249493\", \"selfLink\": \"/api/v1/namespaces/knative-eventing\", \"uid\": \"f300ea38-c8c2-4653-b432-b66103e412db\" }, \"spec\": { #从此行开始删除 \"finalizers\": [] }, # 删到此行 \"status\": { \"phase\": \"Terminating\" } }5）开启proxy $ kubectl proxy执行该命令后，当前终端会被卡住 6）打开新的一个窗口，执行以下命令 $ curl -k -H \"Content-Type: application/json\" -X PUT --data-binary @tmp.json http://127.0.0.1:8001/api/v1/namespaces/\u003cterminating-namespace\u003e/finalize输出信息如下： { \"kind\": \"Namespace\", \"apiVersion\": \"v1\", \"metadata\": { \"name\": \"istio-system\", \"selfLink\": \"/api/v1/namespaces/istio-system/finalize\", \"uid\": \"2e274537-727f-4a8f-ae8c-397473ed619a\", \"resourceVersion\": \"3249492\", \"creationTimestamp\": \"2019-11-20T15:18:06Z\", \"deletionTimestamp\": \"2020-01-16T02:50:02Z\" }, \"spec\": { }, \"status\": { \"phase\": \"Terminating\" } }7）确认处于Terminating 状态的namespace已经被删除 $ kubectl get namespaces如果还有处于Terminating 状态的namespace，重复以上操作，删除即可！ ","date":"2023-08-14","objectID":"/posts/kubernetes/advanced/k8s%E5%BC%BA%E5%88%B6%E5%88%A0%E9%99%A4podpvpvc%E5%92%8Cnsnamespace%E6%96%B9%E6%B3%95/:3:0","tags":["Kubernetes"],"title":"k8s强制删除pod\u0026pv\u0026pvc和ns\u0026namespace方法","uri":"/posts/kubernetes/advanced/k8s%E5%BC%BA%E5%88%B6%E5%88%A0%E9%99%A4podpvpvc%E5%92%8Cnsnamespace%E6%96%B9%E6%B3%95/"},{"categories":["Kubernetes"],"content":"k8s中部署单节点redis https://gitee.com/zdevops/k8s-yaml/blob/main/redis/single/ ","date":"2023-08-14","objectID":"/posts/kubernetes/advanced/redis-on-k8scluster/:0:0","tags":["Kubernetes"],"title":"k8s中部署单节点redis","uri":"/posts/kubernetes/advanced/redis-on-k8scluster/"},{"categories":["Kubernetes"],"content":"redis-cm.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: redis-config namespace: zdevops data: redis-config: | appendonly yes protected-mode no dir /data port 6379 requirepass redis@abc.com ","date":"2023-08-14","objectID":"/posts/kubernetes/advanced/redis-on-k8scluster/:1:0","tags":["Kubernetes"],"title":"k8s中部署单节点redis","uri":"/posts/kubernetes/advanced/redis-on-k8scluster/"},{"categories":["Kubernetes"],"content":"redis-sts.yaml --- apiVersion: apps/v1 kind: StatefulSet metadata: name: redis namespace: zdevops labels: app: redis spec: serviceName: redis-headless replicas: 1 selector: matchLabels: app: redis template: metadata: labels: app: redis spec: containers: - name: redis image: 'registry.zdevops.com.cn/library/redis:6.2.7' command: - \"redis-server\" args: - \"/etc/redis/redis.conf\" ports: - name: redis-6379 containerPort: 6379 protocol: TCP volumeMounts: - name: config mountPath: /etc/redis - name: data mountPath: /data resources: limits: cpu: '2' memory: 4000Mi requests: cpu: 100m memory: 500Mi volumes: - name: config configMap: name: redis-config items: - key: redis-config path: redis.conf volumeClaimTemplates: - metadata: name: data namespace: zdevops spec: accessModes: [ \"ReadWriteOnce\" ] storageClassName: \"glusterfs\" resources: requests: storage: 5Gi --- apiVersion: v1 kind: Service metadata: name: redis-headless namespace: zdevops labels: app: redis spec: ports: - name: redis-6379 protocol: TCP port: 6379 targetPort: 6379 selector: app: redis clusterIP: None type: ClusterIP","date":"2023-08-14","objectID":"/posts/kubernetes/advanced/redis-on-k8scluster/:2:0","tags":["Kubernetes"],"title":"k8s中部署单节点redis","uri":"/posts/kubernetes/advanced/redis-on-k8scluster/"},{"categories":["Kubernetes"],"content":"使用 Kubernetes NFS Subdir External Provisioner 插件来动态为 Kubernetes 提供 PV（Persistent Volume）卷，并且该插件本身并不提供 NFS 存储，而是依赖于现有的 NFS 服务器来提供存储。 持久卷目录的命名规则为: ","date":"2023-08-14","objectID":"/posts/kubernetes/advanced/k8s-replace-nfs-storage/:0:0","tags":["Kubernetes","NFS"],"title":"生产案例-Kubernetes 替换后端存储","uri":"/posts/kubernetes/advanced/k8s-replace-nfs-storage/"},{"categories":["Kubernetes"],"content":"情况说明 业务使用 Kubernetes NFS Subdir External Provisioner 插件来动态为 Kubernetes 提供 PV（Persistent Volume）卷，并且该插件本身并不提供 NFS 存储，而是依赖于现有的 NFS 服务器来提供存储。 命名规则 ${namespace}-${pvcName}-${pvName} 出现磁盘空间严重不足情况，需要更换存储。 项目地址: https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner/blob/master/charts/nfs-subdir-external-provisioner/README.md Kubernetes 提供了一种动态创建 Persistent Volumes (PV) 的机制，称为 Dynamic Provisioning。这个机制允许集群管理员事先配置 StorageClass，然后用户只需创建 Persistent Volume Claims (PVC)，PV 就会自动根据 StorageClass 规则动态创建。 ","date":"2023-08-14","objectID":"/posts/kubernetes/advanced/k8s-replace-nfs-storage/:1:0","tags":["Kubernetes","NFS"],"title":"生产案例-Kubernetes 替换后端存储","uri":"/posts/kubernetes/advanced/k8s-replace-nfs-storage/"},{"categories":["Kubernetes"],"content":"示例 nfs: server: 013f2dsb-4m2d.cn-beijing.extreme.nas.aliyuncs.com #NFS 服务器的主机名（必填） path: /share #要使用的挂载点的基本路径 mountOptions: #安装选项（例如“nfs.vers=3”） - vers=3 - noacl - nolock - proto=tcp - rsize=1048576 - wsize=1048576 - hard - timeo=600 - retrans=2 - noresvport storageClass: name: nfs-client-speed # 存储类的名称 defaultClass: false #设置为默认 StorageClass allowVolumeExpansion: true #允许扩大卷 reclaimPolicy: Delete #回收策略 provisionerName: nfs-client-speed #动态卷分配者名称，必须和创建的\"provisioner\"变量中设置的name一致 archiveOnDelete: true ##设置为\"false\"时删除PVC不会保留数据,\"true\"则保留数据 ","date":"2023-08-14","objectID":"/posts/kubernetes/advanced/k8s-replace-nfs-storage/:1:1","tags":["Kubernetes","NFS"],"title":"生产案例-Kubernetes 替换后端存储","uri":"/posts/kubernetes/advanced/k8s-replace-nfs-storage/"},{"categories":["Kubernetes"],"content":"StorageClass回收策略 由 StorageClass 动态创建的 PersistentVolume 会在类的 reclaimPolicy 字段中指定回收策略，可以是 Delete 或者 Retain。 如果 StorageClass 对象被创建时没有指定 reclaimPolicy，它将默认为 Delete。 回收（Reclaiming） 当用户不再使用其存储卷时，他们可以从 API 中将 PVC 对象删除， 从而允许该资源被回收再利用。PersistentVolume 对象的回收策略告诉集群， 当其被从申领中释放时如何处理该数据卷。 目前，数据卷可以被 Retained（保留）、Recycled（回收）或 Deleted（删除）。 保留（Retain） 回收策略 Retain 使得用户可以手动回收资源。当 PersistentVolumeClaim 对象被删除时，PersistentVolume 卷仍然存在，对应的数据卷被视为\"已释放（released）\"。 由于卷上仍然存在这前一申领人的数据，该卷还不能用于其他申领。 管理员可以通过下面的步骤来手动回收该卷： 删除 PersistentVolume 对象。与之相关的、位于外部基础设施中的存储资产 （例如 AWS EBS、GCE PD、Azure Disk 或 Cinder 卷）在 PV 删除之后仍然存在。 根据情况，手动清除所关联的存储资产上的数据。 手动删除所关联的存储资产。 如果你希望重用该存储资产，可以基于存储资产的定义创建新的 PersistentVolume 卷对象。 删除（Delete） 对于支持 Delete 回收策略的卷插件，删除动作会将 PersistentVolume 对象从 Kubernetes 中移除，同时也会从外部基础设施（如 AWS EBS、GCE PD、Azure Disk 或 Cinder 卷）中移除所关联的存储资产。 动态制备的卷会继承其 StorageClass 中设置的回收策略， 该策略默认为 Delete。管理员需要根据用户的期望来配置 StorageClass； 否则 PV 卷被创建之后必须要被编辑或者修补。 参阅更改 PV 卷的回收策略。 ","date":"2023-08-14","objectID":"/posts/kubernetes/advanced/k8s-replace-nfs-storage/:1:2","tags":["Kubernetes","NFS"],"title":"生产案例-Kubernetes 替换后端存储","uri":"/posts/kubernetes/advanced/k8s-replace-nfs-storage/"},{"categories":["Kubernetes"],"content":"1.将多块磁盘组合成 LVM卷 ","date":"2023-08-14","objectID":"/posts/kubernetes/advanced/k8s-replace-nfs-storage/:2:0","tags":["Kubernetes","NFS"],"title":"生产案例-Kubernetes 替换后端存储","uri":"/posts/kubernetes/advanced/k8s-replace-nfs-storage/"},{"categories":["Kubernetes"],"content":"1.1 安装gdisk工具 yum install gdisk","date":"2023-08-14","objectID":"/posts/kubernetes/advanced/k8s-replace-nfs-storage/:2:1","tags":["Kubernetes","NFS"],"title":"生产案例-Kubernetes 替换后端存储","uri":"/posts/kubernetes/advanced/k8s-replace-nfs-storage/"},{"categories":["Kubernetes"],"content":"1.2 磁盘分区 使用 gdisk 或其他分区工具（例如 fdisk）来为每个磁盘(sdc,sdd,sde,sdf 共8T)创建分区。 [root@xx-worker-05 ~]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT fd0 2:0 1 4K 0 disk sda 8:0 0 1T 0 disk ├─sda1 8:1 0 1G 0 part /boot ├─sda2 8:2 0 199G 0 part │ ├─centos-root 253:0 0 50G 0 lvm / │ ├─centos-swap 253:1 0 3.9G 0 lvm │ └─centos-home 253:2 0 145.1G 0 lvm /home └─sda3 8:3 0 824G 0 part /nfs-server sdb 8:16 0 1T 0 disk sdc 8:32 0 2T 0 disk sdd 8:48 0 2T 0 disk sde 8:64 0 2T 0 disk sdf 8:80 0 2T 0 disk sr0 11:0 1 1024M 0 rom在 gdisk 中，你可以使用 n 命令创建新的分区，然后按照提示创建分区。重复这个步骤来创建所需数量的分区。 然后，对每个硬盘进行相同的操作。 Command (? for help): ? b back up GPT data to a file c change a partition's name d delete a partition i show detailed information on a partition l list known partition types n add a new partition o create a new empty GUID partition table (GPT) p print the partition table q quit without saving changes r recovery and transformation options (experts only) s sort partitions t change a partition's type code v verify disk w write table to disk and exit x extra functionality (experts only) ? print this menu将硬盘模式改为lvm，代码是8e00 Linux LVM Command (? for help): l 0700 Microsoft basic data 0c01 Microsoft reserved 2700 Windows RE 3000 ONIE boot 3001 ONIE config 4100 PowerPC PReP boot 4200 Windows LDM data 4201 Windows LDM metadata 7501 IBM GPFS 7f00 ChromeOS kernel 7f01 ChromeOS root 7f02 ChromeOS reserved 8200 Linux swap 8300 Linux filesystem 8301 Linux reserved 8302 Linux /home 8400 Intel Rapid Start 8e00 Linux LVM a500 FreeBSD disklabel a501 FreeBSD boot a502 FreeBSD swap a503 FreeBSD UFS a504 FreeBSD ZFS a505 FreeBSD Vinum/RAID a580 Midnight BSD data a581 Midnight BSD boot a582 Midnight BSD swap a583 Midnight BSD UFS a584 Midnight BSD ZFS a585 Midnight BSD Vinum a800 Apple UFS a901 NetBSD swap a902 NetBSD FFS a903 NetBSD LFS a904 NetBSD concatenated a905 NetBSD encrypted a906 NetBSD RAID ab00 Apple boot af00 Apple HFS/HFS+ af01 Apple RAID af02 Apple RAID offline af03 Apple label af04 AppleTV recovery af05 Apple Core Storage be00 Solaris boot bf00 Solaris root bf01 Solaris /usr \u0026 Mac Z bf02 Solaris swap bf03 Solaris backup bf04 Solaris /var bf05 Solaris /home bf06 Solaris alternate se bf07 Solaris Reserved 1 bf08 Solaris Reserved 2 bf09 Solaris Reserved 3 bf0a Solaris Reserved 4 bf0b Solaris Reserved 5 c001 HP-UX data c002 HP-UX service ea00 Freedesktop $BOOT eb00 Haiku BFS ed00 Sony system partitio ed01 Lenovo system partit[root@xx-worker-05 ~]# gdisk /dev/sdc GPT fdisk (gdisk) version 0.8.10 Partition table scan: MBR: protective BSD: not present APM: not present GPT: present Found valid GPT with protective MBR; using GPT. Command (? for help): p Disk /dev/sdc: 4294967296 sectors, 2.0 TiB Logical sector size: 512 bytes Disk identifier (GUID): B62A55C1-A5DE-445A-B8CA-AC489E6B49DE Partition table holds up to 128 entries First usable sector is 34, last usable sector is 4294967262 Partitions will be aligned on 2048-sector boundaries Total free space is 2014 sectors (1007.0 KiB) Number Start (sector) End (sector) Size Code Name 1 2048 4294967262 2.0 TiB 8E00 Linux LVM","date":"2023-08-14","objectID":"/posts/kubernetes/advanced/k8s-replace-nfs-storage/:2:2","tags":["Kubernetes","NFS"],"title":"生产案例-Kubernetes 替换后端存储","uri":"/posts/kubernetes/advanced/k8s-replace-nfs-storage/"},{"categories":["Kubernetes"],"content":"1.3 创建物理卷（Physical Volumes） pvcreate /dev/sdc1 /dev/sdd1 /dev/sde1 /dev/sdf1[root@xx-worker-05 ~]# pvdisplay --- Physical volume --- PV Name /dev/sda2 VG Name centos PV Size \u003c199.00 GiB / not usable 3.00 MiB Allocatable yes PE Size 4.00 MiB Total PE 50943 Free PE 1 Allocated PE 50942 PV UUID 9ALLQS-tNe2-f2zD-WG09-2rn1-xH21-lkXfhm --- Physical volume --- PV Name /dev/sdd1 VG Name k8s_nfs_data PV Size \u003c2.00 TiB / not usable 2.98 MiB Allocatable yes (but full) PE Size 4.00 MiB Total PE 524287 Free PE 0 Allocated PE 524287 PV UUID GRJKYB-DPcn-XE28-YAze-eoDU-526F-XydILb --- Physical volume --- PV Name /dev/sdc1 VG Name k8s_nfs_data PV Size \u003c2.00 TiB / not usable 2.98 MiB Allocatable yes (but full) PE Size 4.00 MiB Total PE 524287 Free PE 0 Allocated PE 524287 PV UUID ulkd9R-aoaU-104h-e2PM-eKl7-bZY3-FCEm3W --- Physical volume --- PV Name /dev/sde1 VG Name k8s_nfs_data PV Size \u003c2.00 TiB / not usable 2.98 MiB Allocatable yes (but full) PE Size 4.00 MiB Total PE 524287 Free PE 0 Allocated PE 524287 PV UUID 0hLr2k-IVYf-XRHi-2VSZ-PH6N-Z0ma-OG4A3w --- Physical volume --- PV Name /dev/sdf1 VG Name k8s_nfs_data PV Size \u003c2.00 TiB / not usable 2.98 MiB Allocatable yes PE Size 4.00 MiB Total PE 524287 Free PE 26210 Allocated PE 498077 PV UUID H1aYjs-pPDV-STyr-IvPe-6RcF-xUC4-eOoWiI","date":"2023-08-14","objectID":"/posts/kubernetes/advanced/k8s-replace-nfs-storage/:2:3","tags":["Kubernetes","NFS"],"title":"生产案例-Kubernetes 替换后端存储","uri":"/posts/kubernetes/advanced/k8s-replace-nfs-storage/"},{"categories":["Kubernetes"],"content":"1.4 创建卷组（Volume Group） #创建一个卷组，将物理卷添加到卷组中： vgcreate k8s_nfs_data /dev/sdc1 /dev/sdd1 /dev/sde1 /dev/sdf1#这里 k8s_nfs_data 是卷组的名称，你可以自己命名。 [root@xx-worker-05 ~]# vgdisplay k8s_nfs_data --- Volume group --- VG Name k8s_nfs_data System ID Format lvm2 Metadata Areas 4 Metadata Sequence No 2 VG Access read/write VG Status resizable MAX LV 0 Cur LV 1 Open LV 1 Max PV 0 Cur PV 4 Act PV 4 VG Size \u003c8.00 TiB PE Size 4.00 MiB Total PE 2097148 Alloc PE / Size 2070938 / 7.90 TiB Free PE / Size 26210 / 102.38 GiB VG UUID etAQEx-IDaa-soo5-6d5k-9YHo-JpHX-OpG9Oy","date":"2023-08-14","objectID":"/posts/kubernetes/advanced/k8s-replace-nfs-storage/:2:4","tags":["Kubernetes","NFS"],"title":"生产案例-Kubernetes 替换后端存储","uri":"/posts/kubernetes/advanced/k8s-replace-nfs-storage/"},{"categories":["Kubernetes"],"content":"创建逻辑卷（Logical Volume）： 使用 lvcreate 命令创建逻辑卷，并指定大小： lvcreate -n k8s_nfs_lvmdata_1 -L 7.9T k8s_nfs_data这里 my_lv 是逻辑卷的名称，7.9T 是逻辑卷的大小。 --- Logical volume --- LV Path /dev/k8s_nfs_data/k8s_nfs_lvmdata_1 LV Name k8s_nfs_lvmdata_1 VG Name k8s_nfs_data LV UUID T9NOtS-ElEw-y287-mG41-dzul-b30H-en1fV1 LV Write Access read/write LV Creation host, time xx-worker-05, 2023-08-18 15:55:29 +0800 LV Status available # open 1 LV Size 7.90 TiB Current LE 2070938 Segments 4 Allocation inherit Read ahead sectors auto - currently set to 8192 Block device 253:3[root@xx-worker-05 ~]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT fd0 2:0 1 4K 0 disk sda 8:0 0 1T 0 disk ├─sda1 8:1 0 1G 0 part /boot ├─sda2 8:2 0 199G 0 part │ ├─centos-root 253:0 0 50G 0 lvm / │ ├─centos-swap 253:1 0 3.9G 0 lvm │ └─centos-home 253:2 0 145.1G 0 lvm /home └─sda3 8:3 0 824G 0 part /nfs-server sdb 8:16 0 1T 0 disk sdc 8:32 0 2T 0 disk └─sdc1 8:33 0 2T 0 part └─k8s_nfs_data-k8s_nfs_lvmdata_1 253:3 0 7.9T 0 lvm /nfs-server2 sdd 8:48 0 2T 0 disk └─sdd1 8:49 0 2T 0 part └─k8s_nfs_data-k8s_nfs_lvmdata_1 253:3 0 7.9T 0 lvm /nfs-server2 sde 8:64 0 2T 0 disk └─sde1 8:65 0 2T 0 part └─k8s_nfs_data-k8s_nfs_lvmdata_1 253:3 0 7.9T 0 lvm /nfs-server2 sdf 8:80 0 2T 0 disk └─sdf1 8:81 0 2T 0 part └─k8s_nfs_data-k8s_nfs_lvmdata_1 253:3 0 7.9T 0 lvm /nfs-server2 sr0 11:0 1 1024M 0 rom","date":"2023-08-14","objectID":"/posts/kubernetes/advanced/k8s-replace-nfs-storage/:2:5","tags":["Kubernetes","NFS"],"title":"生产案例-Kubernetes 替换后端存储","uri":"/posts/kubernetes/advanced/k8s-replace-nfs-storage/"},{"categories":["Kubernetes"],"content":"1.5 格式化逻辑卷 格式化逻辑卷以准备使用： mkfs.ext4 /dev/k8s_nfs_data/k8s_nfs_lvmdata_1使用的是 ext4 文件系统，你可以根据需要选择其他文件系统。 ","date":"2023-08-14","objectID":"/posts/kubernetes/advanced/k8s-replace-nfs-storage/:2:6","tags":["Kubernetes","NFS"],"title":"生产案例-Kubernetes 替换后端存储","uri":"/posts/kubernetes/advanced/k8s-replace-nfs-storage/"},{"categories":["Kubernetes"],"content":"1.6 挂载逻辑卷 创建一个目录来挂载逻辑卷，并将逻辑卷挂载到该目录： mkdir /nfs-server2 mount /dev/k8s_nfs_data/k8s_nfs_lvmdata_1 /nfs-server2","date":"2023-08-14","objectID":"/posts/kubernetes/advanced/k8s-replace-nfs-storage/:2:7","tags":["Kubernetes","NFS"],"title":"生产案例-Kubernetes 替换后端存储","uri":"/posts/kubernetes/advanced/k8s-replace-nfs-storage/"},{"categories":["Kubernetes"],"content":"2. 更换后端存储 vim /etc/exports #/nfs-server *(rw,sync,insecure,no_root_squash) /nfs-server2 *(rw,sync,insecure,no_root_squash) systemctl restart nfs systemctl restart rpcbind","date":"2023-08-14","objectID":"/posts/kubernetes/advanced/k8s-replace-nfs-storage/:3:0","tags":["Kubernetes","NFS"],"title":"生产案例-Kubernetes 替换后端存储","uri":"/posts/kubernetes/advanced/k8s-replace-nfs-storage/"},{"categories":["Kubernetes"],"content":"2.1更改NFS服务器地址 vim nfs-client.values nfs: server: 192.168.10.135 path: /nfs-server mountOptions: - vers=4 - minorversion=0 - rsize=1048576 - wsize=1048576 - hard - timeo=600 - retrans=2 - noresvport storageClass: name: nfs-client defaultClass: true allowVolumeExpansion: true reclaimPolicy: Delete provisionerName: nfs-client archiveOnDelete: truehelm install -n kube-system -f nfs-client.values.yaml nfs-client rmxc/nfs-subdir-external-provisioner #-n 命名空间 #-f 指定values文件 #nfs-client helm项目名字 # rmxc/nfs-subdir-external-provisioner CHART文件","date":"2023-08-14","objectID":"/posts/kubernetes/advanced/k8s-replace-nfs-storage/:3:1","tags":["Kubernetes","NFS"],"title":"生产案例-Kubernetes 替换后端存储","uri":"/posts/kubernetes/advanced/k8s-replace-nfs-storage/"},{"categories":["Kubernetes"],"content":"2.2 查看当前pvc 需要删除当前pvc，然后重新自动获取绑定pv kubectl delete pvc -n xx-prod pvc-0770e2e6-70c4-4426-9a43-75e2cab94290 #如果卡住直接删除k8s etcd数据库中的记录 $ kubectl patch pv xxx -p '{\"metadata\":{\"finalizers\":null}}' $ kubectl patch pvc xxx -p '{\"metadata\":{\"finalizers\":null}}'查看原PVC的数据目录 [xx@xx-worker-05 nfs-server]$ ll total 24 drwxrwxrwx 3 root root 37 Nov 19 2021 xx-prod-app-data-pvc-0770e2e6-70c4-4426-9a43-75e2cab94290 drwxrwxrwx 60 root root 8192 May 19 21:26 xx-prod-fe-nginx-data-pvc-b2d082a0-9df3-42a2-8817-dc4eb4a587ca drwxrwxrwx 2 root root 227 Feb 17 2022 xx-prod-fe-nginx-log-pvc-66e5f78a-e952-4a18-8a9f-6173d7d2485a drwxrwxrwx 21 root root 4096 Mar 27 15:32 xx-prod-log-pvc-d717b996-7b65-4c23-be9c-fcdc988266be drwxrwxrwx 15 root root 4096 Aug 22 18:06 xx-prod-mysql57-data-pvc-d4968732-0cf1-4395-9854-fd4581d2906d drwxrwxrwx 2 root root 45 Nov 14 2021 xx-prod-mysql57-log-pvc-7f831088-8828-4ab5-8e20-03ab5b048cdd drwxrwxrwx 2 root root 44 Aug 23 09:27 xx-prod-redis-data-redis-master-0-pvc-7f2b4078-a441-400a-b8e1-8046c3999a8f查看新PVC的数据目录 [xxu@xx-worker-05 nfs-server2]$ ll drwxrwxrwx 3 root root 4096 Nov 19 2021 xx-prod-app-data-pvc-0770e2e6-70c4-4426-9a43-75e2cab94291 drwxrwxrwx 3 root root 4096 Aug 25 17:04 xx-prod-app-data-pvc-51174db7-fd78-4448-b9fb-21b6f8577e58 drwxrwxrwx 2 root root 4096 Sep 1 14:21 xx-prod-data-redis-0-pvc-45801e2d-282d-47cc-8020-a157fd9b14de drwxrwxrwx 60 root root 4096 May 19 21:26 xx-prod-fe-nginx-data-pvc-4ef671bb-4cb3-4012-b637-0564b27637f9 drwxrwxrwx 2 root root 4096 Aug 25 09:46 xx-prod-fe-nginx-log-pvc-14f3a9a9-415d-43ea-840b-c39fa5a28ab7 drwxrwxrwx 10 root root 4096 Aug 25 10:32 xx-prod-log-pvc-fe904a2f-a94d-4f5f-8587-a425417d8612 drwxrwxrwx 15 root root 4096 Aug 26 05:23 xx-prod-mysql57-data2-pvc-085dee0e-1c0e-4a0e-9278-d5ea45ab37c6 drwxrwxrwx 2 root root 4096 Nov 14 2021 xx-prod-mysql57-log2-pvc-67bf033b-f1a4-4df0-8f77-59e47539f3d5","date":"2023-08-14","objectID":"/posts/kubernetes/advanced/k8s-replace-nfs-storage/:3:2","tags":["Kubernetes","NFS"],"title":"生产案例-Kubernetes 替换后端存储","uri":"/posts/kubernetes/advanced/k8s-replace-nfs-storage/"},{"categories":["Kubernetes"],"content":"2.3 使用rsync同步数据 停止各服务，将老PVC中的数据同步到新PVC中，Rsync使用守护进程方式后台运行避免数据同步时间过长导致中断。 服务端配置文件 # /etc/rsyncd: configuration file for rsync daemon mode # See rsyncd.conf man page for more options. # configuration example: uid = root gid = root use chroot = no max connections = 200 timeout = 6000 ignore errors read only = false list = false auth users = rsync_backup secrets file = /etc/rsync.passwd log file = /var/log/rsyncd.log # pid file = /var/run/rsyncd.pid # exclude = lost+found/ # transfer logging = yes # timeout = 900 # ignore nonreadable = yes # dont compress = *.gz *.tgz *.zip *.z *.Z *.rpm *.deb *.bz2 [nfs-backup] comment = welcome to backup! path = /nfs-server [mysql-data-backup] comment = welcome to backup! path = /nfs-server2/xx-prod-mysql57-data-pvc-d4968732-0cf1-4395-9854-fd4581d2906d [mysql-log-backup] comment = welcome to backup! path = /nfs-server2/xx-prod-mysql57-log-pvc-7f831088-8828-4ab5-8e20-03ab5b048cdd [tree-file-backup] comment = welcome to backup! path = /nfs-server2/xx-prod-app-data-pvc-0770e2e6-70c4-4426-9a43-75e2cab94291 # [ftp] # path = /home/ftp # comment = ftp export area","date":"2023-08-14","objectID":"/posts/kubernetes/advanced/k8s-replace-nfs-storage/:3:3","tags":["Kubernetes","NFS"],"title":"生产案例-Kubernetes 替换后端存储","uri":"/posts/kubernetes/advanced/k8s-replace-nfs-storage/"},{"categories":["Kubernetes"],"content":"2.4 同步命令 nohup rsync -avzrogpP --append-verify rsync_backup@127.0.0.1::tree-file-backup /nfs-server2/xx-prod-app-data-pvc-51174db7-fd78-4448-b9fb-21b6f8577e58 \u003e /var/log/rsync-log-treefile.log 2\u003e\u00261 \u0026","date":"2023-08-14","objectID":"/posts/kubernetes/advanced/k8s-replace-nfs-storage/:3:4","tags":["Kubernetes","NFS"],"title":"生产案例-Kubernetes 替换后端存储","uri":"/posts/kubernetes/advanced/k8s-replace-nfs-storage/"},{"categories":["Kubernetes"],"content":"2.5 查看日志 tail -f /var/log/rsync-log-treefile.log tree-file-server-for-pt/storage/high/2022/02/14/86b7f9c882e744a5b51be2c551f24a4b.jpg 32,827 100% 244.71kB/s 0:00:00 (xfr#138425, ir-chk=149321/287796) tree-file-server-for-pt/storage/high/2022/02/14/86b834f605614e07bccc92feaa9ab687.jpg 35,251 100% 256.90kB/s 0:00:00 (xfr#138426, ir-chk=149320/287796) tree-file-server-for-pt/storage/high/2022/02/14/86b8ac41b8394c529c00994427078f0f.png 1,345 100% 9.80kB/s 0:00:00 (xfr#138427, ir-chk=149319/287796) tree-file-server-for-pt/storage/high/2022/02/14/86b8b2ed44dc45c3ae182eccff7b4499.png 1,722 100% 12.55kB/s 0:00:00 (xfr#138428, ir-chk=149318/287796) tree-file-server-for-pt/storage/high/2022/02/14/86b932ab32e4472b8bb582c160885877.jpg 34,690 100% 250.94kB/s 0:00:00 (xfr#138429, ir-chk=149317/287796) tree-file-server-for-pt/storage/high/2022/02/14/86b9ad6ecdec45c9846aee66df52769f.png 1,978 100% 14.31kB/s 0:00:00 (xfr#138430, ir-chk=149316/287796) tree-file-server-for-pt/storage/high/2022/02/14/86ba0c2fd3a7462597c2c1e688a081d9.jpg 37,435 100% 270.80kB/s 0:00:00 (xfr#138431, ir-chk=149315/287796) tree-file-server-for-pt/storage/high/2022/02/14/86ba59e758674905ba39073f647db788.png 1,184 100% 8.50kB/s 0:00:00 (xfr#138432, ir-chk=149314/287796) tree-file-server-for-pt/storage/high/2022/02/14/86ba8bf9b31e464fa53b5cef03b29d52.jpg 31,866 100% 227.15kB/s 0:00:00 (xfr#138433, ir-chk=149313/287796) tree-file-server-for-pt/storage/high/2022/02/14/86bb4df3f5e8490f8f1eb421706828aa.jpg 33,953 100% 240.27kB/s 0:00:00 (xfr#138434, ir-chk=149312/287796) tree-file-server-for-pt/storage/high/2022/02/14/86bbc7d02a5f4076a6a2545f6cfc729d.jpg 30,784 100% 216.28kB/s 0:00:00 (xfr#138435, ir-chk=149311/287796) tree-file-server-for-pt/storage/high/2022/02/14/86bc058ff7e847dd9c6b6c7a69d551a8.jpg","date":"2023-08-14","objectID":"/posts/kubernetes/advanced/k8s-replace-nfs-storage/:3:5","tags":["Kubernetes","NFS"],"title":"生产案例-Kubernetes 替换后端存储","uri":"/posts/kubernetes/advanced/k8s-replace-nfs-storage/"},{"categories":["Kubernetes"],"content":"3. 验证pv状态 [root@xx-master-01 ~]# kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pv-nfs-client-nfs-subdir-external-provisioner 10Mi RWO Retain Bound kube-system/pvc-nfs-client-nfs-subdir-external-provisioner 9d pvc-085dee0e-1c0e-4a0e-9278-d5ea45ab37c6 50Gi RWO Delete Bound xx-prod/mysql57-data2 nfs-client 9d pvc-14f3a9a9-415d-43ea-840b-c39fa5a28ab7 8Gi RWX Delete Bound xx-prod/fe-nginx-log nfs-client 7d6h pvc-45801e2d-282d-47cc-8020-a157fd9b14de 8Gi RWO Delete Bound xx-prod/data-redis-0 nfs-client 8d pvc-4ef671bb-4cb3-4012-b637-0564b27637f9 8Gi RWX Delete Bound xx-prod/fe-nginx-data nfs-client 7d6h pvc-51174db7-fd78-4448-b9fb-21b6f8577e58 8Gi RWX Delete Bound xx-prod/app-data nfs-client 7d23h pvc-67bf033b-f1a4-4df0-8f77-59e47539f3d5 50Gi RWO Delete Bound xx-prod/mysql57-log2 nfs-client 9d pvc-d4968732-0cf1-4395-9854-fd4581d2906d 50Gi RWO Delete Bound xx-prod/mysql57-data nfs-client 655d pvc-fe904a2f-a94d-4f5f-8587-a425417d8612 8Gi RWX Delete Bound xx-prod/log nfs-client 8d","date":"2023-08-14","objectID":"/posts/kubernetes/advanced/k8s-replace-nfs-storage/:4:0","tags":["Kubernetes","NFS"],"title":"生产案例-Kubernetes 替换后端存储","uri":"/posts/kubernetes/advanced/k8s-replace-nfs-storage/"},{"categories":null,"content":"Ryan's friends","date":"2023-07-26","objectID":"/friends/","tags":null,"title":"友情链接","uri":"/friends/"},{"categories":null,"content":"交换名片 - nickname: Ryan's NoteBooks avatar: https://cdn1.ryanxin.live/ryan-ai11.png url: http://xinn.cc description: 记录文字，遇见共鸣","date":"2023-07-26","objectID":"/friends/:1:0","tags":null,"title":"友情链接","uri":"/friends/"},{"categories":["Hugo"],"content":"语法 admonition shortcode 支持 12 种 帮助你在页面中插入提示的横幅。 支持 Markdown 或者 HTML 格式。 注意\r一个 注意 横幅\r摘要\r一个 摘要 横幅\r信息\r一个 信息 横幅\r技巧\r一个 技巧 横幅\r成功\r一个 成功 横幅\r问题\r一个 问题 横幅\r警告\r一个 警告 横幅\r失败\r一个 失败 横幅\r危险\r一个 危险 横幅\rBug\r一个 Bug 横幅\r示例\r一个 示例 横幅\r引用\r一个 引用 横幅\radmonition shortcode 有以下命名参数： type [必需]（第一个位置参数） admonition 横幅的类型，默认值是 note。 title [可选]（第二个位置参数） admonition 横幅的标题，默认值是 type 参数的值。（支持 markdown） open [可选]（第三个位置参数） 横幅内容是否默认展开，默认值是 true。 一个 admonition 示例： {{\u003c admonition type=tip title=\"This is a tip\" open=false \u003e}}\r一个 **技巧** 横幅\r{{\u003c /admonition \u003e}}\r或者\r{{\u003c admonition tip \"This is a tip\" false \u003e}}\r一个 **技巧** 横幅\r{{\u003c /admonition \u003e}}呈现的输出效果如下： This is a tip\r一个 技巧 横幅\r","date":"2023-07-24","objectID":"/posts/blog/%E4%BA%94%E8%8A%B1%E5%85%AB%E9%97%A8%E7%9A%84-markdown-admonitions/:1:0","tags":["个人网站"],"title":"五花八门的 Markdown Admonition","uri":"/posts/blog/%E4%BA%94%E8%8A%B1%E5%85%AB%E9%97%A8%E7%9A%84-markdown-admonitions/"},{"categories":null,"content":"1. 开通 Github Aciton 上传代码一般已Github仓库为主，但Jenkins由于网络原因经常无法拉取Github上的代码，于是考虑将Github仓库自动同步到Gitee上，拉取国内仓库代码进行自动部署。 ","date":"2023-06-12","objectID":"/posts/cicd/github-synctogitee/:1:0","tags":["CI/CD","持续集成"],"title":"Github Actions 自动同步到 Gitee","uri":"/posts/cicd/github-synctogitee/"},{"categories":null,"content":"1.1 在Github仓库下开通Actions的功能 点击Actions选项卡→ 点击右下角Create a new workflow，命名为SyncToGitee.yml即可 ","date":"2023-06-12","objectID":"/posts/cicd/github-synctogitee/:1:1","tags":["CI/CD","持续集成"],"title":"Github Actions 自动同步到 Gitee","uri":"/posts/cicd/github-synctogitee/"},{"categories":null,"content":"1.2 编写workflow的yml代码 可以复制如下代码到自己yml中，需要更改的地方，在代码中已经标出。 name: SyncToGitee on: push: branches: - main jobs: repo-sync: runs-on: ubuntu-latest steps: - name: Checkout source codes uses: actions/checkout@v3 - name: Mirror the Github organization repos to Gitee. uses: Yikun/hub-mirror-action@master with: src: 'github/ryanxin7' # 这里改为自己github账号名称，如github/ryanxin7 dst: 'gitee/ryanxin' # 这里改为gitee上账号名称，如gitee/ryanxin dst_key: ${{ secrets.GITEE_PRIVATE_KEY }} # 这是本地生成的私钥，Github拿着私钥调用Gitee公钥 dst_token: ${{ secrets.GITEE_TOKEN }} # 这是gitee上生成的token，下面会讲 force_update: true static_list: \"xxlog\" # 同步的仓库名称，这里为xxlog，意思是会自动同步该仓库到gitee下同名仓库 debug: true","date":"2023-06-12","objectID":"/posts/cicd/github-synctogitee/:1:2","tags":["CI/CD","持续集成"],"title":"Github Actions 自动同步到 Gitee","uri":"/posts/cicd/github-synctogitee/"},{"categories":null,"content":"2.配置公钥私钥和Gitee Token ","date":"2023-06-12","objectID":"/posts/cicd/github-synctogitee/:2:0","tags":["CI/CD","持续集成"],"title":"Github Actions 自动同步到 Gitee","uri":"/posts/cicd/github-synctogitee/"},{"categories":null,"content":"2.1 配置Gitee私钥 配置公钥和私钥：公钥是Gitee这里拿着，私钥是Github拿着。因为是Github这里要同步到Gitee. 生成私钥和公钥：ssh-kengen -t ed25529 -C xxxx@xxx.com ，具体可参见：生成/添加SSH公钥 生成完之后，会在指定目录下有两个文件：id_ed25519和id_ed25519.public，前者是私钥，后者是公钥 将id_ed25519用记事本打开，复制里面内容，粘贴到Github个人仓库下的secret中。 步骤：点击仓库首页选项卡setting，会看到如下图，点击新建New repository secret： 输入Name为GITEE_PRIVATE_KEY, Value为复制id_ed25519的私钥内容 ","date":"2023-06-12","objectID":"/posts/cicd/github-synctogitee/:2:1","tags":["CI/CD","持续集成"],"title":"Github Actions 自动同步到 Gitee","uri":"/posts/cicd/github-synctogitee/"},{"categories":null,"content":"2.2 配置Gitee 公钥 输入标题为GITEE_PUB_KEY, Value为复制id_ed25519.pub的私钥内容 ","date":"2023-06-12","objectID":"/posts/cicd/github-synctogitee/:2:2","tags":["CI/CD","持续集成"],"title":"Github Actions 自动同步到 Gitee","uri":"/posts/cicd/github-synctogitee/"},{"categories":null,"content":"2.3 配置私人令牌 打开Gitee个人账号的设置页面 → 点击安全设置下的私人令牌 → 右上角生成新令牌，如下图所示： 需要添加以下权限： 点击提交之后，会得到类似下图所示的私人令牌，将其复制，并配置到Github的secret界面，类似上一步的私钥那样。 配置到Github的secret界面 最终Github这里配置的Actions secrets如下： ","date":"2023-06-12","objectID":"/posts/cicd/github-synctogitee/:2:3","tags":["CI/CD","持续集成"],"title":"Github Actions 自动同步到 Gitee","uri":"/posts/cicd/github-synctogitee/"},{"categories":null,"content":"3.查看同步状态 成功同步 ","date":"2023-06-12","objectID":"/posts/cicd/github-synctogitee/:2:4","tags":["CI/CD","持续集成"],"title":"Github Actions 自动同步到 Gitee","uri":"/posts/cicd/github-synctogitee/"},{"categories":null,"content":"Jenkins 安装与基础配置 ","date":"2023-06-12","objectID":"/posts/cicd/jenkins-install/:1:0","tags":["CI/CD"],"title":"Jenkins 安装与基础配置","uri":"/posts/cicd/jenkins-install/"},{"categories":null,"content":"配置java环境 Jdk下载：https://www.oracle.com/java/technologies/downloads/ 版本jdk要求: tar -xf #创建软连接 root@etcd02[11:05:51]/apps/Jenkins #:ln -sv /apps/jdk1.8.0_371/ /usr/local/jdk '/usr/local/jdk' -\u003e '/apps/Jenkins/jdk1.8.0_371/' root@etcd02[11:07:53]/apps/Jenkins #:ln -sv /apps/jdk1.8.0_371/bin/java /usr/bin/java '/usr/bin/java' -\u003e '/apps/Jenkins/jdk1.8.0_371/bin/java'root@server:/apps# ln -sv /apps/jdk-17.0.6/ /usr/local/jdk '/usr/local/jdk' -\u003e '/apps/jdk-17.0.6/' root@server:/apps# ln -sv /apps/jdk-17.0.6/bin/java /usr/bin/java '/usr/bin/java' -\u003e '/apps/jdk-17.0.6/bin/java' apt-get install fontconfig","date":"2023-06-12","objectID":"/posts/cicd/jenkins-install/:1:1","tags":["CI/CD"],"title":"Jenkins 安装与基础配置","uri":"/posts/cicd/jenkins-install/"},{"categories":null,"content":"配置环境变量 vim /etc/profile.d/jdk-bin-path.sh export JAVA_HOME=/usr/local/jdk export PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$PATH export CLASSPATH=.$CLASSPATH:$JAVA_HOME/lib:$JAVA_HOME/jre/lib:$JAVA_HOME/lib/tools.jar source /etc/profile.d/jdk-bin-path.shroot@etcd02[13:55:57]/etc/profile.d #:java -version java version \"1.8.0_371\" Java(TM) SE Runtime Environment (build 1.8.0_371-b11) Java HotSpot(TM) 64-Bit Server VM (build 25.371-b11, mixed mode)","date":"2023-06-12","objectID":"/posts/cicd/jenkins-install/:1:2","tags":["CI/CD"],"title":"Jenkins 安装与基础配置","uri":"/posts/cicd/jenkins-install/"},{"categories":null,"content":"安装Jenkins Ubuntu 安装包下载： https://mirrors.tuna.tsinghua.edu.cn/jenkins/debian-stable/ 安装安装依赖 apt install net-tools dpkg -i jenkins_2.361.4_all.deb 获取密码 设置清华源 该url是国内的清华大学的镜像地址（建议使用清华大学的镜像服务器，修改后刷新页面即可. https://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.json find / -name *.UpdateCenter.xml /var/lib/jenkins/hudson.model.UpdateCenter.xml vim /var/lib/jenkins/hudson.model.UpdateCenter.xml \u003c?xml version='1.1' encoding='UTF-8'?\u003e \u003csites\u003e \u003csite\u003e \u003cid\u003edefault\u003c/id\u003e \u003curl\u003ehttps://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.json\u003c/url\u003e \u003c/site\u003e \u003c/sites\u003e下载插件 ","date":"2023-06-12","objectID":"/posts/cicd/jenkins-install/:1:3","tags":["CI/CD"],"title":"Jenkins 安装与基础配置","uri":"/posts/cicd/jenkins-install/"},{"categories":null,"content":"1.安装插件 ","date":"2023-06-12","objectID":"/posts/cicd/jenkins-autodeploy/:1:0","tags":["CI/CD","持续集成"],"title":"Jenkins实践-自动构建并发布前端项目","uri":"/posts/cicd/jenkins-autodeploy/"},{"categories":null,"content":"1.1 安装NodeJS插件 点击系统管理,然后点击插件管理,在可选插件里面搜索NodeJS插件,然后安装 ","date":"2023-06-12","objectID":"/posts/cicd/jenkins-autodeploy/:1:1","tags":["CI/CD","持续集成"],"title":"Jenkins实践-自动构建并发布前端项目","uri":"/posts/cicd/jenkins-autodeploy/"},{"categories":null,"content":"1.2 安装连接SSH的插件 Publish Over SSH用于连接远程服务器 ","date":"2023-06-12","objectID":"/posts/cicd/jenkins-autodeploy/:1:2","tags":["CI/CD","持续集成"],"title":"Jenkins实践-自动构建并发布前端项目","uri":"/posts/cicd/jenkins-autodeploy/"},{"categories":null,"content":"1.3 安装把应用发布到远程服务器的插件 **Deploy to container **插件用于把打包的应用发布到远程服务 ","date":"2023-06-12","objectID":"/posts/cicd/jenkins-autodeploy/:1:3","tags":["CI/CD","持续集成"],"title":"Jenkins实践-自动构建并发布前端项目","uri":"/posts/cicd/jenkins-autodeploy/"},{"categories":null,"content":"2. 配置git和NodeJS环境 ","date":"2023-06-12","objectID":"/posts/cicd/jenkins-autodeploy/:2:0","tags":["CI/CD","持续集成"],"title":"Jenkins实践-自动构建并发布前端项目","uri":"/posts/cicd/jenkins-autodeploy/"},{"categories":null,"content":"2.1 安装配置git #安装git root@server:~# apt install git root@server:~# whereis git #查看git的执行文件位置, 默认是在 /usr/bin/git whereis git git: /usr/bin/git /usr/share/man/man1/git.1.gz ","date":"2023-06-12","objectID":"/posts/cicd/jenkins-autodeploy/:2:1","tags":["CI/CD","持续集成"],"title":"Jenkins实践-自动构建并发布前端项目","uri":"/posts/cicd/jenkins-autodeploy/"},{"categories":null,"content":"2.2 安装配置NodeJS NodeJs 下载地址：https://nodejs.org/dist/ cd /apps tar -zxvf node-v16.18.1-linux-x64.tar.gz #创建软连接 ln -sv node-v16.18.1-linux-x64/ /usr/local/node填写本地node路径 ","date":"2023-06-12","objectID":"/posts/cicd/jenkins-autodeploy/:2:2","tags":["CI/CD","持续集成"],"title":"Jenkins实践-自动构建并发布前端项目","uri":"/posts/cicd/jenkins-autodeploy/"},{"categories":null,"content":"3. 新建项目部署信息 ","date":"2023-06-12","objectID":"/posts/cicd/jenkins-autodeploy/:3:0","tags":["CI/CD","持续集成"],"title":"Jenkins实践-自动构建并发布前端项目","uri":"/posts/cicd/jenkins-autodeploy/"},{"categories":null,"content":"3.1 源码管理 填写项目仓库地址 配置免密公钥认证 ","date":"2023-06-12","objectID":"/posts/cicd/jenkins-autodeploy/:3:1","tags":["CI/CD","持续集成"],"title":"Jenkins实践-自动构建并发布前端项目","uri":"/posts/cicd/jenkins-autodeploy/"},{"categories":null,"content":"3.2 构建触发器 定时每五分钟检查一次代码仓库有没有新的提交，如果有新的提交就自动构建项目并发布到目标前端服务器。 ","date":"2023-06-12","objectID":"/posts/cicd/jenkins-autodeploy/:3:2","tags":["CI/CD","持续集成"],"title":"Jenkins实践-自动构建并发布前端项目","uri":"/posts/cicd/jenkins-autodeploy/"},{"categories":null,"content":"3.3 构建环境 3.4 执行Shell命令 npm config get registry npm install --legacy-peer-deps npm run docs:build cd src/.vuepress/dist export DIST_NAME=\"dist-v\"$(date +\"%Y%m%d%H%M%S\")\"\" tar -zcf $WORKSPACE/deployment/$DIST_NAME.tar.gz ./* \\cp $WORKSPACE/deployment/$DIST_NAME.tar.gz $WORKSPACE/deployment/dist-latest.tar.gz rm -rf $WORKSPACE/src/.vuepress/dist","date":"2023-06-12","objectID":"/posts/cicd/jenkins-autodeploy/:3:3","tags":["CI/CD","持续集成"],"title":"Jenkins实践-自动构建并发布前端项目","uri":"/posts/cicd/jenkins-autodeploy/"},{"categories":null,"content":"3.5 构建后操作 用到SSH Publishers 插件，将项目代码文件推送到目标主机。 SSH Publishers 配置 系统管理—\u003e 系统配置 —\u003e Publish over SSH Passphrase: 公钥密码 Name:目标服务器名称 Hostname：目标服务器IP地址 Username: 目标主机用户名 Remote Directory：目标主机存放目录 ","date":"2023-06-12","objectID":"/posts/cicd/jenkins-autodeploy/:3:4","tags":["CI/CD","持续集成"],"title":"Jenkins实践-自动构建并发布前端项目","uri":"/posts/cicd/jenkins-autodeploy/"},{"categories":null,"content":"4.测试项目自动发布 ","date":"2023-06-12","objectID":"/posts/cicd/jenkins-autodeploy/:4:0","tags":["CI/CD","持续集成"],"title":"Jenkins实践-自动构建并发布前端项目","uri":"/posts/cicd/jenkins-autodeploy/"},{"categories":null,"content":"4.1 测试手动构建发布 立即构建 控制台输出查看任务进度 构建成功 前端服务器目录下验证 ","date":"2023-06-12","objectID":"/posts/cicd/jenkins-autodeploy/:4:1","tags":["CI/CD","持续集成"],"title":"Jenkins实践-自动构建并发布前端项目","uri":"/posts/cicd/jenkins-autodeploy/"},{"categories":null,"content":"4.2 测试自动构建发布 代码更新后自动构建并发布 ","date":"2023-06-12","objectID":"/posts/cicd/jenkins-autodeploy/:4:2","tags":["CI/CD","持续集成"],"title":"Jenkins实践-自动构建并发布前端项目","uri":"/posts/cicd/jenkins-autodeploy/"},{"categories":["小记"],"content":"前情回顾： 开发在Jenkins发版出现问题 然后更新了一下证书时间 Jenkins 还是无法更新，这次是timeout 于是查看helm 执行历史，还是timed out ","date":"2023-05-16","objectID":"/posts/notes/k8s-certificate-missery/:1:0","tags":["Kubernetes"],"title":"K8S集群证书到期问题","uri":"/posts/notes/k8s-certificate-missery/"},{"categories":["小记"],"content":"思路一 重新创建一个sa 账号 apiVersion: v1\rkind: ServiceAccount\rmetadata:\rname: rmxc-jenkinss\rnamespace: rmxc-prod\r# 授权namespace rmxc-dev给rmxc-jenkins\r---\rapiVersion: rbac.authorization.k8s.io/v1\rkind: RoleBinding\rmetadata:\rname: rmxc-jenkins-rolebinding\rnamespace: rmxc-dev\rroleRef:\rapiGroup: rbac.authorization.k8s.io\rkind: ClusterRole\rname: edit\rsubjects:\r- kind: ServiceAccount\rname: rmxc-jenkins\rnamespace: rmxc-prod获取token 填写到 context中 ServiceAccount 的Secrets 是空的 🤮 1.20（含1.20）之前的版本，在创建sa时会自动创建一个secret，然后这个会把这个secret通过投射卷挂载到pod里。 1.21~1.23这几个版本，在创建sa时也会自动创建secret，但是在pod里并不会使用secret里的token，而是由kubelet到token Request api去申请一个token。 1.24版本，在创建sa时不再自动创建secret了，是由kubelet 到 tokenRequest api去申请token。 在1.21到1.23的版本里，当我们创建了一个service account之后，系统也会自动创建一个secret。 这个secret里会有一个永不过期的token。但是在pod里并不会使用secret的这个token。 可是当前集群环境是 v1.21.1 应该自动创建才对,然而并没有自动创建🤮 [root@master-01 kubernetes]# realkubeadm version\rkubeadm version: \u0026version.Info{Major:\"1\", Minor:\"21\", GitVersion:\"v1.21.1\", GitCommit:\"5e58841cce77d4bc13713ad2b91fa0d961e69192\", GitTreeState:\"clean\", BuildDate:\"2021-05-12T14:17:27Z\", GoVersion:\"go1.16.4\", Compiler:\"gc\", Platform:\"linux/amd64\"} 1.24.x 手动创建secret方法： 让k8s帮我们填好永不过期token apiVersion: v1 kind: Secret metadata: name: secret-sa-sample annotations: kubernetes.io/service-account.name: \"sa-name\" type: kubernetes.io/service-account-token","date":"2023-05-16","objectID":"/posts/notes/k8s-certificate-missery/:2:0","tags":["Kubernetes"],"title":"K8S集群证书到期问题","uri":"/posts/notes/k8s-certificate-missery/"},{"categories":["小记"],"content":"思路二 此时已经感觉K8S集群有点不对劲，跟被什么东西卡住了一样🙃,开始排查四大金刚 这时发现 kube-controller-manager 2023-05-15T06:05:01.346Z | W0515 06:05:01.346004 1 garbagecollector.go:703] failed to discover some groups: map[metrics.k8s.io/v1beta1:the server is currently unable to handle the request] 2023-05-15T06:05:30.134Z | E0515 06:05:30.134484 1 resource_quota_controller.go:409] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: the server is currently unable to handle the request 2023-05-15T06:05:31.374Z | W0515 06:05:31.373915 1 garbagecollector.go:703] failed to discover some groups: map[metrics.k8s.io/v1beta1:the server is currently unable to handle the request] 2023-05-15T06:06:00.149Z | E0515 06:06:00.148786 1 resource_quota_controller.go:409] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: the server is currently unable to handle the request 2023-05-15T06:06:01.406Z | W0515 06:06:01.406088 1 garbagecollector.go:703] failed to discover some groups: map[metrics.k8s.io/v1beta1:the server is currently unable to handle the request] 2023-05-15T06:06:30.163Z | E0515 06:06:30.162904 1 resource_quota_controller.go:409] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: the server is currently unable to handle the request 2023-05-15T06:06:31.435Z | W0515 06:06:31.435710 1 garbagecollector.go:703] failed to discover some groups: map[metrics.k8s.io/v1beta1:the server is currently unable to handle the request] 2023-05-15T06:07:00.233Z | E0515 06:07:00.233046 1 resource_quota_controller.go:409] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: the server is currently unable to handle the requestkube-scheduler 2023-05-15T05:47:31.747Z | E0515 05:47:31.746995 1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1beta1.CSIStorageCapacity: failed to list *v1beta1.CSIStorageCapacity: Unauthorized 2023-05-15T05:47:37.761Z | E0515 05:47:37.761643 1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Node: failed to list *v1.Node: Unauthorized 2023-05-15T05:47:38.024Z | E0515 05:47:38.024302 1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: Unauthorized 2023-05-15T05:47:41.673Z | E0515 05:47:41.673742 1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Pod: failed to list *v1.Pod: Unauthorized 2023-05-15T05:47:44.933Z | E0515 05:47:44.933850 1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Unauthorized 2023-05-15T05:47:46.148Z | E0515 05:47:46.148038 1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: Unauthorized 2023-05-15T05:47:51.624Z | E0515 05:47:51.624779 1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: Unauthorized 2023-05-15T05:47:55.284Z | E0515 05:47:55.284566 1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: Unauthorized 2023-05-15T05:47:55.736Z | E0515 05:47:55.736620 1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: failed to list *v1.Service: Unauthorized 2023-05-15T05:47:56.893Z | E0515 05:47:56.893650 1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Unauthorizedkube-apiserver 2023-05-15T05:20:00.977Z | E0515 05:20:00.976949 1 authentication.go:63] \"Unable to authenticate the request\" err=\"[x509: certificate has expired or is not yet valid: current time 2023-05-15T05:20:00Z is after 2023-05-14T05:49:22Z, verifying certificate SN=7385935559219004232, SKID=, AKID=9A:A4:90:20:28:E3:8C:F6:35:07:07:0F:17:AA:73:39:B5:8","date":"2023-05-16","objectID":"/posts/notes/k8s-certificate-missery/:3:0","tags":["Kubernetes"],"title":"K8S集群证书到期问题","uri":"/posts/notes/k8s-certificate-missery/"},{"categories":["小记"],"content":"解决方法 经过以上日志分析应该是证书更新成功了，但是服务组件开发没重启到位。 干脆在来一遍。 ","date":"2023-05-16","objectID":"/posts/notes/k8s-certificate-missery/:4:0","tags":["Kubernetes"],"title":"K8S集群证书到期问题","uri":"/posts/notes/k8s-certificate-missery/"},{"categories":["小记"],"content":"一、检查证书有效期 1.通过 kubeadm 安装的 Kubernetes 集群的证书有效期为 1 年，可以使用相关命令查看证书的有效期： [root@master-01 kubernetes]# realkubeadm certs check-expiration [check-expiration] Reading configuration from the cluster... [check-expiration] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml' CERTIFICATE EXPIRES RESIDUAL TIME CERTIFICATE AUTHORITY EXTERNALLY MANAGED admin.conf May 14, 2024 04:15 UTC 364d no apiserver May 14, 2024 03:18 UTC 364d ca no apiserver-etcd-client May 14, 2024 03:18 UTC 364d etcd-ca no apiserver-kubelet-client May 14, 2024 03:18 UTC 364d ca no controller-manager.conf May 14, 2024 04:15 UTC 364d no etcd-healthcheck-client May 14, 2024 03:18 UTC 364d etcd-ca no etcd-peer May 14, 2024 03:18 UTC 364d etcd-ca no etcd-server May 14, 2024 03:18 UTC 364d etcd-ca no front-proxy-client May 14, 2024 03:18 UTC 364d front-proxy-ca no scheduler.conf May 14, 2024 04:15 UTC 364d no CERTIFICATE AUTHORITY EXPIRES RESIDUAL TIME EXTERNALLY MANAGED ca May 28, 2031 15:52 UTC 8y no etcd-ca May 28, 2031 15:52 UTC 8y no front-proxy-ca May 28, 2031 15:52 UTC 8y no 可以看到除了 ca 证书，其他证书的有效期都是一年。如果证书到期，则整个集群都会挂掉。 解决这个问题的办法一般有两种： 如果还没有安装集群，可以通过修改源码设置证书有效期。 如果集群已经运行，可以通过重新签发有效期更长的证书。 ","date":"2023-05-16","objectID":"/posts/notes/k8s-certificate-missery/:4:1","tags":["Kubernetes"],"title":"K8S集群证书到期问题","uri":"/posts/notes/k8s-certificate-missery/"},{"categories":["小记"],"content":"二、更新证书 为了更新的安全性，更新之前可以将所有 Master 节点的配置目录做一个备份： cp -r /etc/kubernetes /etc/kubernetes_$(date +%F) cp -r /var/lib/etcd /var/lib/etcd_$(date +%F)通过执行证书更新命令查看： kubeadm certs renew --help可以看到证书更新是支持更新指定服务的证书，也可以更新单个服务的证书，但都是集群服务的证书。 # 所有 Master 节点更新所有证书\rkubeadm certs renew all\rsystemctl restart kubelet systremctl restart containerd 可以看到提示让重启 kube-apiserver, kube-controller-manager, kube-scheduler 和 etcd 服务证书才能生效。 # 重启组件 for i in $(kubectl get pods -A | grep -E \"etcd|kube-apiserver|kube-controller-manager|kube-scheduler\" | awk '{print $2}');do kubectl delete pod $i -n kube-system sleep 3 done此时查看证书已经是新的了，也可以通过命令查看： echo | openssl s_client -showcerts -connect 127.0.0.1:6443 -servername api 2\u003e/dev/null | openssl x509 -noout -enddate同时，由于在初始化 Master 集群的时候采用的是设置环境变量 export KUBECONFIG=/etc/kubernetes/admin.conf 的方法，不需要再更新该文件。如果不是该方法，还需要使用新的 admin.conf 替换掉复制的 /root/.kube/config 配置文件。 重启containerd 运行镜像 crictl stop 9731cb9e5b723\rcrictl stop 977896873866e\rcrictl stop 24430601db1d1\rcrictl stop 7a7bad1c7dd70重启后,查看相关日志 服务正常了 token也出来了 Jenkins 构建也成功了 😆 ","date":"2023-05-16","objectID":"/posts/notes/k8s-certificate-missery/:4:2","tags":["Kubernetes"],"title":"K8S集群证书到期问题","uri":"/posts/notes/k8s-certificate-missery/"},{"categories":["小记"],"content":"一、安装 Oh My Posh for PowerShell winget install JanDeDobbeleer.OhMyPosh -s winget","date":"2023-05-16","objectID":"/posts/notes/powerline/:0:1","tags":["Terminal"],"title":"Windows 11 Terminal 安装PowerLine教程","uri":"/posts/notes/powerline/"},{"categories":["小记"],"content":"二、选择主题 主题列表：https://ohmyposh.dev/docs/themes 选择主题，并使用此命令更新 PowerShell 配置文件。 （可以将 notepad 替换为你选择的文本编辑器。） notepad $PROFILE将以下项添加到 PowerShell 配置文件的末尾，以设置 cloud-native-azure 主题。 （如果想替换将 cloud-native-azure 替换为你选择的主题。） oh-my-posh init pwsh --config \"$env:POSH_THEMES_PATH\\cloud-native-azure.omp.json\" | Invoke-Expression","date":"2023-05-16","objectID":"/posts/notes/powerline/:0:2","tags":["Terminal"],"title":"Windows 11 Terminal 安装PowerLine教程","uri":"/posts/notes/powerline/"},{"categories":["小记"],"content":"三、安装 Nerd Font 如果你的字体不包含相应字形，则在整个提示符中，你可能会看到若干 Unicode 替换字符“▯”。 若要在终端中查看所有字形，建议安装 Nerd Font 官方推荐字体：MesloLGM NF #安装字体 oh-my-posh font install 结果老是timeout 在字体库直接下载 https://www.nerdfonts.com/font-downloads #直接导入字体 oh-my-posh font install .\\Meslo.zip 修改配置文件使用 \"profiles\": { \"defaults\": { \"font\": { \"face\": \"MesloLGM NF\" }, },找不到所选字体 MesloLGM NF 🤮 找不到？ 字体名称是 MesloLGM Nerd Font 替换一下 ？ \"profiles\": {\r\"defaults\": {\r\"font\":\r{\r\"face\": \"MesloLGM Nerd Font\" },\r},可以看到已经正常显示了 ","date":"2023-05-16","objectID":"/posts/notes/powerline/:0:3","tags":["Terminal"],"title":"Windows 11 Terminal 安装PowerLine教程","uri":"/posts/notes/powerline/"},{"categories":["小记"],"content":"四、Oh My Posh 与 Clink 集成 Windows CMD 没有开箱即用的支持。然而，有一种方法可以使用Clink来实现，同时可以增强您的 cmd 体验。按照安装说明进行操作，并确保选择自动启动。 将 Oh My Posh 与 Clink 集成很容易：在 Clink 脚本目录中创建一个名为 oh-my-posh.lua 的新文件（在clink infocmd 中运行以找到该文件的位置）。 load(io.popen('oh-my-posh init cmd'):read(\"*a\"))()添加后，重新启动 cmd 以使更改生效。 ","date":"2023-05-16","objectID":"/posts/notes/powerline/:0:4","tags":["Terminal"],"title":"Windows 11 Terminal 安装PowerLine教程","uri":"/posts/notes/powerline/"},{"categories":["小记"],"content":"安装 Jenkins 遇到的问题 ","date":"2023-05-16","objectID":"/posts/notes/jenkins-error/:1:0","tags":["Jenkins"],"title":"安装 Jenkins 遇到的问题","uri":"/posts/notes/jenkins-error/"},{"categories":["小记"],"content":"问题一 root@server:/apps# systemctl restart jenkins.service Job for jenkins.service failed because the control process exited with error code. See \"systemctl status jenkins.service\" and \"journalctl -xe\" for details. root@server:/apps# root@server:/apps# systemctl status jenkins.service ● jenkins.service - Jenkins Continuous Integration Server Loaded: loaded (/lib/systemd/system/jenkins.service; enabled; vendor preset: enabled) Active: failed (Result: exit-code) since Tue 2023-05-16 16:36:43 CST; 5s ago Process: 168567 ExecStart=/usr/bin/jenkins (code=exited, status=1/FAILURE) Main PID: 168567 (code=exited, status=1/FAILURE) May 16 16:36:43 server systemd[1]: jenkins.service: Scheduled restart job, restart counter is at 5. May 16 16:36:43 server systemd[1]: Stopped Jenkins Continuous Integration Server. May 16 16:36:43 server systemd[1]: jenkins.service: Start request repeated too quickly. May 16 16:36:43 server systemd[1]: jenkins.service: Failed with result 'exit-code'. May 16 16:36:43 server systemd[1]: Failed to start Jenkins Continuous Integration Server.网上查询解决方法 修改/lib/systemd/system/jenkins.service # The Java home directory. When left empty, JENKINS_JAVA_CMD and PATH are consulted. Environment=\"JAVA_HOME=/usr/local/jdk/\" systemctl daemon-reload 修改后仍然Failed to start Jenkins Continuous Integration Server 经过在官网查询 2.361.1 以后的版本需要 java 11 或 java 17 ，java 8 无法正常使用。 更换java 17 root@server:/apps# rm /usr/local/jdk root@server:/apps# rm /usr/bin/java root@server:/apps# ln -sv /apps/jdk-17.0.6/ /usr/local/jdk '/usr/local/jdk' -\u003e '/apps/jdk-17.0.6/' root@server:/apps# ln -sv /apps/jdk-17.0.6/bin/java /usr/bin/java '/usr/bin/java' -\u003e '/apps/jdk-17.0.6/bin/java' #查看版本 java version \"17.0.6\" 2023-01-17 LTS Java(TM) SE Runtime Environment (build 17.0.6+9-LTS-190) Java HotSpot(TM) 64-Bit Server VM (build 17.0.6+9-LTS-190, mixed mode, sharing)","date":"2023-05-16","objectID":"/posts/notes/jenkins-error/:2:0","tags":["Jenkins"],"title":"安装 Jenkins 遇到的问题","uri":"/posts/notes/jenkins-error/"},{"categories":["小记"],"content":"问题二： 启动 直接卡住半天 🤣 root@server:/apps# systemctl start jenkins.service\r^C 查看日志 也看不出来啥 root@server:/apps# journalctl -xe May 16 16:53:44 server jenkins[300962]: at java.desktop/sun.font.SunFontManager$2.run(SunFontManager.java:358) May 16 16:53:44 server jenkins[300962]: at java.desktop/sun.font.SunFontManager$2.run(SunFontManager.java:315) May 16 16:53:44 server jenkins[300962]: at java.base/java.security.AccessController.doPrivileged(AccessController.java:318) May 16 16:53:44 server jenkins[300962]: at java.desktop/sun.font.SunFontManager.\u003cinit\u003e(SunFontManager.java:315) May 16 16:53:44 server jenkins[300962]: at java.desktop/sun.awt.FcFontManager.\u003cinit\u003e(FcFontManager.java:35) May 16 16:53:44 server jenkins[300962]: at java.desktop/sun.awt.X11FontManager.\u003cinit\u003e(X11FontManager.java:56) May 16 16:53:44 server jenkins[300962]: Caused: java.lang.reflect.InvocationTargetException May 16 16:53:44 server jenkins[300962]: at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) May 16 16:53:44 server jenkins[300962]: at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77) May 16 16:53:44 server jenkins[300962]: at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) May 16 16:53:44 server jenkins[300962]: at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499) May 16 16:53:44 server jenkins[300962]: at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480) May 16 16:53:44 server jenkins[300962]: at java.desktop/sun.font.FontManagerFactory$1.run(FontManagerFactory.java:85) May 16 16:53:44 server jenkins[300962]: Caused: java.lang.InternalError May 16 16:53:44 server jenkins[300962]: at java.desktop/sun.font.FontManagerFactory$1.run(FontManagerFactory.java:87) May 16 16:53:44 server jenkins[300962]: at java.base/java.security.AccessController.doPrivileged(AccessController.java:318) May 16 16:53:44 server jenkins[300962]: at java.desktop/sun.font.FontManagerFactory.getInstance(FontManagerFactory.java:75) May 16 16:53:44 server jenkins[300962]: at java.desktop/java.awt.Font.getFont2D(Font.java:526) May 16 16:53:44 server jenkins[300962]: at java.desktop/java.awt.Font.getFamily(Font.java:1436) May 16 16:53:44 server jenkins[300962]: at java.desktop/java.awt.Font.getFamily_NoClientCode(Font.java:1410) May 16 16:53:44 server jenkins[300962]: at java.desktop/java.awt.Font.getFamily(Font.java:1402) May 16 16:53:44 server jenkins[300962]: at java.desktop/java.awt.Font.toString(Font.java:1895) May 16 16:53:44 server jenkins[300962]: at hudson.util.ChartUtil.\u003cclinit\u003e(ChartUtil.java:270) May 16 16:53:44 server jenkins[300962]: at hudson.WebAppMain.contextInitialized(WebAppMain.java:217) May 16 16:53:44 server jenkins[300962]: Caused: hudson.util.AWTProblem May 16 16:53:44 server jenkins[300962]: at hudson.WebAppMain.contextInitialized(WebAppMain.java:218) May 16 16:53:44 server jenkins[300962]: at org.eclipse.jetty.server.handler.ContextHandler.callContextInitialized(ContextHandler.java:1048) May 16 16:53:44 server jenkins[300962]: at org.eclipse.jetty.servlet.ServletContextHandler.callContextInitialized(ServletContextHandler.java:624) May 16 16:53:44 server jenkins[300962]: at org.eclipse.jetty.server.handler.ContextHandler.contextInitialized(ContextHandler.java:983) May 16 16:53:44 server jenkins[300962]: at org.eclipse.jetty.servlet.ServletHandler.initialize(ServletHandler.java:740) May 16 16:53:44 server jenkins[300962]: at org.eclipse.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:392) May 16 16:53:44 server jenkins[300962]: at org.eclipse.jetty.webapp.WebAppContext.startContext(WebAppContext.java:1304) May 16 16:53:44 server jenkins[300962]: at org.eclipse.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:900) May 16 16:53:44 server jenkins[300962]: at org.eclipse.jetty.servlet.ServletContextHan","date":"2023-05-16","objectID":"/posts/notes/jenkins-error/:3:0","tags":["Jenkins"],"title":"安装 Jenkins 遇到的问题","uri":"/posts/notes/jenkins-error/"},{"categories":["小记"],"content":"问题三： Jenkins 一直卡在 Please wait while Jenkins is getting ready to work … find / -name *.UpdateCenter.xml /var/lib/jenkins/hudson.model.UpdateCenter.xml find: ‘/proc/1834928’: No such file or directory find: ‘/proc/1834940’: No such file or directory find: ‘/proc/1834942’: No such file or directoryhttps://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.json 该url是国内的清华大学的镜像地址（建议使用清华大学的镜像服务器，修改后刷新页面即可. vim /var/lib/jenkins/hudson.model.UpdateCenter.xml \u003c?xml version='1.1' encoding='UTF-8'?\u003e \u003csites\u003e \u003csite\u003e \u003cid\u003edefault\u003c/id\u003e \u003curl\u003ehttps://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.json\u003c/url\u003e \u003c/site\u003e \u003c/sites\u003e","date":"2023-05-16","objectID":"/posts/notes/jenkins-error/:4:0","tags":["Jenkins"],"title":"安装 Jenkins 遇到的问题","uri":"/posts/notes/jenkins-error/"},{"categories":["Kubernetes"],"content":"1.为什么需要容器编排系统？ Docker在管理单个容器时表现出色，对于一些由有限几个或十几个容器构建的应用程序来说，直接在Docker引擎上自主运行、部署和管理是相对容易的。然而，当涉及到企业级应用程序，其中包含数百甚至上千个容器时，仅依赖Docker引擎来进行管理将变得异常复杂，甚至难以实现。 容器编排是一套自动化管理容器应用的流程，包括部署、管理、扩展和联网等各种操作，旨在应对大规模容器集群的挑战。它能够自动化许多任务，例如容器的调度和部署、资源的分配、应用规模的弹性扩缩、在主机故障或资源不足时的容器迁移、负载均衡，以及对容器和主机运行状况的监控等。 对于企业级应用程序，其中容器数量庞大、复杂性高，容器编排成为不可或缺的工具。通过容器编排平台，如Kubernetes，可以实现高度自动化的容器管理和协调，无论是在多台主机上还是跨多个数据中心中。容器编排能够实现复杂的任务调度、负载均衡、故障恢复，同时提供灵活的扩展性和资源管理。 因此，容器编排在处理大规模容器集群时变得至关重要。它提供了一种结构化的方式来管理大量容器，使得企业能够更高效、可靠地运行复杂的应用程序，从而克服了仅仅依赖Docker引擎进行管理所面临的挑战。 2.Kubernetes 集群概述 Kubernetes 是一个跨多主机的容器编排平台，它使用共享网络将多个主机构建成统一的集群，其中master作为控制中心负载整个集群系统，余下的主机运行为worker节点，这些工作节点使用本地和外部资源接收请求并以pod形式运行工作负载。 Master节点\rMaster节点：Master节点是Kubernetes集群的控制中心，负责管理和监控整个集群的状态和操作。它包括以下核心组件： API Server（API服务器）：提供了集群内部的API，用于与Kubernetes进行通信，提交和管理操作请求。 Controller Manager（控制器管理器）：负责监控集群状态，并根据期望状态对集群进行调整和控制。 Scheduler（调度器）：负责将新的Pod分配到Worker节点上，以实现负载均衡和资源利用的最佳化。 etcd（分布式键值存储）：用于保存集群的配置数据和状态信息。 Worker节点\rWorker节点：Worker节点是集群中实际运行容器的地方，它们接收Master节点的指令并执行相关操作。每个Worker节点包括以下组件： Kubelet（节点代理）：负责管理节点上的容器，确保Pod按照预期状态运行。 Kube Proxy（代理）：负责维护网络规则，实现Pod之间和外部的网络通信。 Container Runtime（容器运行时）：负责在节点上运行容器，常用的包括Docker、containerd等。 Pod：Pod是Kubernetes中的最小部署单元，它可以包含一个或多个容器，并共享相同的网络和存储空间。Pod作为部署、扩展和管理的基本单位，可以容纳应用及其依赖，并提供了更好的资源隔离和灵活性。 3.Kubernetes API对象 Kubernetes的RESTful API以资源的形式抽象了多种概念来描述应用程序及其周边组件。这些抽象出的程序和组件被统称为API对象，它们都有特定的类型和属性，每个API对象都使用”名称“作为唯一标识符。 对象类型\r以下是一些常见的Kubernetes API对象类型和它们的一些属性： Pod（Pod）：Pod是最小的可部署单元，可以包含一个或多个容器，它们共享网络和存储资源。Pod通常用于封装紧密耦合的应用组件。 ReplicaSet（副本集）：ReplicaSet确保指定数量的Pod副本在集群中运行。它适用于应用部署和扩展。 Deployment（部署）：Deployment建立在ReplicaSet之上，提供了声明式的方式来管理Pod副本的创建和更新。它用于应用的滚动更新和版本控制。 Service（服务）：Service定义了一组Pod的网络访问方式，为Pod提供了稳定的IP地址和DNS名称。它用于内部或外部网络访问。 Ingress（入口）：Ingress允许从集群外部访问Service，提供了HTTP和HTTPS路由的规则。 ConfigMap（配置映射）：ConfigMap用于将配置数据从应用代码中分离出来，使配置的管理更加灵活。 Secret（密钥）：Secret用于存储敏感信息，如密码、API密钥等，以安全地传递给Pod。 Namespace（命名空间）：Namespace用于将集群分割为多个虚拟集群，每个命名空间中的对象相互隔离，有助于多租户的管理。 StatefulSet（有状态副本集）：类似于ReplicaSet，但用于有状态应用，可以为每个Pod分配稳定的网络标识和存储。 DaemonSet（守护进程集）：DaemonSet确保每个节点上都运行一个Pod副本，适用于在每个节点上运行特定任务的情况。 Job（任务）：Job用于运行一次性的任务，确保任务成功完成，例如批量处理任务。 Kubernetes使用命名空间（Namespace）为资源提供了作用域，并将大多数资源类型划分到命名空间级别。命名空间可以看作是一个虚拟的集群，用于在物理集群内部划分不同的逻辑工作单元，从而将不同的资源隔离开来。这对于多租户环境、多个项目的隔离和资源管理非常有用。 在Kubernetes中，每个资源对象都属于一个特定的命名空间。一些资源默认在\"default\"命名空间中，但也可以创建自定义的命名空间，将资源放置在其中。 通过使用命名空间，Kubernetes可以实现资源的逻辑隔离，从而不同项目、团队或应用可以在同一个集群中运行，而彼此之间不会干扰。同时，命名空间还有助于对资源进行分类、管理和监控。需要注意的是，不是所有的资源都支持命名空间，一些核心资源如Nodes和PersistentVolumes并不属于命名空间。 4.Kubernetes 配置清单 Manifest 在Kubernetes中，应用程序的部署和管理需要通过配置清单（也称为资源清单或资源配置）来指定所需的状态，并将其提交给Kubernetes的API服务器。这些配置清单描述了要创建的资源对象的属性和规格，包括元数据、期望状态以及观察状态等信息。 配置清单是一个包含资源对象定义的文件，通常采用JSON或YAML格式编码。这些清单文件指定了要部署的应用程序、服务、副本集等的特性和配置。 当你想在Kubernetes中运行应用程序，那么就需要创建一个配置清单文件，其中描述了应用程序的属性和规格，比如名称、镜像、端口等。这个清单可以使用JSON或YAML格式编写。然后，需要将这个配置清单文件提交给Kubernetes的API服务器。 Kubernetes的API服务器会接收并存储这个配置清单，然后根据清单中的信息，在集群中创建相应的资源，如Pod、服务等。API服务器会确保描述的期望状态（比如运行3个副本）与观察状态（实际运行的副本数）保持一致。 提交配置清单通常使用HTTP/HTTPS协议与Kubernetes的API服务器通信。API服务器会验证和处理请求，并将清单中定义的资源创建或更新到集群中。Kubernetes API服务器通过HTTP GET请求查询资源对象的状态，通常以JSON格式进行序列化。同时，Kubernetes支持更高效的protobuf格式来减少数据传输和解析开销。 Kubernetes提供了一种声明式的方法来管理应用程序和资源对象，使得部署和管理变得更加可靠、可重复和可自动化。 ","date":"2023-04-24","objectID":"/posts/kubernetes/primary/1.kubernetes%E5%9F%BA%E7%A1%80/:0:0","tags":["k8s进阶训练营"],"title":"Kubernetes 基础","uri":"/posts/kubernetes/primary/1.kubernetes%E5%9F%BA%E7%A1%80/"},{"categories":["Kubernetes"],"content":"1.kubeadm 部署工具 kubeadm 是kubernetes 集群全生命周期管理工具，可用于实现集群的部署、升级/降级及卸载等。 kubeadm 的核心工具是 kubeadm init 和 kubeadm join ，前者用于创建新的控制平面节点，后者则用于将节点快速连接到指定的控制平面。 kubeadm init\rkubeadm init: 这个命令用于初始化一个新的 Kubernetes 集群的控制平面节点。 在运行这个命令后，它将自动生成一个初始的控制平面配置，创建必要的证书和密钥，以及启动必要的核心组件，如 etcd、API Server、Controller Manager 和 Scheduler。 kubeadm join: 这个命令用于将工作节点快速连接到已经初始化的 Kubernetes 控制平面。 当你运行 kubeadm init 后，它会生成一个令牌 (token) 和一个哈希 (hash)，你可以将它们用于通过 kubeadm join 命令将其他节点加入到集群中。 这些节点将会成为集群的工作节点，与控制平面节点协同工作。 Kubernetes 中的令牌 (token) 在集群中用于节点加入过程的身份认证和安全通信。这些令牌由 kubeadm 或管理员生成，并用于在新节点首次联系 Kubernetes API Server 时进行身份验证。 Kubernetes Token\rKubernetes 令牌是集群加入过程中的关键组成部分，它们帮助确保新节点的身份得到验证，并确保集群的安全性。 加入令牌 (Join Token): 在使用 kubeadm init 初始化控制平面节点后，它会生成一个加入令牌 (Join Token)。这个令牌包括一个令牌字符串和一个哈希值。新节点需要提供这个令牌和哈希值，以便加入到集群中。通过运行 kubeadm join 命令并提供正确的令牌，新节点可以成功加入集群。 预共享密钥 (Pre-shared Key): 加入令牌的哈希值是通过使用预共享密钥计算生成的。这个密钥存储在控制平面节点上，新节点通过令牌中的哈希值和控制平面节点上的密钥进行匹配，从而实现身份验证。这确保了只有具有有效令牌和正确密钥的节点才能加入集群。 有效期: 令牌通常具有一定的有效期，过期后将无法使用。这是为了安全考虑，以确保令牌不会永久存在，从而降低潜在的风险。 令牌轮换: 在一些情况下，管理员可能需要轮换令牌，以提高集群的安全性。kubeadm 提供了命令来生成新的令牌并轮换现有令牌。 其他几个可用的工具包括： kubeadm config: 这个命令用于生成和修改 kubeadm 的配置文件，这些配置文件包括初始化集群和加入节点时使用的配置。通过 kubeadm config 命令，你可以生成初始化或加入节点所需的配置文件，然后对其进行自定义修改。这使得在不同的部署环境中更容易地配置 Kubernetes 集群。 例如，你可以使用 kubeadm config print init-defaults 命令来生成初始化集群所需的默认配置文件，并在需要时进行修改。 kubeadm upgrade: 这个命令用于执行 Kubernetes 集群的升级操作。Kubernetes 的升级通常涉及到控制平面组件 (Control Plane) 和工作节点 (Worker Node) 的升级。kubeadm upgrade 命令提供了一种升级路径，可用于在不中断生产工作负载的情况下将集群升级到新版本。 升级过程通常包括以下步骤： 升级控制平面组件。 升级 kubeadm 工具本身。 升级工作节点。 kubeadm upgrade 命令可用于管理这些升级步骤，使升级过程更加平滑。 kubeadm reset: 这个命令用于将节点恢复到初始状态，即将节点上的 Kubernetes 组件和配置删除，从而卸载 Kubernetes。这在需要重新部署或卸载节点时非常有用。运行 kubeadm reset 将清除节点上的所有 Kubernetes 配置和数据。 请注意，kubeadm reset 只用于卸载 Kubernetes，并不会卸载容器运行时、操作系统组件或其他可能在节点上存在的软件。 ","date":"2023-02-24","objectID":"/posts/kubernetes/primary/1.%E5%88%A9%E7%94%A8-kubeadm%E9%83%A8%E7%BD%B2kubernetes%E9%9B%86%E7%BE%A4/:1:0","tags":["kubeadm"],"title":"利用kubeadm部署kubernetes集群","uri":"/posts/kubernetes/primary/1.%E5%88%A9%E7%94%A8-kubeadm%E9%83%A8%E7%BD%B2kubernetes%E9%9B%86%E7%BE%A4/"},{"categories":["Kubernetes"],"content":"2.集群组件运行模式 Kubernetes 集群可部署为三种部署模式或运行方式。 独立组件模式：Master各组件和Node各组件直接以守护进程方式运行于节点之上，以二进制程序部署的集群隶属于此种模式。 静态pod模式：控制平面的各组件以静态Pod对象形式运行在Master主机之上，而Node主机上的kubelet 和Docker 运行为系统级守护进程，Kube-proxy托管于集群上的DaemonSet控制器。 静态 Pod\r以下是静态 Pod 的一些关键特点和用途： 由 kubelet 管理：静态 Pod 是由 kubelet 直接管理的，而不是通过 Kubernetes API Server 和控制器来管理。kubelet 会定期检查节点上特定目录中的静态 Pod 配置文件，并启动/停止相应的容器来维护这些 Pod。 节点级别的应用程序：静态 Pod 通常用于运行与节点相关的应用程序，例如容器化的监控代理、网络插件、日志收集器等。这些应用程序对整个节点的性能和资源利用具有重要影响，因此它们通常以静态 Pod 的方式运行。 配置文件位置：静态 Pod 的配置文件通常存储在节点上的 /etc/kubernetes/manifests 目录下。kubelet 会监视该目录并启动 Pod。每个配置文件对应一个静态 Pod。 不受控制器管理：与常规 Pod 不同，静态 Pod 不受 ReplicationController、Deployment 或其他控制器的管理。这意味着它们不会受到控制器的自动伸缩或滚动升级的影响。 用途示例：一些常见的静态 Pod 用途包括运行网络插件（如 Flannel、Calico）、容器运行时（如 Docker 或 Containerd）、kube-proxy（用于服务代理）等。 自托管 self-hosted模式：类型于第二种模式，但控制平面的各组件运行为Pod对象非静态，并且这些Pod 对象同样托管运行在集群之上，同样受控于DaemonSet类型控制器。 使用Kubeadm部署Kubernetes 集群可运行为第二种或第三种模式，默认为静态Pod 对象模式，当需要使用自托管模式时，可使用Kubeadm init 命令的 --features-gates=selfHosting 选项激活。 --feature-gates=selfHosting 是Kubernetes 中的一个特性门标志（Feature Gate），用于启用或禁用特定的实验性或高级功能。特性门标志允许用户在 Kubernetes 中启用或禁用不同的功能，并且这些功能可能还没有被广泛测试或稳定。 ","date":"2023-02-24","objectID":"/posts/kubernetes/primary/1.%E5%88%A9%E7%94%A8-kubeadm%E9%83%A8%E7%BD%B2kubernetes%E9%9B%86%E7%BE%A4/:2:0","tags":["kubeadm"],"title":"利用kubeadm部署kubernetes集群","uri":"/posts/kubernetes/primary/1.%E5%88%A9%E7%94%A8-kubeadm%E9%83%A8%E7%BD%B2kubernetes%E9%9B%86%E7%BE%A4/"},{"categories":["Kubernetes"],"content":"3.kubeadm inti 工作流程 kubeadm 快速部署一个新的 Kubernetes 集群通常需要两个主要步骤: 在Master节点上运行Kubeadm join命令初始化控制平面,在其他节点上运行kubeadm join 命令逐一加入控制平面，进而成为集群成员。 kubeadm init 命令在内部分为多个阶段，每个阶段执行不同的任务，以确保集群的顺利初始化。 各阶段的简要说明如下： 阶段名称 阶段任务 preflight 初始化前的环境检查，在这个阶段，kubeadm 将检查主机上的一系列预定义条件，以确保它们满足 Kubernetes 的最低要求。这些条件包括操作系统、内核参数、容器运行时等。 kubelet-start 生成kubelet 配置，其中包含了与控制平面的通信信息，如 API Server 的地址和端口，以及与其它 kubelet 节点的通信信息。kubelet 配置文件通常存储在 /etc/kubernetes/ 目录下。 接下来启动或重启kubelet以便于静态Pod运行各服务组件。kubeadm 还会等待初始化 Token 的生成，初始化 Token 是用于后续节点加入集群的凭据。 certs 生成各种数字证书，用于确保集群通信的安全性和认证；1.**CA 证书（Certificate Authority Certificate）**CA 证书是整个证书链的根证书，用于签发其他数字证书。kubeadm 在初始化集群时会生成一个根 CA 证书，用于签发后续组件的证书。2.API Server 证书：API Server 证书用于加密和认证 Kubernetes 控制平面组件的通信。它通常由根 CA 证书签发，控制平面组件使用该证书来与客户端（如 kubectl、kubelet、应用程序）进行安全通信。3.Front Proxy 证书：Front Proxy 证书用于加密和认证前端代理（front proxy）的通信。在 Kubernetes 中，前端代理是一个反向代理，用于处理集群中的控制平面请求。4.etcd 证书：etcd 证书用于加密和认证 etcd 集群的通信。etcd 是 Kubernetes 集群的数据存储后端，用于存储集群状态和配置信息。kubeadm 在初始化集群时会生成这些证书，并将它们分发给各个组件，以便它们能够在安全的环境中运行。 kubeconfig kubeconfig 阶段用于生成与集群和控制平面组件相关的 kubeconfig 文件。1.kube-apiserver kubeconfig: 该文件包含 API Server 的地址和端口，以及与授权和身份验证相关的配置信息。2.kube-controller-manager kubeconfig：该文件包括控制器管理器的监听地址以及与 API Server 进行身份验证所需的信息。3.kube-scheduler kubeconfig： 该文件包括调度器的监听地址以及与 API Server 进行身份验证所需的信息。4.kubelet kubeconfig： 该文件包含 kubelet 与 API Server 通信所需的配置信息，包括 API Server 的地址和与之通信的证书和密钥。5.admin kubeconfig： 该文件包括 API Server 的地址和端口，以及用于身份验证的客户端证书和密钥。这些 kubeconfig 文件是控制平面组件和管理员连接到集群的关键。kubeadm 在 kubeconfig 阶段生成这些文件，并通常将它们存储在 /etc/kubernetes/ 目录中。管理员可以使用这些 kubeconfig 文件配置 kubectl 命令行工具，以便与集群进行通信和管理。 control-plane 该阶段会生成控制平面组件（kube-apiserver、kube-controller-manager 和 kube-scheduler）的静态 Pod 配置清单。这个配置清单包括了 kube-controller-manager 的命令行参数、监听地址、证书文件路径等信息，以及其他与控制器相关的配置。 etcd etcd 阶段用于生成本地 etcd 的静态 Pod 配置清单。这个配置清单用于将 etcd 作为一个静态 Pod 运行在控制平面节点上，负责存储集群的状态和配置信息。其中包括 etcd 的监听地址、证书和密钥文件路径、数据存储目录等信息。配置文件通常位于 /etc/kubernetes/manifests/ 目录下，命名为 etcd.yaml 或类似的文件名。一旦静态 Pod 清单被生成并存储在正确的位置，kubelet 将检测到它，并自动启动。etcd 静态 Pod 运行后，它会将集群的状态和配置信息存储在指定的数据存储目录中，通常默认情况下是 /var/lib/etcd。 upload-config 该阶段的关键目标是将 kubeadm 和 kubelet 配置信息以一种集中化的方式存储在集群中，以便各个组件和节点能够共享和使用这些配置。这些配置在集群初始化过程中至关重要，因为它们决定了控制平面组件和节点如何连接和交互。通常情况下，这些 ConfigMap 资源存储在 kube-system 命名空间中，并具有特定的命名约定，以便其他组件能够轻松找到和使用它们。kubeadm 会生成的 ConfigMap，中包含了与初始化集群相关的配置信息，如 API Server 的地址和端口、证书和密钥的路径、初始化 Token 等。kubelete 也会生成一个 ConfigMap，其中包含了与 kubelet 相关的配置信息，如 kubelet 的 kubeconfig 文件内容、Pod 网络的配置信息等。 upload-certs 该阶段是初始化 Kubernetes 集群过程中的一个关键步骤，它负责将与集群安全性相关的证书文件上传到集群中，并创建一个名为 kubeadm-certs 的 ConfigMap 资源对象来存储这些证书，证书通常存储在 kube-system 命名空间中，以确保集群中的其他组件和后续加入的其他Master节点可以访问这些证书，用于控制平面组件和节点之间的安全通信和身份验证。 mark-control-plane 该阶段的主要目的是将节点标记为控制平面节点，并确保节点具备运行控制平面组件的能力。标记的节点将被集群认可为 Master 节点，这个标记告诉 Kubernetes 集群，当前节点具有控制平面组件的角色，可以运行 API Server、Controller Manager、Scheduler 等控制平面组件。并为该节点设置node-role.kubernetes.io/master:NoSchedule 污点，防止其他工作负载Pod运行在当前节点上。kubeadm 会将控制平面节点的信息存储在 /etc/kubernetes/cloud-config 目录下的 kube-controller-manager 和 kube-scheduler 配置文件中，以便这些组件可以识别和管理控制平面节点。 bootstrap-token 该阶段用于生成引导令牌（bootstrap token），这些令牌允许新的节点加入 Kubernetes 集群的控制平面。引导令牌包含一些重要信息，如令牌的密钥、有效期限、用途等，它们允许新节点基于预共享的密钥与集群中的控制平面节点建立连接并获得身份认证。bootstrap-token 阶段的主要功能和操作：1.生成引导令牌：会生成引导令牌，这是一个用于授权新节点加入集群的令牌。令牌通常包括一个密钥和一些元数据，以便集群可以识别和验证该令牌。2.设置令牌有效期：引导令牌通常具有一个有效期，kubeadm 可以配置这个有效期，以限制令牌的使用时间。一旦令牌过期，它将不再被接受。3.指定令牌用途：引导令牌可以配置为特定用途，例如用于加入节点、重置集群等。不同的令牌用途可能具有不同的权限和限制4.保存令牌信息:kubeadm 会保存生成的令牌信息，通常在节点上的某个文件中，以便新节点可以访问这些信息。5.提供令牌给新节点: 新节点需要具有生成的引导令牌信息才能加入集群。这些信息通常包括令牌字符串、CA 证书等。 新节点可以使用这些信息来与集群中的控制平面节点建立连接，完成节点加入过程。 kubelet-finalize kubelet-finalize 阶段的目标是确保 kubelet 在初始化后能够正确配置，以便它可以安全地连接到集群的控制平面，并开始管理节点上的容器和 Pod。在 TLS 引导阶段，kubelet 被配置为使用安全的连接与集群的控制平面通信。而在 kubelet-finalize 阶段，进一步更新 kubelet 的配置，确保其与集群的其他部分协同工作。这个更新会包括与控制平面的安全连接所需的信息，如 API Server 的地址和证书信息。除了更新 kubeconfig 文件之外，kubelet 的启动参数也可能需要调整。这些参数通常包括 API Server 地址、证书文件路径、密钥文件路径等。在配置更新完成后，通常需要重启 kubelet 服务，以使新的配置生效。 addon 该阶段的主要任务是确保核心附加组件在集群中正确安装、配置和运行。这些组件对于集群的网络功能和 DNS 解析功能至关重要，它们允许 Pod 之间进行通信和发现，并提供了 Kubernetes 服务的负载均衡功能。安装和配置这些组件是 Kubernete","date":"2023-02-24","objectID":"/posts/kubernetes/primary/1.%E5%88%A9%E7%94%A8-kubeadm%E9%83%A8%E7%BD%B2kubernetes%E9%9B%86%E7%BE%A4/:3:0","tags":["kubeadm"],"title":"利用kubeadm部署kubernetes集群","uri":"/posts/kubernetes/primary/1.%E5%88%A9%E7%94%A8-kubeadm%E9%83%A8%E7%BD%B2kubernetes%E9%9B%86%E7%BE%A4/"},{"categories":["Kubernetes"],"content":"4.kubeadm join 工作流程 环境预检 kubeadm join 命令也需要进行环境预检操作，确定所在节点满足可加入集群中的前提条件。 这类关键检测的步骤包括： Kubernetes 版本匹配：kubeadm join 会检查所在节点上的 Kubernetes 组件版本是否与控制平面节点上的版本匹配。版本不匹配可能导致问题。 操作系统支持：kubeadm join 确保所在节点使用的操作系统是 Kubernetes 支持的操作系统之一。通常，Kubernetes 支持多种 Linux 发行版。 Docker 或容器运行时支持：kubeadm join 检查所在节点上是否安装了 Docker 或其他容器运行时，并验证其版本是否与 Kubernetes 兼容。 网络插件支持：如果您的集群使用了网络插件（如 Calico、Flannel 等），kubeadm join 确保已安装并配置了适当的网络插件。 授权检查：确保新节点有权加入集群。通常，kubeadm join 命令需要提供有效的加入令牌（Join Token）以进行身份验证。 网络互通性：kubeadm join 会验证新节点是否可以与集群中的控制平面节点建立网络连接。这包括检查 API Server 的可访问性。 证书和密钥检查：确保节点上存在必要的证书和密钥文件，以便安全地连接到集群。 操作系统设置检查：检查所在节点的操作系统设置，例如防火墙规则、SELinux 设置等，以确保它们不会阻止必要的通信。 同控制平面建立双向信任关系 与控制平面建立双向信任关系是新节点加入 Kubernetes 集群的关键一步。这一信任关系的建立通过证书和令牌进行身份验证，确保新节点和集群的控制平面能够互相信任和安全地通信。 双向信任建立的过程可以分为两个主要阶段：发现阶段和 TLS 引导阶段。 1.发现阶段： 在发现阶段，新节点请求加入集群，通常使用一个特定的令牌（共享令牌 Bootstrap Token）。这个令牌由集群管理员或初始化控制平面节点生成，新节点使用令牌向指定的API Server发送请求以获取集群信息。 2.TLS 引导阶段： Bootstrap Token 主要用于节点确定 API Server 的身份，以进行加入请求的授权。它确保了节点具有加入集群的权限，但在数据传输过程中，并没有提供数据的加密和验证数据真实性。为了确保数据在传输过程中的安全性，TLS 引导程序阶段起到了关键作用。 在 TLS 引导程序阶段，控制平面通过 TLS Bootstrap 机制为新节点签发数字证书，这个证书用于加密通信和验证数据的真实性。这确保了新节点与控制平面之间的通信是安全的，同时确保了数据不会被篡改。 TLS 引导程序的主要步骤包括： 证书签发：kubelet 通 TLS Bootstrap 使用共享令牌向API Server 进行身份验证后提交证书并签署请求（CSR）,随后控制平面自动签署该请求从而生成数字证书。 证书传输：控制平面节点通过安全的 TLS 连接将签发的证书传输给新节点。 配置新节点：新节点接收到证书后，将其保存在本地，并配置节点的 kubelet、kube-proxy 等组件，以使用这些证书与控制平面节点建立安全连接。 ","date":"2023-02-24","objectID":"/posts/kubernetes/primary/1.%E5%88%A9%E7%94%A8-kubeadm%E9%83%A8%E7%BD%B2kubernetes%E9%9B%86%E7%BE%A4/:4:0","tags":["kubeadm"],"title":"利用kubeadm部署kubernetes集群","uri":"/posts/kubernetes/primary/1.%E5%88%A9%E7%94%A8-kubeadm%E9%83%A8%E7%BD%B2kubernetes%E9%9B%86%E7%BE%A4/"},{"categories":["Kubernetes"],"content":"5.部署分布式Kubernetes 集群 ","date":"2023-02-24","objectID":"/posts/kubernetes/primary/1.%E5%88%A9%E7%94%A8-kubeadm%E9%83%A8%E7%BD%B2kubernetes%E9%9B%86%E7%BE%A4/:5:0","tags":["kubeadm"],"title":"利用kubeadm部署kubernetes集群","uri":"/posts/kubernetes/primary/1.%E5%88%A9%E7%94%A8-kubeadm%E9%83%A8%E7%BD%B2kubernetes%E9%9B%86%E7%BE%A4/"},{"categories":["Kubernetes"],"content":"5.1 基础环境设置 系统版本：Ubuntu 18.04.6 容器运行时引擎：Docker 19.03.15 Kubernetes: v1.19 https://mirrors.tuna.tsinghua.edu.cn/ubuntu-releases/18.04.6/ IP 地址 主机名 角色 10.1.0.110 k8s-master01 k8s-master01.ilinux.io k8s-api.ilinux.io master 10.1.0.111 k8s-node01 k8s-node01.ilinux.io node 10.1.0.112 k8s-node02 k8s-node02.ilinux.io node 10.1.0.113 k8s-node03 k8s-node03.ilinux.io node 5.1.1 主机时间同步 #设置时区 timedatectl set-timezone Asia/Shanghai #安装chrony #三节点安装 apt install chrony -y ##服务端配置 vim /etc/chrony/chrony.conf pool time1.aliyun.com iburst maxsources 1 allow all systemctl start chrony systemctl enable chrony5.1.2 Hosts 10.1.0.110 k8s-master01 k8s-master01.ilinux.io k8s-api.ilinux.io\r10.1.0.111 k8s-node01 k8s-node01.ilinux.io\r10.1.0.112 k8s-node02 k8s-node02.ilinux.io\r10.1.0.113 k8s-node03 k8s-node03.ilinux.iosystemctl stop ufw \u0026\u0026 systemctl disable ufw swapoff -a5.1.3关闭swap和SElinux swapoff -a | sed -i '/swap/d' /etc/fstab vim /etc/selinu/config disableswapoff -a | sed -i '/swap/d' /etc/fstab vim /etc/selinu/config disable5.1.4 修改内核参数 在 Kubernetes (K8s) 集群上，为了优化性能、安全性和容器工作负载的稳定性，您可能需要修改 Linux 内核参数。以下是一些常见的内核参数，以及如何进行修改： sysctl 配置文件：sysctl 是用于修改和查询内核参数的工具。您可以通过修改 sysctl 配置文件来永久更改内核参数。配置文件通常位于 /etc/sysctl.conf 或 /etc/sysctl.d/ 目录中。例如，要增加 TCP 连接的最大数量，可以在配置文件中添加以下行： vim /etc/sysctl.conf net.ipv4.ip_local_port_range = 1024 65000 sysctl -p然后，使用 sysctl -p 命令重新加载配置。 ulimit 设置：通过修改用户的 ulimit 设置，您可以更改每个进程可以打开的文件描述符数、进程数等。这可以通过修改 /etc/security/limits.conf 文件实现。 vim /etc/security/limits.conf * soft nofile 65536 * hard nofile 65536 ulimit -acat \u003c\u003cEOF \u003e /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 EOF sysctl -p /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1：允许 IPv6 数据包通过 iptables 进行处理，这对于容器网络通信非常重要。 net.bridge.bridge-nf-call-iptables = 1：允许 iptables 处理桥接的数据包，这对于容器网络和服务发现也很重要。 net.ipv4.ip_forward = 1：启用 IPv4 数据包的转发，这对于在 Kubernetes 集群中进行路由和流量管理很重要。 cat \u003c\u003cEOF \u003e /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 EOF sysctl -p /etc/sysctl.d/k8s.conf","date":"2023-02-24","objectID":"/posts/kubernetes/primary/1.%E5%88%A9%E7%94%A8-kubeadm%E9%83%A8%E7%BD%B2kubernetes%E9%9B%86%E7%BE%A4/:5:1","tags":["kubeadm"],"title":"利用kubeadm部署kubernetes集群","uri":"/posts/kubernetes/primary/1.%E5%88%A9%E7%94%A8-kubeadm%E9%83%A8%E7%BD%B2kubernetes%E9%9B%86%E7%BE%A4/"},{"categories":["Kubernetes"],"content":"5.2 配置容器运行时引擎 apt update apt install -y apt-transport-https ca-certificates \\ curl gnupg-agent software-properties-common添加Docker 官方GPG 证书，以验证程序包签名 curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -添加稳定版本的Docker-CE 仓库 add-apt-repository \\ \"deb [arch=amd64] https://mirrors.aliyun.com/docker-ce/linux/ubuntu \\ $(lsb_release -cs) stable\"在生产系统上，可能会需要应该安装一个特定版本的Docker CE，而不是总是使用最新版本： 列出可用的版本： sudo apt update root@k8s-master01:/sh# apt-cache madison docker-ce docker-ce | 5:24.0.2-1~ubuntu.18.04~bionic | https://download.docker.com/linux/ubuntu bionic/stable amd64 Packages docker-ce | 5:24.0.2-1~ubuntu.18.04~bionic | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packages docker-ce | 5:24.0.1-1~ubuntu.18.04~bionic | https://download.docker.com/linux/ubuntu bionic/stable amd64 Packages docker-ce | 5:24.0.1-1~ubuntu.18.04~bionic | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packages docker-ce | 5:24.0.0-1~ubuntu.18.04~bionic | https://download.docker.com/linux/ubuntu bionic/stable amd64 Packages docker-ce | 5:24.0.0-1~ubuntu.18.04~bionic | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packages docker-ce | 5:23.0.6-1~ubuntu.18.04~bionic | https://download.docker.com/linux/ubuntu bionic/stable amd64 Packages docker-ce | 5:23.0.6-1~ubuntu.18.04~bionic | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packages docker-ce | 5:23.0.5-1~ubuntu.18.04~bionic | https://download.docker.com/linux/ubuntu bionic/stable amd64 Packages docker-ce | 5:23.0.5-1~ubuntu.18.04~bionic | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packages docker-ce | 5:23.0.4-1~ubuntu.18.04~bionic | https://download.docker.com/linux/ubuntu bionic/stable amd64 Packages更新apt索引后安装Docker-ce sudo apt install docker-ce=5:19.03.15~3-0~ubuntu-bionic docker-ce-cli=5:19.03.15~3-0~ubuntu-bionic containerd.io=1.5.11-1 -y**自定义docker配置 ** vim /etc/docker/daemon.json { \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"log-driver\": \"json-file\", \"log-opts\":{ \"max-size\": \"100m\" }, \"storage-driver\": \"overlay2\", \"registry-mirrors\": [\"https://sqytbycc.mirror.aliyuncs.com\"] } exec-opts 配置了 CGroup 驱动程序，将其设置为 “systemd”，这是许多现代 Linux 系统上的推荐设置。 log-driver 配置了容器日志驱动程序，将其设置为 “json-file”，表示容器的日志将以 JSON 格式记录在文件中。 log-opts 用于配置日志选项，其中 max-size 设置容器日志文件的最大大小为 “100m”，即 100 兆字节。 storage-driver 配置了 Docker 存储驱动程序，将其设置为 “overlay2”，这是常见的存储驱动程序之一，用于管理容器的文件系统层 systemctl daemon-reload \u0026\u0026 systemctl restart docker \u0026\u0026 systemctl enable docker","date":"2023-02-24","objectID":"/posts/kubernetes/primary/1.%E5%88%A9%E7%94%A8-kubeadm%E9%83%A8%E7%BD%B2kubernetes%E9%9B%86%E7%BE%A4/:5:2","tags":["kubeadm"],"title":"利用kubeadm部署kubernetes集群","uri":"/posts/kubernetes/primary/1.%E5%88%A9%E7%94%A8-kubeadm%E9%83%A8%E7%BD%B2kubernetes%E9%9B%86%E7%BE%A4/"},{"categories":["Kubernetes"],"content":"5.3 安装kubeadm、kubelet 和kubectl 添加kubernetes 官方程序密钥： curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add -为apt添加kubernetes程序包仓库 vim /etc/apt/sources.list echo deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main \u003e\u003e /etc/apt/sources.list \u0026\u0026 apt update 软件包版本： “kubernetes-xenial” 的软件包版本可能与 Ubuntu 16.04 中的软件包版本相匹配。 “kubernetes-bionic” 的软件包版本可能与 Ubuntu 18.04 中的软件包版本相匹配。 支持周期： “kubernetes-xenial” 针对 Ubuntu 16.04，已于 2021 年 4 月结束官方支持，不再收到常规更新。 “kubernetes-bionic” 针对 Ubuntu 18.04，官方支持周期较长，支持至 2023 年 4 月 5.3.1 更新软件包索引并安装程序包 apt update #查看可用版本 apt-cache madison kubelet #选择 1.19.16-00 版本 apt install -y kubelet=1.19.16-00 kubeadm=1.19.16-00 kubectl=1.19.16-00 sudo systemctl enable kubelet \u0026\u0026 sudo systemctl start kubelet5.3.2 初始化控制平面 sudo kubeadm init \\\r--image-repository registry.aliyuncs.com/google_containers \\\r--kubernetes-version v1.19.16 \\\r--control-plane-endpoint k8s-api.ilinux.io \\\r--apiserver-advertise-address 10.1.0.110 \\\r--pod-network-cidr 10.244.0.0/16 \\\r--token-ttl 0--image-repository 指定要使用的镜像仓库，这里使用的是阿里云的，默认为gcr.io --kubernetes-version kubernetes 程序组件的版本号，他必须与安装的kubelet 程序包的版本相同。 --kubernetes-version 控制平面的固定访问端点，可以是IP地址或DNS 名称，作为集群管理员与集群组件的kubeconfig配置文件的API Server 的访问地址。单控制面板部署时可以不使用该选项。 --apiserver-advertise-address API Server 通告给其他组件的IP地址，一般为Master节点用于集群内通信的IP地址，0.0.0.0 表示节点上所有可用地址。 --pod-network-cidr Pod 网络的地址范围，用于容器间通信的 IP 地址段，通常Flannel 网络插件的默认值为10.244.0.0/16，Project Calico 插件的默认值为 192.168.0.0/16。 --service-cidr Service 的网络地址范围，默认为10.96.0.0/12。通常仅Flannel一类的网络插件需要手动指定该地址。 --token-ttl 共享令牌的过期时长，默认为24小时，0表示永不过期。 为防止不安全的存储等原因导致令牌泄露危及集群安装，建议为其设定过期时长。 Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ You can now join any number of control-plane nodes by copying certificate authorities and service account keys on each node and then running the following as root: kubeadm join k8s-api.ilinux.io:6443 --token 60jbc3.cse8z5eiqgdtt1nf \\ --discovery-token-ca-cert-hash sha256:252d584d53ef359d98219cabcd9d7cb07b3c898058d1045e3feeaf5773585ba6 \\ --control-plane Then you can join any number of worker nodes by running the following on each as root: kubeadm join k8s-api.ilinux.io:6443 --token 60jbc3.cse8z5eiqgdtt1nf \\ --discovery-token-ca-cert-hash sha256:252d584d53ef359d98219cabcd9d7cb07b3c898058d1045e3feeaf5773585ba6vim /etc/profile source \u003c(kubectl completion bash) source /etc/profile5.3.3 配置命令行工具 kubectl 是 Kubernetes 集群的最常用命令行工具，它默认搜索当前用户主目录（保存于环境变量HOME中的值）中名为.kube 的隐藏目录，定位其中名为config 的配置文件以读取必要的配置。包括要接入Kubernetes 集群以及用于集群认证的证书或令牌等信息。 mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/configkubeconfig 文件通常包括以下信息： cluster：描述了要连接的 Kubernetes 集群的名称、API 服务器的地址和证书信息。 user：定义了用于身份验证的用户信息，通常包括用户名和证书。 context：将 cluster 和 user 关联在一起，并指定默认上下文。 current-context：指定默认使用的上下文，这决定了用户与集群的交互。 /etc/kubernetes/admin.conf 通常用于控制平面管理操作，如部署、管理和监视集群的操作。这个配置文件是敏感的，应该受到保护，只允许有权限的用户访问。 用户可在任何能够通过k8s-api.ilinux.io 与 API Server 通信的主机上安装Kubectl，并为其复制或生成的kubeconfig文件以访问控制平面。 ","date":"2023-02-24","objectID":"/posts/kubernetes/primary/1.%E5%88%A9%E7%94%A8-kubeadm%E9%83%A8%E7%BD%B2kubernetes%E9%9B%86%E7%BE%A4/:5:3","tags":["kubeadm"],"title":"利用kubeadm部署kubernetes集群","uri":"/posts/kubernetes/primary/1.%E5%88%A9%E7%94%A8-kubeadm%E9%83%A8%E7%BD%B2kubernetes%E9%9B%86%E7%BE%A4/"},{"categories":["Kubernetes"],"content":"5.4 部署 Flannel 网络插件 通过执行kubectl get node 命令获取集群节点相关状态信息，出现NotReady状态是因为集群中未部署网络插件所致。 root@k8s-master01:~# kubectl get node NAME STATUS ROLES AGE VERSION k8s-master01 NotReady master 9m37s v1.19.16较为流行的为K8S提供Pod网络的插件有 Flannel、Calico和WeaveNet 等。相较来说Flannel以其简单、模式丰富、易部署、易使用等特性颇受用户欢迎。 Flannel同样可运行为K8S的集群附件，用DaemonSet控制器为每个节点（包括Master）运行一个Pod实例。 flannel项目地址 Deploying Flannel with kubectl kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.ymlIf you use custom podCIDR (not 10.244.0.0/16) you first need to download the above manifest and modify the network to match your one. 命令结果显示Pod的状态为Running时表示网络插件Flannel部署完成。 root@k8s-master01:/yaml# kubectl get pods -n kube-flannel | grep flannel kube-flannel-ds-sbwf9 1/1 Running 0 93s当前节点状态也转为Ready状态。 root@k8s-master01:/yaml# kubectl get node NAME STATUS ROLES AGE VERSION k8s-master01 Ready master 24m v1.19.16","date":"2023-02-24","objectID":"/posts/kubernetes/primary/1.%E5%88%A9%E7%94%A8-kubeadm%E9%83%A8%E7%BD%B2kubernetes%E9%9B%86%E7%BE%A4/:5:4","tags":["kubeadm"],"title":"利用kubeadm部署kubernetes集群","uri":"/posts/kubernetes/primary/1.%E5%88%A9%E7%94%A8-kubeadm%E9%83%A8%E7%BD%B2kubernetes%E9%9B%86%E7%BE%A4/"},{"categories":["Kubernetes"],"content":"5.5 添加工作节点 当主机基础环境准备好后可使用Kubeadm join命令加入集群，该命令需要借助共享令牌进行首次与控制平面通信时的认证操作。相关的令牌信息及完成的命令由初始化控制平面的命令结果得出。 kubeadm join k8s-api.ilinux.io:6443 --token 8pq5xn.waqfh238255rpvwl \\ --discovery-token-ca-cert-hash sha256:c204bc29c51c0df7e79b2a2dd1b49b89f3c5152189cb4f6f26c8c82dc525fa56root@k8s-node02:~# kubeadm join k8s-api.ilinux.io:6443 --token 8pq5xn.waqfh238255rpvwl \\ \u003e --discovery-token-ca-cert-hash sha256:c204bc29c51c0df7e79b2a2dd1b49b89f3c5152189cb4f6f26c8c82dc525fa56 [preflight] Running pre-flight checks [WARNING IsDockerSystemdCheck]: detected \"cgroupfs\" as the Docker cgroup driver. The recommended driver is \"systemd\". Please follow the guide at https://kubernetes.io/docs/setup/cri/ [preflight] Reading configuration from the cluster... [preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml' [kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\" [kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\" [kubelet-start] Starting the kubelet [kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap... This node has joined the cluster: * Certificate signing request was sent to apiserver and a response was received. * The Kubelet was informed of the new secure connection details. Run 'kubectl get nodes' on the control-plane to see this node join the cluster.","date":"2023-02-24","objectID":"/posts/kubernetes/primary/1.%E5%88%A9%E7%94%A8-kubeadm%E9%83%A8%E7%BD%B2kubernetes%E9%9B%86%E7%BE%A4/:5:5","tags":["kubeadm"],"title":"利用kubeadm部署kubernetes集群","uri":"/posts/kubernetes/primary/1.%E5%88%A9%E7%94%A8-kubeadm%E9%83%A8%E7%BD%B2kubernetes%E9%9B%86%E7%BE%A4/"},{"categories":["Kubernetes"],"content":"1、安装docker harbor依赖于docker 和docker-compose ","date":"2023-02-22","objectID":"/posts/kubernetes/primary/kubernetes-17/:1:0","tags":["k8s进阶训练营"],"title":"Harbor 使用自签证书支持 Https 访问 (十七)","uri":"/posts/kubernetes/primary/kubernetes-17/"},{"categories":["Kubernetes"],"content":"2、安装docker-compose wget https://github.com/docker/compose/releases/download/v2.17.3/docker-compose-linux-x86_64 [root@harbor ~]# cp docker-compose-linux-x86_64 /usr/local/bin/docker-compose [root@harbor ~]# chmod +x /usr/local/bin/docker-compose","date":"2023-02-22","objectID":"/posts/kubernetes/primary/kubernetes-17/:2:0","tags":["k8s进阶训练营"],"title":"Harbor 使用自签证书支持 Https 访问 (十七)","uri":"/posts/kubernetes/primary/kubernetes-17/"},{"categories":["Kubernetes"],"content":"3、安装harbor https://github.com/goharbor/harbor/releases 解压harbor tar -xvf harbor-offline-installer-v2.6.1.tgz -C /apps/harbor","date":"2023-02-22","objectID":"/posts/kubernetes/primary/kubernetes-17/:3:0","tags":["k8s进阶训练营"],"title":"Harbor 使用自签证书支持 Https 访问 (十七)","uri":"/posts/kubernetes/primary/kubernetes-17/"},{"categories":["Kubernetes"],"content":"4、自签证书 https://goharbor.io/docs/2.4.0/install-config/configure-https/ 如果使用containerd部署容器使用harbor则需要参考官网说明，与传统docker部署的Harbor自签发 SSL证书不同需要使用SAN包含多域名签发对象： ","date":"2023-02-22","objectID":"/posts/kubernetes/primary/kubernetes-17/:4:0","tags":["k8s进阶训练营"],"title":"Harbor 使用自签证书支持 Https 访问 (十七)","uri":"/posts/kubernetes/primary/kubernetes-17/"},{"categories":["Kubernetes"],"content":"4.1 生成证书颁发机构证书 创建证书保存目录 $ mkdir -p /apps/harbor/certs \u0026\u0026 cd /apps/harbor/certs生成CA证书私钥 openssl genrsa -out ca.key 4096基于CA证书私钥生成CA证书 openssl req -x509 -new -nodes -sha512 -days 3650 \\ -subj \"/C=CN/ST=Beijing/L=Beijing/O=ceamg/OU=it/CN=harbor.ceamg.com\" \\ -key ca.key \\ -out ca.crtC，Country，代表国家 ST，STate，代表省份 L，Location，代表城市 O，Organization，代表组织，公司 OU，Organization Unit，代表部门 CN，Common Name，代表服务器域名 emailAddress，代表联系人邮箱地址。","date":"2023-02-22","objectID":"/posts/kubernetes/primary/kubernetes-17/:4:1","tags":["k8s进阶训练营"],"title":"Harbor 使用自签证书支持 Https 访问 (十七)","uri":"/posts/kubernetes/primary/kubernetes-17/"},{"categories":["Kubernetes"],"content":"4.2 生成服务证书 创建证书私钥 openssl genrsa -out harbor.ceamg.com.key 4096基于服务证书私钥生成证书签名请求CSR openssl req -sha512 -new \\ -subj \"/C=CN/ST=Hanan/L=Zhengzhou/O=cib/OU=it/CN=harbor.ceamg.com\" \\ -key harbor.ceamg.com.key \\ -out harbor.ceamg.com.csr生成x509 v3扩展文件 cat \u003e v3.ext \u003c\u003c-EOF authorityKeyIdentifier=keyid,issuer basicConstraints=CA:FALSE keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment extendedKeyUsage = serverAuth subjectAltName = @alt_names [alt_names] DNS.1=harbor.ceamg.com DNS.2=harbor.ceamg DNS.3=harbor01 EOF使用v3文件为harbor签发证书 openssl x509 -req -sha512 -days 3650 \\ -extfile v3.ext \\ -CA ca.crt -CAkey ca.key -CAcreateserial \\ -in harbor.ceamg.com.csr \\ -out harbor.ceamg.com.crt","date":"2023-02-22","objectID":"/posts/kubernetes/primary/kubernetes-17/:4:2","tags":["k8s进阶训练营"],"title":"Harbor 使用自签证书支持 Https 访问 (十七)","uri":"/posts/kubernetes/primary/kubernetes-17/"},{"categories":["Kubernetes"],"content":"5、配置证书 ","date":"2023-02-22","objectID":"/posts/kubernetes/primary/kubernetes-17/:5:0","tags":["k8s进阶训练营"],"title":"Harbor 使用自签证书支持 Https 访问 (十七)","uri":"/posts/kubernetes/primary/kubernetes-17/"},{"categories":["Kubernetes"],"content":"5.1 修改harbor配置文件 修改域名和https SSL签发的私钥和证书路径 # Configuration file of Harbor # The IP address or hostname to access admin UI and registry service. # DO NOT use localhost or 127.0.0.1, because Harbor needs to be accessed by external clients. hostname: harbor.ceamg.com # http related config http: # port for http, default is 80. If https enabled, this port will redirect to https port port: 80 # https related config https: # https port for harbor, default is 443 port: 443 # The path of cert and key files for nginx certificate: /data/cert/harbor.ceamg.com.crt private_key: /data/cert/harbor.ceamg.com.key","date":"2023-02-22","objectID":"/posts/kubernetes/primary/kubernetes-17/:5:1","tags":["k8s进阶训练营"],"title":"Harbor 使用自签证书支持 Https 访问 (十七)","uri":"/posts/kubernetes/primary/kubernetes-17/"},{"categories":["Kubernetes"],"content":"5.2 根据新配置重新生成各类资源和配置 ./prepare --with-notary --with-trivy --with-chartmuseum","date":"2023-02-22","objectID":"/posts/kubernetes/primary/kubernetes-17/:5:2","tags":["k8s进阶训练营"],"title":"Harbor 使用自签证书支持 Https 访问 (十七)","uri":"/posts/kubernetes/primary/kubernetes-17/"},{"categories":["Kubernetes"],"content":"5.3 启动服务 docker-compose up -d","date":"2023-02-22","objectID":"/posts/kubernetes/primary/kubernetes-17/:5:3","tags":["k8s进阶训练营"],"title":"Harbor 使用自签证书支持 Https 访问 (十七)","uri":"/posts/kubernetes/primary/kubernetes-17/"},{"categories":["Kubernetes"],"content":"5.4 查看证书 ","date":"2023-02-22","objectID":"/posts/kubernetes/primary/kubernetes-17/:5:4","tags":["k8s进阶训练营"],"title":"Harbor 使用自签证书支持 Https 访问 (十七)","uri":"/posts/kubernetes/primary/kubernetes-17/"},{"categories":["Kubernetes"],"content":"六、配置containerd使用证书访问harbor ","date":"2023-02-22","objectID":"/posts/kubernetes/primary/kubernetes-17/:5:5","tags":["k8s进阶训练营"],"title":"Harbor 使用自签证书支持 Https 访问 (十七)","uri":"/posts/kubernetes/primary/kubernetes-17/"},{"categories":["Kubernetes"],"content":"6.1 HOSTS方式对接 直接在config.toml中配置相关的证书参数对于Containerd默认使用的ctr是不生效的因为ctr不使用CRI；因此它不读取配置中[plugins.\"io.containerd.grpc.v1.cri]配置的认证内容 我们可以使用Containerd支持的hosts方式去进行配置，可以实现ctr和nerdctl去对接Harbor 创建hosts.toml文件或者证书文件存储的目录，注意这个创建的目录名称必须是Harbor的域名(如果不是则报x509)；然后将证书文件或者hosts.toml文件放入该目录下才会生效。 containerd节点创建证书存储目录 mkdir -p /etc/containerd/certs.d/harbor.ceamg.com/创建**hosts.toml**文件 [host.\"https://harbor.ceamg.com\"] capabilities = [\"pull\", \"resolve\",\"push\"] ca = [\"harbor.ceamg.com.crt\"]将harbor服务证书发送到containerd节点 #!/bin/bash #目标主机列表 IP=\" 10.1.0.32 10.1.0.33 \" for node in ${IP};do sshpass -p ceamg.com ssh-copy-id ${node} -o StrictHostKeyChecking=no if [ $? -eq 0 ];then echo \"${node} 秘钥copy完成\" echo \"${node} 秘钥copy完成,准备环境初始化.....\" ssh ${node} \"mkdir /etc/containerd/certs.d/harbor.ceamg.com -p\" echo \"Harbor 证书创建成功!\" scp /etc/containerd/certs.d/harbor.ceamg.com/harbor.ceamg.com.crt ${node}:/etc/containerd/certs.d/harbor.ceamg.com/ echo \"Harbor 证书拷贝成功!\" ssh ${node} \"echo \"10.1.0.38 harbor.ceamg.com\" \u003e\u003e /etc/hosts\" echo \"host 解析添加完成\" #scp -r /root/.docker ${node}:/root/ #echo \"Harbor 认证件拷完成!\" else echo \"${node} 秘钥copy失败\" fi done ","date":"2023-02-22","objectID":"/posts/kubernetes/primary/kubernetes-17/:5:6","tags":["k8s进阶训练营"],"title":"Harbor 使用自签证书支持 Https 访问 (十七)","uri":"/posts/kubernetes/primary/kubernetes-17/"},{"categories":["Kubernetes"],"content":"前言 说到免费的SSL证书，大家首先想到的肯定是Let’s Encrypt，而使用过Let’s Encrypt的同学应该也知道，其有效期只有三个月，三个月后要重新续期。github上也有类似的脚本可以做到自动续期。那如果是在k8s上使用该免费证书，又如何操作的呢？这里cert-manager就派上用场了。 ","date":"2023-02-21","objectID":"/posts/kubernetes/primary/kubernetes-16/:1:0","tags":["k8s进阶训练营"],"title":"实战案例-Cert-Manager 实现 K8s 服务域名证书自动化续签 (十六)","uri":"/posts/kubernetes/primary/kubernetes-16/"},{"categories":["Kubernetes"],"content":"什么是cert-manager ? ","date":"2023-02-21","objectID":"/posts/kubernetes/primary/kubernetes-16/:2:0","tags":["k8s进阶训练营"],"title":"实战案例-Cert-Manager 实现 K8s 服务域名证书自动化续签 (十六)","uri":"/posts/kubernetes/primary/kubernetes-16/"},{"categories":["Kubernetes"],"content":"1.简介 cert-manager 是一个云原生证书管理开源项目，它简化了在 Kubernetes 集群中web服务管理Https证书的过程。 支持从多种证书签发机构申请证书，包括 Let’s Encrypt、HashiCorp Vault 和 Venafi。 它将证书和证书颁发机构添加为 Kubernetes 资源类型，并且简化获取、更新和使用这些证书，并在证书到期前的尝试续订证书，确保证书有效并及时更新。 在Kubernetes集群中使用 HTTPS 协议，需要一个证书管理器、一个证书自动签发服务，主要通过 Ingress 来发布 HTTPS 服务，因此需要Ingress Controller并进行配置，启用 HTTPS 及其路由。 角色 Issuer/ClusterIssuer: 用于指示 cert-manager 用什么方式签发证书，本文主要讲解签发免费证书的 ACME 方式。ClusterIssuer 与 Issuer 的唯一区别就是 Issuer 只能用来签发自己所在 namespace 下的证书，ClusterIssuer 可以签发任意 namespace 下的证书。 Certificate: 用于告诉 cert-manager 我们想要什么域名的证书以及签发证书所需要的一些配置，包括对 Issuer/ClusterIssuer 的引用。 ","date":"2023-02-21","objectID":"/posts/kubernetes/primary/kubernetes-16/:2:1","tags":["k8s进阶训练营"],"title":"实战案例-Cert-Manager 实现 K8s 服务域名证书自动化续签 (十六)","uri":"/posts/kubernetes/primary/kubernetes-16/"},{"categories":["Kubernetes"],"content":"2. 设计理念 Cert-Manager 是将 TLS 证书视为一种资源，就像 Pod、Service 和 Deployment 一样，可以使用 Kubernetes API 进行管理。它使用了自定义资源定义（CRD）机制，通过扩展 Kubernetes API，为证书的生命周期提供了标准化的管理方式。 ","date":"2023-02-21","objectID":"/posts/kubernetes/primary/kubernetes-16/:2:2","tags":["k8s进阶训练营"],"title":"实战案例-Cert-Manager 实现 K8s 服务域名证书自动化续签 (十六)","uri":"/posts/kubernetes/primary/kubernetes-16/"},{"categories":["Kubernetes"],"content":"3.架构设计 Cert-Manager 的架构分为两层：控制层和数据层。 控制层: 负责证书的管理，包括证书的创建、更新和删除等； 数据层: 负责存储证书相关的数据，包括证书的私钥、证书请求、证书颁发机构等。 Cert-Manager 支持多种证书颁发机构，包括自签名证书selfSigned、Let’s Encrypt、HashiCorp Vault、Venafi 等。它还支持多种验证方式，包括 HTTP 验证、DNS 验证和 TLS-SNI 验证等。这些验证方式可以帮助确保证书的颁发机构是可信的，并且确保证书的私钥不会泄露。 ","date":"2023-02-21","objectID":"/posts/kubernetes/primary/kubernetes-16/:2:3","tags":["k8s进阶训练营"],"title":"实战案例-Cert-Manager 实现 K8s 服务域名证书自动化续签 (十六)","uri":"/posts/kubernetes/primary/kubernetes-16/"},{"categories":["Kubernetes"],"content":"4.签发流程 在 Kubernetes 中，cert-manager 通过以下流程创建资源对象以签发证书： 创建一个 CertificateRequest 对象，包含证书的相关信息，例如证书名称、域名等。该对象指定了使用的 Issuer 或 ClusterIssuer，以及证书签发完成后，需要存储的 Secret 的名称。 Issuer 或 ClusterIssuer 会根据证书请求的相关信息，创建一个 Order 对象，表示需要签发一个证书。该对象包含了签发证书所需的域名列表、证书签发机构的名称等信息。 证书签发机构根据 Order 对象中的信息创建一个或多个 Challenge 对象，用于验证证书申请者对该域名的控制权。Challenge 对象包含一个 DNS 记录或 HTTP 服务，证明域名的所有权。 cert-manager 接收到 Challenge 对象的回应ChallengeResponse后，会将其更新为已解决状态。证书签发机构会检查所有的 Challenge 对象，如果全部通过验证，则会签发证书。 签发证书完成后，证书签发机构会将证书信息写入 Secret 对象，同时将 Order 对象标记为已完成。证书信息现在可以被其他部署对象使用。 +-------------+ | | | Ingress/ | | annotations | | | +------+------+ | | watch ingress change | v +-------------+ | | | Issuer/ | | ClusterIssuer | | | +------+------+ | | Create CertificateRequest | v +------+------+ | | |CertificateRequest| | | +------+------+ | | Create Order | v +------+------+ | | | Order | | | +------+------+ | | Create Challenges | v +------+------+ | | | Challenge | | | +------+------+ | | Respond to Challenge | v +------+------+ | | |ChallengeResponse| | | +------+------+ | | Issue Certificate | v +------+------+ | | | Secret | | | +------+------+","date":"2023-02-21","objectID":"/posts/kubernetes/primary/kubernetes-16/:2:4","tags":["k8s进阶训练营"],"title":"实战案例-Cert-Manager 实现 K8s 服务域名证书自动化续签 (十六)","uri":"/posts/kubernetes/primary/kubernetes-16/"},{"categories":["Kubernetes"],"content":"一、安装cert-manager https://cert-manager.io/docs/installation/ wget https://github.com/cert-manager/cert-manager/releases/download/v1.12.0/cert-manager.yaml查看yaml文件中的镜像 quay.io/jetstack/cert-manager-cainjector:v1.12.0 quay.io/jetstack/cert-manager-webhook:v1.12.0 quay.io/jetstack/cert-manager-controller:v1.12.0 quay.io/jetstack/cert-manager-acmesolver:v1.12.0","date":"2023-02-21","objectID":"/posts/kubernetes/primary/kubernetes-16/:3:0","tags":["k8s进阶训练营"],"title":"实战案例-Cert-Manager 实现 K8s 服务域名证书自动化续签 (十六)","uri":"/posts/kubernetes/primary/kubernetes-16/"},{"categories":["Kubernetes"],"content":"1.替换镜像文件 sed -i 's#quay.io\\/jetstack#harbor.ceamg.com\\/baseimages#g' /yaml/cert-manager/cert-manager.yaml","date":"2023-02-21","objectID":"/posts/kubernetes/primary/kubernetes-16/:3:1","tags":["k8s进阶训练营"],"title":"实战案例-Cert-Manager 实现 K8s 服务域名证书自动化续签 (十六)","uri":"/posts/kubernetes/primary/kubernetes-16/"},{"categories":["Kubernetes"],"content":"2.执行yaml文件安装 kubectl apply -f cert-manager.yaml","date":"2023-02-21","objectID":"/posts/kubernetes/primary/kubernetes-16/:3:2","tags":["k8s进阶训练营"],"title":"实战案例-Cert-Manager 实现 K8s 服务域名证书自动化续签 (十六)","uri":"/posts/kubernetes/primary/kubernetes-16/"},{"categories":["Kubernetes"],"content":"3.查看pod运行情况 root@k8s-made-01-32:/yaml/cert-manager# kubectl get pod -n cert-manager NAME READY STATUS RESTARTS AGE cert-manager-5c458b9858-gkxzv 1/1 Running 0 21s cert-manager-cainjector-6fd86f6d9d-w4vhv 1/1 Running 0 21s cert-manager-webhook-5f6c479b6b-mcs4r 1/1 Running 0 21s","date":"2023-02-21","objectID":"/posts/kubernetes/primary/kubernetes-16/:3:3","tags":["k8s进阶训练营"],"title":"实战案例-Cert-Manager 实现 K8s 服务域名证书自动化续签 (十六)","uri":"/posts/kubernetes/primary/kubernetes-16/"},{"categories":["Kubernetes"],"content":"二、创建 cert-manager 的证书颁发实体对象 cert-manager 的 Issuer 和 ClusterIssuer 都是用来定义证书颁发的实体的资源对象。 Issuer 是命名空间级别的资源，用于在命名空间内颁发证书。例如，当您需要使用自签名证书来保护您的服务，或者使用 Let’s Encrypt 等公共证书颁发机构来颁发证书时，可以使用 Issuer。 ClusterIssuer 是集群级别的资源，用于在整个集群内颁发证书。例如，当您需要使用公司的内部 CA 来颁发证书时，可以使用 ClusterIssuer。 知道两者之间的区别之后，你就可以根据自己的使用情况来决定自己的 issuer 的类型。 这里列出几种常用的 issuer 使用模板： ","date":"2023-02-21","objectID":"/posts/kubernetes/primary/kubernetes-16/:4:0","tags":["k8s进阶训练营"],"title":"实战案例-Cert-Manager 实现 K8s 服务域名证书自动化续签 (十六)","uri":"/posts/kubernetes/primary/kubernetes-16/"},{"categories":["Kubernetes"],"content":"1.创建 staging 环境的证书颁发者 issuer apiVersion: cert-manager.io/v1 kind: Issuer metadata: name: letsencrypt-staging spec: acme: # The ACME server URL server: https://acme-staging-v02.api.letsencrypt.org/directory # Email address used for ACME registration email: xxx@qq.com #此处填写你的邮箱地址 # Name of a secret used to store the ACME account private key privateKeySecretRef: name: letsencrypt-staging # Enable the HTTP-01 challenge provider solvers: - http01: ingress: class: nginx 使用 staging 环境颁发的证书无法正常在公网使用，需要本地添加受信任根证书 ","date":"2023-02-21","objectID":"/posts/kubernetes/primary/kubernetes-16/:4:1","tags":["k8s进阶训练营"],"title":"实战案例-Cert-Manager 实现 K8s 服务域名证书自动化续签 (十六)","uri":"/posts/kubernetes/primary/kubernetes-16/"},{"categories":["Kubernetes"],"content":"2.创建 prod 环境的证书颁发者 issuer apiVersion: cert-manager.io/v1 kind: Issuer metadata: name: letsencrypt-prod spec: acme: # The ACME server URL server: https://acme-v02.api.letsencrypt.org/directory # Email address used for ACME registration 欢迎关注·云原生生态圈 email: xxx@qq.com # Name of a secret used to store the ACME account private key privateKeySecretRef: name: letsencrypt-prod # Enable the HTTP-01 challenge provider solvers: - http01: ingress: class: nginx","date":"2023-02-21","objectID":"/posts/kubernetes/primary/kubernetes-16/:4:2","tags":["k8s进阶训练营"],"title":"实战案例-Cert-Manager 实现 K8s 服务域名证书自动化续签 (十六)","uri":"/posts/kubernetes/primary/kubernetes-16/"},{"categories":["Kubernetes"],"content":"3.创建 staging 环境的证书颁发者 ClusterIssuer apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: letsencrypt-staging spec: acme: # The ACME server URL server: https://acme-staging-v02.api.letsencrypt.org/directory # Email address used for ACME registration 欢迎关注·云原生生态圈 email: xxx@qq.com # Name of a secret used to store the ACME account private key privateKeySecretRef: name: letsencrypt-staging # Enable the HTTP-01 challenge provider solvers: - http01: ingress: class: nginx","date":"2023-02-21","objectID":"/posts/kubernetes/primary/kubernetes-16/:4:3","tags":["k8s进阶训练营"],"title":"实战案例-Cert-Manager 实现 K8s 服务域名证书自动化续签 (十六)","uri":"/posts/kubernetes/primary/kubernetes-16/"},{"categories":["Kubernetes"],"content":"4.创建 Prod 环境的证书颁发者 ClusterIssuer apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: letsencrypt-prod spec: acme: # The ACME server URL server: https://acme-v02.api.letsencrypt.org/directory # Email address used for ACME registration 欢迎关注·云原生生态圈 email: xxx@qq.com # Name of a secret used to store the ACME account private key privateKeySecretRef: name: letsencrypt-prod # Enable the HTTP-01 challenge provider solvers: - http01: ingress: class: nginx","date":"2023-02-21","objectID":"/posts/kubernetes/primary/kubernetes-16/:4:4","tags":["k8s进阶训练营"],"title":"实战案例-Cert-Manager 实现 K8s 服务域名证书自动化续签 (十六)","uri":"/posts/kubernetes/primary/kubernetes-16/"},{"categories":["Kubernetes"],"content":"三、应用实践测试 ","date":"2023-02-21","objectID":"/posts/kubernetes/primary/kubernetes-16/:5:0","tags":["k8s进阶训练营"],"title":"实战案例-Cert-Manager 实现 K8s 服务域名证书自动化续签 (十六)","uri":"/posts/kubernetes/primary/kubernetes-16/"},{"categories":["Kubernetes"],"content":"1.创建一个集群级的签发机构 apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: letsencrypt-prod1234 spec: acme: server: https://acme-v02.api.letsencrypt.org/directory email: zxx@ceamg.com privateKeySecretRef: name: letsencrypt-prod1234 solvers: - http01: ingress: class: nginx说明： metadata.name 是我们创建的签发机构的名称，后面我们创建证书的时候会引用它 spec.acme.email 是你自己的邮箱，证书快过期的时候会有邮件提醒，不过 cert-manager 会利用 acme 协议自动给我们重新颁发证书来续期 spec.acme.server 是 acme 协议的服务端，我们这里用 Let’s Encrypt，这个地址就写死成这样就行 spec.acme.privateKeySecretRef 指示此签发机构的私钥将要存储到哪个 Secret 对象中，名称不重要 spec.acme.http01 这里指示签发机构使用 HTTP-01 的方式进行 acme 协议 (还可以用 DNS 方式，acme 协议的目的是证明这台机器和域名都是属于你的，然后才准许给你颁发证书) 执行发布命令 kubectl apply -f cluster-issuer.yaml","date":"2023-02-21","objectID":"/posts/kubernetes/primary/kubernetes-16/:5:1","tags":["k8s进阶训练营"],"title":"实战案例-Cert-Manager 实现 K8s 服务域名证书自动化续签 (十六)","uri":"/posts/kubernetes/primary/kubernetes-16/"},{"categories":["Kubernetes"],"content":"2.配置dns域名解析 ","date":"2023-02-21","objectID":"/posts/kubernetes/primary/kubernetes-16/:5:2","tags":["k8s进阶训练营"],"title":"实战案例-Cert-Manager 实现 K8s 服务域名证书自动化续签 (十六)","uri":"/posts/kubernetes/primary/kubernetes-16/"},{"categories":["Kubernetes"],"content":"3. 创建证书资源 apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: k8s-ceamg-com-cert namespace: default annotations: cert-manager.io/issue-temporary-certificate: \"true\" spec: secretName: k8s-ceamg-com-tls issuerRef: name: letsencrypt-prod1234 kind: ClusterIssuer duration: 2160h renewBefore: 360h dnsNames: - k8s.ceamg.com说明： spec.secretName 指示证书最终存到哪个 Secret 中 spec.issuerRef.kind 值为 ClusterIssuer 说明签发机构不在本 namespace 下，而是在全局 spec.issuerRef.name 我们创建的签发机构的名称 (ClusterIssuer.metadata.name) spec.dnsNames 指示该证书的可以用于哪些域名,与域名解析的一致 renewBefore 字段来控制证书到期前多久会被更新 duration字段来指定自签名证书的期限 kubectl apply -f Certificate.yaml ","date":"2023-02-21","objectID":"/posts/kubernetes/primary/kubernetes-16/:5:3","tags":["k8s进阶训练营"],"title":"实战案例-Cert-Manager 实现 K8s 服务域名证书自动化续签 (十六)","uri":"/posts/kubernetes/primary/kubernetes-16/"},{"categories":["Kubernetes"],"content":"4. 查看cert-manager运行情况 kubectl logs -f $(kubectl get pods -n cert-manager | grep cert-manager | grep -v 'cainjector\\|webhook' | awk '{print $1}') -n cert-manager当然，也可以查看临时生成的专门验证证书的 Ingress 对象的运行情况 临时对象cm-acme-http-solver-xxxx从创建到消亡的过程 kubectl get pod -A | grep \"cm*\"查看certificate创建结果 root@k8s-made-01-32:/etc/kubeasz/clusters/xx-prod# kubectl get certificate NAME READY SECRET AGE k8s-ceamg-com-cert True k8s-ceamg-com-tls 1h当READY为True时即为成功，详细可看cert-manager运行日志。 ","date":"2023-02-21","objectID":"/posts/kubernetes/primary/kubernetes-16/:5:4","tags":["k8s进阶训练营"],"title":"实战案例-Cert-Manager 实现 K8s 服务域名证书自动化续签 (十六)","uri":"/posts/kubernetes/primary/kubernetes-16/"},{"categories":["Kubernetes"],"content":"5.问题排查 问题1： kubectl logs -n cert-manager cert-manager-webhook-5f6c479b6b-mcs4r I0616 06:12:56.720433 1 logs.go:59] http: TLS handshake error from 10.48.87.0:34244: EOF解决方法 在Ingress 或 Certificate resource 添加声明 cert-manager.io/issue-temporary-certificate: \"true\" acme.cert-manager.io/http01-edit-in-place: \"true\" #颁发一个临时的自签名证书在，供ingress controller 在颁发实际证书之前使用问题2： kubectl logs -n cert-manager cert-manager-5c458b9858-gkxzv\rE0616 06:12:27.072543 1 sync.go:190] \"cert-manager/challenges: propagation check failed\" err=\"failed to perform self check GET request 'http://k8s.ceamg.com/.well-known/acme-challenge/OvQFFmEAO5016nAu1YC20RcgDtU2CsCPuoporE13ekw': Get \\\"http://k8s.ceamg.com/.well-known/acme-challenge/OvQFFmEAO5016nAu1YC20RcgDtU2CsCPuoporE13ekw\\\": dial tcp 10.1.0.32:80: connect: connection refused\" resource_name=\"k8s-ceamg-com-cert-9rw4s-2953330747-2092136088\" resource_namespace=\"default\" resource_kind=\"Challenge\" resource_version=\"v1\" dnsName=\"k8s.ceamg.com\" type=HTTP-01解决方法 1.排查DNS解析 2.排查容器内DNS是否可以解析到域名 #启用一个busybox pod 测试pod环境内是否可以解析到域名\rkubectl run --image=busybox:1.28.1 --rm -it -- sh\r/ # ping k8s.ceamg.com\rPING k8s.ceamg.com (10.1.0.91): 56 data bytes\r64 bytes from 10.1.0.91: seq=0 ttl=63 time=0.377 ms\r64 bytes from 10.1.0.91: seq=1 ttl=63 time=0.366 ms3.排查外网防火墙NAT地址转换策略 将公网地址转发到后端Haproxy代理节点，且有匹配数。 4.排查WAF防火墙端口策略 ACME 认证只需要放通80和443端口即可 5.依次检查certificate、challenges、certificaterequests kubectl describe certificate k8s-ceamg-com-certkubectl describe challenges.acme.cert-manager.iokubectl describe certificaterequests.cert-manager.io","date":"2023-02-21","objectID":"/posts/kubernetes/primary/kubernetes-16/:5:5","tags":["k8s进阶训练营"],"title":"实战案例-Cert-Manager 实现 K8s 服务域名证书自动化续签 (十六)","uri":"/posts/kubernetes/primary/kubernetes-16/"},{"categories":["Kubernetes"],"content":"6.部署一个服务测试证书 1.安装ingress-nginx Ingress-nginx详细安装过程 这里因为k8s版本为1.26.2所以选择 V1.71版本 wget https://github.com/kubernetes/ingress-nginx/archive/refs/tag/controller-v1.7.1.tar.gz tar xvf controller-v1.7.1.tar.gz cd ingress-nginx-controller-v1.7.1/deploy/static/provider/baremetal/ #修改当前目录下的deploy.yaml，将镜像修改未国内镜像源 cat deploy.yaml |grep image image: harbor.ceamg.com/k8s-base/ingress-nginx-controller:v1.7.1 image: harbor.ceamg.com/k8s-base/kube-webhook-certgen:v20230312 image: harbor.ceamg.com/k8s-base/kube-webhook-certgen:v202303121.1 指定控制器NodePort地址 vim deploy.yaml apiVersion: v1 kind: Service metadata: labels: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.7.1 name: ingress-nginx-controller namespace: ingress-nginx spec: ipFamilies: - IPv4 ipFamilyPolicy: SingleStack ports: - appProtocol: http name: http port: 80 protocol: TCP targetPort: http nodePort: 30020 - appProtocol: https name: https port: 443 protocol: TCP targetPort: https nodePort: 300211.2 执行安装 kubectl apply -f deploy.yaml amespace/ingress-nginx created serviceaccount/ingress-nginx created serviceaccount/ingress-nginx-admission created role.rbac.authorization.k8s.io/ingress-nginx created role.rbac.authorization.k8s.io/ingress-nginx-admission created clusterrole.rbac.authorization.k8s.io/ingress-nginx created clusterrole.rbac.authorization.k8s.io/ingress-nginx-admission created rolebinding.rbac.authorization.k8s.io/ingress-nginx created rolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx created clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created configmap/ingress-nginx-controller created service/ingress-nginx-controller created service/ingress-nginx-controller-admission created deployment.apps/ingress-nginx-controller created job.batch/ingress-nginx-admission-create created job.batch/ingress-nginx-admission-patch created ingressclass.networking.k8s.io/nginx created validatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission created1.3 查看pod状态 root@k8s-made-01-32:/etc/kubeasz/clusters/xx-prod# kubectl get pod -n ingress-nginx NAME READY STATUS RESTARTS AGE ingress-nginx-admission-create-qhk5g 0/1 Completed 0 12s ingress-nginx-admission-patch-qn9kc 0/1 Completed 0 12s ingress-nginx-controller-589f4f6875-drvlp 1/1 Running 0 12sroot@k8s-made-01-32:/etc/kubeasz/clusters/xx-prod# kubectl get svc -n ingress-nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-nginx-controller NodePort 10.68.201.220 \u003cnone\u003e 80:30020/TCP,443:30021/TCP 15s ingress-nginx-controller-admission ClusterIP 10.68.72.129 \u003cnone\u003e 443/TCP 15s1.4 调整controller 副本数 默认情况下，ingress-nginx-controller只有一个副本，可以按需调整。 kubectl scale -n ingress-nginx deployment ingress-nginx-controller --replicas=3 deployment.apps/ingress-nginx-controller scaled -------------------------------------------------- kubectl get pod -n ingress-nginx NAME READY STATUS RESTARTS AGE ingress-nginx-admission-create-qhk5g 0/1 Completed 0 74s ingress-nginx-admission-patch-qn9kc 0/1 Completed 0 74s ingress-nginx-controller-589f4f6875-drvlp 1/1 Running 0 74s ingress-nginx-controller-589f4f6875-kw5t6 1/1 Running 0 34s ingress-nginx-controller-589f4f6875-pj6g2 1/1 Running 0 74s查看pod 运行位置分布情况 kubectl get pod -n ingress-nginx -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES ingress-nginx-admission-create-qhk5g 0/1 Completed 0 3d12h 10.48.150.71 10.1.0.37 \u003cnone\u003e \u003cnone\u003e ingress-nginx-admission-patch-qn9kc 0/1 Completed 0 3d12h 10.48.245.19 10.1.0.35 \u003cnone\u003e \u003cnone\u003e ingress-nginx-controller-589f4f6875-drvlp 1/1 Running 0 2d22h 10.48.150.72 10.1.0.37 \u003cnone\u003e \u003cnone\u003e ingress-nginx-controller-589f4f6875-kw5t6 1/1 Running 0 5m22s 10.48.245.21 10.1.0.35 \u003cnone\u003e \u003cnone\u003e ingress-nginx-controller-589f4f6875-pj6g2 1/1 Running 0 3d12h 10.48.35.142 10.1.0.34 \u003cnone\u003e \u003cnone\u003eingress-ng","date":"2023-02-21","objectID":"/posts/kubernetes/primary/kubernetes-16/:5:6","tags":["k8s进阶训练营"],"title":"实战案例-Cert-Manager 实现 K8s 服务域名证书自动化续签 (十六)","uri":"/posts/kubernetes/primary/kubernetes-16/"},{"categories":["Kubernetes"],"content":"7.通过域名访问测试 ","date":"2023-02-21","objectID":"/posts/kubernetes/primary/kubernetes-16/:5:7","tags":["k8s进阶训练营"],"title":"实战案例-Cert-Manager 实现 K8s 服务域名证书自动化续签 (十六)","uri":"/posts/kubernetes/primary/kubernetes-16/"},{"categories":["Kubernetes"],"content":"1.发布方式解读 ","date":"2023-02-17","objectID":"/posts/kubernetes/primary/kubernetes-14/:1:0","tags":["k8s进阶训练营"],"title":"K8S 持续集成与部署 (十四)","uri":"/posts/kubernetes/primary/kubernetes-14/"},{"categories":["Kubernetes"],"content":"1.1 金丝雀发布 金丝雀发布这个术语源自20世纪初期，当时英国的煤矿工人在下井采矿之前，会把笼养的金丝雀携带到矿井中，如果矿井中一氧化碳等有毒气体的浓度过高，在影响矿工之前，金丝雀相比人类表现的更加敏感快速，金丝雀中毒之后，煤矿工人就知道该立刻撤离。金丝雀发布是在将整个软件的新版本发布给所有用户之前，先发布给部分用户，用真实的客户流量来测试，以保证软件不会出现严重问题，降低发布风险。 在实践中，金丝雀发布一般会先发布到一个小比例的机器，比如 2% 的服务器做流量验证，然后从中快速获得反馈，根据反馈决定是扩大发布还是回滚。金丝雀发布通常会结合监控系统，通过监控指标，观察金丝雀机器的健康状况。如果金丝雀测试通过，则把剩余的机器全部升级成新版本，否则回滚代码。 优势： 对用户体验影响较小，在金丝雀发布过程中，只有少量用户会受影响 发布安全能够得到保障 劣势： 金丝雀的机器数量比较少, 有一些问题并不能够暴露出来 适用场景： 监控比较完备且与发布系统集成 ","date":"2023-02-17","objectID":"/posts/kubernetes/primary/kubernetes-14/:1:1","tags":["k8s进阶训练营"],"title":"K8S 持续集成与部署 (十四)","uri":"/posts/kubernetes/primary/kubernetes-14/"},{"categories":["Kubernetes"],"content":"1.2 灰度/滚动发布 灰度发布是金丝雀发布的延伸，是将发布分成不同的阶段/批次，每个阶段/批次的用户数量逐级增加。如果新版本在当前阶段没有发现问题，就再增加用户数量进入下一个阶段，直至扩展到全部用户。 灰度发布可以减小发布风险，是一种零宕机时间的发布策略。它通过切换线上并存版本之间的路由权重，逐步从一个版本切换为另一个版本。整个发布过程会持续比较长的时间, 在这段时间内，新旧代码共存，所以在开发过程中，需要考虑版本之间的兼容性，新旧代码共存不能影响功能可用性和用户体验。当新版本代码出现问题时，灰度发布能够比较快的回滚到老版本的代码上。 结合特性开关等技术，灰度发布可以实现更复杂灵活的发布策略。 优势： 用户体验影响比较小, 不需要停机发布 能够控制发布风险 劣势： 发布时间会比较长 需要复杂的发布系统和负载均衡器 需要考虑新旧版本共存时的兼容性 适用场景： 适合可用性较高的生产环境发布 ","date":"2023-02-17","objectID":"/posts/kubernetes/primary/kubernetes-14/:1:2","tags":["k8s进阶训练营"],"title":"K8S 持续集成与部署 (十四)","uri":"/posts/kubernetes/primary/kubernetes-14/"},{"categories":["Kubernetes"],"content":"1.3 滚动发布 kubernetes默认的更新策略也就是主流发布方案是滚动更新。 每次只升级一个或多个服务，升级完成后加入生产环境， 不断执行这个过程，直到集群中的全部旧版升级新版本。 Kubernetes的默认发布策略。 特点：用户无感知，平滑过渡 缺点： 部署周期长（需要健康检查，等它准备就绪，然后升级下一个，健康检查还是需要花费一些时间的） 发布策略较复杂 不易回滚 有影响范围较大 ","date":"2023-02-17","objectID":"/posts/kubernetes/primary/kubernetes-14/:1:3","tags":["k8s进阶训练营"],"title":"K8S 持续集成与部署 (十四)","uri":"/posts/kubernetes/primary/kubernetes-14/"},{"categories":["Kubernetes"],"content":"2.版本升级及回滚 ** 原理**：在指定的deployment 控制器中通过kubectl set image 指定新版本的镜像tag,来实现代码更新的目的。** 例如**:更新deployment中的2个pod,busybox ，pod更新到2.1版本,nginx pod更新到1.21.1版本 。 kubectl set image deployment/nginx busybox=busybox:v2.1 nginx=nginx:1.21.1更新方法有2种: rolling update 滚动更新 先创建一批新POD再删除一批旧Pod,升级完成后再升级下一批pod.(一批的默认值是25%的pod数) 优点:业务不会中断. 缺点:同一时间内会有2个不同版本同时存在。 recreate 重建更新 先删除所有Pod再重新创建Pod. 优点:不会同时有多个版本存在 缺点:在旧版本删除,新版本创建完成之前,该服务无法访问。 ","date":"2023-02-17","objectID":"/posts/kubernetes/primary/kubernetes-14/:2:0","tags":["k8s进阶训练营"],"title":"K8S 持续集成与部署 (十四)","uri":"/posts/kubernetes/primary/kubernetes-14/"},{"categories":["Kubernetes"],"content":"1.1 更新示例环境准备 准备10个pod的deployment,命名空间为cicd kind: Deployment apiVersion: apps/v1 metadata: labels: app: tomcat-app1-deployment-update name: tomcat-app1-deployment namespace: cicd spec: replicas: 10 selector: matchLabels: app: update-tomcat-app1-selector template: metadata: labels: app: update-tomcat-app1-selector spec: containers: - name: update-tomcat-app1-container image: harbor.ceamg.com/xinweb11/tomcat-app1:V2.0 ports: - containerPort: 8080 protocol: TCP name: http --- kind: Service apiVersion: v1 metadata: labels: app: update-tomcat-app1-service-label name: update-tomcat-app1-service namespace: cicd spec: type: NodePort ports: - name: http port: 80 protocol: TCP targetPort: 8080 nodePort: 30022 selector: app: update-tomcat-app1-selector可以看到tomcat-app1-deployment 副本数是10/10 root@master01[15:50:47]~/cicd #:vim test-tomcatapp1-deploy.yaml root@master01[15:51:10]~/cicd #:kubectl apply -f test-tomcatapp1-deploy.yaml deployment.apps/tomcat-app1-deployment created service/update-tomcat-app1-service created root@master01[15:51:12]~/cicd #: root@master01[15:51:12]~/cicd #: root@master01[15:51:12]~/cicd #:kubectl get pod -n cicd NAME READY STATUS RESTARTS AGE tomcat-app1-deployment-64bd79b5b7-49t6g 1/1 Running 0 7s tomcat-app1-deployment-64bd79b5b7-4gz9n 1/1 Running 0 7s tomcat-app1-deployment-64bd79b5b7-829jc 1/1 Running 0 7s tomcat-app1-deployment-64bd79b5b7-fclsh 1/1 Running 0 7s tomcat-app1-deployment-64bd79b5b7-jnkk6 1/1 Running 0 7s tomcat-app1-deployment-64bd79b5b7-ngxsl 1/1 Running 0 7s tomcat-app1-deployment-64bd79b5b7-nscqt 1/1 Running 0 7s tomcat-app1-deployment-64bd79b5b7-td72k 1/1 Running 0 7s tomcat-app1-deployment-64bd79b5b7-wh98d 1/1 Running 0 7s tomcat-app1-deployment-64bd79b5b7-wr8q6 1/1 Running 0 7s root@master01[15:51:19]~/cicd #:kubectl get deployments.apps -n cicd NAME READY UP-TO-DATE AVAILABLE AGE tomcat-app1-deployment 10/10 10 10 2m1s","date":"2023-02-17","objectID":"/posts/kubernetes/primary/kubernetes-14/:2:2","tags":["k8s进阶训练营"],"title":"K8S 持续集成与部署 (十四)","uri":"/posts/kubernetes/primary/kubernetes-14/"},{"categories":["Kubernetes"],"content":"1.2 更新版本 滚动策略 maxSurge: deploy 在更新过程中，Pod 数量可以超过定义的数量，超过的最大的值就叫 maxSurge。该值可以是一个百分比，也可以是一个具体的数字，默认情况下，该值为 25%。 maxUnavailable: 和期望ready的副本数比，不可用副本数最大比例（或最大值），这个值越小，越能保证服务稳定，更新越平滑； kind: Deployment apiVersion: apps/v1 metadata: labels: app: tomcat-app1-deployment-update name: tomcat-app1-deployment namespace: cicd spec: strategy: rollingUpdate: maxSurge: 0 #在更新过程中,Pod 数量可以超过定义的数量5个 maxUnavailable: 1 #在更新过程中，不可用副本数 replicas: 10 selector: matchLabels: app: update-tomcat-app1-selector template: metadata: labels: app: update-tomcat-app1-selector spec: containers: - name: update-tomcat-app1-container image: harbor.ceamg.com/xinweb11/tomcat-app1:1.9 ports: - containerPort: 8080 protocol: TCP name: http --- kind: Service apiVersion: v1 metadata: labels: app: update-tomcat-app1-service-label name: update-tomcat-app1-service namespace: cicd spec: type: NodePort ports: - name: http port: 80 protocol: TCP targetPort: 8080 nodePort: 30022 selector: app: update-tomcat-app1-selectorkubectl set image deployment/tomcat-app1-deployment update-tomcat-app1-container=harbor.ceamg.com/xinweb11/tomcat-app1:2.1 --namespace=cicd --record=true root@master01[16:16:23]~/cicd #:kubectl get pod -n cicd NAME READY STATUS RESTARTS AGE tomcat-app1-deployment-64bd79b5b7-2q7mk 1/1 Running 0 3m49s tomcat-app1-deployment-64bd79b5b7-4mnhr 1/1 Running 0 4m51s tomcat-app1-deployment-64bd79b5b7-55x5h 1/1 Running 0 4m51s tomcat-app1-deployment-64bd79b5b7-csxs2 1/1 Running 0 3m49s tomcat-app1-deployment-64bd79b5b7-h76pj 1/1 Running 0 3m49s tomcat-app1-deployment-64bd79b5b7-h9qcw 1/1 Running 0 4m51s tomcat-app1-deployment-64bd79b5b7-hvv2w 1/1 Running 0 4m51s tomcat-app1-deployment-64bd79b5b7-l6hxc 1/1 Running 0 3m49s tomcat-app1-deployment-64bd79b5b7-wlhzx 1/1 Running 0 4m51s tomcat-app1-deployment-788dbfc749-hjlg2 0/1 ContainerCreating 0 2s因为将 maxSurge，最大超出数量设置成了 0，所以无论如何都不会超过定义的数量。都是先减少再新增，也就是说，更新过程中，只会出现缺少服务数量的情况，不会多。 ","date":"2023-02-17","objectID":"/posts/kubernetes/primary/kubernetes-14/:2:3","tags":["k8s进阶训练营"],"title":"K8S 持续集成与部署 (十四)","uri":"/posts/kubernetes/primary/kubernetes-14/"},{"categories":["Kubernetes"],"content":"1.3 回滚版本 通过命令kubectl rollout history查看更新历史,可以看到除了第一次部署外我们还做了3次升级.当前版本是v3. root@master01[10:26:06]~ #:kubectl rollout history deployment -n cicd tomcat-app1-deployment deployment.apps/tomcat-app1-deployment REVISION CHANGE-CAUSE 2 \u003cnone\u003e 3 \u003cnone\u003e 4 \u003cnone\u003e 5 kubectl set image deployment/tomcat-app1-deployment update-tomcat-app1-container=harbor.ceamg.com/xinweb11/tomcat-app1:2.1 --namespace=cicd --record=true 6 kubectl set image deployment/tomcat-app1-deployment update-tomcat-app1-container=harbor.ceamg.com/xinweb11/tomcat-app1:2.2 --namespace=cicd --record=true 7 kubectl set image deployment/tomcat-app1-deployment update-tomcat-app1-container=harbor.ceamg.com/xinweb11/tomcat-app1:2.3 --namespace=cicd --record=true1.3.1 回滚到上一个版本 假设现在的V3版本有问题,需要回滚到v2的状态 root@master01[10:26:14]~ #:kubectl rollout undo deployment -n cicd tomcat-app1-deployment deployment.apps/tomcat-app1-deployment rolled back root@master01[10:42:21]~ #:kubectl rollout history deployment -n cicd tomcat-app1-deployment deployment.apps/tomcat-app1-deployment REVISION CHANGE-CAUSE 2 \u003cnone\u003e 3 \u003cnone\u003e 4 \u003cnone\u003e 5 kubectl set image deployment/tomcat-app1-deployment update-tomcat-app1-container=harbor.ceamg.com/xinweb11/tomcat-app1:2.1 --namespace=cicd --record=true 7 kubectl set image deployment/tomcat-app1-deployment update-tomcat-app1-container=harbor.ceamg.com/xinweb11/tomcat-app1:2.3 --namespace=cicd --record=true 8 kubectl set image deployment/tomcat-app1-deployment update-tomcat-app1-container=harbor.ceamg.com/xinweb11/tomcat-app1:2.2 --namespace=cicd --record=true 可见最新版本又回到了2.2版本 root@master01[10:44:28]~ #:kubectl describe pod -n cicd tomcat-app1-deployment-b6bbbdfd7-f7r74 | grep \"Image\" Image: harbor.ceamg.com/xinweb11/tomcat-app1:2.21.3.2 回滚到指定版本 那么有没有办法直接回滚到2.1的版本呢?除了用set image将版本指定到想要的版本外是否可以用rollout实现回滚呢? deployment.apps/tomcat-app1-deployment REVISION CHANGE-CAUSE 2 \u003cnone\u003e 3 \u003cnone\u003e 4 \u003cnone\u003e 5 kubectl set image deployment/tomcat-app1-deployment update-tomcat-app1-container=harbor.ceamg.com/xinweb11/tomcat-app1:2.1 --namespace=cicd --record=true 7 kubectl set image deployment/tomcat-app1-deployment update-tomcat-app1-container=harbor.ceamg.com/xinweb11/tomcat-app1:2.3 --namespace=cicd --record=true 8 kubectl set image deployment/tomcat-app1-deployment update-tomcat-app1-container=harbor.ceamg.com/xinweb11/tomcat-app1:2.2 --namespace=cicd --record=true答案显然是可以的,使用参数--to-version就可将版本回滚到指定版本 root@master01[10:54:49]~ #:kubectl rollout undo --to-revision=5 deploy tomcat-app1-deployment -n cicd deployment.apps/tomcat-app1-deployment rolled back ----------------------------------------------------------------------------------------------------- root@master01[10:59:23]~ #:kubectl rollout history deployment -n cicd tomcat-app1-deployment deployment.apps/tomcat-app1-deployment REVISION CHANGE-CAUSE 2 \u003cnone\u003e 3 \u003cnone\u003e 4 \u003cnone\u003e 7 kubectl set image deployment/tomcat-app1-deployment update-tomcat-app1-container=harbor.ceamg.com/xinweb11/tomcat-app1:2.3 --namespace=cicd --record=true 8 kubectl set image deployment/tomcat-app1-deployment update-tomcat-app1-container=harbor.ceamg.com/xinweb11/tomcat-app1:2.2 --namespace=cicd --record=true 9 kubectl set image deployment/tomcat-app1-deployment update-tomcat-app1-container=harbor.ceamg.com/xinweb11/tomcat-app1:2.1 --namespace=cicd --record=true ------------------------------------------------------------------------------------------------------ #查看版本 root@master01[11:00:50]~ #:kubectl describe pod -n cicd tomcat-app1-deployment-6c57484478-2h4d6 | grep \"Image\" Image: harbor.ceamg.com/xinweb11/tomcat-app1:2.1 root@master01[11:01:14]~ #:kubectl rollout undo --to-revision=7 deploy tomcat-app1-deployment -n cicd deployment.apps/tomcat-app1-deployment rolled backhttp://10.1.0.101/mi/ ","date":"2023-02-17","objectID":"/posts/kubernetes/primary/kubernetes-14/:2:4","tags":["k8s进阶训练营"],"title":"K8S 持续集成与部署 (十四)","uri":"/posts/kubernetes/primary/kubernetes-14/"},{"categories":["Kubernetes"],"content":"3. K8S 基于Jenkins CICD ","date":"2023-02-17","objectID":"/posts/kubernetes/primary/kubernetes-14/:3:0","tags":["k8s进阶训练营"],"title":"K8S 持续集成与部署 (十四)","uri":"/posts/kubernetes/primary/kubernetes-14/"},{"categories":["Kubernetes"],"content":"3.1 安装Gitlab 3.1.1准备环境 3.1.1.1 创建nfs共享目录 在nfs服务器创建共享目录，部署的gitlib使用共享目录来进行持久化 $ mkdir -p /data/k8s/gitlab/config $ mkdir -p /data/k8s/gitlab/logs $ mkdir -p /data/k8s/gitlab/data3.1.1.2 添加到共享目录 $ vim /etc/exports /data/k8s/gitlab/config 10.1.0.0/8(rw,sync,no_root_squash) /data/k8s/gitlab/logs 10.1.0.0/8(rw,sync,no_root_squash) /data/k8s/gitlab/data 10.1.0.0/8(rw,sync,no_root_squash)3.1.1.2 重启服务 $ /data/k8s/gitlab #:systemctl restart nfs-server.service #或者 $ exportfs -r3.1.2 部署Gitlab 3.1.2.1 准备部署yaml文件 apiVersion: v1 kind: Service metadata: name: gitlab-cicd namespace: cicd spec: type: NodePort ports: # Port上的映射端口 - port: 443 targetPort: 34443 name: gitlab443 - port: 80 targetPort: 38880 name: gitlab80 - port: 22 targetPort: 32220 name: gitlab22 selector: app: gitlab --- apiVersion: apps/v1 kind: Deployment metadata: name: gitlab-deploy spec: selector: matchLabels: app: gitlab revisionHistoryLimit: 2 #revisionHistoryLimit 可以定义保留的升级记录数。 template: metadata: labels: app: gitlab spec: containers: # 应用的镜像 - image: harbor.ceamg.com/k8s-base/gitlab-ce:15.6.8 name: gitlab imagePullPolicy: IfNotPresent # 应用的内部端口 ports: - containerPort: 443 name: gitlab443 - containerPort: 80 name: gitlab80 - containerPort: 22 name: gitlab22 volumeMounts: # gitlab持久化 - name: gitlab-persistent-config mountPath: /etc/gitlab - name: gitlab-persistent-logs mountPath: /var/log/gitlab - name: gitlab-persistent-data mountPath: /var/opt/gitlab volumes: # 使用nfs互联网存储 - name: gitlab-persistent-config nfs: server: 10.1.0.38 path: /data/k8s/gitlab/config - name: gitlab-persistent-logs nfs: server: 10.1.0.38 path: /data/k8s/gitlab/logs - name: gitlab-persistent-data nfs: server: 10.1.0.38 path: /data/k8s/gitlab/data3.1.2.2 执行部署 $ kubectl apply -f gitlib-ce.yaml3.1.2.3 查看部署结果 $ kubectl get svc -n cicd NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE gitlab-cicd NodePort 10.10.138.150 \u003cnone\u003e 443:34443/TCP,80:38880/TCP,22:32220/TCP 16s update-tomcat-app1-service NodePort 10.10.31.155 \u003cnone\u003e 80:30022/TCP 6d23hhttp://10.1.0.31:38880/users/sign_in 3.1.2.4 初始用户名和密码 初始用户名为root，初始密码gitlib自动创建，在如下文件中： $ cat /etc/gitlab/initial_root_password由于是容器部署，所以，需要进入到容器中，找到对应文件，拷贝密码进行登录。文件内容类似： # WARNING: This value is valid only in the following conditions\r# 1. If provided manually (either via `GITLAB_ROOT_PASSWORD` environment variable or via `gitlab_rails['initial_root_password']` setting in `gitlab.rb`, it was provided before database was seeded for the first time (usually, the first reconfigure run).\r# 2. Password hasn't been changed manually, either via UI or via command line.\r#\r# If the password shown here doesn't work, you must reset the admin password following https://docs.gitlab.com/ee/security/reset_user_password.html#reset-your-root-password.\rPassword: F5o6JeW+jH2qmgyc/yXwlp++DiKX0XchafdYvKB7cdo=\r# NOTE: This file will be automatically deleted in the first reconfigure run after 24 hours进入后修改admin密码 3.1.2 deb包安装 3.1.2.1 下载deb包 3.1.2.2 修改配置文件 $ dpkg -i gitlab-ce_15.7.8-ce.0_amd64.deb $ vim /etc/gitlab/gitlab.rb external_url 'http://10.1.0.35' #生成配置 $ gitlab-ctl reconfigure Notes: Default admin account has been configured with following details: Username: root Password: You didn't opt-in to print initial root password to STDOUT. Password stored to /etc/gitlab/initial_root_password. This file will be cleaned up in first reconfigure run after 24 hours. NOTE: Because these credentials might be present in your log files in plain text, it is highly recommended to reset the password following https://docs.gitlab.com/ee/security/reset_user_password.html#reset-your-root-password. gitlab Reconfigured! #查看密码 $ cat /etc/gitlab/initial_root_password # WARNING: This value is valid only in the following conditions # 1. If provided manually (either via `GITLAB_ROOT_PASSWORD` environment variable or via `gitlab_rails['initial_root_password']` setting in `gitlab.rb`, it was provided before database was seeded for the first time (usually, the first reconfigure run). # 2. Password hasn't been changed ma","date":"2023-02-17","objectID":"/posts/kubernetes/primary/kubernetes-14/:3:1","tags":["k8s进阶训练营"],"title":"K8S 持续集成与部署 (十四)","uri":"/posts/kubernetes/primary/kubernetes-14/"},{"categories":["Kubernetes"],"content":"3.2 安装Jenkins 3.2.1 jenkins 介绍 Jenkins 是一款著名的可扩展的用于自动化部署的开源 CI/CD 工具。Jenkins 是完全用 Java 编写的，是在 MIT 许可下发布的。它有一组强大的功能，可以将软件的构建、测试、部署、集成和发布等相关任务自动化。 这款用于测试的自动化 CI/CD 工具可以在 macOS、Windows 和各种 UNIX 版本（例如 OpenSUSE、Ubuntu、Red Hat 等）系统上使用。除了通过本地安装包安装，它还可以使用war包在任何安装过 Java 运行时环境（Java Runtime Environment，JRE）的机器上单独安装或者作为一个 Docker 安装。 Jenkins 团队已经开发了近 1000 个插件，使得应用程序可以与其它熟悉的技术混合使用。除此之外，还可以使用 Credentials Command 之类的插件。这使得向脚本中添加隐藏的身份验证凭证等变得简单可行。一旦 Jenkins pipeline 开始运行，你还可以验证每个阶段通过与否以及每个阶段的总数。但是，你不能在提供的图形化概览中检查特定作业的状态。你可以做的是跟踪终端中的作业进度。 环境需求 最小硬件需求：256M、1G磁盘空间，通常根据需要Jenkins服务器至少1G内存，50G+的磁盘空间。 软件需求：由于jenkins是使用java语言编写的，所以需要安装java运行时环境(jdk)3.2.2 安装JDK 从 Jenkins 2.357 版本开始，Jenkins只支持Java 11 和 Java 17 3.2.2.1 下载源码包 JDK 下载地址： 11：https://www.oracle.com/java/technologies/downloads/#java11 17：https://www.oracle.com/java/technologies/downloads/#java17 3.2.2.2 解压压缩包 root@etcd01[13:24:27]~ #:mkdir /apps/jdk17 -p root@etcd01[13:24:37]~ #:cd /apps/jdk17 root@etcd01[13:25:06]/apps/jdk17 #:tar -zxvf jdk-17_linux-x64_bin.tar.gz -C /apps/jdk17/3.2.2.3 配置环境变量 vim /etc/profile.d/jdk11.sh export JAVA_HOME=/jdk11/jdk-11.0.18 export PATH=/jdk11/jdk-11.0.18/bin:$PATH source /etc/profile.d/jdk11.sh3.2.2.4 测试java root@etcd01[13:40:02]/jdk11 #:java --version java 17.0.6 2023-01-17 LTS Java(TM) SE Runtime Environment (build 17.0.6+9-LTS-190) Java HotSpot(TM) 64-Bit Server VM (build 17.0.6+9-LTS-190, mixed mode, sharing)3.2.3 war形式安装启动Jenkins 3.2.3.1 下载war包 3.2.3.2 指定Jenkins文件保存路径 如果不设置该变量，Jenkins配置文件等都保存在 ~/.jenkins/ 目录下，不推荐 vim /etc/profile.d/jenkins.sh export JENKINS_HOME=/data/jenkins source /etc/profile.d/jenkins.sh 3.2.3.3 启动Jenkins root@etcd01[13:46:22]/data #:mkdir /apps/jenkins -p root@etcd01[13:46:22]/data #:mkdir -p /data/jenkins/log nohup java -jar -Xms512m -Xmx2048m /apps/jenkins/jenkins.war --httpPort=8181 \u003e /data/jenkins/log/jenkins.log 2\u003e\u00261 \u0026 #nohup 英文全称 no hang up（不挂起），用于在系统后台不挂断地运行命令，退出终端不会影响程序的运行。 #-Xms 指定jvm运行最小运行堆内存，默认为物理内存1/64，用法 ：-Xmx512m 注意：Xmx和512m中间不用添加空格 #-Xmx 指定jvm运行最大运行堆内存，认物理内存1/4，用法： -Xmx1024m 注意：Xmx和1024m中间不用添加空格 #--server.port 指定jar运行的port端口，用法：--server.port=80853.2.3.4 检查服务启动情况 #查看端口是否启动 root@etcd01[13:56:40]/data/jenkins #:lsof -i :8181 COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME java 1914938 root 109u IPv6 290105770 0t0 TCP *:8181 (LISTEN) # 查看日志 root@etcd01[13:56:52]/data/jenkins #:tail -f /data/jenkins/log/jenkins.log at Main.main(Main.java:117) WARNING: An illegal reflective access operation has occurred WARNING: Illegal reflective access by org.codehaus.groovy.vmplugin.v7.Java7$1 (file:/data/jenkins/war/WEB-INF/lib/groovy-all-2.4.21.jar) to constructor java.lang.invoke.MethodHandles$Lookup(java.lang.Class,int) WARNING: Please consider reporting this to the maintainers of org.codehaus.groovy.vmplugin.v7.Java7$1 WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations WARNING: All illegal access operations will be denied in a future release 2023-02-28 05:56:43.644+0000 [id=1] INFO o.e.j.s.handler.ContextHandler#doStart: Started w.@68ac9ec5{Jenkins v2.346.3,/,file:///data/jenkins/war/,AVAILABLE}{/data/jenkins/war} 2023-02-28 05:56:43.670+0000 [id=1] INFO o.e.j.server.AbstractConnector#doStart: Started ServerConnector@6492fab5{HTTP/1.1, (http/1.1)}{0.0.0.0:8181} 2023-02-28 05:56:43.671+0000 [id=1] INFO org.eclipse.jetty.server.Server#doStart: Started @3925ms 2023-02-28 05:56:43.672+0000 [id=24] INFO winstone.Logger#logInternal: Winstone Servlet Engine running: controlPort=disabled3.2.3.5 浏览器访问查看 http://10.1.0.34:8181 ::: warning AWT is not properly configured on this server. Perhaps you need to run your container with “-Djava.awt.headless=true”? See also: https://www.jenkins.io/redirect/troubleshooting/java.awt.headless\\ ::: 由于缺少AWT相关文件导致Jenkins报错，在该参考链接（https://wiki.jenkins.io/display/JENKINS/Jenkins+got+java.awt.headless+problem）中，给出的解决方案是安装ttf-dejavu字体。 解决方法： 安装fontconfig和字体 ttf-dejavu sudo apt-get install fontconfig ttf-dejavu 重启jvm进程来让其生效 root@etcd01[14:11:30]/data/jenkins #:p","date":"2023-02-17","objectID":"/posts/kubernetes/primary/kubernetes-14/:3:2","tags":["k8s进阶训练营"],"title":"K8S 持续集成与部署 (十四)","uri":"/posts/kubernetes/primary/kubernetes-14/"},{"categories":["Kubernetes"],"content":"3.3 对接Jenkins拉取代码 3.3.1 服务器之间免密 需求：服务器之间免密 目的：拉取代码时免密 导出Jenkins服务器秘钥到gitlab服务器中。 root@etcd01[16:20:31]~ #:ssh-keygen -t rsa -q -P \"\" -f ~/.ssh/id_rsa root@etcd01[16:20:31]~ #:cat /root/.ssh/id_rsa.pub ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCvFuqsriXcIcyRQG7KpYbwtM+Fn5BSyJSvfGdDIbOymHt7eFlWPQ/qmsnzdey2V28InALJIBJkQcfRwjmG3OTPsYpcP+ea0jhQ1GJHcamERwDJDxcg7jyk+r+dRwGhxLlWeHdiORGZGdqM2LPp7L3FqkDIKko0WMoL490kmAUMgjICrd3pjAQ7iV66YHxB2Y+w9EdWdj3d3GewtYhfnBlrn1bSaEx73y1KBhf3oy4pNOTeFPb2R5IIBllKiuD1r6J7AznRpVxihiQUadYLVFU4eCnXBHTRgiFTtd8oCghRxfrWgFpm0liBikeaawxM0wDQfYoWjZmKobxgvi47+OxS9xhvOn+yy4Iif2MqbH+V0go+eoAKwUE/FiaqqG0P/J5b6ZKx3ZrBF1FS6JztjI5PnzufizbgetvCqHf58+P4MKl8SuKHEI6SXbVzdf9KNmEpiK15m/flQUmYYIUba1nOiBiRFmZ+bLGvRRqUKLf+4P9XZTU1a0zIYXRaseq9QzU= root@etcd01 测试免密克隆项目代码 3.3.2 测试免密克隆 使用http方式克隆 root@etcd01[15:42:17]/pr #:git clone http://10.1.0.35/cy1/test.git Cloning into 'test'... Username for 'http://10.1.0.35': ryanxin Password for 'http://ryanxin@10.1.0.35': remote: Enumerating objects: 3, done. remote: Counting objects: 100% (3/3), done. remote: Compressing objects: 100% (2/2), done. remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0 Unpacking objects: 100% (3/3), 2.77 KiB | 2.77 MiB/s, done.使用ssh 方式克隆 root@etcd01[15:43:37]/pr #:git clone git@10.1.0.35:cy1/test.git Cloning into 'test'... The authenticity of host '10.1.0.35 (10.1.0.35)' can't be established. ECDSA key fingerprint is SHA256:lhRjKQBhgEhjbqcfKBb6oyle8C9EIOzu48QUoaeISIE. Are you sure you want to continue connecting (yes/no/[fingerprint])? yes Warning: Permanently added '10.1.0.35' (ECDSA) to the list of known hosts. remote: Enumerating objects: 3, done. remote: Counting objects: 100% (3/3), done. remote: Compressing objects: 100% (2/2), done. remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0 Receiving objects: 100% (3/3), done.","date":"2023-02-17","objectID":"/posts/kubernetes/primary/kubernetes-14/:3:3","tags":["k8s进阶训练营"],"title":"K8S 持续集成与部署 (十四)","uri":"/posts/kubernetes/primary/kubernetes-14/"},{"categories":["Kubernetes"],"content":"使用Jenkins对前端工程vue代码打包 ","date":"2023-02-17","objectID":"/posts/kubernetes/primary/kubernetes-14/:4:0","tags":["k8s进阶训练营"],"title":"K8S 持续集成与部署 (十四)","uri":"/posts/kubernetes/primary/kubernetes-14/"},{"categories":["Kubernetes"],"content":"安装npm 国内淘宝源 https://registry.npmmirror.com/binary.html?path=node/ $ tar -zxvf node-v18.14.2-linux-x64.tar.gz $ ln -s node-v18.14.2-linux-x64 node $ vim /etc/profile.d/npm.sh export PATH=$PATH:/software/npm/node/bin $ source /etc/profile.d/npm.sh $ npm -v 9.5.0 #替换npm仓库地址为淘宝镜像地址（推荐） $ npm config set registry https://registry.npm.taobao.org $ npm config get registry https://registry.npm.taobao.org/ npm config set registry http://r.cnpmjs.org #配置后可通过下面方式来验证是否成功 npm config get registry # 或者 npm info express #故需要国内可靠的npm源可以使用 一、国内镜像 1、淘宝NPM镜像 搜索地址：http://npm.taobao.org registry地址：http://registry.npm.taobao.org 2、cnpmjs镜像 搜索地址：http://cnpmjs.org registry地址：http://r.cnpmjs.org测试前端工程编译 cd /xxlog git init Initialized empty Git repository in /xxlog/.git/ git config --global user.name ryanxin7 git config --global user.email xinxincn0506@outlook.com git remote add origin git@github.com:ryanxin7/xxlog.git root@etcd01[15:18:55]/xxlog #:cat /root/.ssh/id_rsa.pub ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCvFuqsriXcIcyRQG7KpYbwtM+Fn5BSyJSvfGdDIbOymHt7eFlWPQ/qmsnzdey2V28InALJIBJkQcfRwjmG3OTPsYpcP+ea0jhQ1GJHcamERwDJDxcg7jyk+r+dRwGhxLlWeHdiORGZGdqM2LPp7L3FqkDIKko0WMoL490kmAUMgjICrd3pjAQ7iV66YHxB2Y+w9EdWdj3d3GewtYhfnBlrn1bSaEx73y1KBhf3oy4pNOTeFPb2R5IIBllKiuD1r6J7AznRpVxihiQUadYLVFU4eCnXBHTRgiFTtd8oCghRxfrWgFpm0liBikeaawxM0wDQfYoWjZmKobxgvi47+OxS9xhvOn+yy4Iif2MqbH+V0go+eoAKwUE/FiaqqG0P/J5b6ZKx3ZrBF1FS6JztjI5PnzufizbgetvCqHf58+P4MKl8SuKHEI6SXbVzdf9KNmEpiK15m/flQUmYYIUba1nOiBiRFmZ+bLGvRRqUKLf+4P9XZTU1a0zIYXRaseq9QzU= root@etcd01 拉取代码 $ git pull origin main The authenticity of host 'github.com (20.205.243.166)' can't be established. ECDSA key fingerprint is SHA256:p2QAMXNIC1TJYWeIOttrVc98/R1BUFWu3/LiyKgUfQM. Are you sure you want to continue connecting (yes/no/[fingerprint])? yes Warning: Permanently added 'github.com,20.205.243.166' (ECDSA) to the list of known hosts. remote: Enumerating objects: 1219, done. remote: Counting objects: 100% (1219/1219), done. remote: Compressing objects: 100% (794/794), done. remote: Total 1219 (delta 532), reused 1073 (delta 397), pack-reused 0 Receiving objects: 100% (1219/1219), 16.95 MiB | 5.16 MiB/s, done. Resolving deltas: 100% (532/532), done. From github.com:ryanxin7/xxlog * branch main -\u003e FETCH_HEAD * [new branch] main -\u003e origin/main $ git checkout main Branch 'main' set up to track remote branch 'main' from 'origin'. Switched to a new branch 'main' root@etcd01[15:22:34]/xxlog #:git branch -a * main master remotes/origin/main #初始化项目所需的node模块 $ npm install -g cnpm npm WARN deprecated @npmcli/move-file@2.0.1: This functionality has been moved to @npmcli/fs changed 420 packages in 2m 11 packages are looking for funding run `npm fund` for details $ npm install up to date in 3s 122 packages are looking for funding run `npm fund` for details $ ls LICENSE node_modules package.json package-lock.json README.md src #测试运行 $ npm run docs:dev \u003e xxlog@2.0.0 docs:dev \u003e vuepress dev src ✔ Initializing and preparing data - done in 3.95s vite v4.0.4 dev server running at: ➜ Local: http://localhost:8080/ ➜ Network: http://10.1.0.34:8080/","date":"2023-02-17","objectID":"/posts/kubernetes/primary/kubernetes-14/:4:1","tags":["k8s进阶训练营"],"title":"K8S 持续集成与部署 (十四)","uri":"/posts/kubernetes/primary/kubernetes-14/"},{"categories":["Kubernetes"],"content":"更新证书 为了更新的安全性，更新之前可以将所有 Master 节点的配置目录做一个备份： cp -r /etc/kubernetes /etc/kubernetes_$(date +%F) cp -r /var/lib/etcd /var/lib/etcd_$(date +%F)通过执行证书更新命令查看： kubeadm certs renew --help可以看到证书更新是支持更新指定服务的证书，也可以更新单个服务的证书，但都是集群服务的证书。 # 所有 Master 节点更新所有证书 kubeadm certs renew all 可以看到提示让重启 kube-apiserver, kube-controller-manager, kube-scheduler 和 etcd 服务证书才能生效。 重启组件脚本 #重启组件 for i in $(kubectl get pods -A | grep -E \"etcd|kube-apiserver|kube-controller-manager|kube-scheduler\" | awk '{print $2}');do kubectl delete pod $i -n kube-system sleep 3 done #重启服务 systemctl restart kubelet systemctl restart containerd查看组件运行情况 kubectl get pods -A | grep -E \"etcd|kube-apiserver|kube-controller-manager|kube-scheduler\" kube-system etcd-tj-master-01 1/1 Running 32 15m kube-system kube-apiserver-tj-master-01 1/1 Running 39 15m kube-system kube-controller-manager-tj-master-01 1/1 Running 163 15m kube-system kube-scheduler-tj-master-01 可以看到证书时间已经更新 同时，由于在初始化 Master 集群的时候采用的是设置环境变量 export KUBECONFIG=/etc/kubernetes/admin.conf 的方法，不需要再更新该文件。如果不是该方法，还需要使用新的 admin.conf 替换掉复制的 /root/.kube/config 配置文件。 cp /etc/kubernetes/admin.conf /root/.kube/config重启containerd 运行镜像 crictl stop 9731cb9e5b723 crictl stop 977896873866e crictl stop 24430601db1d1 crictl stop 7a7bad1c7dd70重启后,查看相关日志 ","date":"2023-02-17","objectID":"/posts/kubernetes/primary/kubernetes-15/:1:0","tags":["k8s进阶训练营"],"title":"K8S集群证书更新 (十五)","uri":"/posts/kubernetes/primary/kubernetes-15/"},{"categories":["Kubernetes"],"content":"1.HPA简介 HPA（Horizontal Pod Autoscaler），Pod水平自动缩放器，可以根据Pod的负载动态调整Pod的副本数量，业务高峰期自动扩容Pod副本以满足业务请求。在业务低峰期自动缩容Pod，实现节约资源的目的。 与HPA相对的是VPA （Vertical Pod Autoscaler），Pod垂直自动缩放器，可以基于Pod的资源利用率，调整对单个Pod的最大资源限制，不能与HPA同时使用。 HPA隶属于autoscaling API群组目前主要有v1和v2两个版本： 版本 描述 autoscaling/v1 只支持基于CPU指标的缩放 autoscaling/v2 支持基于Resource Metrics（资源指标，例如Pod 的CPU和内存）、Custom Metrics（自定义指标）和External Metrics（额外指标）的缩放 ","date":"2023-02-16","objectID":"/posts/kubernetes/primary/kubernetes-13/:1:0","tags":["k8s进阶训练营"],"title":"HPA自动伸缩pod数量 (十三)","uri":"/posts/kubernetes/primary/kubernetes-13/"},{"categories":["Kubernetes"],"content":"2.部署 metrics Server HPA需要通过Metrics Server来获取Pod的资源利用率，所以需要先部署Metrics Server。 Metrics Server是Kubernetes 集群核心监控数据的聚合器，它负责从kubelet收集资源指标，然后对这些指标监控数据进行聚合，并通过Metrics API将它们暴露在Kubernetes apiserver中，供水平Pod Autoscaler和垂直Pod Autoscaler使用。也可以通过kubectl top node/pod查看指标数据。 ","date":"2023-02-16","objectID":"/posts/kubernetes/primary/kubernetes-13/:2:0","tags":["k8s进阶训练营"],"title":"HPA自动伸缩pod数量 (十三)","uri":"/posts/kubernetes/primary/kubernetes-13/"},{"categories":["Kubernetes"],"content":"2.1 准备镜像 #从代理服务器上下载好镜像 root@harbor01[13:32:22]/proxy-images #:docker tag k8s.gcr.io/metrics-server/metrics-server:v0.6.2 harbor.ceamg.com/k8s-base/metrics-server:v0.6.2 root@harbor01[13:32:59]/proxy-images #:docker push harbor.ceamg.com/k8s-base/metrics-server:v0.6.2 The push refers to repository [harbor.ceamg.com/k8s-base/metrics-server] dc5ecd167a15: Pushed 9fce6bd02a21: Pushed v0.6.2: digest: sha256:0542aeb0025f6dd4f75e100ca14d7abdbe0725c75783d13c35e82d391f4735bc size: 739下载yaml文件 Metrics Server releases. V6.0.2 https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.6.2/components.yaml 修改文件中的镜像地址为私有仓库 apiVersion: v1 kind: ServiceAccount metadata: labels: k8s-app: metrics-server name: metrics-server namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: labels: k8s-app: metrics-server rbac.authorization.k8s.io/aggregate-to-admin: \"true\" rbac.authorization.k8s.io/aggregate-to-edit: \"true\" rbac.authorization.k8s.io/aggregate-to-view: \"true\" name: system:aggregated-metrics-reader rules: - apiGroups: - metrics.k8s.io resources: - pods - nodes verbs: - get - list - watch --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: labels: k8s-app: metrics-server name: system:metrics-server rules: - apiGroups: - \"\" resources: - nodes/metrics verbs: - get - apiGroups: - \"\" resources: - pods - nodes verbs: - get - list - watch --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: labels: k8s-app: metrics-server name: metrics-server-auth-reader namespace: kube-system roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: extension-apiserver-authentication-reader subjects: - kind: ServiceAccount name: metrics-server namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: labels: k8s-app: metrics-server name: metrics-server:system:auth-delegator roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:auth-delegator subjects: - kind: ServiceAccount name: metrics-server namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: labels: k8s-app: metrics-server name: system:metrics-server roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:metrics-server subjects: - kind: ServiceAccount name: metrics-server namespace: kube-system --- apiVersion: v1 kind: Service metadata: labels: k8s-app: metrics-server name: metrics-server namespace: kube-system spec: ports: - name: https port: 443 protocol: TCP targetPort: https selector: k8s-app: metrics-server --- apiVersion: apps/v1 kind: Deployment metadata: labels: k8s-app: metrics-server name: metrics-server namespace: kube-system spec: selector: matchLabels: k8s-app: metrics-server strategy: rollingUpdate: maxUnavailable: 0 template: metadata: labels: k8s-app: metrics-server spec: containers: - args: - --cert-dir=/tmp - --secure-port=4443 - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname - --kubelet-use-node-status-port - --metric-resolution=15s image: harbor.ceamg.com/k8s-base/metrics-server:v0.6.2 imagePullPolicy: IfNotPresent livenessProbe: failureThreshold: 3 httpGet: path: /livez port: https scheme: HTTPS periodSeconds: 10 name: metrics-server ports: - containerPort: 4443 name: https protocol: TCP readinessProbe: failureThreshold: 3 httpGet: path: /readyz port: https scheme: HTTPS initialDelaySeconds: 20 periodSeconds: 10 resources: requests: cpu: 100m memory: 200Mi securityContext: allowPrivilegeEscalation: false readOnlyRootFilesystem: true runAsNonRoot: true runAsUser: 1000 volumeMounts: - mountPath: /tmp name: tmp-dir nodeSelector: kubernetes.io/os: linux priorityClassName: system-cluster-critical serviceAccountName: metrics-server volumes: - emptyDir: {} name: tmp-dir --- apiVersion: apiregistration.k8s.io/v1 kind: APIService metadata: labels: k8s-app: metrics-server name: v1beta1.metrics.k8s.io spec: group: metrics.k8s.io groupPriorityMinimum: ","date":"2023-02-16","objectID":"/posts/kubernetes/primary/kubernetes-13/:2:1","tags":["k8s进阶训练营"],"title":"HPA自动伸缩pod数量 (十三)","uri":"/posts/kubernetes/primary/kubernetes-13/"},{"categories":["Kubernetes"],"content":"2.2 创建pod 创建之后查看Pod状态： root@master01[13:41:51]~/metrics #:kubectl apply -f metrics-v6.0.2.yaml serviceaccount/metrics-server created clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created clusterrole.rbac.authorization.k8s.io/system:metrics-server created rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created service/metrics-server created deployment.apps/metrics-server created apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io createdroot@master01[13:41:55]~/metrics #:kubectl get pod -n kube-system NAME READY STATUS RESTARTS AGE calico-kube-controllers-5c8bb696bb-hf2cp 1/1 Running 1 (42d ago) 43d calico-node-4ntd2 1/1 Running 0 42d calico-node-dwnq5 1/1 Running 0 42d calico-node-nskdq 1/1 Running 0 42d calico-node-slx2b 1/1 Running 0 42d coredns-6c496b89f6-hd8vf 1/1 Running 0 42d metrics-server-5cd7bd59b4-tzx2b 1/1 Running 0 2m24s","date":"2023-02-16","objectID":"/posts/kubernetes/primary/kubernetes-13/:2:2","tags":["k8s进阶训练营"],"title":"HPA自动伸缩pod数量 (十三)","uri":"/posts/kubernetes/primary/kubernetes-13/"},{"categories":["Kubernetes"],"content":"2.3 验证资源指标 验证metrics-server是否工作 root@master01[13:44:57]~/metrics #:kubectl top node NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% 10.1.0.30 229m 2% 3731Mi 51% 10.1.0.31 323m 4% 3481Mi 47% 10.1.0.32 417m 5% 4713Mi 64% 10.1.0.33 331m 4% 5453Mi 75% 可以获取node和pod的资源指标就表示metrics-server可以正常工作 ","date":"2023-02-16","objectID":"/posts/kubernetes/primary/kubernetes-13/:2:3","tags":["k8s进阶训练营"],"title":"HPA自动伸缩pod数量 (十三)","uri":"/posts/kubernetes/primary/kubernetes-13/"},{"categories":["Kubernetes"],"content":"3. HPA配置参数 HPA控制器有一些重要配置参数，用于控制Pod缩放的行为，这些参数都可以在kube-controller的启动参数中配置： **–horizontal-pod-autoscaler-sync-period**：查询Pod资源利用率的时间间隔，默认15s查询一次 **–horizontal-pod-autoscaler-downscale-stabilization**：两次缩容操作之间的最小间隔周期，默认5m **–horizontal-pod-autoscaler-cpu-initialization-period**：初始化延迟时间，在此期间内Pod的CPU指标将不生效，默认5m **–horizontal-pod-autoscaler-initial-readiness-delay**：用于设置Pod初始化时间，在此期间内内的Pod被认为未就绪不会被采集数据，默认30s **–horizontal-pod-autoscaler-tolerance**：HPA控制器能容忍的数据差异（浮点数，默认0.1），即当前指标与阈值的差异要在0.1之内，比如阈值设置的是CPU利率50%，如果当前CPU利用率为80%，那么80/50=1.6\u003e1.1，就会触发扩容；如果当前CPU利用率为40%，40/50=0.8\u003c0.9，就会触发缩容。大于1.1扩容，小于0.9缩容 ","date":"2023-02-16","objectID":"/posts/kubernetes/primary/kubernetes-13/:3:0","tags":["k8s进阶训练营"],"title":"HPA自动伸缩pod数量 (十三)","uri":"/posts/kubernetes/primary/kubernetes-13/"},{"categories":["Kubernetes"],"content":"3.1 HPA示例 下面使用HAP v1版本通过CPU指标实现Pod自动扩缩容。 3.1.1 自动缩容示例 先部署一个5副本的nginx deployment，再通过HPA实现缩容： apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deploy labels: app: nginx spec: replicas: 5 selector: matchExpressions: - {key: \"app\", operator: In, values: [\"nginx\"]} template: metadata: labels: app: nginx spec: containers: - name: nginx image: harbor.ceamg.com/pub-images/nginx-base:1.22.1 ports: - name: http containerPort: 80 resources: #如果要通过hpa实现pod的自动扩缩容，在必须对Pod设置资源限制，否则pod不会被hpa统计 requests: cpu: 500m memory: 512Mi limits: cpu: 1 memory: 1Giroot@master01[13:56:32]~/metrics/test #:kubectl get pod NAME READY STATUS RESTARTS AGE emptydirtest 1/1 Running 4 (6h40m ago) 16d net-test2 1/1 Running 10 (31h ago) 42d net-test3 1/1 Running 10 (31h ago) 42d net-test4 1/1 Running 10 (31h ago) 42d nginx-deploy-74d4966b8c-2qs8m 1/1 Running 0 33s nginx-deploy-74d4966b8c-7fcpl 1/1 Running 0 33s nginx-deploy-74d4966b8c-pn6nx 1/1 Running 0 33s nginx-deploy-74d4966b8c-qtdps 1/1 Running 0 33s nginx-deploy-74d4966b8c-rhgm4 1/1 Running 0 33shpa部署文件如下，在hpa中定义了Pod cpu利用率阈值为80%，最小副本数为3，最大副本数为10： apiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler metadata: name: pod-autoscaler-demo spec: minReplicas: 3 #最小副本数 maxReplicas: 10 #最大副本数 scaleTargetRef: #hpa监控的资源对象 apiVersion: apps/v1 kind: Deployment name: nginx-deploy targetCPUUtilizationPercentage: 80 #cpu利用率阈值创建完成后，查看hpa资源： root@master01[15:23:46]~/metrics/test #:kubectl get hpa -o wide NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE pod-autoscaler-demo Deployment/nginx-deploy 0%/80% 3 10 5 21s因为之前创建的nginx pod访问量较低，cpul利用率肯定不超过80%，所以等待一段时间就会触发缩容 因为在hpa中定义的最小副本数为3，所以缩容到3个Pod就不会缩容了 3.1.2 自动扩容示例 使用stress-ng镜像部署3个pod来测试自动扩容，stress-ng是一个压测工具 apiVersion: apps/v1 kind: Deployment metadata: name: stress-ng-deploy labels: app: stress-ng spec: replicas: 3 selector: matchExpressions: - {key: \"app\", operator: In, values: [\"stress-ng\"]} template: metadata: labels: app: stress-ng spec: containers: - name: stress-ng image: lorel/docker-stress-ng args: [\"--vm\", \"2\", \"--vm-bytes\", \"512M\"] resources: requests: cpu: 500m memory: 512Mi limits: cpu: 1 memory: 1Gi``` root@master01[16:34:20]~/metrics/test #:kubectl get pod NAME READY STATUS RESTARTS AGE stress-ng-deploy-5c9d6db588-dmwh8 1/1 Running 0 30s stress-ng-deploy-5c9d6db588-mfr7m 1/1 Running 0 30s stress-ng-deploy-5c9d6db588-vg82w 1/1 Running 0 30shpa部署文件如下： apiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler metadata: name: pod-autoscaler-demo1 spec: minReplicas: 3 maxReplicas: 10 scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: stress-ng-deploy targetCPUUtilizationPercentage: 80查看hpa资源： stress-ng会将Pod的cpu利用率打满，所以等待一段时间hpa就会逐步提高pod的副本数，如下图所示，但是在hpa中定义的最大副本数为10，所以最多扩容到10个Pod就不会扩容了 删除一直处于Terminating 状态的pod root@master01[16:54:36]~/metrics/test #:kubectl get pod NAME READY STATUS RESTARTS AGE emptydirtest 1/1 Terminating 4 (9h ago) 17d net-test2 1/1 Terminating 10 (34h ago) 43d net-test3 1/1 Terminating 10 (34h ago) 43d net-test4 1/1 Running 10 (35h ago) 43d nginx-deploy-74d4966b8c-2qs8m 1/1 Running 0 3h5m nginx-deploy-74d4966b8c-k28hh 1/1 Running 0 14m nginx-deploy-74d4966b8c-pn6nx 1/1 Terminating 0 3h5m nginx-deploy-74d4966b8c-rhgm4 1/1 Running 0 3h5m stress-ng-deploy-5c9d6db588-7q4hq 1/1 Terminating 0 24m stress-ng-deploy-5c9d6db588-dmwh8 1/1 Terminating 0 27m stress-ng-deploy-5c9d6db588-m952t 1/1 Terminating 0 24m stress-ng-deploy-5c9d6db588-mwbgd 1/1 Terminating 0 24m stress-ng-deploy-5c9d6db588-vg82w 1/1 Terminating 0 27m stress-ng-deploy-5c9d6db588-z8n2r 0/1 Terminating 0 24m tomcat-app1-6fd79cfbd4-8tg64 1/1 Terminating 0 2d tomcat-app1-6fd79cfbd4-sts9v 1/1 Running 0 14m tomcat-app2-54b548dfbf-zsgpd 1/1 Running 0 2droot@etcd01[17:03:05]~ #:/usr/local/bin/etcdctl get /registry/pods/default/stress --prefix --keys-only /registry/pods/default/stress-ng-deploy-5c9d6db588-7q4hq /registry/pods/default/stress-ng-deploy-5c9d6db588-dmwh8 /registry/pods/default/stress-ng-deploy-5c9d6db588-m952t /registry/pods/default/stress-ng-deploy-5c9d6db588-m","date":"2023-02-16","objectID":"/posts/kubernetes/primary/kubernetes-13/:3:1","tags":["k8s进阶训练营"],"title":"HPA自动伸缩pod数量 (十三)","uri":"/posts/kubernetes/primary/kubernetes-13/"},{"categories":["Kubernetes"],"content":" Author: Ryan title: 12.ingress 实现基于域名的多虚拟主机,URL转发及多域名HTTPS实现案例 tag: - k8s进阶训练营 category: k8s date: 2022-6-12 12:12:22 lastUpdated: true #sidebar: false breadcrumb: false #contributors: false ","date":"2023-02-15","objectID":"/posts/kubernetes/primary/kubernetes-12/:0:0","tags":["k8s进阶训练营"],"title":"实战案例-基于zookeeper实现微服务动态注册和发现 (十二)","uri":"/posts/kubernetes/primary/kubernetes-12/"},{"categories":["Kubernetes"],"content":"Ingress和Ingress控制器介绍 在k8s中将一个服务暴露出去通常会使用NodePort或LoadBalancer类型的Service，但随着服务数量的增多，使用NodePort会存在一些问题，可用作NodePort的端口是一个有限的范围，不容易记忆，不好管理。另外， 如果在公有云使用LoadBalancer类型的Service上会产生额外的成本。 所以k8s提供了另一种方式，使用Ingress和Ingress控制器来对外暴露服务，Ingress控制器作为统一的流量入口，管理内部各种必要的服务，并通过Ingress资源来描述如何区分流量及内部的路由逻辑。有了Ingress和Ingress控制器，就可以通过定义路由流量的规则来实现服务发布，而无需创建NodePort或LoadBalancer类型的Service，并且流量也会由Ingress控制器直达Pod，不需要再由Service转发。 :::info Ingress资源就是基于HTTP虚拟主机或URL路径的流量转发规则（类似于nginx中的虚拟主机定义或location转发规则定义），它把需要暴露给集群外的每个Service对象，映射为Ingress控制器上的一个虚拟主机或某虚拟主机的一个URL路径。 ::: 如下图所示： Ingress官方文档：https://kubernetes.io/zh-cn/docs/concepts/services-networking/ingress/Ingress控制器官方文档：https://kubernetes.io/zh-cn/docs/concepts/services-networking/ingress-controllers/ ","date":"2023-02-15","objectID":"/posts/kubernetes/primary/kubernetes-12/:1:0","tags":["k8s进阶训练营"],"title":"实战案例-基于zookeeper实现微服务动态注册和发现 (十二)","uri":"/posts/kubernetes/primary/kubernetes-12/"},{"categories":["Kubernetes"],"content":"Ingress控制器 但Ingress资源本身只是一组路由规则定义，这些规则想要真正的生效还需要借助其它功能的辅助，例如监听某套接字、根据路由规则匹配机制将客户端请求进行转发等。实现这些功能的组件就是Ingress控制器(Ingress Controller)。Ingress Controller是Kubernetes的一个附件需要单独部署。 ","date":"2023-02-15","objectID":"/posts/kubernetes/primary/kubernetes-12/:1:1","tags":["k8s进阶训练营"],"title":"实战案例-基于zookeeper实现微服务动态注册和发现 (十二)","uri":"/posts/kubernetes/primary/kubernetes-12/"},{"categories":["Kubernetes"],"content":"Ingress Controller部署 目前可选择使用的Ingress控制器有很多，可以参考官方介绍：https://kubernetes.io/zh-cn/docs/concepts/services-networking/ingress-controllers/ ，下面以nginx Ingress控制器为例进行部署。 nginx Ingress控制器github地址：https://github.com/kubernetes/ingress-nginxnginx Ingress控制器官方文档：https://kubernetes.github.io/ingress-nginx/ **常用的的Ingress控制器部署方式有两种： ** ","date":"2023-02-15","objectID":"/posts/kubernetes/primary/kubernetes-12/:2:0","tags":["k8s进阶训练营"],"title":"实战案例-基于zookeeper实现微服务动态注册和发现 (十二)","uri":"/posts/kubernetes/primary/kubernetes-12/"},{"categories":["Kubernetes"],"content":"1. 以Deployment方式部署Ingress控制器Pod资源 通过NodePort或LoadBalancer类型的Service或者通过拥有外部IP地址（externalIP）的Service对象为其接入集群外部的客户端请求流量。这意味着，在生产环境以这种方式部署一个Ingress控制器时，必须在其前端定义一个负载均衡器，这个负载均衡器可以是LoadBalancer类型的Service，也可以是用户自行管理的负载均衡器。 ","date":"2023-02-15","objectID":"/posts/kubernetes/primary/kubernetes-12/:2:1","tags":["k8s进阶训练营"],"title":"实战案例-基于zookeeper实现微服务动态注册和发现 (十二)","uri":"/posts/kubernetes/primary/kubernetes-12/"},{"categories":["Kubernetes"],"content":"2.以DaemonSet方式部署Ingress控制器 Pod资源Ingress控制器的各Pod分别以单一实例的方式运行在集群的所有节点或部分专用节点之上，并配置这些Pod对象以hostPort或hostNetwork的方式在当前节点接入外部流量。在这种方式下，前端还是需要一个负载均衡器，作为客户端流量的统一入口，然后转发给Ingress控制器Pod 在nginx Ingress控制器官方提供的部署文件中，默认使用第一种方式，使用Deployment+NodePort Service来部署。 ","date":"2023-02-15","objectID":"/posts/kubernetes/primary/kubernetes-12/:2:2","tags":["k8s进阶训练营"],"title":"实战案例-基于zookeeper实现微服务动态注册和发现 (十二)","uri":"/posts/kubernetes/primary/kubernetes-12/"},{"categories":["Kubernetes"],"content":"3. Deployment方式部署 选定好版本，下载对应的部署文件 wget https://github.com/kubernetes/ingress-nginx/archive/refs/tag/controller-v1.3.1.tar.gz tar xvf controller-v1.3.1.tar.gz cd ingress-nginx-controller-v1.3.1/deploy/static/provider/baremetal/ #修改当前目录下的deploy.yaml，将镜像修改未国内镜像源 cat deploy.yaml |grep image image: registry.cn-hangzhou.aliyuncs.com/google_containers/nginx-ingress-controller:v1.3.1 imagePullPolicy: IfNotPresent image: registry.cn-hangzhou.aliyuncs.com/google_containers/kube-webhook-certgen:v1.3.0 imagePullPolicy: IfNotPresent image: registry.cn-hangzhou.aliyuncs.com/google_containers/kube-webhook-certgen:v1.3.0 imagePullPolicy: IfNotPresent修改nodeport地址 apiVersion: v1 kind: Service metadata: labels: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.3.1 name: ingress-nginx-controller namespace: ingress-nginx spec: ipFamilies: - IPv4 ipFamilyPolicy: SingleStack ports: - appProtocol: http name: http port: 80 protocol: TCP targetPort: http nodePort: 30020 - appProtocol: https name: https port: 443 protocol: TCP targetPort: https nodePort: 30021安装 kubectl apply -f deploy.yaml amespace/ingress-nginx created serviceaccount/ingress-nginx created serviceaccount/ingress-nginx-admission created role.rbac.authorization.k8s.io/ingress-nginx created role.rbac.authorization.k8s.io/ingress-nginx-admission created clusterrole.rbac.authorization.k8s.io/ingress-nginx created clusterrole.rbac.authorization.k8s.io/ingress-nginx-admission created rolebinding.rbac.authorization.k8s.io/ingress-nginx created rolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx created clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created configmap/ingress-nginx-controller created service/ingress-nginx-controller created service/ingress-nginx-controller-admission created deployment.apps/ingress-nginx-controller created job.batch/ingress-nginx-admission-create created job.batch/ingress-nginx-admission-patch created ingressclass.networking.k8s.io/nginx created validatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission created查看状态 NAME READY STATUS RESTARTS AGE ingress-nginx-admission-create-cnccm 0/1 Completed 0 21s ingress-nginx-admission-patch-j4l68 0/1 Completed 0 21s ingress-nginx-controller-79658555f4-r2pz5 0/1 Running 0 21sroot@master01[14:23:04]~ #:kubectl get service -n ingress-nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-nginx-controller NodePort 10.10.79.136 \u003cnone\u003e 80:30020/TCP,443:30021/TCP 74m ingress-nginx-controller-admission ClusterIP 10.10.133.115 \u003cnone\u003e 443/TCP 74m默认情况下，ingress-nginx-controller只有一个副本，可以按需调整 root@master01[14:54:09]~ #:kubectl scale -n ingress-nginx deployment ingress-nginx-controller --replicas=3 root@master01[14:54:09]~ #:kubectl get pod -n ingress-nginx NAME READY STATUS RESTARTS AGE ingress-nginx-admission-create-cnccm 0/1 Completed 0 105m ingress-nginx-admission-patch-j4l68 0/1 Completed 0 105m ingress-nginx-controller-79658555f4-gd2zj 1/1 Running 0 22s ingress-nginx-controller-79658555f4-r2pz5 1/1 Running 0 105m ingress-nginx-controller-79658555f4-r6l8s 0/1 Running 0 22skubectl scale -n ingress-nginx deployment ingress-nginx-controller --replicas=3 root@master01[15:44:40]~ #:kubectl get pod -n ingress-nginx NAME READY STATUS RESTARTS AGE ingress-nginx-admission-create-cnccm 0/1 Completed 0 6d2h ingress-nginx-admission-patch-j4l68 0/1 Completed 0 6d2h ingress-nginx-controller-79658555f4-gd2zj 1/1 Running 0 6d ingress-nginx-controller-79658555f4-r2pz5 1/1 Running 0 6d2h ingress-nginx-controller-79658555f4-r6l8s 1/1 Running 0 6d在负载均衡器中添加ingress-nginx-controller后端，以haproxy为例 cat /etc/haproxy/harpoxy.cfg ################################# listen ingress-nginx-controller-80 bind 10.1.0.6:80 option tcplog mode tcp balance source server ingress-controller-server1 10.1.0.31:300","date":"2023-02-15","objectID":"/posts/kubernetes/primary/kubernetes-12/:2:3","tags":["k8s进阶训练营"],"title":"实战案例-基于zookeeper实现微服务动态注册和发现 (十二)","uri":"/posts/kubernetes/primary/kubernetes-12/"},{"categories":["Kubernetes"],"content":"4.DaemonSet方式部署 对前面的deploy.yaml进行修改，主要修改3个配置 删除掉ingress-ingress-controller Service资源定义 将Deployment修改未DaemonSet 配置Pod使用hostNetwork和hostPID apiVersion: apps/v1 kind: DaemonSet #类型修改为DaemonSet metadata: labels: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.3.1 name: ingress-nginx-controller namespace: ingress-nginx spec: minReadySeconds: 0 revisionHistoryLimit: 10 selector: matchLabels: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx template: metadata: labels: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx spec: hostPID: true #Pod使用主机PID名称空间 hostNetwork: true #Pod使用主机网络 containers:同样的，也需要在负载均衡器中添加ingress-nginx-controller后端，以haproxy为例 cat /etc/haproxy/harpoxy.cfg ################################# listen ingress-nginx-controller-80 bind 10.1.0.6:80 option tcplog mode tcp balance source server ingress-controller-server1 10.1.0.31:30020 check inter 2000 fall 3 rise 5 server ingress-controller-server2 10.1.0.32:30020 check inter 2000 fall 3 rise 5 listen ingress-nginx-controller-443 bind 10.1.0.6:443 option tcplog mode tcp balance source server ingress-controller-server1 10.1.0.31:30021 check inter 2000 fall 3 rise 5 server ingress-controller-server2 10.1.0.32:30021 check inter 2000 fall 3 rise 5","date":"2023-02-15","objectID":"/posts/kubernetes/primary/kubernetes-12/:2:4","tags":["k8s进阶训练营"],"title":"实战案例-基于zookeeper实现微服务动态注册和发现 (十二)","uri":"/posts/kubernetes/primary/kubernetes-12/"},{"categories":["Kubernetes"],"content":"3.Ingress示例 ","date":"2023-02-15","objectID":"/posts/kubernetes/primary/kubernetes-12/:3:0","tags":["k8s进阶训练营"],"title":"实战案例-基于zookeeper实现微服务动态注册和发现 (十二)","uri":"/posts/kubernetes/primary/kubernetes-12/"},{"categories":["Kubernetes"],"content":"3.1 Ingress资源规范 Ingress资源的可用字段和含义如下： apiVersion: networkking.k8s.io/v1 kind: Ingress metadata: name: ... namespace: ... annotations: #资源注解 kubernetes.io/ingress.class: \u003cstring\u003e #指明此Ingress资源由哪个Ingress控制器来解析，目前也可以使用spec.ingressClassName字段代替 spec: rules: #Ingress路由规则列表 - host: \u003cstring\u003e #虚拟主机的域名，支持*前缀匹配，但不支持IP，不支持端口 http: paths: #虚拟主机的PATH路径列表，由path和backend组成 - path: \u003cstring\u003e #流量匹配的HTTP URL路径，必须以/开头 pathType: \u003cstring\u003e #URL路径匹配方式，支持Exact(精准匹配)、Prefix(前缀匹配)和ImplementationSpecific，详细介绍可以参考官网文档 backend: #匹配到的流量要转发到的后端定义 service: #后端关联的Service对象定义 name: \u003cstring\u003e #Service对象名称 port: #Service对象端口 number: \u003cint\u003e #端口号 name: \u003cstring\u003e #端口名称 tls: #tls配置，用于指定上边rules字段下哪些host需要使用https - hosts: \u003c[]string\u003e #使用同一组证书的主机名称列表 secretName: \u003cstring\u003e #保存证书的Secret资源名称 defaultBackend: \u003cObject\u003e #默认后端定义，可嵌套使用字段与上面的backend字段相同 ingressClassName: \u003cstring\u003e #ingressClass资源名称，作用类似于上面的注解信息，用于指定适配的Ingress控制器","date":"2023-02-15","objectID":"/posts/kubernetes/primary/kubernetes-12/:3:1","tags":["k8s进阶训练营"],"title":"实战案例-基于zookeeper实现微服务动态注册和发现 (十二)","uri":"/posts/kubernetes/primary/kubernetes-12/"},{"categories":["Kubernetes"],"content":"3.2 单域名访问示例 先创建一下资源，用于测试Ingress功能，部署文件如下，包含两个tomcat-pod和两个对应的Svc apiVersion: apps/v1 kind: Deployment metadata: name: tomcat-app1 spec: replicas: 1 selector: matchLabels: app: tomcat-app1 template: metadata: labels: app: tomcat-app1 spec: containers: - name: tomcat image: harbor.ceamg.com/xinweb11/tomcat-app1:1.9 ports: - name: http containerPort: 8080 --- apiVersion: v1 kind: Service metadata: name: tomcat-app1-svc spec: selector: app: tomcat-app1 ports: - name: http port: 8080 targetPort: 8080 protocol: TCP --- apiVersion: apps/v1 kind: Deployment metadata: name: tomcat-app2 spec: replicas: 1 selector: matchLabels: app: tomcat-app2 template: metadata: labels: app: tomcat-app2 spec: containers: - name: tomcat image: harbor.ceamg.com/xinweb11/tomcat-app1:1.9 ports: - name: http containerPort: 8080 --- apiVersion: v1 kind: Service metadata: name: tomcat-app2-svc spec: selector: app: tomcat-app2 ports: - name: http port: 8080 targetPort: 8080 protocol: TCPpod创建成功后 到里面创建一个测试页面 --------------------------------tomcat-app1--------------------------------------------------- root@tomcat-app1-6fd79cfbd4-8tg64:/data/tomcat/webapps/myapp1# echo \"tomcat app1 ingress\" \u003e index.jsp root@tomcat-app1-6fd79cfbd4-8tg64:/data/tomcat/webapps/myapp1# ls index.jsp root@master01[17:13:13]~/ingress-test #:curl 10.10.103.87:8080/myapp1/ tomcat app1 ingress --------------------------------tomcat-app2--------------------------------------------------- root@tomcat-app2-54b548dfbf-zsgpd:/apps/tomcat/bin# mkdir /data/tomcat/webapps/myapp2 root@tomcat-app2-54b548dfbf-zsgpd:/apps/tomcat/bin# echo \"tomcat app2 ingress\" \u003e /data/tomcat/webapps/myapp2/index.jsp root@tomcat-app2-54b548dfbf-zsgpd:/apps/tomcat/bin# ./catalina.sh stop root@tomcat-app2-54b548dfbf-zsgpd:/apps/tomcat/bin# ./catalina.sh start root@master01[17:19:14]~/ingress-test #:curl 10.10.219.108:8080/myapp2/ tomcat app2 ingress3.2.1 单域名 ingress 资源 将访问www.app1.com 域名的流量转发至tomcat-app1-svc，Ingress部署文件如下： apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress-tomcat-app1 annotations: kubernetes.io/ingress.class: \"nginx\" #指定由哪个Ingress Controller解析 nginx.ingress.kubernetes.io/use-regex: \"true\" ##指定后面rules定义的path可以使用正则表达式 nginx.ingress.kubernetes.io/proxy-connect-timeout: \"600\" ##连接超时时间,默认为5s nginx.ingress.kubernetes.io/proxy-send-timeout: \"600\" ##后端服务器回转数据超时时间,默认为60s nginx.ingress.kubernetes.io/proxy-read-timeout: \"600\" ##后端服务器响应超时时间,默认为60s nginx.ingress.kubernetes.io/proxy-body-size: \"50m\" ##客户端上传文件，最大大小，默认为20m #nginx.ingress.kubernetes.io/rewrite-target: / ##URL重写 nginx.ingress.kubernetes.io/app-root: /index.html spec: rules: - host: www.app1.com http: paths: - path: / pathType: Prefix backend: service: name: tomcat-app1-svc port: number: 80803.2.2 多域名 ingress 资源 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress-tomcat-app annotations: kubernetes.io/ingress.class: \"nginx\" nginx.ingress.kubernetes.io/use-regex: \"true\" nginx.ingress.kubernetes.io/proxy-connect-timeout: \"600\" nginx.ingress.kubernetes.io/proxy-send-timeout: \"600\" nginx.ingress.kubernetes.io/proxy-read-timeout: \"600\" nginx.ingress.kubernetes.io/proxy-body-size: \"50m\" #nginx.ingress.kubernetes.io/rewrite-target: / #nginx.ingress.kubernetes.io/app-root: /index.html spec: rules: - host: www.myapp1.com http: paths: - path: / pathType: Prefix backend: service: name: tomcat-app1-svc port: number: 8080 - host: www.myapp2.com http: paths: - path: / pathType: Prefix backend: service: name: tomcat-app2-svc port: number: 8080root@master01[17:24:26]~/ingress-test #:kubectl apply -f tomcat-app-ingress1-ingress.yaml ingress.networking.k8s.io/ingress-tomcat-app1 created root@master01[17:24:34]~/ingress-test #:kubectl get ingress NAME CLASS HOSTS ADDRESS PORTS AGE ingress-tomcat-app \u003cnone\u003e www.myapp1.com,www.myapp2.com 10.1.0.32,10.1.0.33 80 4m34s3.2.3 配置Haproxy+ keepalived 实现负载均衡 apt install keepalived haproxy -ykeepalived 配置文件如下： ##################################Master################################## global_defs { n","date":"2023-02-15","objectID":"/posts/kubernetes/primary/kubernetes-12/:3:2","tags":["k8s进阶训练营"],"title":"实战案例-基于zookeeper实现微服务动态注册和发现 (十二)","uri":"/posts/kubernetes/primary/kubernetes-12/"},{"categories":["Kubernetes"],"content":"3.3 Ingress 配置TLS 3.3.1 创建证书 首先准备www.myapp1.com 域名的证书，然后将证书保存为Secret mkdir ingress-cert \u0026\u0026 cd ingress-cert openssl genrsa -out ca.key 4096 openssl req -x509 -new -nodes -sha512 -days 100 \\ -subj \"/C=CN/ST=Beijing/L=Beijing/O=example/OU=Personal/CN=www.myapp1.com\" \\ -key ca.key \\ -out ca.crt openssl genrsa -out www.myapp1.com.key 4096 openssl req -sha512 -new \\ -subj \"/C=CN/ST=Beijing/L=Beijing/O=example/OU=Personal/CN=www.myapp1.com\" \\ -key www.myapp1.com.key \\ -out www.myapp1.com.csr openssl x509 -req -sha512 -days 3650 \\ -CA ca.crt -CAkey ca.key -CAcreateserial \\ -in www.myapp1.com.csr \\ -out www.myapp1.com.crt rm -f www.myapp1.com.csr kubectl create secret tls cert-www.myapp1.com --cert ./www.myapp1.com.crt --key ./www.myapp1.com.key查看Secret root@master01[14:41:14]~/ingress-cert #:kubectl describe secrets cert-www.myapp1.com Name: cert-www.myapp1.com Namespace: default Labels: \u003cnone\u003e Annotations: \u003cnone\u003e Type: kubernetes.io/tls Data ==== tls.crt: 1931 bytes tls.key: 3243 bytes3.3.2 Ingress使用TLS证书示例 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress-tls-url annotations: kubernetes.io/ingress.class: \"nginx\" nginx.ingress.kubernetes.io/use-regex: \"true\" nginx.ingress.kubernetes.io/proxy-connect-timeout: \"600\" nginx.ingress.kubernetes.io/proxy-send-timeout: \"600\" nginx.ingress.kubernetes.io/proxy-read-timeout: \"600\" nginx.ingress.kubernetes.io/proxy-body-size: \"50m\" #nginx.ingress.kubernetes.io/rewrite-target: / nginx.ingress.kubernetes.io/app-root: /index.html spec: rules: - host: www.myapp1.com http: paths: - path: /tls1 pathType: Prefix backend: service: name: tomcat-app1-svc port: number: 8080 - path: /tls2 pathType: Prefix backend: service: name: tomcat-app2-svc port: number: 8080 tls: #如果多个域名都使用https，再添加一个列表项即可 - hosts: [\"www.myapp1.com\"] #如果多个域名使用相同的证书，在这里的列表添加一个域名即可 secretName: cert-www.myapp1.com3.3.3 pod 中创建用于测试的目录和页面 mkdir /data/tomcat/webapps/tls1 echo \"www.myapp1.com/tls1/index.html\" \u003e /data/tomcat/webapps/tls1/index.html mkdir /data/tomcat/webapps/tls2 echo \"www.myapp1.com/tls2/index.html\" \u003e /data/tomcat/webapps/tls2/index.html3.3.4 访问测试 ","date":"2023-02-15","objectID":"/posts/kubernetes/primary/kubernetes-12/:3:3","tags":["k8s进阶训练营"],"title":"实战案例-基于zookeeper实现微服务动态注册和发现 (十二)","uri":"/posts/kubernetes/primary/kubernetes-12/"},{"categories":["Kubernetes"],"content":"3.4 证书更新 假设网站的https证书即将过期，在不影响业务的前提下，可以直接更新其引用的Secret中保存的证书来实现网站https证书更新。在生产环境需要提前做好计划，并选择合适时间执行。 首先重新签发一套证书 openssl genrsa -out www.myapp1.com-new.key 4096 openssl req -sha512 -new \\ -subj \"/C=CN/ST=Beijing/L=Beijing/O=example/OU=Personal/CN=www.linux.io\" \\ -key www.myapp1.com-new.key \\ -out www.myapp1.com-new.csr openssl x509 -req -sha512 -days 100 \\ -CA ca.crt -CAkey ca.key -CAcreateserial \\ -in www.myapp1.com-new.csr \\ -out www.myapp1.com-new.crt rm -f www.myapp1.com-new.csr将证书和key的内容进行base64编码，然后编辑相应的Secret对象，修改tls.key和tls.crt的值为编码后的内容 root@master-01:~/resources/ingress-cert# base64 www.myapp1.com-new.key -w 0 LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktLS.............ZLS0tLS0K root@master-01:~/resources/ingress-cert# base64 www.myapp1.com-new.crt -w 0 LS0tLS1CRUdJTiBDRVJ..............FTkQgQ0VSVElGSUNBVEUtLS0tLQo= root@master-01:~# kubectl edit secret/cert-www.myapp1.com 和之前的访问结果进行对比，可以看到证书已经被更新 ","date":"2023-02-15","objectID":"/posts/kubernetes/primary/kubernetes-12/:3:4","tags":["k8s进阶训练营"],"title":"实战案例-基于zookeeper实现微服务动态注册和发现 (十二)","uri":"/posts/kubernetes/primary/kubernetes-12/"},{"categories":["HAProxy"],"content":"六、HAProxy https实现 #配置HAProxy支持https协议，支持ssl会话； bind *:443 ssl crt /PATH/TO/SOME_PEM_FILE #crt 后证书文件为PEM格式，且同时包含证书和所有私钥 cat demo.crt demo.key \u003e demo.pem #把80端口的请求重向定443 bind *:80 redirect scheme https if !{ ssl_fc } #向后端传递用户请求的协议和端口（frontend或backend） http_request set-header X-Forwarded-Port %[dst_port] http_request add-header X-Forwared-Proto https if { ssl_fc }","date":"2023-02-07","objectID":"/posts/haproxy/haproxy-4/:1:0","tags":["负载均衡"],"title":"HAProxy-https实现（四）","uri":"/posts/haproxy/haproxy-4/"},{"categories":["HAProxy"],"content":"6.1 证书制作 #方法1 [root@centos7 ~]mkdir /etc/haproxy/certs/ [root@centos7 ~]cd /etc/haproxy/certs/ [root@centos7 certs]#openssl genrsa -out haproxy.key 2048 [root@centos7 certs]#openssl req -new -x509 -key haproxy.key -out haproxy.crt -subj \"/CN=www.xinblog.org\" #或者用下一条命令实现 [root@centos7 certs]#openssl req -x509 -newkey rsa:2048 -subj \"/CN=www.magedu.org\" -keyout haproxy.key -nodes -days 365 -out haproxy.crt [root@centos7 certs]#cat haproxy.key haproxy.crt \u003e haproxy.pem [root@centos7 certs]#openssl x509 -in haproxy.pem -noout -text #查看证书","date":"2023-02-07","objectID":"/posts/haproxy/haproxy-4/:1:1","tags":["负载均衡"],"title":"HAProxy-https实现（四）","uri":"/posts/haproxy/haproxy-4/"},{"categories":["HAProxy"],"content":"6.2 https配置示例 [root@centos7 ~]#cat /etc/haproxy/conf.d/test.cfg frontend magedu_http_port bind 10.0.0.7:80 bind 10.0.0.7:443 ssl crt /etc/haproxy/certs/haproxy.pem redirect scheme https if !{ ssl_fc } # 注意{ }内的空格 http-request set-header X-forwarded-Port %[dst_port] http-request add-header X-forwarded-Proto https if { ssl_fc } mode http balance roundrobin log global option httplog ###################### acl setting ############################### acl mobile_domain hdr_dom(host) -i mobile.magedu.org ###################### acl hosts ################################# default_backend pc_hosts ################### backend hosts ################################# backend mobile_hosts mode http server web1 10.0.0.17:80 check inter 2000 fall 3 rise 5 backend pc_hosts mode http #http-request set-header X-forwarded-Port %[dst_port] 也可加在此处 #http-request add-header X-forwarded-Proto https if { ssl_fc } server web2 10.0.0.27:80 check inter 2000 fall 3 rise 5 [root@centos7 ~]#ss -ntl State Recv-Q Send-Q Local Address:Port Peer Address:Port LISTEN 0 100 127.0.0.1:25 *:* LISTEN 0 128 10.0.0.7:443 *:* LISTEN 0 128 *:9999 *:* LISTEN 0 128 10.0.0.7:80 *:* LISTEN 0 128 *:22 *:* LISTEN 0 128 [::]:22 [::]:* global maxconn 100000 chroot /var/lib/haproxy stats socket /var/lib/haproxy/haproxy.sock mode 600 level admin #uid 99 #gid 99 user haproxy group haproxy daemon #nbproc 4 #cpu-map 1 0 #cpu-map 2 1 #cpu-map 3 2 #cpu-map 4 3 pidfile /var/lib/haproxy/haproxy.pid log 127.0.0.1 local2 info defaults option http-keep-alive maxconn 100000 option forwardfor mode http timeout connect 300000ms timeout client 300000ms timeout server 300000ms listen stats mode http bind 0.0.0.0:9999 stats enable log global stats uri /haproxy-status stats auth haadmin:123456 listen http_80 mode http bind 10.1.0.6:30013 bind 10.1.0.6:443 ssl crt /etc/haproxy/certs/haproxy.pem redirect scheme https if !{ ssl_fc } http-request set-header X-forwarded-Port %[dst_port] http-request add-header X-forwarded-Proto https if { ssl_fc } balance roundrobin log global option forwardfor server web1 10.1.0.31:30013 check inter 2000 fall 3 rise 5 server web2 10.1.0.32:30013 check inter 2000 fall 3 rise 5 ","date":"2023-02-07","objectID":"/posts/haproxy/haproxy-4/:1:2","tags":["负载均衡"],"title":"HAProxy-https实现（四）","uri":"/posts/haproxy/haproxy-4/"},{"categories":["HAProxy"],"content":"五、HAProxy调度算法 HAProxy通过固定参数 **balance ** 指明对后端服务器的调度算法，该参数可以配置在listen或backend选项中。 HAProxy的调度算法分为静态和动态调度算法，但是有些算法可以根据参数在静态和动态算法中相互转换。 官方文档：http://cbonte.github.io/haproxy-dconv/2.6/configuration.html#balance ","date":"2023-02-06","objectID":"/posts/haproxy/haproxy-3/:1:0","tags":["负载均衡"],"title":"HAProxy-调度算法(三)","uri":"/posts/haproxy/haproxy-3/"},{"categories":["HAProxy"],"content":"5.1 静态算法 :::info ** 静态算法**：按照事先定义好的规则轮询公平调度，不关心后端服务器的当前负载、链接数和响应速度等，且无法实时修改权重，只能靠重启HAProxy生效。 ::: 可以利用 socat工具对服务器动态权重和其它状态的调整，Socat 是 Linux 下的一个多功能的网络工具，名字来由是Socket CAT，Socat 的主要特点就是在两个数据流之间建立通道，且支持众多协议和链接方式。如 IP、TCP、 UDP、IPv6、Socket文件等 **利用工具socat 对服务器动态权重调整 ** [root@centos7 ~]#yum -y install socat [root@centos7 ~]#echo \"show info\" | socat stdio /var/lib/haproxy/haproxy.sock Name: HAProxy Version: 2.1.3 Release_date: 2020/02/12 Nbthread: 4 Nbproc: 1 Process_num: 1 Pid: 2279 Uptime: 0d 0h46m07s Uptime_sec: 2767 Memmax_MB: 0 PoolAlloc_MB: 0 PoolUsed_MB: 0 PoolFailed: 0 Ulimit-n: 200041 Maxsock: 200041 Maxconn: 100000 Hard_maxconn: 100000 CurrConns: 0 CumConns: 1 CumReq: 1 MaxSslConns: 0 CurrSslConns: 0 CumSslConns: 0 Maxpipes: 0 PipesUsed: 0 PipesFree: 0 ConnRate: 0 ConnRateLimit: 0 MaxConnRate: 0 SessRate: 0 SessRateLimit: 0 MaxSessRate: 0 SslRate: 0 SslRateLimit: 0 MaxSslRate: 0 SslFrontendKeyRate: 0 SslFrontendMaxKeyRate: 0 SslFrontendSessionReuse_pct: 0 SslBackendKeyRate: 0 SslBackendMaxKeyRate: 0 SslCacheLookups: 0 SslCacheMisses: 0 CompressBpsIn: 0 CompressBpsOut: 0 CompressBpsRateLim: 0 ZlibMemUsage: 0 MaxZlibMemUsage: 0 Tasks: 19 Run_queue: 1 Idle_pct: 100 node: centos7.wangxiaochun.com Stopping: 0 Jobs: 7 Unstoppable Jobs: 0 Listeners: 6 ActivePeers: 0 ConnectedPeers: 0 DroppedLogs: 0 BusyPolling: 0 FailedResolutions: 0 TotalBytesOut: 0 BytesOutRate: 0 DebugCommandsIssued: 0 [root@centos7 ~]#echo \"show servers state\" | socat stdio /var/lib/haproxy/haproxy.sock1 # be_id be_name srv_id srv_name srv_addr srv_op_state srv_admin_state srv_uweight srv_iweight srv_time_since_last_change srv_check_status srv_check_result srv_check_health srv_check_state srv_agent_state bk_f_forced_id srv_f_forced_id srv_fqdn srv_port srvrecord 2 magedu-test-80 1 web1 10.0.0.17 2 0 2 1 812 6 3 7 6 0 0 0 - 80 - 2 magedu-test-80 2 web2 10.0.0.27 2 0 2 3 812 6 3 4 6 0 0 0 - 80 - 4 web_port 1 web1 127.0.0.1 0 0 1 1 810 8 2 0 6 0 0 0 - 8080 - [root@centos7 ~]#echo \"get weight magedu-test-80/web2\" | socat stdio /var/lib/haproxy/haproxy.sock 3 (initial 3) #修改weight，注意只针对单进程有效 [root@centos7 ~]#echo \"set weight magedu-test-80/web2 2\" | socat stdio /var/lib/haproxy/haproxy.sock [root@centos7 ~]#echo \"get weight magedu-test-80/web2\" | socat stdio /var/lib/haproxy/haproxy.sock 2 (initial 3) #将后端服务器禁用，注意只针对单进程有效 [root@centos7 ~]#echo \"disable server magedu-test-80/web2\" | socat stdio /var/lib/haproxy/haproxy.sock #将后端服务器软下线，即weight设为0 [root@centos7 ~]#echo \"set weight magedu-test-80/web1 0\" | socat stdio /var/lib/haproxy/haproxy.sock #将后端服务器禁用，针对多进程 [root@centos7 ~]#vim /etc/haproxy/haproxy.cfg ...... stats socket /var/lib/haproxy/haproxy1.sock mode 600 level admin process 1 stats socket /var/lib/haproxy/haproxy2.sock mode 600 level admin process 2 nbproc 2 ..... [root@centos7 ~]#echo \"disable server magedu-test-80/web2\" | socat stdio /var/lib/haproxy/haproxy1.sock [root@centos7 ~]#echo \"disable server magedu-test-80/web2\" | socat stdio /var/lib/haproxy/haproxy2.sock [root@haproxy ~]#for i in {1..2};do echo \"set weight magedu-test-80/web$i 10\" | socat stdio /var/lib/haproxy/haproxy$i.sock;done #如果静态算法，如:static-rr，可以更改weight为0或1，但不支持动态更改weight为其它值，否则会提示下面信息 [root@centos7 ~]#echo \"set weight magedu-test-80/web1 0\" | socat stdio /var/lib/haproxy/haproxy.sock [root@centos7 ~]#echo \"set weight magedu-test-80/web1 1\" | socat stdio /var/lib/haproxy/haproxy.sock [root@centos7 ~]#echo \"set weight magedu-test-80/web1 2\" | socat stdio /var/lib/haproxy/haproxy.sock Backend is using a static LB algorithm and only accepts weights '0%' and '100%'.5.1.1 static-rr :::info static-rr：基于权重的轮询调度，不支持权重的运行时利用socat进行动态调整及后端服务器慢启动，其后端主机数量没有限制，相当于LVS中的 wrr ::: listen web_host bind 10.0.0.7:80,:8801-8810,10.0.0.7:9001-9010 mode http log global balance static-rr server web1 10.0.0.17:80 weight 1 check inter 3000 fall 2 rise 5 server web2 10.0.0.27:80 weight 2 check inter 3000 fall 2 rise 55.1.2 first :::warning first：根据服务器在列表中的位置，自上而下进行调度，但是其只会当第一台服务器的连接数达到上限，新请求才会分配给下一台服务，因此会忽略服务器的权","date":"2023-02-06","objectID":"/posts/haproxy/haproxy-3/:1:1","tags":["负载均衡"],"title":"HAProxy-调度算法(三)","uri":"/posts/haproxy/haproxy-3/"},{"categories":["HAProxy"],"content":"5.2 动态算法 :::success 动态算法：基于后端服务器状态进行调度适当调整，优先调度至当前负载较低的服务器，且权重可以在haproxy运行时动态调整无需重启。 ::: 5.2.1 roundrobin roundrobin：基于权重的轮询动态调度算法，支持权重的运行时调整，不同于lvs中的rr轮训模式，HAProxy中的roundrobin支持慢启动(新加的服务器会逐渐增加转发数)，其每个后端backend中最多支持4095个real server，支持对real server权重动态调整，roundrobin为默认调度算法。 listen web_host bind 10.0.0.7:80,:8801-8810,10.0.0.7:9001-9010 mode http log global balance roundrobin server web1 10.0.0.17:80 weight 1 check inter 3000 fall 2 rise 5 server web2 10.0.0.27:80 weight 2 check inter 3000 fall 2 rise 5支持动态调整权重: # echo \"get weight web_host/web1\" | socat stdio /var/lib/haproxy/haproxy.sock 1 (initial 1) # echo \"set weight web_host/web1 3\" | socat stdio /var/lib/haproxy/haproxy.sock # echo \"get weight web_host/web1\" | socat stdio /var/lib/haproxy/haproxy.sock 3 (initial 1)5.2.2 leastconn leastconn加权的最少连接的动态，支持权重的运行时调整和慢启动，即当前后端服务器连接最少的优先调度(新客户端连接)，比较适合长连接的场景使用，比如：MySQL等场景。 listen web_host bind 10.0.0.7:80,:8801-8810,10.0.0.7:9001-9010 mode http log global balance leastconn server web1 10.0.0.17:80 weight 1 check inter 3000 fall 2 rise 5 server web2 10.0.0.27:80 weight 1 check inter 3000 fall 2 rise 55.2.3 random 在1.9版本开始增加一个叫做random的负载平衡算法，其基于随机数作为一致性hash的key，随机负载平衡对于大型服务器场或经常添加或删除服务器非常有用，支持weight的动态调整，weight较大的主机有更大概率获取新请求。 listen web_host bind 10.0.0.7:80,:8801-8810,10.0.0.7:9001-9010 mode http log global balance random server web1 10.0.0.17:80 weight 1 check inter 3000 fall 2 rise 5 server web2 10.0.0.27:80 weight 1 check inter 3000 fall 2 rise 5","date":"2023-02-06","objectID":"/posts/haproxy/haproxy-3/:1:2","tags":["负载均衡"],"title":"HAProxy-调度算法(三)","uri":"/posts/haproxy/haproxy-3/"},{"categories":["HAProxy"],"content":"5.3 其他算法 其它算法即可作为静态算法，又可以通过选项成为动态算法 5.3.1 source 源地址hash，基于用户源地址hash并将请求转发到后端服务器，后续同一个源地址请求将被转发至同一个后端web服务器。 此方式当后端服务器数据量发生变化时，会导致很多用户的请求转发至新的后端服务器，默认为静态方式，但是可以通过hash-type支持的选项更改。 这个算法一般是在不插入Cookie的TCP模式下使用，也可给拒绝会话cookie的客户提供最好的会话粘性，适用于session会话保持但不支持cookie和缓存的场景 源地址有两种转发客户端请求到后端服务器的服务器选取计算方式，分别是取模法和一致性hash 5.3.2 map-base取模法 map-based：取模法，对source地址进行hash计算，再基于服务器总权重的取模，最终结果决定将此请求转发至对应的后端服务器。 此方法是静态的，即不支持在线调整权重，不支持慢启动，可实现对后端服务器均衡调度。 缺点是当服务器的总权重发生变化时，即有服务器上线或下线，都会因总权重发生变化而导致调度结果整体改变，hash-type 指定的默认值为此算法 。 所谓取模运算，就是计算两个数相除之后的余数，**10%7=3, 7%4=3 ** map-based算法：基于权重取模，hash(source_ip)%所有后端服务器相加的总权重 listen web_host bind 10.0.0.7:80,:8801-8810,10.0.0.7:9001-9010 mode tcp log global balance source hash-type map-based server web1 10.0.0.17:80 weight 1 check inter 3000 fall 2 rise 3 server web2 10.0.0.27:80 weight 1 check inter 3000 fall 2 rise 3 [root@haproxy ~]#echo \"set weight web_host/10.0.0.27 10\" | socat stdio /var/lib/haproxy/haproxy.sock Backend is using a static LB algorithm and only accepts weights '0%' and '100%'. [root@haproxy ~]#echo \"set weight web_host/10.0.0.27 0\" | socat stdio /var/lib/haproxy/haproxy.sock [root@haproxy conf.d]#echo \"get weight web_host/10.0.0.27\" | socat stdio /var/lib/haproxy/haproxy.sock 0 (initial 1)5.3.3 一致性hash 一致性哈希，当服务器的总权重发生变化时，对调度结果影响是局部的，不会引起大的变动，hash（o）mod n ，该hash算法是动态的，支持使用 socat等工具进行在线权重调整，支持慢启动 。 算法： 1、key1=hash(source_ip)%(2^32) [0---4294967295] 2、keyA=hash(后端服务器虚拟ip)%(2^32) 3、将key1和keyA都放在hash环上，将用户请求调度到离key1最近的keyA对应的后端服务器** hash环偏斜问题 ** 增加虚拟服务器IP数量，比如：一个后端服务器根据权重为1生成1000个虚拟IP，再hash。而后端服务器权重为2则生成2000的虚拟IP，再hash,最终在hash环上生成3000个节点，从而解决hash环偏斜问题hash对象 Hash对象到后端服务器的映射关系： 一致性hash示意图 后端服务器在线与离线的调度方式 一致性hash配置示例 listen web_host bind 10.0.0.7:80,:8801-8810,10.0.0.7:9001-9010 mode tcp log global balance source hash-type consistent server web1 10.0.0.17:80 weight 1 check inter 3000 fall 2 rise 5 server web2 10.0.0.27:80 weight 1 check inter 3000 fall 2 rise 5","date":"2023-02-06","objectID":"/posts/haproxy/haproxy-3/:1:3","tags":["负载均衡"],"title":"HAProxy-调度算法(三)","uri":"/posts/haproxy/haproxy-3/"},{"categories":["HAProxy"],"content":"四、基础配置详解 官方文档：http://cbonte.github.io/haproxy-dconv/2.6/configuration.html HAProxy 的配置文件haproxy.cfg由两大部分组成，分别是global和proxies部分 **global ：全局配置段 ** 进程及安全配置相关的参数 性能调整相关参数 Debug参数** proxies：代理配置段 ** defaults：为frontend, backend, listen提供默认配置 frontend：前端，相当于nginx中的server {} backend：后端，相当于nginx中的upstream {} listen：同时拥有前端和后端配置","date":"2023-02-05","objectID":"/posts/haproxy/haproxy-2/:1:0","tags":["负载均衡"],"title":"HAProxy-基础配置详解 （二）","uri":"/posts/haproxy/haproxy-2/"},{"categories":["HAProxy"],"content":"4.1 global配置 4.1.1 global 配置参数说明 官方文档：http://cbonte.github.io/haproxy-dconv/2.6/configuration.html chroot #锁定运行目录 deamon #以守护进程运行 stats socket /var/lib/haproxy/haproxy.sock mode 600 level admin process 1 #链接本机的socket文件，定义权限方便对负载进行动态调整 user, group, uid, gid #运行haproxy的用户身份 nbproc n #多进程模式，开启worker进程数，建议与cpu个数相同，默认为1。开启时就不在支持线程模式，没开启时，一个进程下面有多个线程 #nbthread 1 #指定每个haproxy进程开启的线程数，默认为每个进程一个线程,和nbproc互斥（版本有关） #如果同时启用nbproc和nbthread 会出现以下日志的错误，无法启动服务 Apr 7 14:46:23 haproxy haproxy: [ALERT] 097/144623 (1454) : config : cannot enable multiple processes if multiple threads are configured. Please use either nbproc or nbthread but not both. cpu-map 1 0 #绑定haproxy 进程至指定CPU，将第一个work进程绑定至0号CPU cpu-map 2 1 #绑定haproxy 进程至指定CPU，将第二个work进程绑定至1号CPU maxconn 100000 #每个haproxy进程的最大并发连接数 maxsslconn n #每个haproxy进程ssl最大连接数,用于haproxy配置了证书的场景下 maxconnrate n #每个进程每秒创建的最大连接数量 spread-checks n #后端server状态check随机提前或延迟百分比时间，建议2-5(20%-50%)之间，默认值0 pidfile #指定pid文件路径 log 127.0.0.1 local2 info #定义全局的syslog服务器；日志服务器需要开启UDP协议，最多可以定义两个4.4.2 多进程和线程 范例：多进程和socket文件 查看CPU核心数量 [root@localhost haproxy]# cat /proc/cpuinfo | grep \"cores\" | uniq cpu cores : 8[root@centos7 ~]#vim /etc/haproxy/haproxy.cfg global maxconn 100000 chroot /apps/haproxy stats socket /var/lib/haproxy/haproxy.sock1 mode 600 level admin process 1 stats socket /var/lib/haproxy/haproxy.sock2 mode 600 level admin process 2 uid 99 gid 99 daemon nbproc 2 [root@centos7 ~]#systemctl restart haproxy [root@centos7 ~]#pstree -p |grep haproxy |-haproxy(2634)-+-haproxy(2637) | `-haproxy(2638) [root@centos7 ~]#ll /var/lib/haproxy/ total 4 -rw-r--r-- 1 root root 5 Mar 31 18:49 haproxy.pid srw------- 1 root root 0 Mar 31 18:49 haproxy.sock1 srw------- 1 root root 0 Mar 31 18:49 haproxy.sock24.4.3 配置HAProxy记录日志到指定日志文件中 #在global配置项定义： log 127.0.0.1 local{1-7} info #基于syslog记录日志到指定设备，级别有(err、warning、info、debug) listen web_port bind 127.0.0.1:80 mode http log global #开启当前web_port的日志功能，默认不记录日志 server web1 127.0.0.1:8080 check inter 3000 fall 2 rise 5 # systemctl restart haproxyRsyslog配置 vim /etc/rsyslog.conf $ModLoad imudp $UDPServerRun 514 # Save boot messages also to boot.log local7.* /var/log/boot.log local5.* /var/log/haproxy.log local0.* /var/log/haproxy.log # systemctl restart rsyslog验证HAProxy日志 重启syslog服务并访问app页面，然后验证是否生成日志 [root@localhost log]# tail -f haproxy.log Feb 13 11:11:57 localhost haproxy[3127]: Connect from 172.16.32.242:64058 to 10.1.0.6:30013 (web_port/HTTP) Feb 13 11:11:57 localhost haproxy[3127]: Connect from 172.16.32.242:64058 to 10.1.0.6:30013 (web_port/HTTP) Feb 13 11:11:59 localhost haproxy[3127]: Connect from 172.16.32.242:64058 to 10.1.0.6:30013 (web_port/HTTP) Feb 13 11:11:59 localhost haproxy[3127]: Connect from 172.16.32.242:64058 to 10.1.0.6:30013 (web_port/HTTP) Feb 13 11:11:59 localhost haproxy[3127]: Connect from 172.16.32.242:64058 to 10.1.0.6:30013 (web_port/HTTP) Feb 13 11:11:59 localhost haproxy[3127]: Connect from 172.16.32.242:64058 to 10.1.0.6:30013 (web_port/HTTP) Feb 13 11:11:59 localhost haproxy[3127]: Connect from 172.16.32.242:64061 to 10.1.0.6:30013 (web_port/HTTP) Feb 13 11:11:59 localhost haproxy[3127]: Connect from 172.16.32.242:64061 to 10.1.0.6:30013 (web_port/HTTP) Feb 13 11:11:59 localhost haproxy[3127]: Connect from 172.16.32.242:64061 to 10.1.0.6:30013 (web_port/HTTP) Feb 13 11:11:59 localhost haproxy[3127]: Connect from 172.16.32.242:64061 to 10.1.0.6:30013 (web_port/HTTP)","date":"2023-02-05","objectID":"/posts/haproxy/haproxy-2/:1:1","tags":["负载均衡"],"title":"HAProxy-基础配置详解 （二）","uri":"/posts/haproxy/haproxy-2/"},{"categories":["HAProxy"],"content":"4.2 Proxies配置 官方文档：http://docs.haproxy.org/2.6/configuration.html#4 defaults [\u003cname\u003e] #默认配置项，针对以下的frontend、backend和listen生效，可以多个name也可以没有name frontend \u003cname\u003e #前端servername，类似于Nginx的一个虚拟主机 server和LVS服务集群。 backend \u003cname\u003e #后端服务器组，等于nginx的upstream和LVS中的RS服务器 listen \u003cname\u003e #将frontend和backend合并在一起配置，相对于frontend和backend配置更简洁，生产常用**注意：name字段只能使用大小写字母，数字，‘-’(dash)，’_‘(underscore)，’.’ (dot)和 ‘:’(colon)，并且严格区分大小写 ** 4.2.1 Proxies配置-frontend frontend 配置参数： bind： #指定HAProxy的监听地址，可以是IPV4或IPV6，可以同时监听多个IP或端口，可同时用于listen字段中 #格式： bind [\u003caddress\u003e]:\u003cport_range\u003e [, ...] [param*] #注意：如果需要绑定在非本机的IP，需要开启内核参数：net.ipv4.ip_nonlocal_bind=1范例： listen http_proxy #监听http的多个IP的多个端口和sock文件 bind :80,:443,:8801-8810 bind 10.0.0.1:10080,10.0.0.1:10443 bind /var/run/ssl-frontend.sock user root mode 600 accept-proxy listen http_https_proxy #https监听 bind :80 bind :443 ssl crt /etc/haproxy/site.pem #公钥和私钥公共文件 listen http_https_proxy_explicit #监听ipv6、ipv4和unix sock文件 bind ipv6@:80 bind ipv4@public_ssl:443 ssl crt /etc/haproxy/site.pem bind unix@ssl-frontend.sock user root mode 600 accept-proxy listen external_bind_app1 #监听file descriptor bind \"fd@${FD_APP1}\"** 生产示例：** frontend magedu_web_port #可以采用后面形式命名：业务-服务-端口号 bind :80,:8080 bind 10.0.0.7:10080,:8801-8810,10.0.0.17:9001-9010 mode http|tcp #指定负载协议类型 use_backend \u003cbackend_name\u003e #调用的后端服务器组名称4.2.2 Proxies配置-backend 定义一组后端服务器，backend服务器将被frontend进行调用。 mode http|tcp #指定负载协议类型,和对应的frontend必须一致 option #配置选项 server #定义后端real server注意：option后面加** httpchk，smtpchk,mysql-check,pgsql-check，ssl-hello-chk**方法，可用于实现更多应用层检测功能。 option 配置 check #对指定real进行健康状态检查，如果不加此设置，默认不开启检查 addr \u003cIP\u003e #可指定的健康状态监测IP，可以是专门的数据网段，减少业务网络的流量 port \u003cnum\u003e #指定的健康状态监测端口 inter \u003cnum\u003e #健康状态检查间隔时间，默认2000 ms fall \u003cnum\u003e #后端服务器从线上转为线下的检查的连续失效次数，默认为3 rise \u003cnum\u003e #后端服务器从下线恢复上线的检查的连续有效次数，默认为2 weight \u003cweight\u003e #默认为1，最大值为256，0表示不参与负载均衡，但仍接受持久连接 backup #将后端服务器标记为备份状态,只在所有非备份主机down机时提供服务，类似Sorry Server disabled #将后端服务器标记为不可用状态，即维护状态，除了持久模式，将不再接受连接 redirect prefix http://www.baidu.com/ #将请求临时(302)重定向至其它URL，只适用于http模式 redir http://www.baidu.com #将请求临时(302)重定向至其它URL，只适用于http模式 maxconn \u003cmaxconn\u003e #当前后端server的最大并发连接数 backlog \u003cbacklog\u003e #当前端服务器的连接数达到上限后的后援队列长度，注意：不支持backend4.2.3 frontend+backend配置实例 范例1： frontend xin-test-http bind :80,:8080 mode tcp use_backend magedu-test-http-nodes backend magedu-test-http-nodes mode tcp default-server inter 1000 weight 6 server web1 10.0.0.17:80 check weight 2 addr 10.0.0.117 port 8080 server web1 10.0.0.27:80 check范例2： #官网业务访问入口 frontend WEB_PORT_80 bind 10.0.0.7:80 mode http use_backend web_prot_http_nodes backend web_prot_http_nodes mode http option forwardfor server 10.0.0.17 10.0.0.17:8080 check inter 3000 fall 3 rise 5 server 10.0.0.27 10.0.0.27:8080 check inter 3000 fall 3 rise 54.2.4 Proxies配置-listen替代frontend+backend 使用listen替换上面的frontend和backend的配置方式，可以简化设置，通常只用于TCP协议的应用 #官网业务访问入口 listen WEB_PORT_80 bind 10.0.0.7:80 mode http option forwardfor server web1 10.0.0.17:8080 check inter 3000 fall 3 rise 5 server web2 10.0.0.27:8080 check inter 3000 fall 3 rise 5","date":"2023-02-05","objectID":"/posts/haproxy/haproxy-2/:1:2","tags":["负载均衡"],"title":"HAProxy-基础配置详解 （二）","uri":"/posts/haproxy/haproxy-2/"},{"categories":["HAProxy"],"content":"4.3 使用子配置文件保存配置 当业务众多时，将所有配置都放在一个配置文件中，会造成维护困难。可以考虑按业务分类，将配置信息拆分，放在不同的子配置文件中，从而达到方便维护的目的。 #创建子配置目录 [root@centos7 ~]#mkdir /etc/haproxy/conf.d/ #创建子配置文件，注意：必须为cfg后缀 [root@centos7 ~]#vim /etc/haproxy/conf.d/test.cfg listen WEB_PORT_80 bind 10.0.0.7:80 mode http balance roundrobin server web1 10.0.0.17:80 check inter 3000 fall 2 rise 5 server web2 10.0.0.27:80 check inter 3000 fall 2 rise 5 #添加子配置目录到unit文件中 [root@centos7 ~]#vim /lib/systemd/system/haproxy.service [Unit] Description=HAProxy Load Balancer After=syslog.target network.target [Service] ExecStartPre=/usr/sbin/haproxy -f /etc/haproxy/haproxy.cfg -f /etc/haproxy/conf.d/ -c -q ExecStart=/usr/sbin/haproxy -Ws -f /etc/haproxy/haproxy.cfg -f /etc/haproxy/conf.d/ -p /var/lib/haproxy/haproxy.pid ExecReload=/bin/kill -USR2 $MAINPID [Install] WantedBy=multi-user.target [root@centos7 ~]#systemctl daemon-reload [root@centos7 ~]#systemctl restart haproxy","date":"2023-02-05","objectID":"/posts/haproxy/haproxy-2/:1:3","tags":["负载均衡"],"title":"HAProxy-基础配置详解 （二）","uri":"/posts/haproxy/haproxy-2/"},{"categories":["docker"],"content":"1.下载Docker二进制包 Docker 下载地址： https://download.docker.com/win/static/stable/x86_64/ https://mirrors.aliyun.com/docker-ce/linux/static/stable/x86_64/ Docker-compos 下载地址： https://github.com/docker/compose/releases https://github.com/docker/compose/releases/download/v2.20.3/docker-compose-linux-x86_64 ","date":"2023-02-04","objectID":"/posts/docker/docker%E4%BA%8C%E8%BF%9B%E5%88%B6%E5%8C%85%E6%96%B9%E5%BC%8F%E5%AE%89%E8%A3%85/:1:0","tags":["docker"],"title":"Docker 二进制方式安装","uri":"/posts/docker/docker%E4%BA%8C%E8%BF%9B%E5%88%B6%E5%8C%85%E6%96%B9%E5%BC%8F%E5%AE%89%E8%A3%85/"},{"categories":["docker"],"content":"2.安装Docker tar xvf docker-24.0.5.zip cp docker/* /usr/bin cp containerd.service /lib/systemd/system/containerd.service cp docker.service /lib/systemd/system/docker.service cp docker.socket /lib/systemd/system/docker.socket cp docker-compose-Linux-x86_64_2.20.3 /usr/bin/docker-compose groupadd docker \u0026\u0026 useradd docker -g docker systemctl enable containerd.service \u0026\u0026 systemctl restart containerd.service systemctl enable docker.service \u0026\u0026 systemctl restart docker.service systemctl enable docker.socket \u0026\u0026 systemctl restart docker.socket ","date":"2023-02-04","objectID":"/posts/docker/docker%E4%BA%8C%E8%BF%9B%E5%88%B6%E5%8C%85%E6%96%B9%E5%BC%8F%E5%AE%89%E8%A3%85/:2:0","tags":["docker"],"title":"Docker 二进制方式安装","uri":"/posts/docker/docker%E4%BA%8C%E8%BF%9B%E5%88%B6%E5%8C%85%E6%96%B9%E5%BC%8F%E5%AE%89%E8%A3%85/"},{"categories":["docker"],"content":"2.1 containerd.service containerd 是 Docker 的核心组件之一，负责管理容器的生命周期、镜像传输以及容器进程的执行,如创建命名空间、控制组、文件系统等。 Docker 在其架构中使用了容器运行时（Container Runtime）来管理容器的生命周期。containerd 实现了 OCI（Open Container Initiative）标准，这是一个开放的行业标准，旨在定义容器和容器运行时的规范。这使得 containerd 能够与其他符合 OCI 标准的工具和库协同工作。 在 Linux 系统中，containerd 以守护进程的形式运行。为了确保 containerd 在系统启动时自动启动，并能够受到 systemd（一个常用的初始化系统和服务管理器）的管理，需要创建并配置一个 containerd.service 单元。 这个服务单元定义了 containerd 守护进程的启动方式、参数以及其他相关设置。 [Unit] Description=containerd container runtime Documentation=https://containerd.io After=network.target local-fs.target [Service] ExecStartPre=-/sbin/modprobe overlay ExecStart=/usr/bin/containerd Type=notify Delegate=yes KillMode=process Restart=always # Having non-zero Limit*s causes performance problems due to accounting overhead # in the kernel. We recommend using cgroups to do container-local accounting. LimitNPROC=infinity LimitCORE=infinity LimitNOFILE=1048576 # Comment TasksMax if your systemd version does not supports it. # Only systemd 226 and above support this version. TasksMax=infinity [Install] WantedBy=multi-user.target","date":"2023-02-04","objectID":"/posts/docker/docker%E4%BA%8C%E8%BF%9B%E5%88%B6%E5%8C%85%E6%96%B9%E5%BC%8F%E5%AE%89%E8%A3%85/:2:1","tags":["docker"],"title":"Docker 二进制方式安装","uri":"/posts/docker/docker%E4%BA%8C%E8%BF%9B%E5%88%B6%E5%8C%85%E6%96%B9%E5%BC%8F%E5%AE%89%E8%A3%85/"},{"categories":["docker"],"content":"2.2 docker.service docker.service 是一个 Systemd 服务单元，用于管理 Docker 守护进程（dockerd）的运行。Systemd 是一个常用的初始化系统和服务管理器，而服务单元则定义了如何启动、停止和管理特定的服务。 在 Docker 的架构中，dockerd 是 Docker 守护进程，负责管理容器的创建、运行、停止等任务。docker.service 的作用是管理 dockerd 进程的生命周期，使得 Docker 守护进程可以在系统启动时自动启动，并在需要时提供管理和监控。 [Unit] Description=Docker Application Container Engine Documentation=https://docs.docker.com BindsTo=containerd.service After=network-online.target firewalld.service containerd.service Wants=network-online.target Requires=docker.socket [Service] Type=notify # the default is not to use systemd for cgroups because the delegate issues still # exists and systemd currently does not support the cgroup feature set required # for containers run by docker ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock ExecReload=/bin/kill -s HUP $MAINPID TimeoutSec=0 RestartSec=2 Restart=always # Note that StartLimit* options were moved from \"Service\" to \"Unit\" in systemd 229. # Both the old, and new location are accepted by systemd 229 and up, so using the old location # to make them work for either version of systemd. StartLimitBurst=3 # Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230. # Both the old, and new name are accepted by systemd 230 and up, so using the old name to make # this option work for either version of systemd. StartLimitInterval=60s # Having non-zero Limit*s causes performance problems due to accounting overhead # in the kernel. We recommend using cgroups to do container-local accounting. LimitNOFILE=infinity LimitNPROC=infinity LimitCORE=infinity # Comment TasksMax if your systemd version does not support it. # Only systemd 226 and above support this option. TasksMax=infinity # set delegate yes so that systemd does not reset the cgroups of docker containers Delegate=yes # kill only the docker process, not all processes in the cgroup KillMode=process [Install] WantedBy=multi-user.target Description 提供了关于服务的简要描述。 Documentation 可以提供指向 Docker 文档的链接。 ExecStart 指定了如何启动 dockerd 进程，这里的 -H fd:// 告诉 Docker 守护进程通过文件描述符进行通信。 Restart 规定了在发生错误时如何重启服务。 StartLimitIntervalSec 和 StartLimitBurst 规定了在一段时间内尝试启动服务的次数限制，以避免过多的重试。 WantedBy=multi-user.target 表示该服务会在多用户模式下启动，即在系统引导后的一般操作状态下。 ","date":"2023-02-04","objectID":"/posts/docker/docker%E4%BA%8C%E8%BF%9B%E5%88%B6%E5%8C%85%E6%96%B9%E5%BC%8F%E5%AE%89%E8%A3%85/:2:2","tags":["docker"],"title":"Docker 二进制方式安装","uri":"/posts/docker/docker%E4%BA%8C%E8%BF%9B%E5%88%B6%E5%8C%85%E6%96%B9%E5%BC%8F%E5%AE%89%E8%A3%85/"},{"categories":["docker"],"content":"2.3 docker.socket docker.socket 是一个 Systemd 套接字（socket）单元，用于与 Docker 守护进程（dockerd）之间的通信。 具体来说，docker.socket 通过监听一个特定的网络端口或者 Unix 域套接字（Unix Domain Socket），等待来自 Docker 客户端的连接请求。一旦有请求连接，docker.socket 就会将请求转发给 dockerd 进程，然后 dockerd 处理这些请求并执行相应的操作，如创建或管理容器。 [Unit] Description=Docker Socket for the API PartOf=docker.service [Socket] ListenStream=/var/run/docker.sock SocketMode=0660 SocketUser=root SocketGroup=docker [Install] WantedBy=sockets.target","date":"2023-02-04","objectID":"/posts/docker/docker%E4%BA%8C%E8%BF%9B%E5%88%B6%E5%8C%85%E6%96%B9%E5%BC%8F%E5%AE%89%E8%A3%85/:2:3","tags":["docker"],"title":"Docker 二进制方式安装","uri":"/posts/docker/docker%E4%BA%8C%E8%BF%9B%E5%88%B6%E5%8C%85%E6%96%B9%E5%BC%8F%E5%AE%89%E8%A3%85/"},{"categories":["HAProxy"],"content":"三、HAProxy安装及基础配置 介绍HAProxy的基础安装及基础配置 ","date":"2023-02-04","objectID":"/posts/haproxy/haproxy-1/:1:0","tags":["负载均衡"],"title":"HAProxy-安装及基础配置（一）","uri":"/posts/haproxy/haproxy-1/"},{"categories":["HAProxy"],"content":"3.1 源码包安装 官方提供了Ubuntu和Debian的包，没有Centos的包 ubuntu 安装 apt-get install --no-install-recommends software-properties-common #--no-install-recommends 参数来避免安装非必须的文件，从而减小镜像的体积 add-apt-repository ppa:vbernat/haproxy-2.6 apt-get install haproxy=2.6.\\*#安装常用软件包 apt-get install --no-install-recommends software-properties-common -y #--no-install-recommends 参数来避免安装非必须的文件，从而减小镜像的体积 #安装源 root@etcd01[11:10:22]~ #:add-apt-repository ppa:vbernat/haproxy-2.6 HAProxy is a free, very fast and reliable solution offering high availability, load balancing, and proxying for TCP and HTTP-based applications. It is particularly suited for web sites crawling under very high loads while needing persistence or Layer7 processing. Supporting tens of thousands of connections is clearly realistic with todays hardware. Its mode of operation makes its integration into existing architectures very easy and riskless, while still offering the possibility not to expose fragile web servers to the Net. This PPA contains packages for HAProxy 2.6. More info: https://launchpad.net/~vbernat/+archive/ubuntu/haproxy-2.6 Press [ENTER] to continue or Ctrl-c to cancel adding it. Get:1 http://ppa.launchpad.net/vbernat/haproxy-2.6/ubuntu focal InRelease [23.8 kB] Hit:2 http://cn.archive.ubuntu.com/ubuntu focal InRelease Hit:3 http://cn.archive.ubuntu.com/ubuntu focal-updates InRelease Hit:4 http://cn.archive.ubuntu.com/ubuntu focal-backports InRelease Hit:5 http://cn.archive.ubuntu.com/ubuntu focal-security InRelease Get:6 http://ppa.launchpad.net/vbernat/haproxy-2.6/ubuntu focal/main amd64 Packages [1,000 B] Get:7 http://ppa.launchpad.net/vbernat/haproxy-2.6/ubuntu focal/main Translation-en [704 B] Fetched 25.5 kB in 2s (14.0 kB/s) Reading package lists... Done #查看可用版本 root@etcd01[11:11:01]~ #:apt-cache madison haproxy haproxy | 2.6.8-1ppa1~focal | http://ppa.launchpad.net/vbernat/haproxy-2.6/ubuntu focal/main amd64 Packages haproxy | 2.0.29-0ubuntu1.1 | http://cn.archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages haproxy | 2.0.29-0ubuntu1.1 | http://cn.archive.ubuntu.com/ubuntu focal-security/main amd64 Packages haproxy | 2.0.13-2 | http://cn.archive.ubuntu.com/ubuntu focal/main amd64 Packages #安装2.6 apt-get install haproxy=2.6.\\* -y #验证haproxy版本 root@etcd01[13:50:48]~ #:haproxy -v HAProxy version 2.6.8-1ppa1~focal 2023/01/24 - https://haproxy.org/ Status: long-term supported branch - will stop receiving fixes around Q2 2027. Known bugs: http://www.haproxy.org/bugs/bugs-2.6.8.html Running on: Linux 5.4.0-135-generic #152-Ubuntu SMP Wed Nov 23 20:19:22 UTC 2022 x86_64Centos安装 在centos系统上通过yum、编译等多种安装方式。默认yum源默认的base仓库中包含haproxy的安装包文件，但是版本比较旧，是1.5.18的版本，距离当前版本已经有较长时间没有更新，由于版本比较旧所以有很多功能不支持，如果对功能和性能没有要求可以使用此版本，否则推荐使用新版本。 # yum install haproxy -y #验证haproxy版本 # haproxy -v HA-Proxy version 1.5.18 2016/05/10 Copyright 2000-2016 Willy Tarreau \u003cwilly@haproxy.org\u003e","date":"2023-02-04","objectID":"/posts/haproxy/haproxy-1/:1:1","tags":["负载均衡"],"title":"HAProxy-安装及基础配置（一）","uri":"/posts/haproxy/haproxy-1/"},{"categories":["HAProxy"],"content":"3.2 编译安装HAProxy 编译安装HAProxy 2.0 LTS版本，源码包下载地址：http://www.haproxy.org/download/ 3.2.1 解决lua环境 HAProxy支持基于lua实现功能扩展，lua是一种小巧的脚本语言，于1993年由巴西里约热内卢天主教大学（Pontiﬁcal Catholic University of Rio de Janeiro）里的一个研究小组开发，其设计目的是为了嵌入应用程序中，从而为应用程序提供灵活的扩展和定制功能。 Lua 官网：www.lua.org Lua应用场景 游戏开发 独立应用脚本 Web应用脚本 扩展和数据库插件，如MySQL Proxy 安全系统，如入侵检测系统 Centos 环境：由于centos自带的lua版本比较低并不符合HAProxy要求的lua最低版本(5.3)的要求，因此需要编译安装较新版本的lua环境，然后才能编译安装HAProxy，过程如下： #当前系统版本 [root@centos7 ~]#lua -v Lua 5.1.4 Copyright (C) 1994-2008 Lua.org, PUC-Rio #安装基础命令及编译依赖环境 [root@centos7 ~]# yum install gcc readline-devel [root@centos7 ~]# wget http://www.lua.org/ftp/lua-5.3.5.tar.gz [root@centos7 ~]# tar xvf lua-5.3.5.tar.gz -C /usr/local/src [root@centos7 ~]# cd /usr/local/src/lua-5.3.5 [root@centos7 lua-5.3.5]# make linux test [root@localhost lua-5.3.5]# make linux test cd src \u0026\u0026 make linux make[1]: 进入目录“/usr/local/src/lua-5.3.5/src” make all SYSCFLAGS=\"-DLUA_USE_LINUX\" SYSLIBS=\"-Wl,-E -ldl -lreadline\" make[2]: 进入目录“/usr/local/src/lua-5.3.5/src” gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o lapi.o lapi.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o lcode.o lcode.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o lctype.o lctype.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o ldebug.o ldebug.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o ldo.o ldo.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o ldump.o ldump.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o lfunc.o lfunc.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o lgc.o lgc.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o llex.o llex.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o lmem.o lmem.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o lobject.o lobject.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o lopcodes.o lopcodes.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o lparser.o lparser.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o lstate.o lstate.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o lstring.o lstring.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o ltable.o ltable.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o ltm.o ltm.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o lundump.o lundump.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o lvm.o lvm.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o lzio.o lzio.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o lauxlib.o lauxlib.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o lbaselib.o lbaselib.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o lbitlib.o lbitlib.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o lcorolib.o lcorolib.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o ldblib.o ldblib.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o liolib.o liolib.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o lmathlib.o lmathlib.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o loslib.o loslib.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o lstrlib.o lstrlib.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o ltablib.o ltablib.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o lutf8lib.o lutf8lib.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o loadlib.o loadlib.c gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX -c -o linit.o linit.c ar rcu liblua.a lapi.o lcode.o l","date":"2023-02-04","objectID":"/posts/haproxy/haproxy-1/:1:2","tags":["负载均衡"],"title":"HAProxy-安装及基础配置（一）","uri":"/posts/haproxy/haproxy-1/"},{"categories":["HAProxy"],"content":"3.3 验证haproxy状态 3.3.1 验证监听端口 [root@localhost haproxy]# ss -tnl State Recv-Q Send-Q Local Address:Port Peer Address:Port LISTEN 0 128 127.0.0.1:631 *:* LISTEN 0 100 *:8088 *:* LISTEN 0 3 127.0.0.1:31769 *:* LISTEN 0 100 127.0.0.1:25 *:* LISTEN 0 128 *:1883 *:* LISTEN 0 128 *:30013 *:* LISTEN 0 50 *:8161 *:* LISTEN 0 1 127.0.0.1:8005 *:* LISTEN 0 128 *:5672 *:* LISTEN 0 50 *:43178 *:* LISTEN 0 128 *:6379 *:* LISTEN 0 128 *:61613 *:* LISTEN 0 50 *:61614 *:* LISTEN 0 128 *:9999 *:* 3.3.2 查看haproxy的状态页面 浏览器访问：http://haproxy-server:9999/haproxy-status 3.3.3 测试转发 10.1.0.6:30013 转发到 10.1.0.31:30013 ✅ ","date":"2023-02-04","objectID":"/posts/haproxy/haproxy-1/:1:3","tags":["负载均衡"],"title":"HAProxy-安装及基础配置（一）","uri":"/posts/haproxy/haproxy-1/"},{"categories":["Kubernetes"],"content":"Apache Dubbo 最初在 2008 年由 Alibaba 捐献开源，很快成为了国内开源服务框架选型的事实标准框架 ，得到了各行各业的广泛应用。在 2017 年，Dubbo 正式捐献到 Apache 软件基金会并成为 Apache 顶级项目 。 Apache Dubbo 是一款 RPC 服务开发框架，用于解决微服务架构下的服务治理与通信问题，官方提供了 Java、Golang 等多语言 SDK 实现。使用 Dubbo 开发的微服务原生具备相互之间的远程地址发现与通信能力， 利用 Dubbo 提供的丰富服务治理特性，可以实现诸如服务发现、负载均衡、流量调度等服务治理诉求。Dubbo 被设计为高度可扩展，用户可以方便的实现流量拦截、选址的各种定制逻辑。 dubbo 简介 **dubbo 架构 ** ","date":"2023-01-31","objectID":"/posts/kubernetes/primary/kubernetes-11/:0:0","tags":["k8s进阶训练营"],"title":"实战案例-基于zookeeper实现微服务动态注册和发现 (十一)","uri":"/posts/kubernetes/primary/kubernetes-11/"},{"categories":["Kubernetes"],"content":"1.构建Provider镜像 #Dubbo provider FROM harbor.ceamg.com/pub-images/jdk8:3411 MAINTAINER XXXXXXXX RUN mkdir -p /apps/dubbo/provider ADD dubbo-demo-provider-2.1.5/ /apps/dubbo/provider/ ADD run_java.sh /apps/dubbo/provider/bin/ RUN useradd nginx -u 2023 RUN chown -R nginx.nginx /apps/ \u0026\u0026 chmod +x /apps/dubbo/provider/bin/*.sh CMD [\"/apps/dubbo/provider/bin/run_java.sh\"]dubbo-demo-consumer-2.1.5是dubbo consumer的代码，也需要修改下配置，指定zookeeper的地址： ## # Copyright 1999-2011 Alibaba Group. # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. ## dubbo.container=log4j,spring dubbo.application.name=demo-provider dubbo.application.owner= dubbo.registry.address=zookeeper://zookeeper1.xin-zk.svc.ceamg.local:2181 | zookeeper://zookeeper2.xin-zk.svc.ceamg.local:2181 | zookeeper://zookeeper3.xin-zk.svc.ceamg.local:2181 #dubbo.registry.address=zookeeper://127.0.0.1:2181 #dubbo.registry.address=redis://127.0.0.1:6379 #dubbo.registry.address=dubbo://127.0.0.1:9090 dubbo.monitor.protocol=registry dubbo.protocol.name=dubbo dubbo.protocol.port=20880 dubbo.log4j.file=logs/dubbo-demo-provider.log dubbo.log4j.level=WARNzk集群测试连通性 --- zookeeper1.xin-zk.svc.ceamg.local:2181 ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 0.076/0.097/0.118 ms bash-4.3# ping zookeeper2.xin-zk.svc.ceamg.local:2181 PING zookeeper2.xin-zk.svc.ceamg.local:2181 (10.10.101.183): 56 data bytes 64 bytes from 10.10.101.183: seq=0 ttl=64 time=0.059 ms 64 bytes from 10.10.101.183: seq=1 ttl=64 time=0.134 ms ^C --- zookeeper2.xin-zk.svc.ceamg.local:2181 ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 0.059/0.096/0.134 ms bash-4.3# ping zookeeper3.xin-zk.svc.ceamg.local:2181 PING zookeeper3.xin-zk.svc.ceamg.local:2181 (10.10.176.183): 56 data bytes 64 bytes from 10.10.176.183: seq=0 ttl=64 time=0.049 ms 64 bytes from 10.10.176.183: seq=1 ttl=64 time=0.151 ms --- zookeeper3.xin-zk.svc.ceamg.local:2181 ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 0.049/0.100/0.151 msrun_java.sh内容如下： #!/bin/bash su - nginx -c \"/apps/dubbo/provider/bin/start.sh\" tail -f /etc/hosts#!/bin/bash TAG=$1 docker build -t harbor.ceamg.com/pub-images/dubbo-provider:${TAG} . docker push harbor.ceamg.com/pub-images/dubbo-provider:${TAG}root@harbor01[21:28:43]~/dubbo/dubbo-demo-consumer-2.1.5/conf #:chmod +x ./*.sh执行构建，上传镜像 root@harbor01[21:37:19]~/dubbo/provider #:bash build_image_command.sh v1 Sending build context to Docker daemon 11.56MB Step 1/8 : FROM harbor.ceamg.com/pub-images/jdk8:3411 ---\u003e 1328b4d79a67 Step 2/8 : MAINTAINER XXXXXXXX ---\u003e Using cache ---\u003e d68f684b20d3 Step 3/8 : RUN mkdir -p /apps/dubbo/provider ---\u003e Using cache ---\u003e 1eee7aae68c2 Step 4/8 : ADD dubbo-demo-provider-2.1.5/ /apps/dubbo/provider/ ---\u003e Using cache ---\u003e 7d305495d592 Step 5/8 : ADD run_java.sh /apps/dubbo/provider/bin/ ---\u003e 93a53e745acc Step 6/8 : RUN useradd nginx -u 2023 ---\u003e Running in 6ce3cb1b6065 Removing intermediate container 6ce3cb1b6065 ---\u003e 5f0402802b5c Step 7/8 : RUN chown -R nginx.nginx /apps/ \u0026\u0026 chmod +x /apps/dubbo/provider/bin/*.sh ---\u003e Running in 5e52acec648e Removing intermediate container 5e52acec648e ---\u003e 464dd347a8e2 Step 8/8 : CMD [\"/apps/dubbo/provider/bin/run_java.sh\"] ---\u003e Running in 3ae6dd4ef0d7 Removing intermediate container 3ae6dd4ef0d7 ---\u003e c79f2c1a9fd3 Successfully built c79f2c1a9fd3 Successfully tagged harbor.ceamg.com/pub-images/dubbo-provider:v1 The push refers to rep","date":"2023-01-31","objectID":"/posts/kubernetes/primary/kubernetes-11/:1:0","tags":["k8s进阶训练营"],"title":"实战案例-基于zookeeper实现微服务动态注册和发现 (十一)","uri":"/posts/kubernetes/primary/kubernetes-11/"},{"categories":["Kubernetes"],"content":"https://cn.wordpress.org/download/releases/ ","date":"2023-01-29","objectID":"/posts/kubernetes/primary/kubernetes-10/:0:0","tags":["k8s进阶训练营"],"title":"实战案例-WordPress (十)","uri":"/posts/kubernetes/primary/kubernetes-10/"},{"categories":["Kubernetes"],"content":"1.准备PHP镜像 ","date":"2023-01-29","objectID":"/posts/kubernetes/primary/kubernetes-10/:1:0","tags":["k8s进阶训练营"],"title":"实战案例-WordPress (十)","uri":"/posts/kubernetes/primary/kubernetes-10/"},{"categories":["Kubernetes"],"content":"官方PHP镜像 root@harbor01[09:20:39]/dockerfile/web/wordpress #:docker pull php:5.6.40-fpm root@harbor01[09:21:35]/dockerfile/web/wordpress #:docker tag php:5.6.40-fpm harbor.ceamg.com/baseimages/php-fpm:5.6.40 root@harbor01[09:23:12]/dockerfile/web/wordpress #:docker push harbor.ceamg.com/baseimages/php-fpm:5.6.40","date":"2023-01-29","objectID":"/posts/kubernetes/primary/kubernetes-10/:1:1","tags":["k8s进阶训练营"],"title":"实战案例-WordPress (十)","uri":"/posts/kubernetes/primary/kubernetes-10/"},{"categories":["Kubernetes"],"content":"自制PHP镜像 root@harbor01[12:28:25]/dockerfile/web/php #:ls build_image_command.sh Dockerfile php.ini run_php.sh sources.list www.conf编写Dockerfile #PHP Base Image FROM harbor.ceamg.com/baseimages/ubuntu:20.04 RUN apt update -y \\ \u0026\u0026 apt install ca-certificates -y \\ \u0026\u0026 useradd nginx -u 2023 -s /sbin/nologin \\ \u0026\u0026 mkdir /run/php COPY sources.list /etc/apt/sources.list RUN apt update -y \u0026\u0026 apt-get install php7.4-fpm php7.4-mysql -y COPY www.conf /etc/php/7.4/fpm/pool.d/ COPY php.ini /etc/php/7.4/fpm/ ADD run_php.sh /usr/local/bin/run_php.sh EXPOSE 9000 CMD [\"/usr/local/bin/run_php.sh\"] #!/bin/bash TAG=$1 docker build -t harbor.ceamg.com/baseimages/php7.4-wordpress:${TAG} . docker push harbor.ceamg.com/baseimages/php7.4-wordpress:${TAG}#!/bin/bash php-fpm7.4 --nodaemonize[PHP] ;;;;;;;;;;;;;;;;;;; ; About php.ini ; ;;;;;;;;;;;;;;;;;;; ; PHP's initialization file, generally called php.ini, is responsible for ; configuring many of the aspects of PHP's behavior. ; PHP attempts to find and load this configuration from a number of locations. ; The following is a summary of its search order: ; 1. SAPI module specific location. ; 2. The PHPRC environment variable. (As of PHP 5.2.0) ; 3. A number of predefined registry keys on Windows (As of PHP 5.2.0) ; 4. Current working directory (except CLI) ; 5. The web server's directory (for SAPI modules), or directory of PHP ; (otherwise in Windows) ; 6. The directory from the --with-config-file-path compile time option, or the ; Windows directory (usually C:\\windows) ; See the PHP docs for more specific information. ; http://php.net/configuration.file ; The syntax of the file is extremely simple. Whitespace and lines ; beginning with a semicolon are silently ignored (as you probably guessed). ; Section headers (e.g. [Foo]) are also silently ignored, even though ; they might mean something in the future. ; Directives following the section heading [PATH=/www/mysite] only ; apply to PHP files in the /www/mysite directory. Directives ; following the section heading [HOST=www.example.com] only apply to ; PHP files served from www.example.com. Directives set in these ; special sections cannot be overridden by user-defined INI files or ; at runtime. Currently, [PATH=] and [HOST=] sections only work under ; CGI/FastCGI. ; http://php.net/ini.sections ; Directives are specified using the following syntax: ; directive = value ; Directive names are *case sensitive* - foo=bar is different from FOO=bar. ; Directives are variables used to configure PHP or PHP extensions. ; There is no name validation. If PHP can't find an expected ; directive because it is not set or is mistyped, a default value will be used. ; The value can be a string, a number, a PHP constant (e.g. E_ALL or M_PI), one ; of the INI constants (On, Off, True, False, Yes, No and None) or an expression ; (e.g. E_ALL \u0026 ~E_NOTICE), a quoted string (\"bar\"), or a reference to a ; previously set variable or directive (e.g. ${foo}) ; Expressions in the INI file are limited to bitwise operators and parentheses: ; | bitwise OR ; ^ bitwise XOR ; \u0026 bitwise AND ; ~ bitwise NOT ; ! boolean NOT ; Boolean flags can be turned on using the values 1, On, True or Yes. ; They can be turned off using the values 0, Off, False or No. ; An empty string can be denoted by simply not writing anything after the equal ; sign, or by using the None keyword: ; foo = ; sets foo to an empty string ; foo = None ; sets foo to an empty string ; foo = \"None\" ; sets foo to the string 'None' ; If you use constants in your value, and these constants belong to a ; dynamically loaded extension (either a PHP extension or a Zend extension), ; you may only use these constants *after* the line that loads the extension. ;;;;;;;;;;;;;;;;;;; ; About this file ; ;;;;;;;;;;;;;;;;;;; ; PHP comes packaged with two INI files. One that is recommended to be used ; in production environments and one that is recommended to be used in ; development environments. ; php.ini-production contains settings which hold","date":"2023-01-29","objectID":"/posts/kubernetes/primary/kubernetes-10/:1:2","tags":["k8s进阶训练营"],"title":"实战案例-WordPress (十)","uri":"/posts/kubernetes/primary/kubernetes-10/"},{"categories":["Kubernetes"],"content":" 打包镜像 root@harbor01[12:32:54]/dockerfile/web/php #:bash build_image_command.sh v1 Sending build context to Docker daemon 99.84kB Step 1/9 : FROM harbor.ceamg.com/baseimages/ubuntu:20.04 ---\u003e d5447fc01ae6 Step 2/9 : RUN apt update -y \u0026\u0026 apt install ca-certificates -y \u0026\u0026 useradd nginx -u 2023 -s /sbin/nologin \u0026\u0026 mkdir /run/php ---\u003e Using cache ---\u003e d00499d15478 Step 3/9 : COPY sources.list /etc/apt/sources.list ---\u003e Using cache ---\u003e 4b22bb5df136 Step 4/9 : RUN apt update -y \u0026\u0026 apt-get install php7.4-fpm php7.4-mysql -y ---\u003e Using cache ---\u003e 1d827f2430fe Step 5/9 : COPY www.conf /etc/php/7.4/fpm/pool.d/ ---\u003e Using cache ---\u003e 47bb226c528b Step 6/9 : COPY php.ini /etc/php/7.4/fpm/ ---\u003e Using cache ---\u003e 4e1d1d47bb8e Step 7/9 : ADD run_php.sh /usr/local/bin/run_php.sh ---\u003e Using cache ---\u003e b8b7aacc06e0 Step 8/9 : EXPOSE 9000 ---\u003e Using cache ---\u003e a3a4c49d3987 Step 9/9 : CMD [\"/usr/local/bin/run_php.sh\"] ---\u003e Using cache ---\u003e 167baff20de6 Successfully built 167baff20de6 Successfully tagged harbor.ceamg.com/baseimages/php7.4-wordpress:v1 The push refers to repository [harbor.ceamg.com/baseimages/php7.4-wordpress] e3b90ad599ab: Layer already exists 861aac09aa7f: Layer already exists 77abd2e639b1: Layer already exists 5020b1aebc94: Layer already exists 301ecd0cc403: Layer already exists 6df28fa92c54: Layer already exists 0002c93bdb37: Layer already exists v1: digest: sha256:4a5236c50f567e10cbe90f5b8b4eaf7b93261ecf6191838741aa5c4e729fe66c size: 1784测试镜像 root@harbor01[12:24:31]/dockerfile/web/php #:docker run --rm -it harbor.ceamg.com/baseimages/php7.4-wordpress:v1 [01-Feb-2023 04:25:02] NOTICE: fpm is running, pid 6 [01-Feb-2023 04:25:02] NOTICE: ready to handle connections [01-Feb-2023 04:25:02] NOTICE: systemd monitor interval set to 10000ms","date":"2023-01-29","objectID":"/posts/kubernetes/primary/kubernetes-10/:1:3","tags":["k8s进阶训练营"],"title":"实战案例-WordPress (十)","uri":"/posts/kubernetes/primary/kubernetes-10/"},{"categories":["Kubernetes"],"content":"2. 准备nginx镜像 root@harbor01[09:50:02]/dockerfile/web/nginx #:ls build_image_command.sh Dockerfile nginx.conf run_nginx.sh","date":"2023-01-29","objectID":"/posts/kubernetes/primary/kubernetes-10/:2:0","tags":["k8s进阶训练营"],"title":"实战案例-WordPress (十)","uri":"/posts/kubernetes/primary/kubernetes-10/"},{"categories":["Kubernetes"],"content":"编写 Dockefile #nginx wordpress FROM harbor.ceamg.com/pub-images/nginx-base:1.22.1 MAINTAINER admin@163.com COPY nginx.conf /usr/local/nginx/conf/ ADD run_nginx.sh /usr/local/nginx/sbin/ RUN useradd nginx -u 2023 RUN mkdir /usr/local/nginx/html/wordpress RUN chown nginx:nginx /usr/local/nginx/html/wordpress/ EXPOSE 80 443 CMD [\"/usr/local/nginx/sbin/run_nginx.sh\"]#!/bin/bash TAG=$1 echo \"开始镜像构建\" docker build -t harbor.ceamg.com/wordpress/nginx-wordpress:${TAG} . if [ $? -eq 0 ];then echo \"镜像构建成功，开始上传镜像\" docker push harbor.ceamg.com/wordpress/nginx-wordpress:${TAG} if [ $? -eq 0 ];then echo \"镜像上传成功\" else echo \"镜像上传失败\" fi else echo \"镜像构建失败,请检查输出\" fiuser nginx nginx; worker_processes auto; #error_log logs/error.log; #error_log logs/error.log notice; #error_log logs/error.log info; #pid logs/nginx.pid; #daemon off; events { worker_connections 1024; } http { include mime.types; default_type application/octet-stream; #log_format main '$remote_addr - $remote_user [$time_local] \"$request\" ' # '$status $body_bytes_sent \"$http_referer\" ' # '\"$http_user_agent\" \"$http_x_forwarded_for\"'; #access_log logs/access.log main; sendfile on; #tcp_nopush on; keepalive_timeout 65; client_max_body_size 10M; client_body_buffer_size 16k; client_body_temp_path /usr/local/nginx/tmp 1 2 2; gzip on; server { listen 80; server_name blog.ceamg.com; #charset koi8-r; #access_log logs/host.access.log main; location / { root /usr/local/nginx/html/wordpress; index index.php index.html index.htm; #if ($http_user_agent ~ \"ApacheBench|WebBench|TurnitinBot|Sogou web spider|Grid Service\") { # proxy_pass http://www.baidu.com; # #return 403; #} } location ~ \\.php$ { root /usr/local/nginx/html/wordpress; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; #fastcgi_param SCRIPT_FILENAME /scripts$fastcgi_script_name; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; } #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } } }#!/bin/bash nginx tail -f /etc/hosts","date":"2023-01-29","objectID":"/posts/kubernetes/primary/kubernetes-10/:2:1","tags":["k8s进阶训练营"],"title":"实战案例-WordPress (十)","uri":"/posts/kubernetes/primary/kubernetes-10/"},{"categories":["Kubernetes"],"content":"打包镜像 root@harbor01[09:45:23]/dockerfile/web/nginx #:bash build_image_command.sh v1.1 开始镜像构建 Sending build context to Docker daemon 6.656kB Step 1/9 : FROM harbor.ceamg.com/pub-images/nginx-base:1.22.1 ---\u003e c055772e4c77 Step 2/9 : MAINTAINER admin@163.com ---\u003e Using cache ---\u003e c339760b111a Step 3/9 : COPY nginx.conf /usr/local/nginx/conf/ ---\u003e Using cache ---\u003e fd62ef4ba6d3 Step 4/9 : ADD run_nginx.sh /usr/local/nginx/sbin/ ---\u003e 0333f9c88d67 Step 5/9 : RUN useradd nginx -u 2023 ---\u003e Running in b477510d6ec6 Removing intermediate container b477510d6ec6 ---\u003e 18217eeee991 Step 6/9 : RUN mkdir /usr/local/nginx/html/wordpress ---\u003e Running in 0b3fa2320f66 Removing intermediate container 0b3fa2320f66 ---\u003e 0d05360ddcfc Step 7/9 : RUN chown nginx:nginx /usr/local/nginx/html/wordpress/ ---\u003e Running in 16e255057567 Removing intermediate container 16e255057567 ---\u003e 5acd10a728cd Step 8/9 : EXPOSE 80 443 ---\u003e Running in 09164687f69f Removing intermediate container 09164687f69f ---\u003e c65586dcb037 Step 9/9 : CMD [\"/usr/local/nginx/sbin/run_nginx.sh\"] ---\u003e Running in ffe14b4816df Removing intermediate container ffe14b4816df ---\u003e 657c22de8b70 Successfully built 657c22de8b70 Successfully tagged harbor.ceamg.com/wordpress/nginx-wordpress:v1.1 镜像构建成功，开始上传镜像 The push refers to repository [harbor.ceamg.com/wordpress/nginx-wordpress] 591ed3250639: Pushed 04c9fd3db048: Pushed 0252a86a3ed5: Pushed f771a2872392: Pushed 644965591cf9: Layer already exists d8949178f619: Layer already exists da95977bca7c: Layer already exists 03477dd36445: Layer already exists fb82b029bea0: Layer already exists v1.1: digest: sha256:c5bce69a5ec505fac1424a33abca32713cf5844eaabfaba4e4338f3bdbc0514d size: 2200 镜像上传成功","date":"2023-01-29","objectID":"/posts/kubernetes/primary/kubernetes-10/:2:2","tags":["k8s进阶训练营"],"title":"实战案例-WordPress (十)","uri":"/posts/kubernetes/primary/kubernetes-10/"},{"categories":["Kubernetes"],"content":"3. 创建PV/PVC ","date":"2023-01-29","objectID":"/posts/kubernetes/primary/kubernetes-10/:3:0","tags":["k8s进阶训练营"],"title":"实战案例-WordPress (十)","uri":"/posts/kubernetes/primary/kubernetes-10/"},{"categories":["Kubernetes"],"content":"NFS服务器 root@harbor01[09:52:06]/data/k8s/wordpress #:mkdir /data/k8s/wordpress/ -p root@harbor01[09:52:21]/data/k8s/wordpress #:vim /etc/exports /data/k8s/wordpress *(rw,sync,no_root_squash) root@harbor01[09:53:34]/data/k8s/wordpress #:systemctl restart nfs-server.service ","date":"2023-01-29","objectID":"/posts/kubernetes/primary/kubernetes-10/:3:1","tags":["k8s进阶训练营"],"title":"实战案例-WordPress (十)","uri":"/posts/kubernetes/primary/kubernetes-10/"},{"categories":["Kubernetes"],"content":"pv/pvc yaml文件 apiVersion: v1 kind: PersistentVolume metadata: name: wordpress-data-pv spec: accessModes: [\"ReadWriteMany\"] capacity: storage: 50Gi nfs: server: 10.1.0.38 path: /data/k8s/wordpress/ --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: wordpress-data-pvc spec: accessModes: [\"ReadWriteMany\"] resources: requests: storage: 40Gi limits: storage: 40Gi volumeName: wordpress-data-pvroot@master01[09:57:42]~/wordpress-yaml #:kubectl create namespace wordpress-xin namespace/wordpress-xin created root@master01[09:55:58]~/wordpress-yaml #:vim wordpress-pv-pvc.yaml root@master01[09:56:14]~/wordpress-yaml #:kubectl apply -f wordpress-pv-pvc.yaml persistentvolume/wordpress-data-pv created persistentvolumeclaim/wordpress-data-pvc created","date":"2023-01-29","objectID":"/posts/kubernetes/primary/kubernetes-10/:3:2","tags":["k8s进阶训练营"],"title":"实战案例-WordPress (十)","uri":"/posts/kubernetes/primary/kubernetes-10/"},{"categories":["Kubernetes"],"content":"查看pv状态 root@master01[09:58:21]~/wordpress-yaml #:kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE jenkins-datadir-pv 100Gi RWO Retain Bound jenkins-xin/jenkins-datadir-pvc 2d17h jenkins-root-datadir-pv 100Gi RWO Retain Bound jenkins-xin/jenkins-root-datadir-pvc 2d17h mysql-datadir-1 50Gi RWO Retain Bound mysql-sts/data-mysql-1 2d20h mysql-datadir-2 50Gi RWO Retain Bound mysql-sts/data-mysql-2 2d20h mysql-datadir-3 50Gi RWO Retain Bound mysql-sts/data-mysql-0 2d20h wordpress-data-pv 50Gi RWX Retain Bound wordpress-xin/wordpress-data-pvc 74s","date":"2023-01-29","objectID":"/posts/kubernetes/primary/kubernetes-10/:3:3","tags":["k8s进阶训练营"],"title":"实战案例-WordPress (十)","uri":"/posts/kubernetes/primary/kubernetes-10/"},{"categories":["Kubernetes"],"content":"部署wordpress 部署文件如下 apiVersion: apps/v1 kind: Deployment metadata: name: wordpress-deploy spec: replicas: 1 selector: matchLabels: app: wordpress-server template: metadata: labels: app: wordpress-server spec: containers: - name: nginx image: harbor.ceamg.com/wordpress/nginx-wordpress:v1.1 imagePullPolicy: Always ports: - name: http containerPort: 80 - name: https containerPort: 443 volumeMounts: - name: wordpress-data mountPath: /usr/local/nginx/html/wordpress - name: php image: harbor.ceamg.com/baseimages/php7.4-wordpress:v1 imagePullPolicy: Always ports: - name: php containerPort: 9000 volumeMounts: - name: wordpress-data mountPath: /usr/local/nginx/html/wordpress readOnly: false volumes: - name: wordpress-data persistentVolumeClaim: claimName: wordpress-data-pvc --- apiVersion: v1 kind: Service metadata: name: wordpress-svc spec: selector: app: wordpress-server type: NodePort ports: - name: http port: 80 targetPort: 80 nodePort: 30013 protocol: TCP - name: https port: 443 targetPort: 443 nodePort: 30014 protocol: TCP","date":"2023-01-29","objectID":"/posts/kubernetes/primary/kubernetes-10/:4:0","tags":["k8s进阶训练营"],"title":"实战案例-WordPress (十)","uri":"/posts/kubernetes/primary/kubernetes-10/"},{"categories":["Kubernetes"],"content":"查看Pod状态 root@master01[10:19:10]~/wordpress-yaml #:kubectl apply -f wordpress-server.yaml deployment.apps/wordpress-deploy created service/wordpress-svc created root@master01[10:19:22]~/wordpress-yaml #:kubectl get pod -n wordpress-xin NAME READY STATUS RESTARTS AGE wordpress-deploy-dd645ccf9-v4k7j 2/2 Running 0 11s root@master01[10:20:06]~/wordpress-yaml #:kubectl get svc -n wordpress-xin NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE wordpress-svc NodePort 10.10.165.34 \u003cnone\u003e 80:30013/TCP,443:30014/TCP 64s","date":"2023-01-29","objectID":"/posts/kubernetes/primary/kubernetes-10/:4:1","tags":["k8s进阶训练营"],"title":"实战案例-WordPress (十)","uri":"/posts/kubernetes/primary/kubernetes-10/"},{"categories":["Kubernetes"],"content":"初始化安装配置wordpress 下载wordpress安装包，将安装包中文件放置到Pod使用的nfs pv对应的目录安装包下载地址：https://cn.wordpress.org/download/releases/ root@harbor01[10:48:28]/data/k8s/wordpress #:tar -xf wordpress-5.9.2-zh_CN.tar.gz root@harbor01[10:48:43]/data/k8s/wordpress #:mv wordpress/* ./ root@harbor01[10:49:05]/data/k8s/wordpress #:rm wordpress -rf root@harbor01[10:49:37]/data/k8s/wordpress #:ls index.php wordpress-5.9.2-zh_CN.tar.gz wp-blog-header.php wp-content wp-links-opml.php wp-mail.php wp-trackback.php license.txt wp-activate.php wp-comments-post.php wp-cron.php wp-load.php wp-settings.php xmlrpc.php readme.html wp-admin wp-config-sample.php wp-includes wp-login.php wp-signup.php chown -R 2023.2023 /data/k8s/wordpress #修改目录所属者为Pod的容器里的nginx用户，否则可能会出现权限不足无法读写的问题 ","date":"2023-01-29","objectID":"/posts/kubernetes/primary/kubernetes-10/:5:0","tags":["k8s进阶训练营"],"title":"实战案例-WordPress (十)","uri":"/posts/kubernetes/primary/kubernetes-10/"},{"categories":["Kubernetes"],"content":"初始化数据库 mysql\u003e create database wordpress; mysql\u003e grant all on wordpress.* to \"wordpress\"@\"%\" identified by 'wordpresspassword@123'; Query OK, 0 rows affected, 1 warning (0.01 sec) mysql\u003e flush privileges; Query OK, 0 rows affected (0.03 sec) mysql-sts wordpress 数据库填为主库mysql.mysql-sts.svc.ceamg.local mysql.mysql-sts.svc.ceamg.local ","date":"2023-01-29","objectID":"/posts/kubernetes/primary/kubernetes-10/:5:1","tags":["k8s进阶训练营"],"title":"实战案例-WordPress (十)","uri":"/posts/kubernetes/primary/kubernetes-10/"},{"categories":["Kubernetes"],"content":"测试上传图片 ","date":"2023-01-29","objectID":"/posts/kubernetes/primary/kubernetes-10/:5:2","tags":["k8s进阶训练营"],"title":"实战案例-WordPress (十)","uri":"/posts/kubernetes/primary/kubernetes-10/"},{"categories":["Kubernetes"],"content":"本次以jenkins.war 包部署⽅式为例运⾏ ，java war包或jar包，且要求jenkins的数据保存⾄外部存储(NFS或者PVC)，其他java应⽤看实际需求是否需要将数据保存⾄外部存储。 ","date":"2023-01-24","objectID":"/posts/kubernetes/primary/kubernetes-9/:0:0","tags":["k8s进阶训练营"],"title":"实战案例-运行java应用 (九)","uri":"/posts/kubernetes/primary/kubernetes-9/"},{"categories":["Kubernetes"],"content":"构建镜像 #Jenkins Version 2.319.3 FROM harbor.ceamg.com/pub-images/jdk8:3411 MAINTAINER zcc zcc@qq.com ADD jenkins-2.319.3.war /apps/jenkins/ ADD run_jenkins.sh /usr/bin/ EXPOSE 8080 CMD [\"/usr/bin/run_jenkins.sh\"]#!/bin/bash cd /apps/jenkins \u0026\u0026 jave -server -Xms1024m -Xmx1024m -Xss512k -jar jenkins-2.319.3.war --webroot=/apps/jenkins/jenkins-data --httpPort=8080查看Jenkins 支持的参数 java -jar jenkins.war --help#!/bin/bash docker build -t harbor.ceamg.com/pub-images/jenkins:v2.319.3 . echo \"镜像制作完成，即将上传至Harbor服务器\" sleep 1 docker push harbor.ceamg.com/pub-images/jenkins:v2.319.3 echo \"镜像上传完成\"root@harbor01[16:45:23]/dockerfile/jenkins #:bash build-command.sh Sending build context to Docker daemon 72.26MB Step 1/6 : FROM harbor.ceamg.com/pub-images/jdk8:3411 ---\u003e 1328b4d79a67 Step 2/6 : MAINTAINER zcc zcc@qq.com ---\u003e Using cache ---\u003e 35ad6bb5a267 Step 3/6 : ADD jenkins-2.319.3.war /apps/jenkins/ ---\u003e d83e0dff6896 Step 4/6 : ADD run_jenkins.sh /usr/bin/ ---\u003e 4f60478bd327 Step 5/6 : EXPOSE 8080 ---\u003e Running in 84bcd1400981 Removing intermediate container 84bcd1400981 ---\u003e d01106084f38 Step 6/6 : CMD [\"/usr/bin/run_jenkins.sh\"] ---\u003e Running in 9eaaf7204543 Removing intermediate container 9eaaf7204543 ---\u003e 0c48f0d81550 Successfully built 0c48f0d81550 Successfully tagged harbor.ceamg.com/pub-images/jenkins:v2.319.3 镜像制作完成，即将上传至Harbor服务器 The push refers to repository [harbor.ceamg.com/pub-images/jenkins] d74d542bfbb2: Pushed ba3e041a4025: Pushed 3ad8c5bef187: Mounted from pub-images/tomcat-base f4442a8d89b4: Mounted from pub-images/tomcat-base c185ef053da5: Mounted from pub-images/tomcat-base 0002c93bdb37: Mounted from pub-images/tomcat-base v2.319.3: digest: sha256:fbaa1f61491042ddc6ab2dc3e2183900daaeee1d57ecc772c33ac7dfb39f895a size: 1575 镜像上传完成","date":"2023-01-24","objectID":"/posts/kubernetes/primary/kubernetes-9/:1:0","tags":["k8s进阶训练营"],"title":"实战案例-运行java应用 (九)","uri":"/posts/kubernetes/primary/kubernetes-9/"},{"categories":["Kubernetes"],"content":"创建pv --- apiVersion: v1 kind: PersistentVolume metadata: name: jenkins-datadir-pv spec: capacity: storage: 100Gi accessModes: - ReadWriteOnce nfs: server: 10.1.0.38 path: /data/k8s/jenkins/jenkins-data --- apiVersion: v1 kind: PersistentVolume metadata: name: jenkins-root-datadir-pv spec: capacity: storage: 100Gi accessModes: - ReadWriteOnce nfs: server: 10.1.0.38 path: /data/k8s/jenkins/jenkins-root-data","date":"2023-01-24","objectID":"/posts/kubernetes/primary/kubernetes-9/:2:0","tags":["k8s进阶训练营"],"title":"实战案例-运行java应用 (九)","uri":"/posts/kubernetes/primary/kubernetes-9/"},{"categories":["Kubernetes"],"content":"NFS服务器创建应用数据目录 root@harbor01[16:52:33]/data/k8s #:mkdir /data/k8s/jenkins/jenkins-data -p root@harbor01[16:52:47]/data/k8s #:mkdir /data/k8s/jenkins/jenkins-root-data -p vim /etx/exports /data/k8s/jenkins *(rw,sync,no_root_squash) root@harbor01[16:54:13]/data/k8s/jenkins #:systemctl restart nfs-server.serviceroot@master01[16:57:33]~/jenkins-yaml #:kubectl apply -f jenkins-pv.yaml persistentvolume/jenkins-datadir-pv created persistentvolume/jenkins-root-datadir-pv created root@master01[16:57:38]~/jenkins-yaml #:kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE jenkins-datadir-pv 100Gi RWO Retain Available 5s jenkins-root-datadir-pv 100Gi RWO Retain Available 5s","date":"2023-01-24","objectID":"/posts/kubernetes/primary/kubernetes-9/:2:1","tags":["k8s进阶训练营"],"title":"实战案例-运行java应用 (九)","uri":"/posts/kubernetes/primary/kubernetes-9/"},{"categories":["Kubernetes"],"content":"创建PVC --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: jenkins-datadir-pvc namespace: jenkins-xin spec: volumeName: jenkins-datadir-pv accessModes: - ReadWriteOnce resources: requests: storage: 80Gi --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: jenkins-root-datadir-pvc namespace: jenkins-xin spec: volumeName: jenkins-root-datadir-pv accessModes: - ReadWriteOnce resources: requests: storage: 80Giroot@master01[16:58:41]~/jenkins-yaml #:kubectl apply -f jenkins-pvc.yaml persistentvolumeclaim/jenkins-datadir-pvc created persistentvolumeclaim/jenkins-root-datadir-pvc created root@master01[16:58:28]~/jenkins-yaml #:kubectl create namespace jenkins-xin namespace/jenkins-xin created root@master01[16:58:52]~/jenkins-yaml #:kubectl get pvc -n jenkins-xin NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE jenkins-datadir-pvc Bound jenkins-datadir-pv 100Gi RWO 23s jenkins-root-datadir-pvc Bound jenkins-root-datadir-pv 100Gi RWO 23s","date":"2023-01-24","objectID":"/posts/kubernetes/primary/kubernetes-9/:3:0","tags":["k8s进阶训练营"],"title":"实战案例-运行java应用 (九)","uri":"/posts/kubernetes/primary/kubernetes-9/"},{"categories":["Kubernetes"],"content":"创建Jenkins Pod服务 kind: Deployment apiVersion: apps/v1 metadata: labels: app: jenkins-319 name: jenkins-319-deployment namespace: jenkins-xin spec: replicas: 1 selector: matchLabels: app: jenkins-319 template: metadata: labels: app: jenkins-319 spec: containers: - name: jenkins-319-container image: harbor.ceamg.com/pub-images/jenkins:v2.319.3 imagePullPolicy: IfNotPresent #imagePullPolicy: Always ports: - containerPort: 8080 protocol: TCP name: http volumeMounts: - mountPath: \"/apps/jenkins/jenkins-data/\" name: jenkins-app-datadir - mountPath: \"/root/.jenkins\" name: jenkins-root-datadir volumes: - name: jenkins-app-datadir persistentVolumeClaim: claimName: jenkins-datadir-pvc - name: jenkins-root-datadir persistentVolumeClaim: claimName: jenkins-root-datadir-pvc --- kind: Service apiVersion: v1 metadata: labels: app: jenkins-319 name: jenkins-319-service namespace: jenkins-xin spec: type: NodePort ports: - name: http port: 80 protocol: TCP targetPort: 8080 nodePort: 38080 selector: app: jenkins-319","date":"2023-01-24","objectID":"/posts/kubernetes/primary/kubernetes-9/:4:0","tags":["k8s进阶训练营"],"title":"实战案例-运行java应用 (九)","uri":"/posts/kubernetes/primary/kubernetes-9/"},{"categories":["Kubernetes"],"content":"验证服务状态 root@master01[10:36:43]~/jenkins-yaml #:kubectl apply -f jenkins-deployment.yaml deployment.apps/jenkins-319-deployment created service/jenkins-319-service created root@master01[10:39:03]~/jenkins-yaml #:kubectl get pod -n jenkins-xin NAME READY STATUS RESTARTS AGE jenkins-319-deployment-67cb4bf4c9-wnvqb 1/1 Running 0 24s","date":"2023-01-24","objectID":"/posts/kubernetes/primary/kubernetes-9/:4:1","tags":["k8s进阶训练营"],"title":"实战案例-运行java应用 (九)","uri":"/posts/kubernetes/primary/kubernetes-9/"},{"categories":["Kubernetes"],"content":"service root@master01[10:39:16]~/jenkins-yaml #:kubectl get service -n jenkins-xin NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE jenkins-319-service NodePort 10.10.24.94 \u003cnone\u003e 80:38080/TCP 41s 获取密码 ","date":"2023-01-24","objectID":"/posts/kubernetes/primary/kubernetes-9/:4:2","tags":["k8s进阶训练营"],"title":"实战案例-运行java应用 (九)","uri":"/posts/kubernetes/primary/kubernetes-9/"},{"categories":["Kubernetes"],"content":"https://kubernetes.io/zh-cn/docs/tutorials/stateful-application/mysql-wordpress-persistent-volume/Pod调度运⾏时，如果应⽤不需要任何稳定的标示、有序的部署、删除和扩展，则应该使⽤⼀组⽆状态副本的控制器来部署应⽤，例如 Deployment 或 ReplicaSet更适合⽆状态服务需求，⽽StatefulSet适合管理所有有状态的服务，⽐如MySQL、 MongoDB集群等。 运行一个有状态的应用程序：https://kubernetes.io/zh-cn/docs/tasks/run-application/run-replicated-stateful-application/ **StatefulSet **本质上是Deployment的⼀种变体，在v1.9版本中已成为GA版本，它为了解决有状态服务的问题，它所管理的Pod拥有固定的Pod名称，启停顺序，在StatefulSet中， Pod名字称为⽹络标识(hostname)，还必须要⽤到共享存储。 在Deployment中，与之对应的服务是service，⽽在StatefulSet中与之对应的headless service， headlessservice，即⽆头服务，与service的区别就是它没有Cluster IP，解析它的名称时将返回该Headless Service 对应的全部Pod的Endpoint列表。 StatefulSet 特点 给每个pod分配固定且唯⼀的⽹络标识符 给每个pod分配固定且持久化的外部存储 对pod进⾏有序的部署和扩展 对pod进有序的删除和终⽌ 对pod进有序的⾃动滚动更新 ","date":"2023-01-20","objectID":"/posts/kubernetes/primary/kubernetes-8/:0:0","tags":["k8s进阶训练营"],"title":"实战案例-基于StatefulSetMySQL主从架构 (八)","uri":"/posts/kubernetes/primary/kubernetes-8/"},{"categories":["Kubernetes"],"content":"StatefulSet 的组成部分 :::info Headless Service：⽤来定义Pod⽹络标识( DNS domain)，指的是短的serfvice(丢失了domainname)。直接解析到pod。StatefulSet：定义具体应⽤，有多少个Pod副本，并为每个Pod定义了⼀个域名。 volumeClaimTemplates： 存储卷申请模板，创建PVC，指定pvc名称⼤⼩，将⾃动创建pvc，且pvc必须由存储类供应。 ::: ","date":"2023-01-20","objectID":"/posts/kubernetes/primary/kubernetes-8/:1:0","tags":["k8s进阶训练营"],"title":"实战案例-基于StatefulSetMySQL主从架构 (八)","uri":"/posts/kubernetes/primary/kubernetes-8/"},{"categories":["Kubernetes"],"content":" 镜像准备 #准备xtrabackup镜像 root@harbor01[11:17:56]~ #:docker pull registry.cn-hangzhou.aliyuncs.com/hxpdocker/xtrabackup:1.0 root@harbor01[10:27:39]~ #:docker tag registry.cn-hangzhou.aliyuncs.com/hxpdocker/xtrabackup:1.0 harbor.ceamg.com/databases/xtrabackup:1.0 root@harbor01[10:29:24]~ #:docker push harbor.ceamg.com/databases/xtrabackup:1.0 #准备mysql 镜像 root@harbor01[10:30:00]~ #:docker pull mysql:5.7 root@harbor01[10:31:09]~ #:docker tag mysql:5.7 harbor.ceamg.com/databases/mysql:5.7 root@harbor01[10:31:42]~ #:docker push harbor.ceamg.com/databases/mysql:5.7","date":"2023-01-20","objectID":"/posts/kubernetes/primary/kubernetes-8/:3:0","tags":["k8s进阶训练营"],"title":"实战案例-基于StatefulSetMySQL主从架构 (八)","uri":"/posts/kubernetes/primary/kubernetes-8/"},{"categories":["Kubernetes"],"content":"创建PV pvc会⾃动基于PV创建，只需要有多个可⽤的PV即可， PV数量取决于计划启动多少个mysql pod，本次创建5个PV，也就是最多启动5个mysql pod 。 ","date":"2023-01-20","objectID":"/posts/kubernetes/primary/kubernetes-8/:4:0","tags":["k8s进阶训练营"],"title":"实战案例-基于StatefulSetMySQL主从架构 (八)","uri":"/posts/kubernetes/primary/kubernetes-8/"},{"categories":["Kubernetes"],"content":"创建nfs共享存储目录 root@harbor01[10:35:02]/data/k8s #:mkdir /data/k8s/mysqldata/mysql-datadir-1 root@harbor01[10:35:20]/data/k8s #:mkdir /data/k8s/mysqldata/mysql-datadir-2 root@harbor01[10:35:22]/data/k8s #:mkdir /data/k8s/mysqldata/mysql-datadir-3 root@harbor01[10:35:23]/data/k8s #:mkdir /data/k8s/mysqldata/mysql-datadir-4 root@harbor01[10:35:24]/data/k8s #:mkdir /data/k8s/mysqldata/mysql-datadir-5 vim /etc/exports /data/k8s/xinzk *(rw,sync,no_root_squash) /data/k8s/web1 *(rw,sync,no_root_squash) /data/k8s/mysqldata *(rw,sync,no_root_squash) root@harbor01[10:36:33]/data/k8s #:systemctl restart nfs-server.service root@harbor01[10:36:41]/data/k8s #:showmount -e Export list for harbor01: /data/k8s/mysqldata * /data/k8s/web1 * /data/k8s/xinzk *","date":"2023-01-20","objectID":"/posts/kubernetes/primary/kubernetes-8/:4:1","tags":["k8s进阶训练营"],"title":"实战案例-基于StatefulSetMySQL主从架构 (八)","uri":"/posts/kubernetes/primary/kubernetes-8/"},{"categories":["Kubernetes"],"content":" 创建PV yaml文件 --- apiVersion: v1 kind: PersistentVolume metadata: name: mysql-datadir-1 spec: capacity: storage: 50Gi accessModes: - ReadWriteOnce nfs: path: /data/k8s/mysqldata/mysql-datadir-1 server: 10.1.0.38 --- apiVersion: v1 kind: PersistentVolume metadata: name: mysql-datadir-2 spec: capacity: storage: 50Gi accessModes: - ReadWriteOnce nfs: path: /data/k8s/mysqldata/mysql-datadir-2 server: 10.1.0.38 --- apiVersion: v1 kind: PersistentVolume metadata: name: mysql-datadir-3 spec: capacity: storage: 50Gi accessModes: - ReadWriteOnce nfs: path: /data/k8s/mysqldata/mysql-datadir-3 server: 10.1.0.38检查pv状态 root@master01[12:06:23]~/mysql-sts-yaml #:kubectl apply -f mysql-persistentvolume.yaml persistentvolume/mysql-datadir-1 created persistentvolume/mysql-datadir-2 created persistentvolume/mysql-datadir-3 created root@master01[12:06:51]~/mysql-sts-yaml #:kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE mysql-datadir-1 50Gi RWO Retain Available 27s mysql-datadir-2 50Gi RWO Retain Available 27s mysql-datadir-3 50Gi RWO Retain Available 27s","date":"2023-01-20","objectID":"/posts/kubernetes/primary/kubernetes-8/:4:2","tags":["k8s进阶训练营"],"title":"实战案例-基于StatefulSetMySQL主从架构 (八)","uri":"/posts/kubernetes/primary/kubernetes-8/"},{"categories":["Kubernetes"],"content":"创建 ConfigMap apiVersion: v1 kind: ConfigMap metadata: name: mysql namespace: mysql-sts labels: app: mysql app.kubernetes.io/name: mysql data: primary.cnf: | [mysqld] log-bin replica.cnf: | [mysqld] super-read-only $ kubectl apply -f mysql-configmap.yaml 这个 ConfigMap 提供 my.cnf 覆盖设置，使你可以独立控制 MySQL 主服务器和副本服务器的配置。 在这里，你希望主服务器能够将复制日志提供给副本服务器， 并且希望副本服务器拒绝任何不是通过复制进行的写操作。 ","date":"2023-01-20","objectID":"/posts/kubernetes/primary/kubernetes-8/:5:0","tags":["k8s进阶训练营"],"title":"实战案例-基于StatefulSetMySQL主从架构 (八)","uri":"/posts/kubernetes/primary/kubernetes-8/"},{"categories":["Kubernetes"],"content":"创建 无头服务 Headless Service⽆头服务，与service的区别就是它没有Cluster IP，解析它的名称时将返回该Headless Service对应的全部Pod的Endpoint列表。 客户端 Service 为 mysql-read，是一种常规 Service，具有其自己的集群 IP。 该集群 IP 在报告就绪的所有 MySQL Pod 之间分配连接。 可能的端点集合包括 MySQL 主节点和所有副本节点。 **mysql-read是给slave pod使用的mysql只读服务，以此实现读写分离。 ** apiVersion: v1 kind: Service metadata: name: mysql namespace: mysql-sts labels: app: mysql app.kubernetes.io/name: mysql spec: ports: - name: mysql port: 3306 clusterIP: None selector: app: mysql --- apiVersion: v1 kind: Service metadata: name: mysql-read namespace: mysql-sts labels: app: mysql app.kubernetes.io/name: mysql readonly: \"true\" spec: ports: - name: mysql port: 3306 selector: app: mysqlkubectl apply -f mysql-services.yaml","date":"2023-01-20","objectID":"/posts/kubernetes/primary/kubernetes-8/:6:0","tags":["k8s进阶训练营"],"title":"实战案例-基于StatefulSetMySQL主从架构 (八)","uri":"/posts/kubernetes/primary/kubernetes-8/"},{"categories":["Kubernetes"],"content":"创建 StatefulSet 创建MySQL一主多从集群，每个pod分别执行4个容器。具体作用如下： 初始化容器1：根据mysql-0数字标记为master，其它为slave，并分发不同配置文件。 初始化容器2：mysql-0不动，mysql-1从mysql-0全量拷贝数据，mysql-2再从mysql-1全量拷贝，以此类推。 主容器mysql：数据库主程序，都有读写功能。读写分离依靠mysql和mysql-read服务实现。 主容器xtrabackup：实现主从复制自动备份，除刚创建外都从mysql-0拷贝。开放3307端口供后一位pod全量复制。 apiVersion: apps/v1 kind: StatefulSet metadata: name: mysql namespace: mysql-sts spec: selector: matchLabels: app: mysql app.kubernetes.io/name: mysql serviceName: mysql replicas: 3 template: metadata: labels: app: mysql app.kubernetes.io/name: mysql spec: initContainers: - name: init-mysql image: harbor.ceamg.com/databases/mysql:5.7 command: - bash - \"-c\" - | set -ex # 基于 Pod 序号生成 MySQL 服务器的 ID。 [[ $HOSTNAME =~ -([0-9]+)$ ]] || exit 1 ordinal=${BASH_REMATCH[1]} echo [mysqld] \u003e /mnt/conf.d/server-id.cnf # 添加偏移量以避免使用 server-id=0 这一保留值。 echo server-id=$((100 + $ordinal)) \u003e\u003e /mnt/conf.d/server-id.cnf # 将合适的 conf.d 文件从 config-map 复制到 emptyDir。 if [[ $ordinal -eq 0 ]]; then cp /mnt/config-map/primary.cnf /mnt/conf.d/ else cp /mnt/config-map/replica.cnf /mnt/conf.d/ fi volumeMounts: - name: conf mountPath: /mnt/conf.d - name: config-map mountPath: /mnt/config-map - name: clone-mysql image: harbor.ceamg.com/databases/xtrabackup:1.0 command: - bash - \"-c\" - | set -ex # 如果已有数据，则跳过克隆。 [[ -d /var/lib/mysql/mysql ]] \u0026\u0026 exit 0 # 跳过主实例（序号索引 0）的克隆。 [[ `hostname` =~ -([0-9]+)$ ]] || exit 1 ordinal=${BASH_REMATCH[1]} [[ $ordinal -eq 0 ]] \u0026\u0026 exit 0 # 从原来的对等节点克隆数据。 ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql # 准备备份。 xtrabackup --prepare --target-dir=/var/lib/mysql volumeMounts: - name: data mountPath: /var/lib/mysql subPath: mysql - name: conf mountPath: /etc/mysql/conf.d containers: - name: mysql image: harbor.ceamg.com/databases/mysql:5.7 env: - name: MYSQL_ALLOW_EMPTY_PASSWORD value: \"1\" ports: - name: mysql containerPort: 3306 volumeMounts: - name: data mountPath: /var/lib/mysql subPath: mysql - name: conf mountPath: /etc/mysql/conf.d resources: requests: cpu: 500m memory: 1Gi livenessProbe: exec: command: [\"mysqladmin\", \"ping\"] initialDelaySeconds: 30 periodSeconds: 10 timeoutSeconds: 5 readinessProbe: exec: # 检查我们是否可以通过 TCP 执行查询（skip-networking 是关闭的）。 command: [\"mysql\", \"-h\", \"127.0.0.1\", \"-e\", \"SELECT 1\"] initialDelaySeconds: 5 periodSeconds: 2 timeoutSeconds: 1 - name: xtrabackup image: harbor.ceamg.com/databases/xtrabackup:1.0 ports: - name: xtrabackup containerPort: 3307 command: - bash - \"-c\" - | set -ex cd /var/lib/mysql # 确定克隆数据的 binlog 位置（如果有的话）。 if [[ -f xtrabackup_slave_info \u0026\u0026 \"x$(\u003cxtrabackup_slave_info)\" != \"x\" ]]; then # XtraBackup 已经生成了部分的 “CHANGE MASTER TO” 查询 # 因为我们从一个现有副本进行克隆。(需要删除末尾的分号!) cat xtrabackup_slave_info | sed -E 's/;$//g' \u003e change_master_to.sql.in # 在这里要忽略 xtrabackup_binlog_info （它是没用的）。 rm -f xtrabackup_slave_info xtrabackup_binlog_info elif [[ -f xtrabackup_binlog_info ]]; then # 我们直接从主实例进行克隆。解析 binlog 位置。 [[ `cat xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1 rm -f xtrabackup_binlog_info xtrabackup_slave_info echo \"CHANGE MASTER TO MASTER_LOG_FILE='${BASH_REMATCH[1]}',\\ MASTER_LOG_POS=${BASH_REMATCH[2]}\" \u003e change_master_to.sql.in fi # 检查我们是否需要通过启动复制来完成克隆。 if [[ -f change_master_to.sql.in ]]; then echo \"Waiting for mysqld to be ready (accepting connections)\" until mysql -h 127.0.0.1 -e \"SELECT 1\"; do sleep 1; done echo \"Initializing replication from clone position\" mysql -h 127.0.0.1 \\ -e \"$(\u003cchange_master_to.sql.in), \\ MASTER_HOST='mysql-0.mysql', \\ MASTER_USER='root', \\ MASTER_PASSWORD='', \\ MASTER_CONNECT_RETRY=10; \\ START SLAVE;\" || exit 1 # 如果容器重新启动，最多尝试一次。 mv change_master_to.sql.in change_master_to.sql.orig fi # 当对等点请求时，启动服务器发送备份。 exec ncat --listen --keep-open --send-only --max-conns=1 3307 -c \\ \"xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1 --user=root\" volumeMounts: - name: data mountPath: /var/lib/mysql subPath: mysql - name: conf mountPath: /etc/mysql/conf.d resources: requests: cpu: 100m memory: 100Mi volumes: - name: conf emptyDir: {} - name: con","date":"2023-01-20","objectID":"/posts/kubernetes/primary/kubernetes-8/:7:0","tags":["k8s进阶训练营"],"title":"实战案例-基于StatefulSetMySQL主从架构 (八)","uri":"/posts/kubernetes/primary/kubernetes-8/"},{"categories":["Kubernetes"],"content":"运⾏mysql服务 root@master01[13:33:35]~/mysql-sts-yaml #:kubectl get pod -n mysql-sts NAME READY STATUS RESTARTS AGE mysql-0 2/2 Running 0 102s mysql-1 2/2 Running 0 64s mysql-2 1/2 Running 0 18s ","date":"2023-01-20","objectID":"/posts/kubernetes/primary/kubernetes-8/:7:1","tags":["k8s进阶训练营"],"title":"实战案例-基于StatefulSetMySQL主从架构 (八)","uri":"/posts/kubernetes/primary/kubernetes-8/"},{"categories":["Kubernetes"],"content":"验证MySQL主从同步是否正常 ","date":"2023-01-20","objectID":"/posts/kubernetes/primary/kubernetes-8/:7:2","tags":["k8s进阶训练营"],"title":"实战案例-基于StatefulSetMySQL主从架构 (八)","uri":"/posts/kubernetes/primary/kubernetes-8/"},{"categories":["Ceph"],"content":"Ceph dashboard是通过一个 web界面，对已经运行的ceph集群进行状态查看及功能配置等功能，早期ceph使用的是第三方的dashboard组件，如: **Calamari: **Calamari 对外提供了十分漂亮的Web管理和监控界面，以及一套改进的REST API接口(不同于Ceph自身的REST API)，在一定程度上简化了Ceph的管理，最初Calamari是作为 Inktank公司的Ceph企业级商业产品来销售，红帽2015年收购Inktank 后为了更好地推动Ceph的发展，对外宣布Calamari 开源https://github.com/ceph/calamari优点: 管理功能好 界面友好 可以利用它来部署Ceph和监控Ceph 缺点: 非官方 依赖OpenStack某些包 (ceph@ceph-deploy ceph-cluster]$ ceph-deploy -h ....... calamari Install and configure Calamari nodes. Assumes that a repository with Calamari packages is already configured. Refer to the docs for examples (http://ceph.com/ceph-deploy/docs/conf.html)VSM:Virtual Storage Manager (VSM)是Intel公司研发并且开源的一款Ceph集群管理和监控软件，简化了一些Ceph集群部署的一些步骤， 可以简单的通过 WEB页面来操作.https://github.com/intel/virtual-storage-manager 优点: 易部署 轻量级 灵活(可以自定义开发功能) 缺点: 监控选项少 缺乏Ceph管理功能 Inkscope:Inkscope是一个Ceph的管理和监控系统，依赖于Ceph提供的API,使用MongoDB来存储实时的监控数据和历史信息。https://github.com/inkscope/inkscope优点: 易部署 轻量级 灵活(可以自定义开发功能) 缺点: 监控选项少 缺乏Ceph管理功能 Ceph-Dash: Ceph-Dash是用 Python 开发的一一个Ceph的监控面板，用来监控Ceph的运行状态。同时提供REST API来访问状态数据。http://cephdash.crapworks.de/ 优点: 易部署 轻量级 灵活(可以自定义开发功能) 缺点: 功能相对简单 ","date":"2023-01-18","objectID":"/posts/ceph/10.-ceph-dashboard%E5%8F%8A%E7%9B%91%E6%8E%A7/:0:0","tags":["分布式存储"],"title":"Ceph Dashboard及监控 （十）","uri":"/posts/ceph/10.-ceph-dashboard%E5%8F%8A%E7%9B%91%E6%8E%A7/"},{"categories":["Ceph"],"content":"10.1 启用dashboard插件 https://docs.ceph.com/en/mimic/mgr/https://docs.ceph.com/en/latest/mgr/dashboard/https://packages.debian.org/unstable/ceph-mgr-dashboard #15 版本有依赖需要单独解决Ceph mgr 是一个多插件(模块化)的组件，其组件可以单独的启用或关闭,以下为在ceph-deploy服务器操作:新版本需要安装 dashboard 包，而且必须安装在mgr节点，否则报错如下: The following packages have unmet dependencies: ceph-mgr-dashboard : Depends: ceph-mgr (= 15.2.13-1-bpo10+1) but it is not going to be installed E: Unable to correct problems, you have held broken packages. root@ceph-mgr1:~# apt-cache madison ceph-mgr-dashboard root@ceph-mgr1:~# apt install ceph-mgr-dashboard[ceph@ceph-deploy ceph-cluster]$ ceph mgr module -h #查看帮助 [ceph@ceph-deploy ceph-cluster|$ ceph mgr module ls #列出所有模块状态 { \"enabled_modules\": [ #已开启的模块 \"balancer\", \"crash\", \"iostat\", \"restful\", \"status\" ], \"disabled_modules\": [ #已关闭的模块 { \"name\": \"dashboard\"， \"can_ run\": true, #是否可以启用 \"error string\": \"\" }, { \"name\": \"hello\", \"can_run\": true, \"error_string\":\"\" }, ------ [ceph@ceph-deploy ceph-cluster]$ ceph mgr module enable dashboard #启用模块注:模块启用后还不能直接访问，需要配置关闭SSL或启用SSL及指定监听地址. ","date":"2023-01-18","objectID":"/posts/ceph/10.-ceph-dashboard%E5%8F%8A%E7%9B%91%E6%8E%A7/:1:0","tags":["分布式存储"],"title":"Ceph Dashboard及监控 （十）","uri":"/posts/ceph/10.-ceph-dashboard%E5%8F%8A%E7%9B%91%E6%8E%A7/"},{"categories":["Ceph"],"content":"10.1.2 启用dashboard模块 Ceph dashboard在mgr节点进行开启设置，并且可以配置开启或者关闭SSL，如下: [ceph@ceph-deploy ceph-cluster]$ ceph config set mgr mgr/dashboard/ssl false #关闭mgr SSL [ceph@ceph-deploy ceph-clusterl$ ceph config set mgr mgr/dashboard/ceph-mgr1/server_addr 172.31.6.107 #指定dashboard监听地址 [ceph@ceph-deploy ceph-cluster]$ ceph config set mgr mgr/dashboard/ceph-mgr1/server_port 9009 #指定dashboard监听端口 #验证ceph集群状态: (ceph@ceph-deploy ceph-cluster]$ ceph -s cluster: id: 23b0f9f2-8db3-477f-99a7-35a90eaf3dab health: HEALTH_ OK services: mon: 3 daemons, quorum ceph-mon1 ,ceph-mon2,ceph-mon3 mgr: ceph-mgr1(active), standbys: ceph-mgr2 mds: mycephfs-2/2/2 up {0=ceph-mgr1=up:active, 1=ceph-mgr2=upactive}, 1 up:standby osd: 12 osds: 12 up, 12 in 如果有以下报错: Module 'dashboard' has failed: error(\"No socket could be created'，) 需要检查mgr服务是否正常运行，可以重启一遍mgr服务","date":"2023-01-18","objectID":"/posts/ceph/10.-ceph-dashboard%E5%8F%8A%E7%9B%91%E6%8E%A7/:1:1","tags":["分布式存储"],"title":"Ceph Dashboard及监控 （十）","uri":"/posts/ceph/10.-ceph-dashboard%E5%8F%8A%E7%9B%91%E6%8E%A7/"},{"categories":["Ceph"],"content":"10.1.3 在mgr节点验证端口与进程 [root@ceph-mgr1 ~]# lsof -i:9009 COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME ceph-mgr 2338 ceph 28u IPv4 23986 OtO TCP *:pichat (LISTEN)","date":"2023-01-18","objectID":"/posts/ceph/10.-ceph-dashboard%E5%8F%8A%E7%9B%91%E6%8E%A7/:1:2","tags":["分布式存储"],"title":"Ceph Dashboard及监控 （十）","uri":"/posts/ceph/10.-ceph-dashboard%E5%8F%8A%E7%9B%91%E6%8E%A7/"},{"categories":["Ceph"],"content":"10.1.4 dashboard访问验证 ","date":"2023-01-18","objectID":"/posts/ceph/10.-ceph-dashboard%E5%8F%8A%E7%9B%91%E6%8E%A7/:1:3","tags":["分布式存储"],"title":"Ceph Dashboard及监控 （十）","uri":"/posts/ceph/10.-ceph-dashboard%E5%8F%8A%E7%9B%91%E6%8E%A7/"},{"categories":["Ceph"],"content":"10.1.5 设置dashboard账户及密码 Ubuntu: ceph@ceph-deploy:/home/ceph/ceph-clustqr$ touch pass.txt ceph@ceph-deploy:/home/ceph/ceph-cluster$ echo \"12345678\" \u003e pass.txt ceph@ceph-deploy:/home/ceph/ceph-cluster$ ceph dashboard set-login-credentials jack -i pass.txt ******************************************************************************** ***WARNING: this command is deprecated. *** Please use the ac-user-* related commands to manage users. *** ******************************************************************************** Username and password updated早期方式： [ceph@ceph-deploy ceph-cluster]$ ceph dashboard set-login-credentials -h #命令格式 Monitor commands: ==================== Dashboard set-login-credentials \u003cusername\u003e \u003cpassword\u003e Set the login credentials [ceph@ceph-deploy ceph-cluster]$ ceph dashboard set-login-credentials jack 123456 Username and password updated #设置jack用户密码为123456 ","date":"2023-01-18","objectID":"/posts/ceph/10.-ceph-dashboard%E5%8F%8A%E7%9B%91%E6%8E%A7/:1:4","tags":["分布式存储"],"title":"Ceph Dashboard及监控 （十）","uri":"/posts/ceph/10.-ceph-dashboard%E5%8F%8A%E7%9B%91%E6%8E%A7/"},{"categories":["Ceph"],"content":"10.1.7 dashboard SSL 如果要使用SSL访问。则需要配置签名证书.证书可以使用ceph命令生成，或是opessl命令生成.https://docs.ceph.com/en/latest/mgr/dashboard/ 10.1.7.1 ceph自签名证书 #生成证书: [ceph@ceph-deploy ceph-cluster]$ ceph dashboard create-self-signed-cert #启用SSL: [ceph@ceph-deploy ceph-cluster]$ ceph config set mgr mgr/dashboard/ssl true #查看当前dashboard状态: [ceph@ceph-deploy ceph-cluster]$ ceph mgr services { \"dashboard\": \"http://172.31.6.107:9009/\" } #重启mgr服务: [root@ceph-mgr1 ~]# systemctl restart ceph-mgr@ceph-mgr1 #再次验证dashboard: [ceph@ceph-deploy ceph-cluster}$ ceph mgr services { \"dashboard\": \"https://172.31.6.107:9009/\" }10.1.7.2 验证证书信息 10.1.7.4 登陆成功 ","date":"2023-01-18","objectID":"/posts/ceph/10.-ceph-dashboard%E5%8F%8A%E7%9B%91%E6%8E%A7/:1:5","tags":["分布式存储"],"title":"Ceph Dashboard及监控 （十）","uri":"/posts/ceph/10.-ceph-dashboard%E5%8F%8A%E7%9B%91%E6%8E%A7/"},{"categories":["Ceph"],"content":"10.2 通过prometheus监控ceph node节点 https://prometheus.io/ ","date":"2023-01-18","objectID":"/posts/ceph/10.-ceph-dashboard%E5%8F%8A%E7%9B%91%E6%8E%A7/:2:0","tags":["分布式存储"],"title":"Ceph Dashboard及监控 （十）","uri":"/posts/ceph/10.-ceph-dashboard%E5%8F%8A%E7%9B%91%E6%8E%A7/"},{"categories":["Ceph"],"content":"10.2.1 部署prometheus [root@ceph-mgr1 ~]# mkdir /apps [root@ceph-mgr1 ~]# cd /apps/ root@ceph-mgr1 apps]# tar xvf prometheus-2.27.1.linux-amd64.tar.gz [root@ceph-mgr1 apps]# ln -sv /apps/prometheus-2.27.1.linux-amd64 /apps/prometheus '/apps/prometheus’-\u003e' /apps/ prometheus-2.27.1.linux-amd64'[root@ceph-node1 prometheus]# cat /etc/systemd/system/prometheus.service [Unit] Description=Prometheus Server Documentation=https://prometheus.io/docs/introduction/overview/ After=network.target [Service] Restart=on-failure WorkingDirectory=/apps/prometheus/ ExecStart=/apps/prometheus/prometheus --config.file=/apps/prometheus/prometheus.yml [Istall] WantedBy=multi-user.target root@ceph-mgr1 apps]# systemctl daemon-reload root@ceph-mgr1 apps]# systemctl restart prometheus root@ceph-mgr1 apps]# systemctl enable prometheus","date":"2023-01-18","objectID":"/posts/ceph/10.-ceph-dashboard%E5%8F%8A%E7%9B%91%E6%8E%A7/:2:1","tags":["分布式存储"],"title":"Ceph Dashboard及监控 （十）","uri":"/posts/ceph/10.-ceph-dashboard%E5%8F%8A%E7%9B%91%E6%8E%A7/"},{"categories":["Ceph"],"content":"10.2.2 访问prometheus ","date":"2023-01-18","objectID":"/posts/ceph/10.-ceph-dashboard%E5%8F%8A%E7%9B%91%E6%8E%A7/:2:2","tags":["分布式存储"],"title":"Ceph Dashboard及监控 （十）","uri":"/posts/ceph/10.-ceph-dashboard%E5%8F%8A%E7%9B%91%E6%8E%A7/"},{"categories":["Ceph"],"content":"10.2.3 部署node_exporter 各node节点安装node_exporter [root@ceph-node1 ~]# mkdir /apps [root@ceph-node1 ~]# cd /apps/ [root@ceph-node1 apps]# tar xvf node_exporter-1.0.1.inux. amd64.tar.gz root@ceph-node1 apps]# ln -sv /apps/node_exporter-1.0.1.linux -amd64 /apps/node_exporter rootaceph-node1:/apps# scp node_exporter-1.0.1.linux-amd64.tar.gz 172.31.6.107:/apps/[root@ceph-node2 apps]# cat /etc/systemd/system/node-exporter.service [Unit] Description=Prometheus Node Exporter After-network.target [Service] ExecStart=/apps/node_exporter/node_exporter [Instal] WantedBy=multi-user.target root@ceph-node1 apps]# systemctl daemon-reload [root@ceph-node1 apps]# systemctl restart node-exporter [root@ceph-node1 apps]# svstemctl enable node-exporter ","date":"2023-01-18","objectID":"/posts/ceph/10.-ceph-dashboard%E5%8F%8A%E7%9B%91%E6%8E%A7/:2:3","tags":["分布式存储"],"title":"Ceph Dashboard及监控 （十）","uri":"/posts/ceph/10.-ceph-dashboard%E5%8F%8A%E7%9B%91%E6%8E%A7/"},{"categories":["Ceph"],"content":"10.2.4 配置prometheus server数据并验证 vim /apps/prometheus-2.23.0.linux-amd64/prometheus.yaml scrape configs : # The job name is added as a Label. job=\u003cjob_ name\u003e to any timeseries scraped from this config. - job_name: 'prometheus' # metrics_path defaults to ' /metrics' # scheme defaults to 'http'. static_configs: - targets: ['localhost:9090'] - job_name: 'ceph-node monitor' static_configs: - targets: ['172.31.6.106:9100','172.31.6.107:9100'] ","date":"2023-01-18","objectID":"/posts/ceph/10.-ceph-dashboard%E5%8F%8A%E7%9B%91%E6%8E%A7/:2:4","tags":["分布式存储"],"title":"Ceph Dashboard及监控 （十）","uri":"/posts/ceph/10.-ceph-dashboard%E5%8F%8A%E7%9B%91%E6%8E%A7/"},{"categories":["Ceph"],"content":"10.3 通过prometheus监控ceph服务 Ceph manager内部的模块中包含了prometheus 的监控模块,并监听在每个 manager 节点的9283端口，该端口用于将采集到的信息通过 http接口向prometheus 提供数据.https://docs.ceph.com/en/mimic/mgr/prometheus/?highlight=prometheus ","date":"2023-01-18","objectID":"/posts/ceph/10.-ceph-dashboard%E5%8F%8A%E7%9B%91%E6%8E%A7/:3:0","tags":["分布式存储"],"title":"Ceph Dashboard及监控 （十）","uri":"/posts/ceph/10.-ceph-dashboard%E5%8F%8A%E7%9B%91%E6%8E%A7/"},{"categories":["Ceph"],"content":"10.3.1 启用 prometheus 监控模块 [ceph@ceph-deploy ceph-cluster]$ ceph mgr module enable prometheus ","date":"2023-01-18","objectID":"/posts/ceph/10.-ceph-dashboard%E5%8F%8A%E7%9B%91%E6%8E%A7/:3:1","tags":["分布式存储"],"title":"Ceph Dashboard及监控 （十）","uri":"/posts/ceph/10.-ceph-dashboard%E5%8F%8A%E7%9B%91%E6%8E%A7/"},{"categories":["Ceph"],"content":"10.3.2 验证manager 数据 ","date":"2023-01-18","objectID":"/posts/ceph/10.-ceph-dashboard%E5%8F%8A%E7%9B%91%E6%8E%A7/:3:2","tags":["分布式存储"],"title":"Ceph Dashboard及监控 （十）","uri":"/posts/ceph/10.-ceph-dashboard%E5%8F%8A%E7%9B%91%E6%8E%A7/"},{"categories":["Ceph"],"content":"10.3.3 配置Prometheus 采集数据 vim /apps/prometheus-2.23.0.linux-amd64/prometheus.yaml - job_name: 'ceph-cluster-monitor' static_configs: - targets:['172.31.6.105:9283'] systemctl restart prometheus.service","date":"2023-01-18","objectID":"/posts/ceph/10.-ceph-dashboard%E5%8F%8A%E7%9B%91%E6%8E%A7/:3:3","tags":["分布式存储"],"title":"Ceph Dashboard及监控 （十）","uri":"/posts/ceph/10.-ceph-dashboard%E5%8F%8A%E7%9B%91%E6%8E%A7/"},{"categories":["Ceph"],"content":"10.3.4 验证数据 ","date":"2023-01-18","objectID":"/posts/ceph/10.-ceph-dashboard%E5%8F%8A%E7%9B%91%E6%8E%A7/:3:4","tags":["分布式存储"],"title":"Ceph Dashboard及监控 （十）","uri":"/posts/ceph/10.-ceph-dashboard%E5%8F%8A%E7%9B%91%E6%8E%A7/"},{"categories":["Ceph"],"content":"10.4 通过grafana显示监控数据 通过granfana 显示对ceph的集群监控数据及node 数据. ","date":"2023-01-18","objectID":"/posts/ceph/10.-ceph-dashboard%E5%8F%8A%E7%9B%91%E6%8E%A7/:4:0","tags":["分布式存储"],"title":"Ceph Dashboard及监控 （十）","uri":"/posts/ceph/10.-ceph-dashboard%E5%8F%8A%E7%9B%91%E6%8E%A7/"},{"categories":["Ceph"],"content":"10.4.1 安装grafana [root@ceph-mgr1 apps]# yum localinstall grafana-7.5.7-1.x86_64.rpm [root@ceph-mgr1 apps]# systemctl enable grafana-server [root@ceph-mgr1 apps]# systemctl restart grafana-server","date":"2023-01-18","objectID":"/posts/ceph/10.-ceph-dashboard%E5%8F%8A%E7%9B%91%E6%8E%A7/:4:1","tags":["分布式存储"],"title":"Ceph Dashboard及监控 （十）","uri":"/posts/ceph/10.-ceph-dashboard%E5%8F%8A%E7%9B%91%E6%8E%A7/"},{"categories":["Ceph"],"content":"10.4.2 登陆 grafana 账号admin 密码 admin ","date":"2023-01-18","objectID":"/posts/ceph/10.-ceph-dashboard%E5%8F%8A%E7%9B%91%E6%8E%A7/:4:2","tags":["分布式存储"],"title":"Ceph Dashboard及监控 （十）","uri":"/posts/ceph/10.-ceph-dashboard%E5%8F%8A%E7%9B%91%E6%8E%A7/"},{"categories":["Ceph"],"content":"10.4.3 添加数据源 ","date":"2023-01-18","objectID":"/posts/ceph/10.-ceph-dashboard%E5%8F%8A%E7%9B%91%E6%8E%A7/:4:3","tags":["分布式存储"],"title":"Ceph Dashboard及监控 （十）","uri":"/posts/ceph/10.-ceph-dashboard%E5%8F%8A%E7%9B%91%E6%8E%A7/"},{"categories":["Ceph"],"content":"10.4.4 导入模板 ceph OSDhttps://grafana.com/grafana/dashboards/5336 ceph pool https://grafana.com/grafana/dashboards/5342 ","date":"2023-01-18","objectID":"/posts/ceph/10.-ceph-dashboard%E5%8F%8A%E7%9B%91%E6%8E%A7/:4:4","tags":["分布式存储"],"title":"Ceph Dashboard及监控 （十）","uri":"/posts/ceph/10.-ceph-dashboard%E5%8F%8A%E7%9B%91%E6%8E%A7/"},{"categories":["Kubernetes"],"content":"Pod状态 **第一阶段 ** **Pending **正在创建Pod但是Pod中的容器还没有全部被创建完成=[处于此状态的Pod应该检查Pod依赖的存储是否有权限挂载、镜像是否可以下载、调度是否正常等。 **Failed **Pod中有容器启动失败而导致pod工作异常。 **Unknown **由于某种原因无法获得pod的当前状态，通常是由于与pod所在的node节点通信错误。 Succeeded Pod中的所有容器都被成功终止即pod里所有的containers均已terminated. 第二阶段 **Unschedulable **Pod不能被调度，kube-scheduler没有匹配到合适的node节点。 Podscheduled pod正处于调度中，在kube-scheduler刚开始调度的时候，还没有将pod分配到指定的node，在筛选出合适的节点后就会更新etcd数据，将pod分配到指定的node。 Initialized 所有pod中的初始化容器已经完成了。 **ImagePullBackoff **Pod所在的node节点下载镜像失败 **Running **Pod内部的容器已经被创建并且启动。 **Ready **表示pod中的容器已经可以提供访问服务 Error: #pod 启动过程中发生错误 NodeLost: #Pod 所在节点失联 Unkown: #Pod 所在节点失联或其它未知异常 waiting: #Pod 等待启动 Pending: #Pod 等待被调度 Terminating: #Pod 正在被销毁 CrashLoopBackoff: #pod，但是kubelet正在将它重启 InvalidImageName: #node节点无法解析镜像名称导致的镜像无法下载 ImageInspectError: #无法校验镜像，镜像不完整导致 ErrImageNeverPull: #策略禁止拉取镜像，镜像中心权限是私有等 ImagePullBackoff: #镜像拉取失败，但是正在重新拉取 RegistryUnavailable: #镜像服务器不可用，网络原因或harbor宕机 ErrImagePull: #镜像拉取出错，超时或下载被强制终止 CreateContainerConfigError: #不能创建kubelet使用的容器配置 CreateContainerError: #创建容器失败 PreStartContainer: #执行prestart hook报错，Pod hook(钩子)是由 Kubernetes 管理的 kubelet 发起的，当容器中的进程启动前或者容器中的进程终止之前运行，比如容器创建完成后里面的服务启动之前可以检查一下依赖的其它服务是否启动，或者容器退出之前可以把容器中的服务先通过命令停止。 PoststartHookError: #执行 poststart hook 报错 RunContainerError: #pod运行失败，容器中没有初始化PID为1的守护进程等 ContainersNotInitialized: #pod没有初始化完毕 ContainersNotReady: #pod没有准备完毕 ContainerCreating: #pod正在创建中 PodInitializing: #pod正在初始化中 DockerDaemonNotReady: #node节点decker服务没有启动 NetworkPluginNotReady: #网络插件还没有完全启动","date":"2023-01-18","objectID":"/posts/kubernetes/primary/kubernetes-7/:0:0","tags":["k8s进阶训练营"],"title":"Pod的状态和探针 (七)","uri":"/posts/kubernetes/primary/kubernetes-7/"},{"categories":["Kubernetes"],"content":"Pod 探针 https://kubernetes.io/zh-cn/docs/concepts/workloads/pods/pod-lifecycle/ ","date":"2023-01-18","objectID":"/posts/kubernetes/primary/kubernetes-7/:1:0","tags":["k8s进阶训练营"],"title":"Pod的状态和探针 (七)","uri":"/posts/kubernetes/primary/kubernetes-7/"},{"categories":["Kubernetes"],"content":"探针简介 探针 是由 kubelet 对容器执行的定期诊断，以保证Pod的状态始终处于运行状态，要执行诊断，kubelet 调用由容器实现的Handler(处理程序)，有三种类型的处理程序: ExecAction #在容器内执行指定命令，如果命令退出时返回码为0则认为诊断成功。 TcPSocketAction #对指定端口上的容器的IP地址进行TCP检查，如果端口打开，则诊断被认为是成功的。 HTTPGetAction #对指定的端口和路径上的容器的IP地址执行HTTPGet请求，如果响应的状态码大于等于200且小于 400，则诊断被认为是成功的。每次探测都将获得以下三种结果之一： 成功: 容器通过了诊断 失败: 容器未通过诊断 未知: 诊断失败，因此不会采取任何行动 ","date":"2023-01-18","objectID":"/posts/kubernetes/primary/kubernetes-7/:1:1","tags":["k8s进阶训练营"],"title":"Pod的状态和探针 (七)","uri":"/posts/kubernetes/primary/kubernetes-7/"},{"categories":["Kubernetes"],"content":"配置探针 实现对Pod的状态检测 ","date":"2023-01-18","objectID":"/posts/kubernetes/primary/kubernetes-7/:2:0","tags":["k8s进阶训练营"],"title":"Pod的状态和探针 (七)","uri":"/posts/kubernetes/primary/kubernetes-7/"},{"categories":["Kubernetes"],"content":"探针类型 livenessProbe #存活探针，检测容器容器是否正在运行，如果存活探测失败，则kubelet会杀死容器，并且容器将受到其重启策略的影响，如果容器不提供存活探针，则默认状态为 Success，livenessProbe用于控制是否重启pod。 readinessProbe #就绪探针，如果就绪探测失败，端点控制器将从与Pod匹配的所有Service的端点中删除该Pod的IP地址，初始延迟之前的就绪状态默认为Failure(失败)，如果容器不提供就绪探针，则默认状态为 Success，readinessProbe用于控制pod是否添加至service。","date":"2023-01-18","objectID":"/posts/kubernetes/primary/kubernetes-7/:2:1","tags":["k8s进阶训练营"],"title":"Pod的状态和探针 (七)","uri":"/posts/kubernetes/primary/kubernetes-7/"},{"categories":["Kubernetes"],"content":"探针配置 https://kubernetes.io/zh-cn/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/探针有很多配置字段，可以使用这些字段精确的控制存活和就绪检测的行为 initialDelaySeconds: 120 #初始化延迟时间，告诉kubelet在执行第一次探测前应该等待多少秒，默认是0秒，最小值是0 periodseconds: 60 #探测周期间隔时间，指定了kubelet应该每多少秒秒执行一次存活探测，默认是 10 秒。最小值是 1 timeoutseconds: 5 #单次探测超时时间，探测的超时后等待多少秒，默认值是1秒，最小值是1。 successThreshold: 1 #从失败转为成功的重试次数，探测器在失败后，被视为成功的最小连续成功数，默认值是1，存活探测的这个值必须是1，最小值是 1。 failureThreshold: 3 #从成功转为失败的重试次数，当Pod启动了并目探测到失败，Kubernetes的重试次数，存活探测情况下的放弃就意味着重新启动容器，就绪探测情况下的放弃Pod 会被打上未就绪的标签，默认值是3，最小值是1。","date":"2023-01-18","objectID":"/posts/kubernetes/primary/kubernetes-7/:2:2","tags":["k8s进阶训练营"],"title":"Pod的状态和探针 (七)","uri":"/posts/kubernetes/primary/kubernetes-7/"},{"categories":["Kubernetes"],"content":"HTTP 探测器可以在 httpGet 上配置额外的字段 host: #连接使用的主机名，默认是Pod的 IP，也可以在HTTP头中设置 “Host” 来代替 scheme: http #用于设置连接主机的方式 (HTTP 还是 HTTPS)，默认是 HTTP。 path: /monitor/index.html #访问 HTTP 服务的路径。 httpHeaders : #请求中自定义的 HTTP 头，HTTP 头字段允许重复 port: 80 #访问容器的端口号或者端口名，如果数字必须在 1 ~ 65535 之间","date":"2023-01-18","objectID":"/posts/kubernetes/primary/kubernetes-7/:2:3","tags":["k8s进阶训练营"],"title":"Pod的状态和探针 (七)","uri":"/posts/kubernetes/primary/kubernetes-7/"},{"categories":["Kubernetes"],"content":"HTTP探针示例 #apiVersion: extensions/v1beta1 apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: replicas: 1 selector: matchLabels: #rs or deployment app: ng-deploy-80 #matchExpressions: # - {key: app, operator: In, values: [ng-deploy-80,ng-rs-81]} template: metadata: labels: app: ng-deploy-80 spec: containers: - name: ng-deploy-80 image: nginx:1.17.5 ports: - containerPort: 80 #readinessProbe: livenessProbe: httpGet: #path: /monitor/monitor.html path: /index.html port: 80 initialDelaySeconds: 5 periodSeconds: 3 timeoutSeconds: 5 successThreshold: 1 failureThreshold: 3 --- apiVersion: v1 kind: Service metadata: name: ng-deploy-80 spec: ports: - name: http port: 81 targetPort: 80 nodePort: 40012 protocol: TCP type: NodePort selector: app: ng-deploy-80验证http探针： ","date":"2023-01-18","objectID":"/posts/kubernetes/primary/kubernetes-7/:2:4","tags":["k8s进阶训练营"],"title":"Pod的状态和探针 (七)","uri":"/posts/kubernetes/primary/kubernetes-7/"},{"categories":["Kubernetes"],"content":"TCP 探针示例 apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: replicas: 1 selector: matchLabels: #rs or deployment app: ng-deploy-80 #matchExpressions: # - {key: app, operator: In, values: [ng-deploy-80,ng-rs-81]} template: metadata: labels: app: ng-deploy-80 spec: containers: - name: ng-deploy-80 image: nginx:1.17.5 ports: - containerPort: 80 livenessProbe: #readinessProbe: tcpSocket: port: 80 #port: 8080 initialDelaySeconds: 5 periodSeconds: 3 timeoutSeconds: 5 successThreshold: 1 failureThreshold: 3 --- apiVersion: v1 kind: Service metadata: name: ng-deploy-80 spec: ports: - name: http port: 81 targetPort: 80 nodePort: 40012 protocol: TCP type: NodePort selector: app: ng-deploy-80","date":"2023-01-18","objectID":"/posts/kubernetes/primary/kubernetes-7/:2:5","tags":["k8s进阶训练营"],"title":"Pod的状态和探针 (七)","uri":"/posts/kubernetes/primary/kubernetes-7/"},{"categories":["Kubernetes"],"content":"ExecAction探针 可以基于指定的命令对Pod进⾏特定的状态检查。 apiVersion: apps/v1 kind: Deployment metadata: name: redis-deployment spec: replicas: 1 selector: matchLabels: #rs or deployment app: redis-deploy-6379 #matchExpressions: # - {key: app, operator: In, values: [redis-deploy-6379,ng-rs-81]} template: metadata: labels: app: redis-deploy-6379 spec: containers: - name: redis-deploy-6379 image: redis ports: - containerPort: 6379 livenessProbe: #readinessProbe: exec: command: #- /apps/redis/bin/redis-cli - /usr/local/bin/redis-cli - quit initialDelaySeconds: 5 periodSeconds: 3 timeoutSeconds: 5 successThreshold: 1 failureThreshold: 3 --- apiVersion: v1 kind: Service metadata: name: redis-deploy-6379 spec: ports: - name: http port: 6379 targetPort: 6379 nodePort: 40016 protocol: TCP type: NodePort selector: app: redis-deploy-6379如果端⼝检测连续超过指定的三次都没有通过，则Pod状态如下： ","date":"2023-01-18","objectID":"/posts/kubernetes/primary/kubernetes-7/:2:6","tags":["k8s进阶训练营"],"title":"Pod的状态和探针 (七)","uri":"/posts/kubernetes/primary/kubernetes-7/"},{"categories":["Kubernetes"],"content":"livenessProbe和readinessProbe的对⽐ 配置参数⼀样 livenessProbe 连续探测失败会重启、重建pod， readinessProbe不会执⾏重启或者重建Pod操作livenessProbe 连续检测指定次数失败后会将容器置于(Crash Loop BackOff)且不可⽤，readinessProbe不会 readinessProbe 连续探测失败会从service的endpointd中删除该Pod， livenessProbe不具备此功能，但是会将容器挂起livenessProbe livenessProbe⽤户控制是否重启pod， readinessProbe⽤于控制pod是否添加⾄service **建议：两个探针都配置 ** ","date":"2023-01-18","objectID":"/posts/kubernetes/primary/kubernetes-7/:2:7","tags":["k8s进阶训练营"],"title":"Pod的状态和探针 (七)","uri":"/posts/kubernetes/primary/kubernetes-7/"},{"categories":["Kubernetes"],"content":"Pod重启策略 k8s在Pod出现异常的时候会⾃动将Pod重启以恢复Pod中的服务。 :::success restartPolicyAlways：当容器异常时， k8s⾃动重启该容器， ReplicationController/Replicaset/Deployment。OnFailure：当容器失败时(容器停⽌运⾏且退出码不为0)， k8s⾃动重启该容器。Never：不论容器运⾏状态如何都不会重启该容器,Job或CronJob。 ::: ","date":"2023-01-18","objectID":"/posts/kubernetes/primary/kubernetes-7/:3:0","tags":["k8s进阶训练营"],"title":"Pod的状态和探针 (七)","uri":"/posts/kubernetes/primary/kubernetes-7/"},{"categories":["Kubernetes"],"content":"镜像拉取策略 IfNotPresent node节点没有此镜像就去指定的镜像仓库拉取， node有就使⽤node本地镜像。Always 每次重建pod都会重新拉取镜像Never **从不到镜像中⼼拉取镜像，只使⽤本地镜像 ** ","date":"2023-01-18","objectID":"/posts/kubernetes/primary/kubernetes-7/:4:0","tags":["k8s进阶训练营"],"title":"Pod的状态和探针 (七)","uri":"/posts/kubernetes/primary/kubernetes-7/"},{"categories":["Ceph"],"content":"场景：集群内有固态和机械硬盘 如何让不重要的业务放到机械盘。 Ceph 集群中由 mon 服务器维护的的五种运行图: Monitor map 监视器运行图 OSD map OSD运行图 各个每隔6s汇报状态同时监控其他OSD的状态，超过20秒就会被踢出去 PG map PG运行图 （一个存储池有哪些pg） Crush map (Controllers replication under scalable hashing) 可控的、可复制的、可伸缩的一致性hash算法。crush运行图，当新建存储池时会基于OSD map创建出新的PG组合列表用于存储数据 MDS map cephfs metadata运行图 数据的访问： obj–\u003epg hash(oid)%pg=pgid 先将文件计算成一个hash值，这个数取pg数量的余数 最终得到分配到那个pg中Obj–\u003eOSD crush 根据当前的mon运行图返回pg内的最新的OSD组合,数据即可开始往主的写然后往副本OSD同步 crush 算法针对目的节点的选择:目前有5种算法来实现节点的选择，包括Uniform、List、 Tree、 Straw、 Straw2, 早期版本使用的是ceph项目的发起者发明的算法straw,目前已经发展到straw2版本。抽签算法 ","date":"2023-01-17","objectID":"/posts/ceph/9.-ceph-crush%E8%BF%9B%E9%98%B6/:0:0","tags":["分布式存储"],"title":"Ceph crush算法进阶 （九）","uri":"/posts/ceph/9.-ceph-crush%E8%BF%9B%E9%98%B6/"},{"categories":["Ceph"],"content":"9.1 PG与OSD映射调整 默认情况下,crush算法自行对创建的pool中的PG分配OSD,但是可以手动基于权重设置 crush 算法分配数据的倾向性， 比如1T 的磁盘权重是1, 2T 的就是2, 推荐使用相同大小的设备。 调整的方法有两种： 1. 调整weight值 调整reweight值 ","date":"2023-01-17","objectID":"/posts/ceph/9.-ceph-crush%E8%BF%9B%E9%98%B6/:1:0","tags":["分布式存储"],"title":"Ceph crush算法进阶 （九）","uri":"/posts/ceph/9.-ceph-crush%E8%BF%9B%E9%98%B6/"},{"categories":["Ceph"],"content":"9.1.1 查看当前状态 ceph osd df 基于存储空间**weight **表示设备(device)的容量相对值，比如 1TB 对应1.00 ,那么500G的 OSD 的 weight 就应该是0.5, weight 是基于磁盘空间分配PG的数量，让crush算法尽可能往磁盘空间大的OSD多分配OSD。往磁盘空间小的OSD分配较少的OSD。 那个磁盘快满了调整一下释放资源**Reweight **参数的目的是重新平衡 ceph的CRUSH算法随机分配的PG,默认的分配是概率上的均衡，即使OSD都是一 样的磁盘空间也会产生一些PG分布不均匀的情况， 此时可以通过调整reweight参数，让ceph集群立即重新平衡当前磁盘的PG,以达到数据均衡分布的目的，REWEIGHT 是PG已经分配完成，要在cepg集群重新平衡PG的分布。 ","date":"2023-01-17","objectID":"/posts/ceph/9.-ceph-crush%E8%BF%9B%E9%98%B6/:1:1","tags":["分布式存储"],"title":"Ceph crush算法进阶 （九）","uri":"/posts/ceph/9.-ceph-crush%E8%BF%9B%E9%98%B6/"},{"categories":["Ceph"],"content":"9.1.2 修改WEIGHT并验证 #修改某个指定ID的osd的权重 调整完立即生效 root@ceph-deploy:~# ceph osd crush reweight osd.10 1.5 #验证OSD权重: root@ceph-deploy:~# ceph osd df","date":"2023-01-17","objectID":"/posts/ceph/9.-ceph-crush%E8%BF%9B%E9%98%B6/:1:2","tags":["分布式存储"],"title":"Ceph crush算法进阶 （九）","uri":"/posts/ceph/9.-ceph-crush%E8%BF%9B%E9%98%B6/"},{"categories":["Ceph"],"content":"9.1.3 修改 REWEIGHT 并验证 OSD 的 REWEIGHT的值默认为1 ,值可以调整，范围在0~1之间，值越低PG越小，如果调整了任何一个OSD的REWEIGHT值，那么OSD的PG会立即和其它OSD进行重新平衡，即数据的重新分配，用于当某个OSD的PG相对较多需要降低其PG 数量的场景。 root@ceph-deploy:~# ceph osd reweight 9 0.6 ","date":"2023-01-17","objectID":"/posts/ceph/9.-ceph-crush%E8%BF%9B%E9%98%B6/:1:3","tags":["分布式存储"],"title":"Ceph crush算法进阶 （九）","uri":"/posts/ceph/9.-ceph-crush%E8%BF%9B%E9%98%B6/"},{"categories":["Ceph"],"content":"9.2 crush运行图管理 通过工具将 ceph 的crush 运行图导出并进行编辑，然后导入 ","date":"2023-01-17","objectID":"/posts/ceph/9.-ceph-crush%E8%BF%9B%E9%98%B6/:2:0","tags":["分布式存储"],"title":"Ceph crush算法进阶 （九）","uri":"/posts/ceph/9.-ceph-crush%E8%BF%9B%E9%98%B6/"},{"categories":["Ceph"],"content":"9.2.1 导出crush运行图 注: 导出的 crush 运行图为二进制格式,无法通过文本编辑器直接打开，需要使用crushtool工具转换为文本格式后才能通过vim等文本编辑宫工具打开和编辑。 root@ceph-deploy:~# mkdir /data/ceph -p #导出 root@ceph-deploy:~# ceph osd getcrushmap -o /data/ceph/crushmap 67","date":"2023-01-17","objectID":"/posts/ceph/9.-ceph-crush%E8%BF%9B%E9%98%B6/:2:1","tags":["分布式存储"],"title":"Ceph crush算法进阶 （九）","uri":"/posts/ceph/9.-ceph-crush%E8%BF%9B%E9%98%B6/"},{"categories":["Ceph"],"content":"9.2.2 将运行图转换为文本: 导出的运行图不能直接编辑，需要转换为文本格式再进行查看与编辑 root@ceph-deploy:~# apt install ceph-base root@ceph-deploy:~# crushtool -d /data/ceph/crushmap \u003e /data/ceph/crushmap.txt root@ceph-deploy:~# file /data/ceph/crushmap.txt /data/ceph/crushmap.txt: ASCII text","date":"2023-01-17","objectID":"/posts/ceph/9.-ceph-crush%E8%BF%9B%E9%98%B6/:2:2","tags":["分布式存储"],"title":"Ceph crush算法进阶 （九）","uri":"/posts/ceph/9.-ceph-crush%E8%BF%9B%E9%98%B6/"},{"categories":["Ceph"],"content":"9.2.3 编辑文本 #自定义修改 root@ceph-deploy:~# vim /data/ceph/crushmap.txt # begin crush map #可调整的crush map参数 tunable choose local tries 0 # devices #当前的设备列表 device 0 osd.0 class hdd device 1 osd.1 class hdd # types #当前支持的bucket类型 以什么为单位 type 0 osd #osd守护进程，对应到一个磁盘设备 type 1 host #一个主机 (默认放在不同的主机) type 2 chassis #刀片服务器的机箱 type 3 rack #包含若干个服务器的机柜/机架 type 4 row #包含若干个机柜的一排机柜 type 5 pdu #机柜的接入电源插座 type 7 room #包含若干机柜的房间，一个数据中心有好多这样的房间组成 type 8 datacenter #一个数据中心或IDS type 9 region #一个区域，比如AWS宁夏中卫数据中心 type 10 root #bucket分层的最顶部，根算法 item osd.0 weight 0.098 #osd0权重比例，crush 会自动根据磁盘空间计算，不同的磁盘空间的权重不一样 tem osd.1 weight 0.098 tem osd.2 weight 0.098 tem osd.3 weight 0.098 tem osd.4 weight 0.098 } ... root default { #根的配置 id -1 # do not change unnecessarily id -2 class hdd # do not change unnecessarily # weight 3.256 alg straw2 hash 0 # rjenkins1 item ceph-node1 weight 0.488 item ceph-node2 weight 0.488 item ceph-node3 weight 0.488 item ceph-node4 weight 0.488 } # buckets host ceph-node1 { 类型Host 名称为ceph-node1 id -3 # do not change unnecessarily #ceph生成的OSD ID,非必要不要改 id -4 class hdd # do not change unnecessarily # weight 0.488 alg straw2 #crush算法，管理OSD角色 hash 0 # rjenkins1 #使用是哪个hash算法，0表示选择rjenkins1这种hash算法 item osd.0 weight 0.098 #osd 0 权重比例，crush 会自动根据磁盘空间计算，不同的磁盘空间的权重不一样 item osd.1 weight 0.098 item osd.2 weight 0.098 item osd.3 weight 0.098 item osd.4 weight 0.098 } host ceph-node2 { id -5 # do not change unnecessarily id -6 class hdd # do not change unnecessarily # weight 0.461 alg st raw2 hash 0 # rjenkins1 item osd.5 weight 0.098 item osd.6 weight 0.098 item osd.7 weight 0.070 item osd.8 weight 0.098 item osd.9 weight 0.098 } root default { #根的配置 id -1 # do not change unnecessarily id -2 class hdd # do not change unnecessarily # weight 3.256 alg straw2 hash0 # rjenkins1 item ceph-node1 weight 0.488 item ceph-node2 weight 0.488 item ceph-node3 weight 0.488 item ceph-node4 weight 0.488 } # rules rule replicated _rule { #副本池的默认配置 id 0 type replicated min_size 1 max_size 10 #默认最大副本为10 step take default #基于default定义的主机分配OSD step chooseleaf firstn 0 type host #选择主机，故障域类型为主机 step emit #弹出配置即返回给客户端 } rule erasure-code { #纠删码池的默认配置 type erasure min_size 3 max_size 4 step set_chooseleaf_tries 5 step set_choose_tries 100 step take default step chooseleaf indep 0 type host step emit }# rules rule replicated rule { id 0 type rep Licated min_size 1 max_size 6 #修改最大副本数 step take default step chooseleaf firstn 0 type host step emit }","date":"2023-01-17","objectID":"/posts/ceph/9.-ceph-crush%E8%BF%9B%E9%98%B6/:2:3","tags":["分布式存储"],"title":"Ceph crush算法进阶 （九）","uri":"/posts/ceph/9.-ceph-crush%E8%BF%9B%E9%98%B6/"},{"categories":["Ceph"],"content":"9.2.3 将文本转换为crush格式 root@ceph-deploy:~# crushtool -c /data/ceph/crushmap.txt -o /data/ceph/newcrushmap","date":"2023-01-17","objectID":"/posts/ceph/9.-ceph-crush%E8%BF%9B%E9%98%B6/:2:4","tags":["分布式存储"],"title":"Ceph crush算法进阶 （九）","uri":"/posts/ceph/9.-ceph-crush%E8%BF%9B%E9%98%B6/"},{"categories":["Ceph"],"content":"9.2.4 导入新的crush 导入的运行图会立即覆盖原有的运行图并立即生效. root@ceph-deploy:~# ceph osd setcrushmap -i /data/ceph/newcrushmap","date":"2023-01-17","objectID":"/posts/ceph/9.-ceph-crush%E8%BF%9B%E9%98%B6/:2:5","tags":["分布式存储"],"title":"Ceph crush算法进阶 （九）","uri":"/posts/ceph/9.-ceph-crush%E8%BF%9B%E9%98%B6/"},{"categories":["Ceph"],"content":"9.2.5 验证crush运行图是否生效 root@ceph-deploy:~# ceph osd crush rule dump [ { \"rule_id\": 0, \"rule_name\": \"replicated_rule\", \"ruleset\": 0, \"type\": 1, \"min_size\": 1, \"max_size\": 6, ","date":"2023-01-17","objectID":"/posts/ceph/9.-ceph-crush%E8%BF%9B%E9%98%B6/:2:6","tags":["分布式存储"],"title":"Ceph crush算法进阶 （九）","uri":"/posts/ceph/9.-ceph-crush%E8%BF%9B%E9%98%B6/"},{"categories":["Ceph"],"content":"9.3 crush数据分类管理 Cephcrush 算法分配的PG的时候可以将PG分配到不同主机的OSD上，以实现以主机为单位的高可用，这也是默认机制，但是无法保证不同PG位于不同机柜或者机房的主机。 如果要实现基于机柜或者是更高级的IDC等方式的数据高可用，而且也不能实现A项目的数据在SSD,B项目的数据在机械磁盘，如果想要实现此功能则需要导出crush运行图并手动编辑，之后再导入并覆盖原有的crush运行图。 ","date":"2023-01-17","objectID":"/posts/ceph/9.-ceph-crush%E8%BF%9B%E9%98%B6/:3:0","tags":["分布式存储"],"title":"Ceph crush算法进阶 （九）","uri":"/posts/ceph/9.-ceph-crush%E8%BF%9B%E9%98%B6/"},{"categories":["Ceph"],"content":"9.3.1 导出cursh运行图 root@ceph-deploy:~# mkdir /opt/ceph/ root@ceph-deploy:~# ceph osd getcrushmap -o /opt/ceph/crushmap 68","date":"2023-01-17","objectID":"/posts/ceph/9.-ceph-crush%E8%BF%9B%E9%98%B6/:3:1","tags":["分布式存储"],"title":"Ceph crush算法进阶 （九）","uri":"/posts/ceph/9.-ceph-crush%E8%BF%9B%E9%98%B6/"},{"categories":["Ceph"],"content":"9.3.2 将运行图转换为文本 root@ceph-deploy:~# crushtool -d /opt/ceph/crushmap \u003e /opt/ceph/crushmap.txt root@ceph-deploy:- # file /opt/ceph/crushmap.txt /opt/ceph/crushmap.txt: ASCII text","date":"2023-01-17","objectID":"/posts/ceph/9.-ceph-crush%E8%BF%9B%E9%98%B6/:3:2","tags":["分布式存储"],"title":"Ceph crush算法进阶 （九）","uri":"/posts/ceph/9.-ceph-crush%E8%BF%9B%E9%98%B6/"},{"categories":["Ceph"],"content":"9.3.3 添加自定义配置 root@ceph-deploy:~# cat topt/ceph/crushmap.txt # begin crush map tunable choose_local_tries 0 tunable choose_local_fallback_tries 0 tunable choose_total_tries 50 tunable chooseleaf_descend_once 1 tunable chooseleaf_vary_r 1 tunable chooseleaf_stable 1 tunable straw_calc_version 1 tunable allowed_bucket_algs 54 # devices device 0 osd.0 class hdd device 1 osd.1 class hdd device 2 osd.2 class hdd device 3 osd.3 class hdd device 4 osd.4 class hdd device 5 osd.5 class hdd device 6 osd.6 class hdd device 7 osd.7 class hdd device 8 osd.8 class hdd device 9 osd.9 class hdd device 10 osd.10 class hdd device 11 osd.11 class hdd device 12 osd.12 class hdd device 13 osd.13 class hdd device 14 osd.14 class hdd device 15 osd.15 class hdd device 16 osd.16 class hdd device 17 osd.17 class hdd device 18 osd.18 class hdd device 19 osd.19 class hdd # types type 0 osd type 1 host type 2 chassis type 3 rack type 4 row type 5 pdu type 6 pod type 7 room type 8 datacenter type 9 zone type 10 region type 11 root # buckets host ceph-node1 { id -3 # do not change unnecessarily id -4 class hdd # do not change unnecessarily # weight 0.490 alg straw2 hash0 # rjenkins1 item osd.0 weight 0.098 item osd.1 weight 0.098 item osd.2 weight 0.098 item osd.3 weight 0.098 } host ceph-node2 { id -5 # do not change unnecessarily id -6 class hdd # do not change unnecessarily # weight 0.490 alg straw2 hash0 # rjenkins1 item osd.5 weight 0.098 item osd.6 weight 0.098 item osd.7 weight 0.098 item osd.8 weight 0.098 item osd.9 weight 0.098 } host ceph-node3 { id -7 # do not change unnecessarily id -8 class hdd # do not change unnecessarily # weight 1.792 alg straw2 hash 0 # rjenkins1 item osd.10 weight 1.400 item osd.11 weight 0.098 item osd.12 weight 0.098 item osd.13 weight 0.098 item osd.14 weight 0.098 } host ceph-node4 { id -9 # do not change unnecessarily id -10 class hdd # do not change unnecessarily # weight 0.490 alg straw2 hash 0 # rjenkins1 item osd.15 weight 0.098 item osd.16 weight 0.098 item osd.17 weight 0.098 item osd.18 weight 0.098 item osd.19 weight 0.098 } root default { id -1 # do not change unnecessarily id -2 class hdd # do not change unnecessarily # weight 3.255 alg straw2 hash 0 # rjenkins1 item ceph-node1 weight 0.488 item ceph-node2 weight 0.488 item ceph-node3 weight 1.791 item ceph-node4 weight 0.488 } #前面是机械节点，后面定义ssd节点，ID不能冲突 #magedu ssd node host ceph-ssdnode1 { id -103 # do not change unnecessarily id -104 class hdd # do not change unnecessarily # weight 0.098 alg straw2 hash 0 # rjenkins1 item osd.0 weight 0.098 } host ceph-ssdnode2 { id -105 # do not change unnecessarily id -106 class hdd # do not change unnecessarily # weight 0.098 alg straw2 hash0 # rjenkins1 item osd.5 weight 0.098 } host ceph-ssdnode3 { id -107 # do not change unnecessarily id -108 class hdd # do not change unnecessarily # weight 0.098 alg straw2 hash 0 # rjenkins1 item osd.10 weight 0.098 } host ceph-ssdnode4 { id -109 # do not change unnecessarily id -110 class hdd # do not change unnecessarily # weight 0.098 alg straw2 hash 0 # rjenkins1 item osd.15 weight 0.098 } #magedu bucket 把定义node加进来 root ssd { id -127 # do not change unnecessarily id -111 class hdd # do not change unnecessarily # weight 1.952 alg straw hash 0 # rjenkins1 item ceph-ssdnode1 weight 0.488 item ceph-ssdnode2 weight 0.488 item ceph-ssdnode3 weight 0.488 item ceph-ssdnode4 weight 0.488 } #magedu ssd-rules rule magedu_ssd_rule { id 20 type replicated #类型副本池 min_size 1 max_size 5 step take ssd #定义使用的bucket step chooseleaf firstn 0 type host #选择方法 选择当前副本数个主机下的OSD， 高可用类型host step emit } ## chooseleaf表示在确定故障域后，还必须选出该域下面的OSD节点 rule replicated rule { id 0 type replicated min_size 1 max_size 10 step take default step chooseleaf firstn 0 type host step emit } rule erasure-code { id 1 type erasure min_size 3 max_size 4 step set_chooseleaf_tries 5 step set_choose_tries 100 step take default step chooseleaf indep 0 type host step emit } rule erasure-code { id 1 type erasure min_size 3 max_size 4","date":"2023-01-17","objectID":"/posts/ceph/9.-ceph-crush%E8%BF%9B%E9%98%B6/:3:3","tags":["分布式存储"],"title":"Ceph crush算法进阶 （九）","uri":"/posts/ceph/9.-ceph-crush%E8%BF%9B%E9%98%B6/"},{"categories":["Ceph"],"content":"9.3.4 转换为crush二进制格式 root@ceph-deploy:~# crushtool -C /opt/ceph/crushmap.txt -o /opt/ceph/newcrushmap","date":"2023-01-17","objectID":"/posts/ceph/9.-ceph-crush%E8%BF%9B%E9%98%B6/:3:4","tags":["分布式存储"],"title":"Ceph crush算法进阶 （九）","uri":"/posts/ceph/9.-ceph-crush%E8%BF%9B%E9%98%B6/"},{"categories":["Ceph"],"content":"9.3.5 导入新的crush运行图 root@ceph-deploy:~# ceph osd setcrushmap -i /opt/ceph/crushmap 70","date":"2023-01-17","objectID":"/posts/ceph/9.-ceph-crush%E8%BF%9B%E9%98%B6/:3:5","tags":["分布式存储"],"title":"Ceph crush算法进阶 （九）","uri":"/posts/ceph/9.-ceph-crush%E8%BF%9B%E9%98%B6/"},{"categories":["Ceph"],"content":"9.3.6 验证crush运行图是否生效 root@ceph-deploy:~# ceph osd crush rule dump","date":"2023-01-17","objectID":"/posts/ceph/9.-ceph-crush%E8%BF%9B%E9%98%B6/:3:6","tags":["分布式存储"],"title":"Ceph crush算法进阶 （九）","uri":"/posts/ceph/9.-ceph-crush%E8%BF%9B%E9%98%B6/"},{"categories":["Ceph"],"content":"9.3.7 测试创建存储池 root@ceph-deploy:~# peph osd pool create magedu-ssdpool 32 32 magedu_ssd_rule pool 'magedu-ssdpool' created #指定存储池 rule 默认是defuse","date":"2023-01-17","objectID":"/posts/ceph/9.-ceph-crush%E8%BF%9B%E9%98%B6/:3:7","tags":["分布式存储"],"title":"Ceph crush算法进阶 （九）","uri":"/posts/ceph/9.-ceph-crush%E8%BF%9B%E9%98%B6/"},{"categories":["Ceph"],"content":"9.3.8 验证pgp状态 ceph pg ls-by-pool magedu-ssdpool awk '{print $1, $2,$15} ' ","date":"2023-01-17","objectID":"/posts/ceph/9.-ceph-crush%E8%BF%9B%E9%98%B6/:3:8","tags":["分布式存储"],"title":"Ceph crush算法进阶 （九）","uri":"/posts/ceph/9.-ceph-crush%E8%BF%9B%E9%98%B6/"},{"categories":["Kubernetes"],"content":"rbd结合k8s提供存储卷及动态存储卷使用案例 目的： 让k8s 中的 pod 可以访问 ceph中rbd 提供的镜像作为存储设备。 需要在 ceph 创建rbd并且让 k8s node 节点能够通过 ceph 的认证k8s在使用 ceph 作为动态存储卷的时候，需要 **kube-controller-manager **组件能够访问ceph，因此需要在包括k8s master及 node节点在内的每一个node 同步认证文件。 ","date":"2023-01-16","objectID":"/posts/kubernetes/primary/kubernetes-6/:1:0","tags":["k8s进阶训练营"],"title":"k8s实战案例-nginx与tomcat实现动静分离 (六)","uri":"/posts/kubernetes/primary/kubernetes-6/"},{"categories":["Kubernetes"],"content":"1.创建初始化RBD #创建新的rbd [ceph@ceph-deploy ~]$ ceph osd pool create shijie-rbd-pool1 32 32 pool'xin-rbd-pool1' created #验证存储池: [ceph@ceph-deploy ~]$ ceph osd pool ls mypool myrdb1 .rgw.root default.rgw.controldefault.rgw.meta default.rgw.log cephfs-metadata cephfs-datarbd-data1 xin-rbd-pool1 #确认存储池已经存在 #存储池启用rbd ceph@ceph-deploy ~]$ ceph osd pool application enable xin-rbd-pool1 rbd enabled application 'rbd' on pool 'xin-rbd-pool1 #初始化 rbdceph@ceph-deploy ~]$ rbd pool init -p xin-rbd-pool1","date":"2023-01-16","objectID":"/posts/kubernetes/primary/kubernetes-6/:2:0","tags":["k8s进阶训练营"],"title":"k8s实战案例-nginx与tomcat实现动静分离 (六)","uri":"/posts/kubernetes/primary/kubernetes-6/"},{"categories":["Kubernetes"],"content":"2 创建image #创建镜像 [ceph@ceph-deploy -]$ rbd create xin-img-img1 --size 3G --pool xin-rbd-pool1 --image-format 2 --image-feature layering #验证镜像 [ceph@ceph-deploy ~]$ rbd ls --pool xin-rbd-pool1 shijie-img-img1 #验证镜像信息 [ceph@ceph-deploy ~]$ rbd --image xin-img-img1 --pool xin-rbd-pool1 inforbd image 'xin-img-img1': size 3 GiBin 768 objects order 22 (4 MiB objects) id:1e7356b8b4567b lock_name_prefix:rbd_data.1e7356b8b4567 format: 2 features: layering op_features: flags: create timestamp: Wed Jan 611:01:51 2021","date":"2023-01-16","objectID":"/posts/kubernetes/primary/kubernetes-6/:3:0","tags":["k8s进阶训练营"],"title":"k8s实战案例-nginx与tomcat实现动静分离 (六)","uri":"/posts/kubernetes/primary/kubernetes-6/"},{"categories":["Kubernetes"],"content":"客户端安装ceph-common 分别在 k8s master 与各 node 节点安装 ceph-common 组件包 #下载key文件 root@k8s-master1:$ wget -q -0- 'https://download.ceph.com/keys/release.asc' | sudo apt-key add - root@k8s-master2:$ wget -q -0- 'https://download.ceph.com/keys/release.asc' | sudo apt-key add - root@k8s-worker1:$ wget -q -0- 'https://download.ceph.com/keys/release.asc' | sudo apt-key add - root@k8s-worker2:$ wget -q -0- 'https://download.ceph.com/keys/release.asc' | sudo apt-key add - #各 master与node 节点配置apt源 root@k8s-master1:~$ cat /etc/apt/sources.list # 默认注释了源码镜像以提高 apt update 速度，如有需要可自行取消注释 deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-updates main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-updates main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-backports main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-backports main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-security main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-security main restricted universe multiverse #更新软件源 root@k8s-node3:~$ apt update #验证 ceph 版本 root@k8s-master1:~$ apt-cache madison ceph-common #各节点安装和当前 ceph 集群相同版本的 ceph-common root@k8s-node3:~$ apt install ceph-common=13.2.10-1bionic","date":"2023-01-16","objectID":"/posts/kubernetes/primary/kubernetes-6/:4:0","tags":["k8s进阶训练营"],"title":"k8s实战案例-nginx与tomcat实现动静分离 (六)","uri":"/posts/kubernetes/primary/kubernetes-6/"},{"categories":["Kubernetes"],"content":"创建ceph 用户与授权 ceph@ceph-deploy ceph-clusterl$ ceph auth get-or-create client.xinceph-zcc mon 'allow r' osd 'allow * pool=xin-rbd-pool1' client.xinceph-zcc] key=AQB4L79g/he7HBAAvJQ7sl3zdSsTUL21Nx6zLQ== #验证用户 [ceph@ceph-deploy ceph-cluster]$ ceph auth get client.xinceph-zcc exported keyring for client.xinceph-zcc [clientmagedu-shijie] key= AQB4L79g/he7HBAAvJQ7sl3zdSsTUL21Nx6zLQ== caps mon =\"allow r\" caps osd =\"allow* pool=xin-rbd-pool1\" #导出用户信息至keyring文件 ceph@ceph-deploy ceph-cluster]$ceph auth get client.xinceph-zcc -o client.xinceph-zcc.keyring exported keyring for client.xinceph-zcc #同步认证文件到 k8s各master 及node节点 ceph@ceph-deploy ceph-cluster]$ scp ceph.conf ceph.client.xinceph-zcc.keyring root@10.1.0.30:/etc/ceph ceph@ceph-deploy ceph-cluster]$ scp ceph.conf ceph.client.xinceph-zcc.keyring root@10.1.0.31:/etc/ceph ceph@ceph-deploy ceph-cluster]$ scp ceph.conf ceph.client.xinceph-zcc.keyring root@10.1.0.32:/etc/ceph ceph@ceph-deploy ceph-cluster]$ scp ceph.conf ceph.client.xinceph-zcc.keyring root@10.1.0.33:/etc/ceph #添加hosts 172.31.6.101 ceph-node1.jie.local ceph-node1 172.31.6.102 ceph-node2.jie.ocal ceph-node2 172.31.6.103 ceph-node3.jie.ocal ceph-node3 172.31.6.104 ceph-mon1.jie.local ceph-mon1 172.31.6.105 ceph-mon2.jie.local ceph-mon2 172.31.6.106 ceph-mon3.jie.local ceph-mon3 172.31.6.107 ceph-mgr1.jie.local ceph-mgr1 172.31.6.108 ceph-mgr2.jie.local ceph-mgr2 172.31.6.109 ceph-deploy.jie.local ceph-deploy #在k8snode节点验证用户权限 root@k8s-node1:~$ ceph --user xinceph-zcc -s ","date":"2023-01-16","objectID":"/posts/kubernetes/primary/kubernetes-6/:5:0","tags":["k8s进阶训练营"],"title":"k8s实战案例-nginx与tomcat实现动静分离 (六)","uri":"/posts/kubernetes/primary/kubernetes-6/"},{"categories":["Kubernetes"],"content":"通过 keyring 文件挂载 rbd 基于 ceph 提供的rbd 实现存储卷的动态提供，由两种实现方式，一是通过宿主机的 keyring文件挂载rbd，另外一个是通过将 keyring 中key 定义为 k8s中的 secret,然后 pod 通过secret 挂载 rbd。 ","date":"2023-01-16","objectID":"/posts/kubernetes/primary/kubernetes-6/:6:0","tags":["k8s进阶训练营"],"title":"k8s实战案例-nginx与tomcat实现动静分离 (六)","uri":"/posts/kubernetes/primary/kubernetes-6/"},{"categories":["Kubernetes"],"content":"通过 keyring 文件直接挂载-busybox #podyaml文件 root@k8s-master1:/opt/ceph-case# cat case1-busybox-keyring.yaml apiVersion: v1 kind: Pod metadata: name: busybox namespace: default spec: containers: - image: busybox command: - sleep - \"3600\" imagePullPolicy: Always name: busybox #restartPolicy: Always volumeMounts: - name: rbd-data1 mountPath: /data volumes: - name: rbd-data1 rbd: monitors: - '172.31.6.101:6789' - '172.31.6.102:6789' - '172.31.6.103:6789' pool: xin-rbd-pool1 image: xin-img-img1 fsType: ext4 readOnly: false user: xinceph-zcc keyring: /etc/ceph/ceph.client.xinceph-zcc.keyring #创建 pod root@k8s-master1:/opt/ceph-case# kubectl apply -f case1-busybox-keyring.yamlpod/busybox created pod/busybox created #到pod验证rbd是否挂载成功 ","date":"2023-01-16","objectID":"/posts/kubernetes/primary/kubernetes-6/:6:1","tags":["k8s进阶训练营"],"title":"k8s实战案例-nginx与tomcat实现动静分离 (六)","uri":"/posts/kubernetes/primary/kubernetes-6/"},{"categories":["Kubernetes"],"content":"通过secret 挂载rbd 将key定义为secret ，然后在挂载至pod，每个k8s node 就不用保存keyring文件了。 ","date":"2023-01-16","objectID":"/posts/kubernetes/primary/kubernetes-6/:7:0","tags":["k8s进阶训练营"],"title":"k8s实战案例-nginx与tomcat实现动静分离 (六)","uri":"/posts/kubernetes/primary/kubernetes-6/"},{"categories":["Kubernetes"],"content":"创建secret 首先创建secret ，secret中主要就是包含ceph中被授权用户的keyring文件中的key,需要将key内容通过 base64编码后即可创建secret。 #将 key 进行编码 ceph auth print-key client.xinceph-zcc AQB4L79g/he7HBAAvJQ7sl3zdSsTUL21Nx6zLQ== ceph auth print-key client.xinceph-zcc | base64 QVFDbm1HSmg2L0dCTGhBQWtXQlRUTmg2R1RHWGpreXFtdFo5RHc9PQo=apiVersion: v1 kind: Secret metadata: name: ceph-secret-xinceph-zcc type: \"kubernetes.io/rbd\" data: key: QVFDbm1HSmg2L0dCTGhBQWtXQlRUTmg2R1RHWGpreXFtdFo5RHc9PQo= #查看secret root@k8s-master1:~/ceph-case$ kubectl get secrets NAME TYPE DATA AGE ceph-secret-xinceph-zcc kubernetes.io/service-account-token 1 3sapiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: replicas: 1 selector: matchLabels: #rs or deployment app: ng-deploy-80 template: metadata: labels: app: ng-deploy-80 spec: containers: - name: ng-deploy-80 image: nginx ports: - containerPort: 80 volumeMounts: - name: rbd-data1 mountPath: /data volumes: - name: rbd-data1 rbd: monitors: - '172.31.6.101:6789' - '172.31.6.102:6789' - '172.31.6.103:6789' pool: xin-rbd-pool1 image: xin-img-img1 fsType: ext4 readOnly: false user: xinceph-zcc secretRef: name: ceph-secret-xinceph-zcc查看pod挂载情况 ","date":"2023-01-16","objectID":"/posts/kubernetes/primary/kubernetes-6/:7:1","tags":["k8s进阶训练营"],"title":"k8s实战案例-nginx与tomcat实现动静分离 (六)","uri":"/posts/kubernetes/primary/kubernetes-6/"},{"categories":["Kubernetes"],"content":"ceph 持久化存储 admin secret 用于k8s master 节点 连接到ceph 自动创建pv ，并关联pvc提供给pod 使用。uer secret 用于pod挂载。 apiVersion: v1 kind: Secret metadata: name: ceph-secret-admin type: \"kubernetes.io/rbd\" data: key: QVFBM2RoZGhNZC9VQUJBQXIyU05wSitoY0sxZEQ1bDJIajVYTWc9PQo= ","date":"2023-01-16","objectID":"/posts/kubernetes/primary/kubernetes-6/:8:0","tags":["k8s进阶训练营"],"title":"k8s实战案例-nginx与tomcat实现动静分离 (六)","uri":"/posts/kubernetes/primary/kubernetes-6/"},{"categories":["Kubernetes"],"content":"创建存储类 apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: ceph-storage-class-xin annotations: storageclass.kubernetes.io/is-default-class: \"true\" #设置为默认存储类 provisioner: kubernetes.io/rbd parameters: monitors: 172.31.6.101:6789,172.31.6.102:6789,172.31.6.103:6789 adminId: admin adminSecretName: ceph-secret-admin adminSecretNamespace: default pool: xin-rbd-pool1 userId: xinceph-zcc userSecretName: ceph-secret-xinceph-zcc","date":"2023-01-16","objectID":"/posts/kubernetes/primary/kubernetes-6/:8:1","tags":["k8s进阶训练营"],"title":"k8s实战案例-nginx与tomcat实现动静分离 (六)","uri":"/posts/kubernetes/primary/kubernetes-6/"},{"categories":["Kubernetes"],"content":"创建 secret root@k8s-master1:~/ceph-cease$ kubectl apply -f case3-secret-client-shijie.yaml root@k8s-master1:~/ceph-cease$ kubectl apply -f case4-secret-client-admin.yaml","date":"2023-01-16","objectID":"/posts/kubernetes/primary/kubernetes-6/:8:2","tags":["k8s进阶训练营"],"title":"k8s实战案例-nginx与tomcat实现动静分离 (六)","uri":"/posts/kubernetes/primary/kubernetes-6/"},{"categories":["Kubernetes"],"content":"创建pvc 关联 apiVersion: v1 kind: PersistentVolumeClaim metadata: name: mysql-data-pvc spec: accessModes: - ReadWriteOnce storageClassName: ceph-storage-class-xin resources: requests: storage: '5Gi' #查看pvc root@k8s-master1:~/ceph-cease$ kubectl get pvc apiVersion: apps/v1 kind: Deployment metadata: name: mysql spec: selector: matchLabels: app: mysql strategy: type: Recreate template: metadata: labels: app: mysql spec: containers: - image: mysql:5.6.46 name: mysql env: # Use secret in real usage - name: MYSQL_ROOT_PASSWORD value: 123456 ports: - containerPort: 3306 name: mysql volumeMounts: - name: mysql-persistent-storage mountPath: /var/lib/mysql volumes: - name: mysql-persistent-storage persistentVolumeClaim: claimName: mysql-data-pvc --- kind: Service apiVersion: v1 metadata: labels: app: mysql-service-label name: mysql-service spec: type: NodePort ports: - name: http port: 3306 protocol: TCP targetPort: 3306 nodePort: 43306 selector: app: mysql查看ceph使用空间 ceph df ","date":"2023-01-16","objectID":"/posts/kubernetes/primary/kubernetes-6/:8:3","tags":["k8s进阶训练营"],"title":"k8s实战案例-nginx与tomcat实现动静分离 (六)","uri":"/posts/kubernetes/primary/kubernetes-6/"},{"categories":["Kubernetes"],"content":"cephFs 实现多主机的挂载共享数据 ","date":"2023-01-16","objectID":"/posts/kubernetes/primary/kubernetes-6/:9:0","tags":["k8s进阶训练营"],"title":"k8s实战案例-nginx与tomcat实现动静分离 (六)","uri":"/posts/kubernetes/primary/kubernetes-6/"},{"categories":["Kubernetes"],"content":"配置cephfs 步骤如下：https://www.yuque.com/ryanxx/ga3673/bz0645hbae3emovp ","date":"2023-01-16","objectID":"/posts/kubernetes/primary/kubernetes-6/:9:1","tags":["k8s进阶训练营"],"title":"k8s实战案例-nginx与tomcat实现动静分离 (六)","uri":"/posts/kubernetes/primary/kubernetes-6/"},{"categories":["Kubernetes"],"content":"创建nginx pod同时挂载cephfs apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: replicas: 3 selector: matchLabels: #rs or deployment app: ng-deploy-80 template: metadata: labels: app: ng-deploy-80 spec: containers: - name: ng-deploy-80 image: nginx ports: - containerPort: 80 volumeMounts: - name: xinceph-staticdata-cephfs mountPath: /usr/share/nginx/html/ volumes: - name: xinceph-staticdata-cephfs cephfs: monitors: - '172.31.6.101:6789' - '172.31.6.102:6789' - '172.31.6.103:6789' path: / user: admin secretRef: name: ceph-secret-admin","date":"2023-01-16","objectID":"/posts/kubernetes/primary/kubernetes-6/:9:2","tags":["k8s进阶训练营"],"title":"k8s实战案例-nginx与tomcat实现动静分离 (六)","uri":"/posts/kubernetes/primary/kubernetes-6/"},{"categories":["Kubernetes"],"content":" Author: Ryan title: 5.k8s实战案例-nginx与tomcat实现动静分离 tag: - k8s进阶训练营 category: k8s date: 2022-6-5 12:12:22 lastUpdated: true #sidebar: false breadcrumb: false #contributors: false ","date":"2023-01-16","objectID":"/posts/kubernetes/primary/kubernetes-5/:0:0","tags":["k8s进阶训练营"],"title":"k8s实战案例-nginx与tomcat实现动静分离 (五)","uri":"/posts/kubernetes/primary/kubernetes-5/"},{"categories":["Kubernetes"],"content":"自定义镜像-运行nginx与tomcat实现动静分离 ","date":"2023-01-16","objectID":"/posts/kubernetes/primary/kubernetes-5/:1:0","tags":["k8s进阶训练营"],"title":"k8s实战案例-nginx与tomcat实现动静分离 (五)","uri":"/posts/kubernetes/primary/kubernetes-5/"},{"categories":["Kubernetes"],"content":"1. 系统基础镜像 harbor.ceamg.com/baseimages/centos:7.8.2003","date":"2023-01-16","objectID":"/posts/kubernetes/primary/kubernetes-5/:1:1","tags":["k8s进阶训练营"],"title":"k8s实战案例-nginx与tomcat实现动静分离 (五)","uri":"/posts/kubernetes/primary/kubernetes-5/"},{"categories":["Kubernetes"],"content":"2. 构建 nginx 基础镜像 root@harbor01:/dockerfile/web/pub-images# pwd /dockerfile/web/pub-images root@harbor01:/dockerfile/web/pub-images# tree . ├── build-command.sh ├── Dockerfile └── nginx-1.22.1.tar.gz 0 directories, 3 files#!/bin/bash TAG=$1 docker build -t harbor.ceamg.com/pub-images/nginx-base:${TAG} . sleep 1 docker push harbor.ceamg.com/pub-images/nginx-base:${TAG}#Nginx 1.22.1 FROM harbor.ceamg.com/pub-images/nginx-base:1.22.1 ADD nginx.conf /usr/local/nginx/conf/nginx.conf ADD app1.tar.gz /usr/local/nginx/html/webapp/ ADD index.html /usr/local/nginx/html/index.html #静态资源挂载路径 RUN mkdir -p /usr/local/nginx/html/webapp/static /usr/local/nginx/html/webapp/images \u0026\u0026 useradd nginx -u 2023 -s /sbin/nologin -M EXPOSE 80 443 CMD [\"nginx\"]","date":"2023-01-16","objectID":"/posts/kubernetes/primary/kubernetes-5/:1:2","tags":["k8s进阶训练营"],"title":"k8s实战案例-nginx与tomcat实现动静分离 (五)","uri":"/posts/kubernetes/primary/kubernetes-5/"},{"categories":["Kubernetes"],"content":"3. 构建 nginx 业务镜像 root@harbor01:/dockerfile/web/xin-01/nginx# pwd\r/dockerfile/web/xin-01/nginx\rroot@harbor01:/dockerfile/web/xin-01/nginx# tree\r.\r├── app1.tar.gz\r├── bulid-command.sh\r├── Dockerfile\r├── index.html\r└── nginx.conf#!/bin/bash\rTAG=$1\rdocker build -t harbor.ceamg.com/xinweb11/nginx-web1:${TAG} .\rsleep 1\rdocker push harbor.ceamg.com/xinweb11/nginx-web1:${TAG}xxx in xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\rxxx /usr/local/nginx/html/index.html xx\rxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxuser nginx nginx; worker_processes auto; #error_log logs/error.log; #error_log logs/error.log notice; #error_log logs/error.log info; #pid logs/nginx.pid; daemon off; events { worker_connections 1024; } http { include mime.types; default_type application/octet-stream; #log_format main '$remote_addr - $remote_user [$time_local] \"$request\" ' # '$status $body_bytes_sent \"$http_referer\" ' # '\"$http_user_agent\" \"$http_x_forwarded_for\"'; #access_log logs/access.log main; sendfile on; #tcp_nopush on; #keepalive_timeout 0; keepalive_timeout 65; #gzip on; upstream tomcat_webserver { server magedu-tomcat-app1-service.magedu.svc.magedu.local:80; } server { listen 80; server_name localhost; #charset koi8-r; #access_log logs/host.access.log main; location / { root html; index index.html index.htm; } location /webapp { root html; index index.html index.htm; } location /myapp { proxy_pass http://tomcat_webserver; proxy_set_header Host $host; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Real-IP $remote_addr; } #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } # proxy the PHP scripts to Apache listening on 127.0.0.1:80 # #location ~ \\.php$ { # proxy_pass http://127.0.0.1; #} # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000 # #location ~ \\.php$ { # root html; # fastcgi_pass 127.0.0.1:9000; # fastcgi_index index.php; # fastcgi_param SCRIPT_FILENAME /scripts$fastcgi_script_name; # include fastcgi_params; #} # deny access to .htaccess files, if Apache's document root # concurs with nginx's one # #location ~ /\\.ht { # deny all; #} } # another virtual host using mix of IP-, name-, and port-based configuration # #server { # listen 8000; # listen somename:8080; # server_name somename alias another.alias; # location / { # root html; # index index.html index.htm; # } #} # HTTPS server # #server { # listen 443 ssl; # server_name localhost; # ssl_certificate cert.pem; # ssl_certificate_key cert.key; # ssl_session_cache shared:SSL:1m; # ssl_session_timeout 5m; # ssl_ciphers HIGH:!aNULL:!MD5; # ssl_prefer_server_ciphers on; # location / { # root html; # index index.html index.htm; # } #} }#Nginx 1.22.1\rFROM harbor.ceamg.com/pub-images/nginx-base:1.22.1\rADD nginx.conf /usr/local/nginx/conf/nginx.conf\rADD app1.tar.gz /usr/local/nginx/html/webapp/\rADD index.html /usr/local/nginx/html/index.html\r#静态资源挂载路径\rRUN mkdir -p /usr/local/nginx/html/webapp/static /usr/local/nginx/html/webapp/images\rEXPOSE 80 443\rCMD [\"nginx\"]测试nginx业务镜像 root@harbor01:/dockerfile/web/xin-01/nginx# docker run -it --rm -p 8888:80 harbor.ceamg.com/xinweb11/nginx-web1:v1.6 curl http://10.1.0.38:8888 curl http://10.1.0.38:8888/webapp/","date":"2023-01-16","objectID":"/posts/kubernetes/primary/kubernetes-5/:1:3","tags":["k8s进阶训练营"],"title":"k8s实战案例-nginx与tomcat实现动静分离 (五)","uri":"/posts/kubernetes/primary/kubernetes-5/"},{"categories":["Kubernetes"],"content":"4. ubuntu基础镜像 docker pull ubuntu:20.04 docker tag docker.io/library/ubuntu:20.04 harbor.ceamg.com/baseimages/ubuntu:20.04 docker push harbor.ceamg.com/baseimages/ubuntu:20.04","date":"2023-01-16","objectID":"/posts/kubernetes/primary/kubernetes-5/:1:4","tags":["k8s进阶训练营"],"title":"k8s实战案例-nginx与tomcat实现动静分离 (五)","uri":"/posts/kubernetes/primary/kubernetes-5/"},{"categories":["Kubernetes"],"content":"5. 构建jdk8 root@harbor01:/dockerfile/web/pub-images/jdk# pwd /dockerfile/web/pub-images/jdk root@harbor01:/dockerfile/web/pub-images/jdk# tree . ├── build-command.sh ├── Dockerfile ├── jdk-8u212-linux-x64.tar.gz ├── jdk-8u341-linux-x64.tar.gz └── profile#JDK Base Image FROM harbor.ceamg.com/baseimages/ubuntu:20.04 MAINTAINER Ryan \"ryanxin.com\" ADD jdk-8u341-linux-x64.tar.gz /usr/local/src/ RUN ln -sv /usr/local/src/jdk1.8.0_341 /usr/local/jdk ADD profile /etc/profile ENV JAVA_HOME /usr/local/jdk ENV JRE_HOME $JAVA_HOME/jre ENV CLASSPATH $JAVA_HOME/lib/:$JRE_HOME/lib/ ENV PATH $PATH:$JAVA_HOME/bin#!/bin/bash TAG=$1 docker build -t harbor.ceamg.com/pub-images/jdk8:${TAG} . sleep 3 docker push harbor.ceamg.com/pub-images/jdk8:${TAG}# /etc/profile: system-wide .profile file for the Bourne shell (sh(1)) # and Bourne compatible shells (bash(1), ksh(1), ash(1), ...). if [ \"${PS1-}\" ]; then if [ \"${BASH-}\" ] \u0026\u0026 [ \"$BASH\" != \"/bin/sh\" ]; then # The file bash.bashrc already sets the default PS1. # PS1='\\h:\\w\\$ ' if [ -f /etc/bash.bashrc ]; then . /etc/bash.bashrc fi else if [ \"$(id -u)\" -eq 0 ]; then PS1='# ' else PS1='$ ' fi fi fi if [ -d /etc/profile.d ]; then for i in /etc/profile.d/*.sh; do if [ -r $i ]; then . $i fi done unset i fi export LANG=en_US.UTF-8 export HISTTIMEFORMAT=\"%F %T `whoami` \" export JAVA_HOME=/usr/local/jdk export JRE_HOME=${JAVA_HOME}/jre export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib export PATH=${JAVA_HOME}/bin:$PATH测试镜像 root@harbor01:/dockerfile/web/pub-images/jdk# docker run -it --rm harbor.ceamg.com/pub-images/jdk8:3411 root@494e5aeb25af:/# java -v Unrecognized option: -v Error: Could not create the Java Virtual Machine. Error: A fatal exception has occurred. Program will exit. root@494e5aeb25af:/# java -version java version \"1.8.0_341\" Java(TM) SE Runtime Environment (build 1.8.0_341-b10) Java HotSpot(TM) 64-Bit Server VM (build 25.341-b10, mixed mode)","date":"2023-01-16","objectID":"/posts/kubernetes/primary/kubernetes-5/:1:5","tags":["k8s进阶训练营"],"title":"k8s实战案例-nginx与tomcat实现动静分离 (五)","uri":"/posts/kubernetes/primary/kubernetes-5/"},{"categories":["Kubernetes"],"content":"6. 构建 Tomcat 镜像 root@harbor01:/dockerfile/web/pub-images/tomcat# pwd /dockerfile/web/pub-images/tomcat root@harbor01:/dockerfile/web/pub-images/tomcat# tree . ├── apache-tomcat-8.5.43.tar.gz ├── build-command.sh └── Dockerfile#!/bin/bash docker build -t harbor.ceamg.com/pub-images/tomcat-base:v8.5.43 . sleep 3 docker push harbor.ceamg.com/pub-images/tomcat-base:v8.5.43#Tomcat 8.5.43基础镜像 FROM harbor.ceamg.com/pub-images/jdk8:3411 MAINTAINER Ryanxin ryanxin@outlook.com RUN mkdir /apps /data/tomcat/webapps /data/tomcat/logs -pv ADD apache-tomcat-8.5.43.tar.gz /apps RUN useradd nginx -u 2022 \u0026\u0026 ln -sv /apps/apache-tomcat-8.5.43 /apps/tomcat \u0026\u0026 chown -R nginx.nginx /apps /data -R","date":"2023-01-16","objectID":"/posts/kubernetes/primary/kubernetes-5/:1:6","tags":["k8s进阶训练营"],"title":"k8s实战案例-nginx与tomcat实现动静分离 (五)","uri":"/posts/kubernetes/primary/kubernetes-5/"},{"categories":["Kubernetes"],"content":"7. 构建Tomcat 业务镜像 root@harbor01:/dockerfile/web/xin-01/tomcat-app1# tree . ├── app1.tar ├── app1.tar.gz ├── build-command.sh ├── catalina.sh ├── Dockerfile ├── run_tomcat.sh └── server.xml#tomcat web1 FROM harbor.ceamg.com/pub-images/tomcat-base:v8.5.43.1 ADD catalina.sh /apps/tomcat/bin/catalina.sh ADD server.xml /apps/tomcat/conf/server.xml ADD app1.tar /data/tomcat/webapps/myapp/ ADD run_tomcat.sh /apps/tomcat/bin/run_tomcat.sh RUN mkdir /home/nginx -p \\ \u0026\u0026 chmod 755 /home/nginx \\ \u0026\u0026 cp -a /etc/skel/. /home/nginx \\ \u0026\u0026 chown -R nginx.nginx /data/ /apps/ EXPOSE 8080 8443 CMD [\"/apps/tomcat/bin/run_tomcat.sh\"]#!/bin/bash #echo \"nameserver 223.6.6.6\" \u003e /etc/resolv.conf #echo \"192.168.7.248 k8s-vip.example.com\" \u003e\u003e /etc/hosts #/usr/share/filebeat/bin/filebeat -e -c /etc/filebeat/filebeat.yml -path.home /usr/share/filebeat -path.config /etc/filebeat -path.data /var/lib/filebeat -path.logs /var/log/filebeat \u0026 su -c \"/apps/tomcat/bin/catalina.sh start\" nginx tail -f /etc/hostsserver.xml catalina.sh 测试镜像 docker run -it --rm -p 9900:8080 harbor.ceamg.com/xinweb11/tomcat-app1:1.9 ","date":"2023-01-16","objectID":"/posts/kubernetes/primary/kubernetes-5/:1:7","tags":["k8s进阶训练营"],"title":"k8s实战案例-nginx与tomcat实现动静分离 (五)","uri":"/posts/kubernetes/primary/kubernetes-5/"},{"categories":["Kubernetes"],"content":"在k8s中跑起来 启动tomcat pod kind: Deployment #apiVersion: extensions/v1beta1 apiVersion: apps/v1 metadata: labels: app: xin-tomcat-app1-deployment-label name: xin-tomcat-app1-deployment namespace: xin-web spec: replicas: 1 selector: matchLabels: app: xin-tomcat-app1-selector template: metadata: labels: app: xin-tomcat-app1-selector spec: containers: - name: xin-tomcat-app1-container image: harbor.ceamg.com/xinweb11/tomcat-app1:1.9 #command: [\"/apps/tomcat/bin/run_tomcat.sh\"] #imagePullPolicy: IfNotPresent imagePullPolicy: Always ports: - containerPort: 8080 protocol: TCP name: http env: - name: \"password\" value: \"123456\" - name: \"age\" value: \"18\" resources: limits: cpu: 1 memory: \"512Mi\" requests: cpu: 500m memory: \"512Mi\" volumeMounts: - name: xin-images mountPath: /usr/local/nginx/html/webapp/images readOnly: false - name: xin-static mountPath: /usr/local/nginx/html/webapp/static readOnly: false volumes: - name: xin-images nfs: server: 10.1.0.38 path: /data/k8s/web1/images - name: xin-static nfs: server: 10.1.0.38 path: /data/k8s/web1/static # nodeSelector: # project: xin # app: tomcat --- kind: Service apiVersion: v1 metadata: labels: app: xin-tomcat-app1-service-label name: xin-tomcat-app1-service namespace: xin-web spec: type: NodePort ports: - name: http port: 80 protocol: TCP targetPort: 8080 nodePort: 40003 selector: app: xin-tomcat-app1-selector检测后端Tomcat SVC 连通性 ping xin-tomcat-app1-service.xin-web.svc.ceamg.local 启动nginx pod kind: Deployment apiVersion: apps/v1 metadata: labels: app: web1-nginx-deployment-label name: web1-nginx-deployment namespace: xin-web spec: replicas: 1 selector: matchLabels: app: web1-nginx-selector template: metadata: labels: app: web1-nginx-selector spec: containers: - name: web1-nginx-container image: harbor.ceamg.com/xinweb11/nginx-web1:v1.0 #command: [\"/apps/tomcat/bin/run_tomcat.sh\"] #imagePullPolicy: IfNotPresent imagePullPolicy: Always ports: - containerPort: 80 protocol: TCP name: http - containerPort: 443 protocol: TCP name: https env: - name: \"password\" value: \"123456\" - name: \"age\" value: \"20\" resources: limits: cpu: 2 memory: 2Gi requests: cpu: 500m memory: 1Gi volumeMounts: - name: xin-images mountPath: /usr/local/nginx/html/webapp/images readOnly: false - name: xin-static mountPath: /usr/local/nginx/html/webapp/static readOnly: false volumes: - name: xin-images nfs: server: 10.1.0.38 path: /data/k8s/web1/images - name: xin-static nfs: server: 10.1.0.38 path: /data/k8s/web1/static #nodeSelector: # group: magedu --- kind: Service apiVersion: v1 metadata: labels: app: web1-nginx-service-label name: web1-nginx-service namespace: xin-web spec: type: NodePort ports: - name: http port: 80 protocol: TCP targetPort: 80 nodePort: 40002 - name: https port: 443 protocol: TCP targetPort: 443 nodePort: 40443 selector: app: web1-nginx-selectornginx 配置文件启用location 和 upstream 后端主机 upstream tomcat_webserver { server xin-tomcat-app1-service.xin-web.svc.ceamg.local:80; } server { listen 80; server_name localhost; #charset koi8-r; #access_log logs/host.access.log main; location / { root html; index index.html index.htm; } location /webapp { root html; index index.html index.htm; } location /myapp { proxy_pass http://tomcat_webserver; proxy_set_header Host $host; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Real-IP $remote_addr; } ","date":"2023-01-16","objectID":"/posts/kubernetes/primary/kubernetes-5/:1:8","tags":["k8s进阶训练营"],"title":"k8s实战案例-nginx与tomcat实现动静分离 (五)","uri":"/posts/kubernetes/primary/kubernetes-5/"},{"categories":["Ceph"],"content":"Ceph中独立的服务如果不用可以不启用。 类似阿里云OSS对象存储。遵循亚马逊S3标准数据存在bucket中 官方文档：http:/docs.ceph.org.cn/radosgw/数据不需要放置在目录层次结构中，而是存在于平面地址空间内的同一级别，应用通过唯一地址来识别每个单独数据对象。访问的时候传递的URL是固定的。每个对象可包含有助于检索的元数据通过RESTful API在应用级别(而非用户级别)进行访问 ","date":"2023-01-16","objectID":"/posts/ceph/8.-%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%BD%91%E5%85%B3rgw/:0:0","tags":["分布式存储"],"title":"RGW对象存储网关 （八）","uri":"/posts/ceph/8.-%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%BD%91%E5%85%B3rgw/"},{"categories":["Ceph"],"content":"8.1 RadosGW对象存储简介 RadosGW是对象存储(OSS,Object Storage Service)的一种实现方式，** RADOS网关也称为Ceph对象网关、RadosGW、RGW,是-种服务， 使客户端能够利用标准对象存储API来访问Ceph集群，它支持AWSS3和SwiftAPI**，在ceph0.8版本之后使用Civetweb(https://github.com/civetweb/civetweb))的web服务器来响应api请求，客户端使用http/https协议通过RESTful API与RGW通信默认端口7480，而RGW则通过librados与ceph集群通信，RGW客户端通过s3或者swift api使用RGW用户进行身份验证，然后RGW网关代表用户利用cephx与ceph存储进行身份验证.S3由Amazon于2006年推出，全称为Simple Storage Service,S3定义了对象存储，是对象存储事实上的标准，从某种意义上说，S3 就是对象存储，对象存储就是S3,它是对象存储市场的霸主，后续的对象存储都是对S3的模仿。 ","date":"2023-01-16","objectID":"/posts/ceph/8.-%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%BD%91%E5%85%B3rgw/:0:1","tags":["分布式存储"],"title":"RGW对象存储网关 （八）","uri":"/posts/ceph/8.-%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%BD%91%E5%85%B3rgw/"},{"categories":["Ceph"],"content":"8.2 对象存储特点 通过对象存储将数据存储为对象，每个对象除了包含数据，还包含数据自身的元数据。 对象通过Object ID来检索，无法通过普通文件系统的方式通过文件路径及文件名称操作来直接访问对象，只能通过API来访问，或者第三方客户端(实际上也是对API的封装)。 对象存储中的对象不整理到目录树中，而是存储在扁平的命名空间中，Amazon S3将这个扁平命名空间称为bucket,而swift则将其称为容器。无论是bucket还是容器，都不能嵌套。 bucket需要被授权才能访问到，一个帐户可以对多个bucket授权，而权限可以不同。方便横向扩展、快速检索数据。 不支持客户端挂载，且需要客户端在访问的时候指定文件名称。不是很适用于文件过于频繁修改及删除的场景。 ceph使用bucket作为存储桶(存储空间)，实现对象数据的存储和多用户隔离，数据存储在bucket中，用户的权限也是针对bucket进行授权，可以设置用户对不同的bucket拥有不同的权限，以实现权限管理。 基于bucket对项目隔离 对象存储运维需要做的事情：启用RGW做好RGW的高可用创建bucket创建用户并授权做好监控 ","date":"2023-01-16","objectID":"/posts/ceph/8.-%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%BD%91%E5%85%B3rgw/:1:0","tags":["分布式存储"],"title":"RGW对象存储网关 （八）","uri":"/posts/ceph/8.-%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%BD%91%E5%85%B3rgw/"},{"categories":["Ceph"],"content":"8.2.1 bucket 特性 存储空间是用于存储对象(Object) 的容器，所有的对象都必须隶属于某个存储空间，可以设置和修改存储空间属性用来控制地域、访问权限、生命周期等，这些属性设置直接作用于该存储空间内所有对象，因此可以通过灵活创建不同的存储空间来完成不同的管理功能。 同一个存储空间的内部是扁平的，没有文件系统的目录等概念，所有的对象都直接隶属于其对应的存储空间。 每个用户可以拥有多个存储空间 存储空间的名称在OSS范围内必须是全局唯一的，一旦创建之后无法修改名称。 存储空间内部的对象数目没有限制。 ","date":"2023-01-16","objectID":"/posts/ceph/8.-%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%BD%91%E5%85%B3rgw/:1:1","tags":["分布式存储"],"title":"RGW对象存储网关 （八）","uri":"/posts/ceph/8.-%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%BD%91%E5%85%B3rgw/"},{"categories":["Ceph"],"content":"8.2.2 bucket命名规范 https://docs.amazonaws.cn/AmazonS3/latest/userguide/bucketnamingrules.html 只能包括小写字母、数字和短横线(-) 必须以小写字母或者数字开头和结尾. 长度必须在3-63字节之间。 存储桶名称不能使用用IP地址格式. Bucket名称必须全局唯一 ","date":"2023-01-16","objectID":"/posts/ceph/8.-%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%BD%91%E5%85%B3rgw/:1:2","tags":["分布式存储"],"title":"RGW对象存储网关 （八）","uri":"/posts/ceph/8.-%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%BD%91%E5%85%B3rgw/"},{"categories":["Ceph"],"content":"8.3 对象存储访问对比 目前主流有三种访问标准： AmEizon S3:提供了user、bucket 和object分别表示用户、存储桶和对象，其中bucket隶属于user,可以针对user设置不同bucket的名称空间的访问权限，而且不同用户允许访问相同的bucket。（使用最多-最广泛） OpenStack Swift:提供了user、 container 和object分别对应于用户、存储桶和对象，不过它还额外为user提供了父级组件account, account用于表示一个项目或租户(OpenStack用户)，因此一个account中可包含一到多个user,它们可共享使用同一组 container, 并为container提供名称空间。 RadosGW: 提供了user、 subuser、bucket 和object,其中的user对应于S3的user,而subuser则对应于Swift的user,不过user和subuser都不支持为bucket提供名称空间，因此，不同用户的存储桶也不允许同名;不过，自Jewel版本起，RadosGW引入了tenant(租户)用于为user和bucket提供名称空间，但它是个可选组件，RadosGW 基于ACL为不同的用户设置不同的权限控制，如:Read读权限Write写权限Readwrite读写权限full-control全部控制权限 ","date":"2023-01-16","objectID":"/posts/ceph/8.-%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%BD%91%E5%85%B3rgw/:1:3","tags":["分布式存储"],"title":"RGW对象存储网关 （八）","uri":"/posts/ceph/8.-%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%BD%91%E5%85%B3rgw/"},{"categories":["Ceph"],"content":"8.4 部署RadosGW服务 将ceph-mgr1、ceph-mgr2 服务器部署为高可用的radosGW服务 ","date":"2023-01-16","objectID":"/posts/ceph/8.-%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%BD%91%E5%85%B3rgw/:2:0","tags":["分布式存储"],"title":"RGW对象存储网关 （八）","uri":"/posts/ceph/8.-%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%BD%91%E5%85%B3rgw/"},{"categories":["Ceph"],"content":"8.4.1 安装radosgw服务并初始化 Ubuntu #apt install radosgwCentOS [root@ceph-mgr1 ~]# yum install ceph-radosgw [root@ceph-mgr2 ~]# yum install ceph-radosgw #在ceph deploy服务器将ceph-mgr1初始化为radosGW服务: [ceph@ceph-deploy ~]$ cd ceph-cluster/ [ceph@ceph-deploy ceph-cluster]$ ceph-deploy rgw create ceph-mgr2 [ceph@ceph-deploy ceph-cluster]$ ceph-deploy rgw create ceph-mgr1","date":"2023-01-16","objectID":"/posts/ceph/8.-%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%BD%91%E5%85%B3rgw/:2:1","tags":["分布式存储"],"title":"RGW对象存储网关 （八）","uri":"/posts/ceph/8.-%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%BD%91%E5%85%B3rgw/"},{"categories":["Ceph"],"content":"8.4.2 验证radosgw服务状态 ","date":"2023-01-16","objectID":"/posts/ceph/8.-%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%BD%91%E5%85%B3rgw/:2:2","tags":["分布式存储"],"title":"RGW对象存储网关 （八）","uri":"/posts/ceph/8.-%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%BD%91%E5%85%B3rgw/"},{"categories":["Ceph"],"content":"8.4.3 验证radosgw服务进程 ps -ef | grep radosgw","date":"2023-01-16","objectID":"/posts/ceph/8.-%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%BD%91%E5%85%B3rgw/:2:3","tags":["分布式存储"],"title":"RGW对象存储网关 （八）","uri":"/posts/ceph/8.-%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%BD%91%E5%85%B3rgw/"},{"categories":["Ceph"],"content":"8.4.4 radosgw的存储池类型 启动radosgw 服务自动创建default zone区域存储池，这些存储池的功能是不一样的 root@ceph-deploy:~# ceph osd pool ls device_health_metrics cephfs-metadata cephfs-data .rgw.root default.rgw.log default.rgw.control default.rgw.meta default.rgw.buckets.index default.rgw.buckets.data root@ceph-deploy:~# ceph osd pool get default.rgw.buckets.data crush_rule crush_rule: replicated_rule #默认是副本池 root@ceph-deploy:~# ceph osd pool get default.rgw.buckets.data size size: 3 #默认的副本数 root@ceph-deploy:~# ceph osd pool get default.rgw.buckets.data pgp_num pgp_num: 32 #默认的pgp数量 root@ceph-deploy:~# ceph osd pool get default.rgw.buckets.data pg_num pg_num: 32 #默认的pg数量","date":"2023-01-16","objectID":"/posts/ceph/8.-%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%BD%91%E5%85%B3rgw/:2:4","tags":["分布式存储"],"title":"RGW对象存储网关 （八）","uri":"/posts/ceph/8.-%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%BD%91%E5%85%B3rgw/"},{"categories":["Ceph"],"content":"8.4.5 RGW存储池功能 root@ceph-deploy:~ # ceph osd lspools 1 device_health_metrics 3 cephfs-metadata 4 cephfs-data 9.rgw.root 10 default.rgw.log 11 default.rgw.control 12 default.rgw.meta 13 default.rgw.buckets.index 14 default.rgw.buckets.data 15 default.rgw.buckets.non-ec.rgw.root: 包含 realm(领域信息)，比如zone和zonegroup。（用于不同区域多机房之间）default.rgw.log: 存储日志信息，用于记录各种log信息。default.rgw.control: 系统控制池，在有数据更新时，通知其它RGW更新缓存。default.rgw.meta: 元数据存储池，通过不同的名称空间分别存储不同的rados对象，这些名称空间包括用户UID及其bucket映射信息的名称空间users.uid、用户的密钥名称空间users.keys、用戶的email名称空间users.email、用户的subuser的名称空间users. swift、以及bucket的名称空间root等。default.rgw.buckets.index: 存放 bucket到object的索引信息。default.rgw.buckets.data: 存放对象的数据 。default.rgw. buckets.non-ec ：数据的额外信息存储池。 ","date":"2023-01-16","objectID":"/posts/ceph/8.-%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%BD%91%E5%85%B3rgw/:2:5","tags":["分布式存储"],"title":"RGW对象存储网关 （八）","uri":"/posts/ceph/8.-%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%BD%91%E5%85%B3rgw/"},{"categories":["Ceph"],"content":"8.4.6 验证RGW zone信息 root@ceph-deploy:~# radosgw-admin zone get --rgw-zone=default","date":"2023-01-16","objectID":"/posts/ceph/8.-%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%BD%91%E5%85%B3rgw/:2:6","tags":["分布式存储"],"title":"RGW对象存储网关 （八）","uri":"/posts/ceph/8.-%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%BD%91%E5%85%B3rgw/"},{"categories":["Ceph"],"content":"8.5 radosgw服务高可用配置 ","date":"2023-01-16","objectID":"/posts/ceph/8.-%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%BD%91%E5%85%B3rgw/:3:0","tags":["分布式存储"],"title":"RGW对象存储网关 （八）","uri":"/posts/ceph/8.-%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%BD%91%E5%85%B3rgw/"},{"categories":["Ceph"],"content":"8.5.1 radosgw http高可用 8.5.1.1 自定义http端口 配置文件可以在ceph deploy服务器修改然后统一推送，或者单独修改每个radosgw服务器的配置为统一配置，然后重启RGW服务。 https://docs. ceph.com/en/latest/radosaw/frontends/[链接](https://docs. ceph.com/en/latest/radosaw/frontends/) [root@ceph-mgr2 ~]# vim /etc/ceph/ceph.conf [client.rgw.ceph-mgr2] #在最后面添加针对当前节点的自定义配置如下: rgw_host=ceph-mgr2 rrgw_frontends=civetweb port=9900 #重启服务 [root@ceph-mgr2 ~]# systemctl restart ceph-radosgw@rgw.ceph-mgr2.service 8.5.1.3 测试http反向代理 使用haproxy代理 rgw两个节点 yum install haproxy vim /etc/haproxy/haproxy.cfg listen ceph-rgw bind 172.31.6.201:80 mode tcp server rgw1 172.31.6.104:7480 check inter 3s fall 3 rise 5 server rgw2 172.31.6.105:7480 check inter 3s fall 3 rise 5 systemctl restart haproxy.service ","date":"2023-01-16","objectID":"/posts/ceph/8.-%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%BD%91%E5%85%B3rgw/:3:1","tags":["分布式存储"],"title":"RGW对象存储网关 （八）","uri":"/posts/ceph/8.-%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%BD%91%E5%85%B3rgw/"},{"categories":["Ceph"],"content":"8.5.2 radosgw https 在rgw节点生成签名证书并配置radosgw启用SSL方式一： 在反向代理配置SSL证书方式二： 内置自签证书（浏览器提升不安全证书） 8.5.2.1 自签名证书 [root@ceph-mgr2 ~]# cd /etc/ceph/ [root@ceph-mgr2 ceph]# mkdir certs [root@ceph-mgr2 ceph]# cd certs/ #生成key root@ceph-mgr2 certs]# openssl genrsa -out civetweb.key 2048 #自签发 [root@ceph-mgr2 certs]# openssl req -new -x509 -key civetweb.key -out civetweb.crt -subj \"/CN=rgw.magedu.net\" #将key和私钥放一块 [root@ceph-mgr2 certs]# cat civetweb.key civetweb.crt \u003e civetweb.pem [root@ceph-mgr2 certs]# tree8.5.2.2 SSL配置 [root@ceph-mgr2 certs]# vim /etc/ceph/ceph.conf [client.rgw.ceph-mgr2] rgw_host = ceph-mgr2 rgw_frontends = \"civetweb port-9900+9443s ssl_certificate=/etc/ceph/certs/civetweb.pem\" root@ceph-mgr2 certs]# systemctl restart ceph-radosgw@rgw.ceph-mgr2.service8.5.2.3 验证https端口 ss -tnl","date":"2023-01-16","objectID":"/posts/ceph/8.-%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%BD%91%E5%85%B3rgw/:3:2","tags":["分布式存储"],"title":"RGW对象存储网关 （八）","uri":"/posts/ceph/8.-%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%BD%91%E5%85%B3rgw/"},{"categories":["Ceph"],"content":"8.5.3 radosgw https高可用 通过负载均衡对radosgw进行反向代理，实现高可用 8.5.3.1 域名解析至VIP 先将域名解析至负裁均衡的IP172.31.6.201 rgw.magedu.net 8.5.3.2 负载均衡配置 负载均衡配置监听及realserver ,把SSL证书放在了rgw节点上面（也放在负载均衡上面） #ceph http access listen ceph-rgw bind 172.31.6.201:80 mode tcp server 172.31.6.104 172.31.6.104:9900 check inter 3s fall 3 rise 5 server 172.31.6.105 172.31.6.105:9900 check inter 3s fall 3 rise 5 #ceph https access listen ceph-rgw bind 172.31.6.201:443 mode tcp server 172.31.6.104 172.31 .6.104:9443 check inter 3s fall 3 rise 5 server 172.31.6.105 172.31 .6.105:9443 check inter 3s fall 3 rise 58.5.3.3 重启负载均衡 [root@ceph-client1-centos7 ~]# systemctl restart haproxy8.5.3.4 测试访问 ","date":"2023-01-16","objectID":"/posts/ceph/8.-%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%BD%91%E5%85%B3rgw/:3:3","tags":["分布式存储"],"title":"RGW对象存储网关 （八）","uri":"/posts/ceph/8.-%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%BD%91%E5%85%B3rgw/"},{"categories":["Ceph"],"content":"8.5.4 日志及其它优化配置 #创建日志目录: [root@ceph-mgr2 certs]# mkdir /var/log/radosgw [root@ceph-mgr2 certs]# chown ceph.ceph /var/log/radosgw #当前配置 [root@ceph-mgr2 ceph]# vim ceph.conf [client.rgw.ceph-mgr2] rgw_host = ceph-mgr2 rgw_frontends = \"civetweb port=9900+8443s ssl_certificate=/etc/ceph/certs/civetweb.pem error_log_file=/var/log/radosgw/civetweb.error.log access_log_file=/var/log/radosgw/civetweb.access.log request_timeout_ms=30000 num_threads=200\" #请求超过30s 报超时 #100线程 每个显示处理一个请求（2000） #https://docs.ceph.com/en/mimic/radosgw/config-ref/ num_threads默认值等于 rgw_thread_pool_size=100 #重启服务 [root@ceph-mgr2 certs]# systemctl restart ceph-radosgw@rgw.ceph-mgr2.service #访问测试: [root@ceph-mgr2 certs]# curl -k https://172.31.6.108:8443 #验证日志 tail /var/log/radosgw/civetweb/access.log","date":"2023-01-16","objectID":"/posts/ceph/8.-%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%BD%91%E5%85%B3rgw/:3:4","tags":["分布式存储"],"title":"RGW对象存储网关 （八）","uri":"/posts/ceph/8.-%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%BD%91%E5%85%B3rgw/"},{"categories":["Ceph"],"content":"8.6 测试数据读写 8.6.1 RGW Server配置: 在实际的生产环境，RGW1和RGW的配置参数是完全一样的. jack@ceph-mgr2:-$ sudo cat /etc/ceph/ceph.conf [global] fsid = 1883278f-95fe-4f85-b027-3a6eba444861 public_network = 172.31.0.0/21 cluster_network = 192.168.0.0/21 mon_initial members = ceph-mon1 mon_host= 172.31.6.101 auth_cluster_required = cephx auth_service_required = cephx auth_client_required = cephx [client.rgw.ceph-mgr1] rgw_host = ceph-mgr1 rgw_frontends = civetweb port:=9900 rgw_dns_name = rgw.magedu.net [client.rgw.ceph-mgr2] rgw_host = ceph-mgr2 rgw_frontends = civetweb port:=9900 rgw_dns_name = rgw.magedu.net","date":"2023-01-16","objectID":"/posts/ceph/8.-%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%BD%91%E5%85%B3rgw/:4:0","tags":["分布式存储"],"title":"RGW对象存储网关 （八）","uri":"/posts/ceph/8.-%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%BD%91%E5%85%B3rgw/"},{"categories":["Ceph"],"content":"8.6.2 创建RGW 账号 ceph@ceph-deploy:/home/ceph/ceph cluster$ rdosgw admin user create --uid=\"user1\" --display-name=\"user1\" #创建用户会生成key 是访问SGW存储的凭证 \"keys\": [ { \"user\" : \"\"use1\"\", \"access_key\": \"T119RIWTRMMI9BBJEC66\"， \"secret_key\": \"r8kwaYi9hdZJyCKW23hucEUABli 5xOAXSGs8worB\" } ],","date":"2023-01-16","objectID":"/posts/ceph/8.-%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%BD%91%E5%85%B3rgw/:4:1","tags":["分布式存储"],"title":"RGW对象存储网关 （八）","uri":"/posts/ceph/8.-%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%BD%91%E5%85%B3rgw/"},{"categories":["Ceph"],"content":"8.6.3 安装s3cmd客户端 s3cmd 是一个通过命令行访问ceph RGW实现创建存储同桶、上传、下载以及管理数据到对象存储的命令行客户端工具。 ceph@ceph-deploy:/home/ceph/ceph-cluster$ sudo apt-cache madison s3cmd ceph@ceph-deploy:/home/ceph/ceph-cluster$ sudo apt install s3cmd","date":"2023-01-16","objectID":"/posts/ceph/8.-%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%BD%91%E5%85%B3rgw/:4:2","tags":["分布式存储"],"title":"RGW对象存储网关 （八）","uri":"/posts/ceph/8.-%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%BD%91%E5%85%B3rgw/"},{"categories":["Ceph"],"content":"8.6.4 配置客户端执行环境 8.6.4.1 s3cmd客户端添加域名解析 vim /etc/hosts 127.0.0.1 localhost 127.0.1.1 ubuntu.example.local ubuntu 172.31.6.108 ceph-node3.example.local ceph-node3 172.31.6.109 ceph-node4.example.local ceph-node4 172.31.6.201 rgw.magedu.net8.6.4.2 配置命令执行环境 jack@ceph-deploy:~$ sudo su - root root@ceph-deploy:~# s3cmd --help root@ceph-deploy:~# s3cmd --configure Enter new values or accept defaults in brackets with Enter. Refer to user manual for detailed description of all options. Access key and Secret key are your identifiers for Amazon S3. Leave them empty for using the env variables. Access Key: JIJX25OFEJ40JEBECDZV #输入用户 access key Secret Key: vBa23pj4AhGk9GPeSrhL9NLaldShudVfjQ4AC90E #输入用户secret key Default Region [US]: #region 选项 Use \"s3.amazonaws.com\" for S3 Endpoint and not modify it to the target Amazon S3. S3 Endpoint [s3.amazonaws.com]: rgw.magedu.net:9900 #RGW 域名 Use \"%(bucket)s.s3.amazonaws.com\" to the target Amazon S3. \"%(bucket)s\" and \"%(location)s\" vars can be used if the target S3 system supports dns based buckets. DNS-style bucket+hostname:port template for accessing a bucket [%(bucket)s.s3.amazonaws.com]: rgw.magedu.net:9900/%(bucket) #bucket城名格式 Encryption password is used to protect your files from reading by unauthorized persons while in transfer to S3 Encryption password : #秘钥是否使用密码加密 Path to GPG program [/usr/bin/gpg]: #使用gpg进行加密（系统自带不需要安装） #是否使用HTTPS When using secure HTTPS protocol all communication with Amazon S3 servers is protected from 3rd party eavesdropping. This method is slower than plain HTTP, and can only be proxied with Python 2.7 or newe r Use HTTPS protocol [Yes]: No ##是否使用代理服务器 On some networks all internet access must go through a HTTP proxy. Try setting it here if you can't connect to S3 directly HTTP Proxy server name : #生成的信息 New settings: Access Key: T119RIWTRMMI9BBJEC66 Secret Key: r8kWaY i9hdZJyCKW23hucEUABli5x0AXSGs8worB Default Region: US S3 Endpoint: rgw.magedu.net:9900 DNS-style bucket+hostname:port template for accessing a bucket: rgw.magedu.net:9900/%(bucket) Enc ryption password: Path to GPG prog ram:/usr/bin/gpg Use HTTPS protocol: False HTTP Proxy server name: HTTP Proxy server port: 0 Test access with supplied credentials? [Y/n]] Y #测试连接 PLease wait, attempting to list all buckets.. . Success. Your access key and secret key worked fine :-) #如果连接成功就弹出保存配置 Now verifying that encryption works. . . Not conf igured. Never mind. Save settings? [y/N] y Configuration saved to '/root/.s3cfg' #测试命令 s3cmd la","date":"2023-01-16","objectID":"/posts/ceph/8.-%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%BD%91%E5%85%B3rgw/:4:3","tags":["分布式存储"],"title":"RGW对象存储网关 （八）","uri":"/posts/ceph/8.-%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%BD%91%E5%85%B3rgw/"},{"categories":["Ceph"],"content":"8.6.5 命令行客户端s3cmd 验证数据上传 8.6.5.1 查看帮助信息 s3cmd --help8.6.5.2 创建bucket 以验证权限 存储空间(Bucket) 是用于存储对象(Object) 的容器， 在上传任意类型的Object前，您需要先创建Bucket. Make bucket s3cmd mb s3://BUCKET ------------------------------------------------- root@ceph-deploy:~# s3cmd mb s3://mybucket Bucket 's3://mybucket/' created root@ceph-deploy:~# s3cmd mb s3://css Bucket 's3://css/' created root@ceph-deploy:~# s3cmd mb s3://images Bucket 's3://images/' created8.6.5.3 验证上传数据 Put file into bucket s3cmd put FILE [FILE..] s3://BUCKET[/PREFIX] root@ceph-deploy:~# wget http://www.magedu.com/wp-content/uploads/2019/07/2019070508503489.png #上传数据 root@ceph-deploy:~# s3cmd put 2019070508503489.png s3://mages/ upload: '2019070508503489.png -\u003e 's3://images/2019070508503489.png' [1 of 1] 40703 of 40703 100% in 0s 1911.84 kB/s done root@ceph-deploy:~# s3cmd put 2019070508503489.png s3://images/png/ upload: '2019070508503489.png' -\u003e 's3://images/png/2019070508503489.png' [1 of 1] 40703 of 40703 100% in 2s 16.24 kB/s done #验证数据 root@ceph-deploy:~# s3cmd ls s3://images/ DIR s3://images/png/ 2021-08-26 13:35 40703 s3://images/2019070508503489.png root@ceph-deploy:~# s3cmd ls s3://images/png/ 2021-08-26 13:35 40703 s3://images/png/2019070508503489.png8.6.5.4 验证下载文件 Get file from bucket s3cmd get s3://BUCKET/OBJECT LOCAL_FILE ------------------------------------------------------------------------ root@ceph-deploy:~# s3cmd get s3://images/2019070508503489.png /opt/ download: 's3://images/2019070508503489.png' -\u003e /opt/2019070508503489.png' [1of 1] 40703 of 40703 100% in 0s 5.80 MB/s done root@ceph-deploy:~# ls /opt/2019070508503489.png8.6.5.5 删除文件 Delete file from bucket (alias for del) s3cmd rm s3://BUCKET/OBJECT --------------------------------------------------- root@ceph-deploy: ~# s3cmd ls s3://images/ #验证当前文件 DIR s3://images/png/ 2021-08-26 13:35 40703 s3://images/2019070508503489.png root@ceph-deploy: ~# s3cmd rm s3://images/2019070508503489.png #删除文件 delete: 's3://images/2019070508503489.png' root@ceph-deploy:~# s3cmd ls s3://images/ #验证是否被删除 DIR s3://images/png/ #查看存储池pg ceph pg ls-by-pool default.rgw.buckets.data awk '{print $1, $2, $15}'","date":"2023-01-16","objectID":"/posts/ceph/8.-%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%BD%91%E5%85%B3rgw/:4:4","tags":["分布式存储"],"title":"RGW对象存储网关 （八）","uri":"/posts/ceph/8.-%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%BD%91%E5%85%B3rgw/"},{"categories":["Ceph"],"content":"ceph FS即ceph filesystem，可以实现文件系统共享功能，客户端通过ceph协议挂载并使用ceph集群作为数据存储服务器。 （类似NFS） Ceph FS需要运行Meta Data Services(MDS)服务，其守护进程为ceph-mds, ceph-mds进程管理与Ceph FS上存储的文件相关的元数据，并协调对ceph存储集群的访问。http://docs.ceph.org.cn/cephfs/Ceph FS的元数据使用的动态子树分区,把元数据划分名称空间对应到不同的mds,写入元数据的时候将元数据按照名称保存到不同主mds上，有点类似于nginx中的缓存目录分层一样。但是最终元数据都会保存在ceph 元数据池中。 ","date":"2023-01-15","objectID":"/posts/ceph/7.-ceph-fs%E4%BD%BF%E7%94%A8/:0:0","tags":["分布式存储"],"title":"Ceph FS （七）","uri":"/posts/ceph/7.-ceph-fs%E4%BD%BF%E7%94%A8/"},{"categories":["Ceph"],"content":"7.1 部署MDS 服务 如果要使用cephFS,需要部署cephfs服务。 Ubuntu: root@ceph-mgr1:~# apt-cache madison ceph-mds root@ceph-mgr1:~# apt install ceph-mds Centos: root@ceph-mgr1 ~]# yum install ceph-mds $ ceph-deploy mds create ceph-mgr1","date":"2023-01-15","objectID":"/posts/ceph/7.-ceph-fs%E4%BD%BF%E7%94%A8/:1:0","tags":["分布式存储"],"title":"Ceph FS （七）","uri":"/posts/ceph/7.-ceph-fs%E4%BD%BF%E7%94%A8/"},{"categories":["Ceph"],"content":"7.2 创建CephFS metadata和data存储池 使用CephFS之前需要事先于集群中创建一个文件系统，并为其分别指定元数据和数据相关的存储池。下面创建一一个名为cephfs的文件系统用于测试，它使用cephfs-metadata为元数据存储池，使用cephfs-data 为数据存储池。 #保存metadata的pool [ceph@ceph-deploy ceph-cluster]$ ceph osd pool create cephfs-metadata 32 32 pool 'cephfs-metadata' created #保存数据的pool [ceph@ceph-deploy ceph-cluster]$ ceph osd pool create cephfs data 64 64 pool 'cephfs-data' created (ceph@ceph-deploy ceph-cluster]$ ceph -s #当前ceph状态 cluster: id:80a34e06-4458-4 1a8- 8d19-1 c0501152d69 health: HEALTH_ OK services: mon: 3 daemons, quorum ceph-mon1 ,ceph-mon2,ceph-mon3 mgr: ceph-mgr1(active), standbys: ceph-mgr2 osd: 12 osds: 12 up, 12 in rgw: 1 daemon active data: pools: 8 pools, 224 pgs objects: 278 objects, 302 MiB usage: 13 GiB used, 1.2 TiB / 1.2 TiB avail pgs: 224 active+clean","date":"2023-01-15","objectID":"/posts/ceph/7.-ceph-fs%E4%BD%BF%E7%94%A8/:2:0","tags":["分布式存储"],"title":"Ceph FS （七）","uri":"/posts/ceph/7.-ceph-fs%E4%BD%BF%E7%94%A8/"},{"categories":["Ceph"],"content":"7.3 创建cephFS并验证 [ceph@ceph-deploy ceph-cluster]$ ceph fs new mycephfs cephfs-metadata cephfs-data new fs with metadata pool 7 and data pool 8 [ceph@ceph-deploy ceph-cluster]$ ceph fs ls name: mycephfs, metadata pool: cephfs-metadata, data pools: [cephfs-data ]、 #查看指定cephFS状态 [ceph@ceph-deploy ceph-cluster$ ceph fs status mycephfs ","date":"2023-01-15","objectID":"/posts/ceph/7.-ceph-fs%E4%BD%BF%E7%94%A8/:3:0","tags":["分布式存储"],"title":"Ceph FS （七）","uri":"/posts/ceph/7.-ceph-fs%E4%BD%BF%E7%94%A8/"},{"categories":["Ceph"],"content":"7.4 验证cepfFS服务状态 #现在已经转变为活动状态 $ ceph mds stat mycephfs-1/1/1 up {0=ceph-mgr1=up:active} ","date":"2023-01-15","objectID":"/posts/ceph/7.-ceph-fs%E4%BD%BF%E7%94%A8/:4:0","tags":["分布式存储"],"title":"Ceph FS （七）","uri":"/posts/ceph/7.-ceph-fs%E4%BD%BF%E7%94%A8/"},{"categories":["Ceph"],"content":"7.5 创建客户端账户 #创建账户 [ceph@ceph-deploy ceph-cluster]$ ceph auth add client.yanyan mon 'allow r' mds 'allow rw' osd 'allow rwx pool=cephfs-data' added key for client.yanyan #验证账户 [ceph@ceph-deploy ceph-cluster]$ ceph auth get client.yanyan exported keyring for client.yanyan [client.yanyan] key = AQCxpdhfjQt1OxAAGe0mqTMveNu2ZMEem3tb0g== caps mds = \"allow rw\" caps mon = \"allow r\" caps osd = \"allow rwx pool=cephfs-data\" #创建用keyring文件 [ceph@ceph-deploy ceph-cluster]$ceph auth get client.yanyan -o ceph.client.yanyan.keyring #创建key文件: [ceph@ceph-deploy ceph-cluster]$ ceph auth print-key client.yanyan \u003e yanyan.key #验证用户的keyring文件 [ceph@ceph-deploy ceph-clusterl$ cat ceph.client.yanyan.keyring [client.yanyan] key = AQCxpdhfjQt1OxAAGe0mqTMveNu2ZMEem3tb0g== caps mds = \"allow rw\" caps mon = \"allow r\" caps osd = \"allow rwx pool=cephfs-data\"","date":"2023-01-15","objectID":"/posts/ceph/7.-ceph-fs%E4%BD%BF%E7%94%A8/:5:0","tags":["分布式存储"],"title":"Ceph FS （七）","uri":"/posts/ceph/7.-ceph-fs%E4%BD%BF%E7%94%A8/"},{"categories":["Ceph"],"content":"7.6 安装ceph客户端 [root@ceph-client3 ~]# yum install epel-release -y [root@ceph-client3~]# yum install htts://mirs.aliyun.com/ceph/rpm-octopus/el7/noarch/ceph-release- 1-1.el7.noarch.rpm","date":"2023-01-15","objectID":"/posts/ceph/7.-ceph-fs%E4%BD%BF%E7%94%A8/:6:0","tags":["分布式存储"],"title":"Ceph FS （七）","uri":"/posts/ceph/7.-ceph-fs%E4%BD%BF%E7%94%A8/"},{"categories":["Ceph"],"content":"7.7 同步客户端认证文件 [ceph@ceph-deploy ceph-cluster]$ scp ceph.conf ceph.client.yanyan.keyring yanyan.key root@172.31.6.203:/etc/ceph/","date":"2023-01-15","objectID":"/posts/ceph/7.-ceph-fs%E4%BD%BF%E7%94%A8/:7:0","tags":["分布式存储"],"title":"Ceph FS （七）","uri":"/posts/ceph/7.-ceph-fs%E4%BD%BF%E7%94%A8/"},{"categories":["Ceph"],"content":"7.8 客户端验证权限 [root@ceph-client3 ~]# ceph --user yanyan -s","date":"2023-01-15","objectID":"/posts/ceph/7.-ceph-fs%E4%BD%BF%E7%94%A8/:8:0","tags":["分布式存储"],"title":"Ceph FS （七）","uri":"/posts/ceph/7.-ceph-fs%E4%BD%BF%E7%94%A8/"},{"categories":["Ceph"],"content":"7.9 内核空间挂载ceph-fs 客户端挂载有两种方式，一是内核空间，二是用户空间,内核空间挂载需要内核支持ceph模块，用户空间挂载需要安装ceph-fuse。 内核\u003e 2.6.34默认支持ceph ","date":"2023-01-15","objectID":"/posts/ceph/7.-ceph-fs%E4%BD%BF%E7%94%A8/:9:0","tags":["分布式存储"],"title":"Ceph FS （七）","uri":"/posts/ceph/7.-ceph-fs%E4%BD%BF%E7%94%A8/"},{"categories":["Ceph"],"content":"7.9.1 客户端通过key文件挂载 root@ceph-client3 ~]# mkdir /data [root@ceph-client3~]# mount -t ceph 172.31.6.104:6789,172.31.6.105:6789,172.31.6.106:6789:/ /data -o name-yanyan,secretfile=/etc/ceph/yanyan.key ","date":"2023-01-15","objectID":"/posts/ceph/7.-ceph-fs%E4%BD%BF%E7%94%A8/:9:1","tags":["分布式存储"],"title":"Ceph FS （七）","uri":"/posts/ceph/7.-ceph-fs%E4%BD%BF%E7%94%A8/"},{"categories":["Ceph"],"content":"7.9.2 客户端通过key挂载 [root@ceph-client3 ~]# tail /etc/ceph/yanyan.key AQCxpdhfjQt1OxAAGe0mqTMveNu2ZMEem3tb0g== root@ceph-client3 ~]# umount /data/ [root@ceph-client3 ~]# mount -t ceph 172.31.6.104:6789,172.31.6.105:6789,172.31.6.106:6789:/ /data -o name=yanyan,secret=AQCxpdhfjQt1OxAAGe0mqTMveiJu2ZMEem3tb0g==","date":"2023-01-15","objectID":"/posts/ceph/7.-ceph-fs%E4%BD%BF%E7%94%A8/:9:2","tags":["分布式存储"],"title":"Ceph FS （七）","uri":"/posts/ceph/7.-ceph-fs%E4%BD%BF%E7%94%A8/"},{"categories":["Ceph"],"content":"7.9.3 开机挂载 root@ceph-client3 ~]# cat /etc/fstab 172.31.6.104:6789,172.31.6.105:6789,172.31.6.106:6789:/ /data ceph defaults,name=yanyan,secretfile=/etc/ceph/yanyan.key,_netdev 0 0 [root@ceph-client3 ~]# mount -a #secret挂载 172.31.6.101:6789,172.31.6.102:6789,172.31.6.103:6789:/ /data/cephfs ceph defaults ,name=yanyan,secret=AQBpxyBhUXlrIBAA9bW3UG2rdv6hQm0Is9MC7Q==,_netdev 0 0","date":"2023-01-15","objectID":"/posts/ceph/7.-ceph-fs%E4%BD%BF%E7%94%A8/:9:3","tags":["分布式存储"],"title":"Ceph FS （七）","uri":"/posts/ceph/7.-ceph-fs%E4%BD%BF%E7%94%A8/"},{"categories":["Ceph"],"content":"7.9.4 客户端模块 客户端内核加载ceph.ko模块挂载cephfs文件系统 ","date":"2023-01-15","objectID":"/posts/ceph/7.-ceph-fs%E4%BD%BF%E7%94%A8/:9:4","tags":["分布式存储"],"title":"Ceph FS （七）","uri":"/posts/ceph/7.-ceph-fs%E4%BD%BF%E7%94%A8/"},{"categories":["Ceph"],"content":"7.10 用户空间挂载ceph-fs 如果内核本较低而没有ceph模块,那么可以安装ceph-fuse 挂载，但是推荐使用内核模块挂载。 ","date":"2023-01-15","objectID":"/posts/ceph/7.-ceph-fs%E4%BD%BF%E7%94%A8/:10:0","tags":["分布式存储"],"title":"Ceph FS （七）","uri":"/posts/ceph/7.-ceph-fs%E4%BD%BF%E7%94%A8/"},{"categories":["Ceph"],"content":"7.10.1 安装ceph-fuse http://docs.ceph.org.cn/man/8/ceph-fuse/在一台新的客户端或还原快照，然后安装ceph-fuse [root@ceph- client2 ~]# yum install epel-release -y [root@ceph-client2 ~]# yum install https://mirrors.aliyun.com/ceph/rpm-octopus/el7/noarch/ceph-release-1-1.el7.noarch.rpm -y [ceph@ceph-deploy ceph-cluster]$ scp /etc/yum.repos.d/ceph.repo /etc/yum.repos.d/epel* root@172.31.6.111:/etc/yum.repos.d/ root@ceph-client2 ~]# yum install ceph-fuse ceph-common -y","date":"2023-01-15","objectID":"/posts/ceph/7.-ceph-fs%E4%BD%BF%E7%94%A8/:10:1","tags":["分布式存储"],"title":"Ceph FS （七）","uri":"/posts/ceph/7.-ceph-fs%E4%BD%BF%E7%94%A8/"},{"categories":["Ceph"],"content":"7.10.2 ceph-fuse 挂载 #同步认证及配置文件: [ceph@ceph-deploy ceph-cluster]$ scp ceph.conf ceph.client.yanyan.keyring root@172.31 .6.111:/etc/ceph/ root@172.31.6.111's password: #通过ceph-fuse挂载ceph [root@ceph-client2 ~]# mkdir /data [root@ceph-client2 ~]# ceph-fuse --name client.yanyan -m 172.31.6.104:6789,172.31.6.105:6789,172.31.6.106:6789 /data ceph-fuse[1628]: starting ceph client 2021-06-08 10:51:24.332 7f5a3898ec00 -1 init, newargv = 0x556a48c77da0 newargc=7 ceph-fuse[1628]: starting fuse #开机挂载,指定用户会自动根据用户名称加载授权文件及配置文件ceph.conf root@ceph-cient2 ~]# vim /etc/fstab none /data fuse.ceph ceph.id=yanyan,ceph.conf=/etc/ceph/ceph.conf,_netdev,defaults 0 0 [root@ceph-client2 ~]# umount /data [root@ceph-client2 ~]# mount -a ceph-fuse[1760]: starting ceph client 2021-06-08 10:56:57.602 7f24d91b9c00 -1 init, newargv = 0x55999f6cda40 newargc=9 ceph-fuse[1 7601. startina fuse","date":"2023-01-15","objectID":"/posts/ceph/7.-ceph-fs%E4%BD%BF%E7%94%A8/:10:2","tags":["分布式存储"],"title":"Ceph FS （七）","uri":"/posts/ceph/7.-ceph-fs%E4%BD%BF%E7%94%A8/"},{"categories":["Ceph"],"content":"7.11 ceph mds高可用 基于多mds服务器，在业务高并发时频繁读写元数据的场景。 Ceph mds(etadata service)作为ceph的访问入口，需要实现高性能及数据备份，假设启动4个MDS进程，设置2个Rank.这时候有2个MDS进程会分配给两个Rank，还剩下2个MDS进程分别作为另外个的备份。 通过参数指定那主的备是谁设置每个Rank的备份MDS,也就是如果此Rank当前的MDS出现问题马上切换到另个MDS，设置备份的方法有很多，常用选项如下。mds_standby_replay: 值为true或false, true 表示开启replay 模式，这种模式下主MDS内的数量将实时与从MDS同步，如果主宕机，从可以快速的切换，如果为false只有宕机的时候才去同步数据，这样会有一段时间的中断.mds_standby_for_name:设置当前MDS进程只用于备份于指定名称的MDS.mds_standby_for_rank: 设置当前MDS进程只用于备份于哪个Rank,通常为Rank编号另外在存在之个CephFS文件系统中，还可以使用mds_standby_for fscid 参数来为指定不同的文件系统。mds_standby_for fscid: 指定CephFS文件系统ID,需要联合mds_standby_for_rank生效，如果设置mds_standby_for_rank, 那么就是用于指定文件系统的指定Rank,如果没有设置，就是指定文件系统的所有Rank。 ","date":"2023-01-15","objectID":"/posts/ceph/7.-ceph-fs%E4%BD%BF%E7%94%A8/:11:0","tags":["分布式存储"],"title":"Ceph FS （七）","uri":"/posts/ceph/7.-ceph-fs%E4%BD%BF%E7%94%A8/"},{"categories":["Ceph"],"content":"7.11.1 当前mds服务器状态 [ceph@ceph-deploy ceph-cluster]$ ceph mds stat mycephfs-1/1/1 up {0=ceph-mgr1=up:active}","date":"2023-01-15","objectID":"/posts/ceph/7.-ceph-fs%E4%BD%BF%E7%94%A8/:11:1","tags":["分布式存储"],"title":"Ceph FS （七）","uri":"/posts/ceph/7.-ceph-fs%E4%BD%BF%E7%94%A8/"},{"categories":["Ceph"],"content":"7.11.2 添加MDS服务器 将ceph-mgr2和ceph-mon2和ceph-mon3作为mds服务角色添加至ceph集群.最后实现两主两备的mds高可用和高性能结构。 #mds服务器安装ceph-mds服务 [root@ceph-mgr2 ~]# yum install ceph-mds -y [root@ceph-mon2 ~]# yum install ceph-mds -y [root@ceph-mon3 ~]# yum install ceph-mds -y #添加mds服务器 [ceph@ceph-deploy ceph-cluster]$ ceph-deploy mds create ceph-mgr2 [ceph@ceph-deploy ceph-cluster]$ ceph-deploy mds create ceph-mon2 [ceph@ceph-deploy ceph-cluster]$ ceph-deploy mds create ceph-mon3 #验证mds服务器当前状态: [ceph@ceph-deploy ceph-cluster]$ ceph mds stat mycephfs-1/1/1 up {0=ceph-mgr1 =up:active}, 3 up:standby","date":"2023-01-15","objectID":"/posts/ceph/7.-ceph-fs%E4%BD%BF%E7%94%A8/:11:2","tags":["分布式存储"],"title":"Ceph FS （七）","uri":"/posts/ceph/7.-ceph-fs%E4%BD%BF%E7%94%A8/"},{"categories":["Ceph"],"content":"7.11.3 验证ceph集群当前状态 当前处于激活状态的mds服务器有一台，处于备份状态的mds服务器有三台. [ceph@ceph-deploy ceph-cluster]$ ceph fs status ","date":"2023-01-15","objectID":"/posts/ceph/7.-ceph-fs%E4%BD%BF%E7%94%A8/:11:3","tags":["分布式存储"],"title":"Ceph FS （七）","uri":"/posts/ceph/7.-ceph-fs%E4%BD%BF%E7%94%A8/"},{"categories":["Ceph"],"content":"7.11.4 当前的文件系统状态 [ceph@ceph-deploy ceph-cluster]$ ceph fs get mycephfs Filesystem 'mycephfs' (1) fs_name mycephfs epoch 4 flags 12 created 2021-06-01 17:09:25.850256 modified 2021-06-0117:09:26.854640 tableserver 0 root 0 session_timeout 60","date":"2023-01-15","objectID":"/posts/ceph/7.-ceph-fs%E4%BD%BF%E7%94%A8/:11:4","tags":["分布式存储"],"title":"Ceph FS （七）","uri":"/posts/ceph/7.-ceph-fs%E4%BD%BF%E7%94%A8/"},{"categories":["Ceph"],"content":"7.11.5 设置处于激活状态的mds的数量 目前有四个mds服务器，但是有一个主三个备，可以优化一 下部署架构，设置为为两主两备 [ceph@ceph-deploy ceph-cluster]$ cepn fs set mycephfs max_mds 2 #设置同时活跃的主mds最大值为2 cephaceph-dep loy :~/ ceph-c luster$ ceph fs status mycephfs - 1 clients ======== RANK STATE MDS ACTIVITY DNS INOS DIRS CAPS 0 active ceph-mgr1 Reqs: 0 /s 32 19 12 7 1 active ceph-mon2 Reqs: 0 /s 10 13 11 0 POOL TYPE USED AVAIL cephfs-metadata metadata 1347k 630G cephfs-data data 849M 630G STANDBY MDS ceph-mon1 ceph-mon3 MDS version: ceph version 16.2.5 ( 0883bdea7337b95e4b611c768c0279868462204a) pacific (stable)","date":"2023-01-15","objectID":"/posts/ceph/7.-ceph-fs%E4%BD%BF%E7%94%A8/:11:5","tags":["分布式存储"],"title":"Ceph FS （七）","uri":"/posts/ceph/7.-ceph-fs%E4%BD%BF%E7%94%A8/"},{"categories":["Ceph"],"content":"7.11.6 MDS高可用优化 目前的状态是ceph-mgr1和ceph-mon2分别是active 状态，ceph-mon3 和ceph-mgr2分别处于standby状态。现在可以将ceph-mgr2设置为ceph-mgr1的standby,将ceph-mon3设置为ceph-mon2的standby,以实现每个主都有一个固定备份角色的结构，则修改配置文件如下: [ceph@ceph-deploy ceph-cluster]$ vim ceph.conf [global] fsid = 23b0f9f2-8db3-477f-99a7-35a90eaf3dab public_ network = 172.31.0.0/21 cluster_ network = 192.168.0.0/21 mon_ initial members = ceph-mon1 monhost= 172.31.6.104 auth_ cluster_ required = cephx auth service_ required = cephx auth_ client required = cephx mon clock drift allowed = 2 mon clock drift warn backoff = 30 [mds.ceph-mgr2] #指定mgr的配置 #mds_standby_for_fscid = mycephfs mds_standby_for_name = ceph-mgr1 #指定主是谁 mds_standby_replay = true [mds_ceph-mon3] #指定mon3的配置 mds_standby_for name = ceph-mon2 #指定主是谁 mds_standby_replay = true","date":"2023-01-15","objectID":"/posts/ceph/7.-ceph-fs%E4%BD%BF%E7%94%A8/:11:6","tags":["分布式存储"],"title":"Ceph FS （七）","uri":"/posts/ceph/7.-ceph-fs%E4%BD%BF%E7%94%A8/"},{"categories":["Ceph"],"content":"7.11.7 分发配置文件并重启mds服务 #分发配置文件保证各mds服务重启有效 $ ceph-deploy --overwrite-conf config push ceph-mon3 $ ceph-deploy --overwrite-conf config push ceph-mon2 $ ceph-deploy --overwrite-conf config push ceph-mgr1 $ ceph-deploy --overwrite-conf config push ceph-mgr2 root@ceph-mon2 ~]# systemctl restart ceph-mds@ceph-mon2 .service root@ceph-mon3 ~]# systemctl restart ceph-mds@ceph-mon3.service root@ceph-mgr2 ~]# systemctl restart ceph-mds@ceph-mgr2.service root@ceph-mgr1 ~]# systemctl restart ceph-mds@ceph-mgr1.service","date":"2023-01-15","objectID":"/posts/ceph/7.-ceph-fs%E4%BD%BF%E7%94%A8/:11:7","tags":["分布式存储"],"title":"Ceph FS （七）","uri":"/posts/ceph/7.-ceph-fs%E4%BD%BF%E7%94%A8/"},{"categories":["Ceph"],"content":"7.11.8 ceph集群mds高可用状态 ceph fs status","date":"2023-01-15","objectID":"/posts/ceph/7.-ceph-fs%E4%BD%BF%E7%94%A8/:11:8","tags":["分布式存储"],"title":"Ceph FS （七）","uri":"/posts/ceph/7.-ceph-fs%E4%BD%BF%E7%94%A8/"},{"categories":["Ceph"],"content":"7.12 通过ganesha将cephfs导出为NFS 通过ganesha将cephfs通过NFS协议共享使用。 https://www.server-world.info/en/note?os=Ubuntu_20.04\u0026p=ceph15\u0026f=8 把Ceph Fs 中转成NFS协议 ","date":"2023-01-15","objectID":"/posts/ceph/7.-ceph-fs%E4%BD%BF%E7%94%A8/:12:0","tags":["分布式存储"],"title":"Ceph FS （七）","uri":"/posts/ceph/7.-ceph-fs%E4%BD%BF%E7%94%A8/"},{"categories":["Ceph"],"content":"7.12.1 服务端配置 root@ceph-mgr1:~# apt install nfs-ganesha-ceph root@ceph-mgr1:~# cd /etc/ganesha/ root@ceph-mgr1:/etc/ganesha# cat ganesha.conf # create new NFS_CORE_PARAM { # disable NLM Enable_ NLM = false; # disable RQUOTA (not suported on CephFS) Enable_ RQUOTA = false; # NFS protocol Protocols = 4; } EXPORT_DEFAULTS { # default access mode Access_Type = RW; } EXPORT { # uniq ID Export_ld = 1; # mount path of CephFS Path = \"/\"; FSAL { name = CEPH; # hostname or IP address of this Node hostname=\"172.31.6.104\"; } # setting for root Squash Squash=\"No_root_squash\"; # NFSv4 Pseudo path Pseudo= */magedu\"; # allowed security options SecType = \"sys\"; } LOG { # default log level Default_ _Log_ Level = WARN; } root@ceph-mgr1:/etc/ganesha# systemctl restart nfs-ganesha","date":"2023-01-15","objectID":"/posts/ceph/7.-ceph-fs%E4%BD%BF%E7%94%A8/:12:1","tags":["分布式存储"],"title":"Ceph FS （七）","uri":"/posts/ceph/7.-ceph-fs%E4%BD%BF%E7%94%A8/"},{"categories":["Ceph"],"content":"7.12.2 客户端挂载测试 root@ceph- client3-ubuntu1804:~# mount -t nfs 172.31.6.104:/magedu /data 验证挂载: df -TH #客户端测试写人数据: root@ceph-client3-ubuntu1804:~# echo \"ganesha v11111111\" \u003e\u003e /data/magedu/data/magedu.txt","date":"2023-01-15","objectID":"/posts/ceph/7.-ceph-fs%E4%BD%BF%E7%94%A8/:12:2","tags":["分布式存储"],"title":"Ceph FS （七）","uri":"/posts/ceph/7.-ceph-fs%E4%BD%BF%E7%94%A8/"},{"categories":["Ceph"],"content":"6.1 RBD架构图 Ceph可以同时提供对象存储RADOSGW 、块存储RBD、文件系统存储Ceph FSRBD即RADOS Block Device的简称，RIBD 块存储是常用的存储类型之一，RBD块设备类似磁盘可以被挂载，RBD 块设备具有快照、多副本、克隆和一致性等特性，数据以条带化的方式存储在Ceph集群的多个OSD中，条带化技术就是一种自动的将1/0 的负载均衡到多个物理磁盘上的技术，条带化技术就是将一块连续的数据分成很多小部分并把他们分别存储到不同磁盘上去.这就能使多个进程同时访问数据的多个不同部分而不会造成磁盘冲突，而且在需要对这种数据进行顺序访问的时候可以获得最大程度上的/O并行能力，从而获得非常好的性能。 ","date":"2023-01-14","objectID":"/posts/ceph/6.-ceph-rbd%E4%BD%BF%E7%94%A8/:1:0","tags":["分布式存储"],"title":"Ceph RBD （六）","uri":"/posts/ceph/6.-ceph-rbd%E4%BD%BF%E7%94%A8/"},{"categories":["Ceph"],"content":"6.2 创建存储池 [ceph@ceph-deploy ceph-cluster]$ ceph osd pool create rbd-data1 32 32 pool 'rbd-data1' created #在存储池启用rbd [ceph@ceph-deploy ceph-cluster]$ ceph osd pool appliclition enable rbd-data1 rbd enabled application 'rbd' on pool 'rbd-data1' #初始化rbd: [ceph@ceph-deploy ceph-cluster]$ rbd pool init -P rbd-data1","date":"2023-01-14","objectID":"/posts/ceph/6.-ceph-rbd%E4%BD%BF%E7%94%A8/:2:0","tags":["分布式存储"],"title":"Ceph RBD （六）","uri":"/posts/ceph/6.-ceph-rbd%E4%BD%BF%E7%94%A8/"},{"categories":["Ceph"],"content":"6.3 创建img镜像 rbd存储池并不能直接用于块设备，而是需要事先在其中按需创建映像(image) ，并把映像文件作为块设备使用。rbd 命令可用于创建、查看及删除块设备相在的映像(image) ，以及克隆映像、创建快照.将映像回滚到快照和查看快照等管理操作。例如，下面的命令能够在指定的RBD即rbd-data1创建一个名为myimg1的映像: ","date":"2023-01-14","objectID":"/posts/ceph/6.-ceph-rbd%E4%BD%BF%E7%94%A8/:3:0","tags":["分布式存储"],"title":"Ceph RBD （六）","uri":"/posts/ceph/6.-ceph-rbd%E4%BD%BF%E7%94%A8/"},{"categories":["Ceph"],"content":"6.3.1 创建镜像 创建镜像命令格式: #创建两个镜像: $ rbd create data-img1 --size 3G --pool rbd-data1 --image-format 2 --image-feature layering $ rbd create data-img2 --size 5G --pool rbd-data1 --image-format 2 --image-feature layering #验证镜像: $ rbd ls --pool rbd-data1 data-img1 data-img2 #列出镜像个多信息: $ rbd Is --pool rbd-data1 -| 1 NAME SIZE PARENT FMT PROT LOCK data-img1 3 GiB 2 data-img2 5 GiB 2","date":"2023-01-14","objectID":"/posts/ceph/6.-ceph-rbd%E4%BD%BF%E7%94%A8/:3:1","tags":["分布式存储"],"title":"Ceph RBD （六）","uri":"/posts/ceph/6.-ceph-rbd%E4%BD%BF%E7%94%A8/"},{"categories":["Ceph"],"content":"6.3.2 查看镜像详细信息 $ rbd --image data-img2 --pool rbd-data1 info rbd image 'data-img2': size 5 GiB in 1280 objects order 22 (4 MiB objects) #对象大小，每个对象是2^22/1024/1024=4MiB id: d42b6b8b4567 #镜像id block_name_prefix: rbd_data.d42b6b8b4567 #size里面的1280个对象名称前缀 format:2 #镜像文件格式版本 features:layering #特性，layering 支持分层快照以写时复制 op_features: flags: create timestamp: Mon Dec 14 12:22:27 2020","date":"2023-01-14","objectID":"/posts/ceph/6.-ceph-rbd%E4%BD%BF%E7%94%A8/:3:2","tags":["分布式存储"],"title":"Ceph RBD （六）","uri":"/posts/ceph/6.-ceph-rbd%E4%BD%BF%E7%94%A8/"},{"categories":["Ceph"],"content":"6.3.3 以json格式显示镜像信息: $ rbd ls --pool rbd-data1 -| --format json --pretty-format [ { \"image\": \"data-img1\", \"size\": 3221225472, \"format\": 2 }, { \"image\": \"data-img2\", \"size\": 5368709120, \"format\": 2 } ]","date":"2023-01-14","objectID":"/posts/ceph/6.-ceph-rbd%E4%BD%BF%E7%94%A8/:3:3","tags":["分布式存储"],"title":"Ceph RBD （六）","uri":"/posts/ceph/6.-ceph-rbd%E4%BD%BF%E7%94%A8/"},{"categories":["Ceph"],"content":"6.3.4 镜像的其他特性 特性简介layering:支持镜像分层快照特性，用于快照及写时复制，可以对image创建快照并保护，然后从快照克隆出新的image出来，父子image之间采用COW技术，共享对象数据。striping:支持条带化v2，类似raid 0 ，只不过在ceph环境中的数据被分散到不同的对象中，可改善顺序读写场景较多情况下的性能。 exclusive-lock: 支持独占锁，限制-一个镜像只能被一个客户端 使用.object-map: 支持对象映射(依赖exclusive-lock),加速数据导人导出及已用空间统计等，此特性开启的时候，会记录image所有对象的一个位图，用以标记对象是否真的存在，在一些场景下可以加速io。 fast-diff: 快速计算镜像与快照数据差异对比(依赖object-map).deep-flatten: 支持快照扁平化操作，用于快照管理时解决快照依赖关系等。journaling: 修改数据是否记录日志，该特性可以通过记录日志并通过日志恢复数据(依赖独占锁)，开启此特性会增加系统磁盘I0使用. jewel默认开启的特性包括: layering/exlcusive lock/object map/fast diff/deep flatten [ceph@ceph-deploy ceph-cluster]$ rbd help feature enable usage: rbd feature enable [--pool \u003cpool\u003e] [-image \u003cimage\u003e] [--journal-splay-width \u003cjourmal-splay-width\u003e] [--jourmal-object- size \u003cjournal-object- size\u003e] [--journal-pool \u003cjournal-pool\u003e] \u003cimage-spec\u003e \u003cfeatures\u003e [\u003cfeatures\u003e ... ] Enable the specified image feature. Positional arguments \u003cimage-spec\u003e image specification (example: [\u003cpool-name\u003e/\u003cimage-name\u003e) \u003cfeatures\u003e image features [exclusive-lock, object-map, fast-diff, journaling] Optional arguments -P [--pool] arg pool name --image arg image name --journal-splay-width arg number of active journal objects --joumal-object-size arg size of journal objects [4K \u003c= size \u003c= 64M] --joumnal-pool arg pool for journal objects","date":"2023-01-14","objectID":"/posts/ceph/6.-ceph-rbd%E4%BD%BF%E7%94%A8/:3:4","tags":["分布式存储"],"title":"Ceph RBD （六）","uri":"/posts/ceph/6.-ceph-rbd%E4%BD%BF%E7%94%A8/"},{"categories":["Ceph"],"content":"6.3.6 镜像特性的启用 启用指定存储池中的指定镜像的特性: $ rbd feature enable exclusive-lock --pool rbd-data1 --image data-img1 $ rbd feature enable object-map --pool rbd-data1 --image data-img1 $ rbd feature enable fast-diff --pool rbd-data1 --image data-img1 #验证镜像特性: $ rbd -imnage data-img1 --pool rbd-data1 info rbd image 'data-img1': size 3 GiB in 768 objects order 22 (4 MiB objects) id: d45b6b8b4567 block_name_prefix: rbd_data.d45b6b8b4567 format: 2 features: layering, exclusive-lock, object-map, fast-diff op_features: flags: object map invalid, fast diff invalid create_ timestamp: Mon Dec 14 12:35:44 2020","date":"2023-01-14","objectID":"/posts/ceph/6.-ceph-rbd%E4%BD%BF%E7%94%A8/:3:5","tags":["分布式存储"],"title":"Ceph RBD （六）","uri":"/posts/ceph/6.-ceph-rbd%E4%BD%BF%E7%94%A8/"},{"categories":["Ceph"],"content":"6.3.7 镜像特性的禁用 禁用指定存储池中指定镜像的特性 $ rbd feature disable fast-diff --pool rbd-data1 --image data-img1 #验证镜像特性: $ rbd --image data-img1 --pool rbd-data1 info rbd image 'data-img1': size 3 GiB in 768 objects order 22 (4 MiB objects) id: d45b6b8b4567 block_name_prefix: rbd_data.d45b6b8b4567 format: 2 features: layering, exclusive-lock, object-map #少了一个fast-diff 特性 op_features: flags: object map invalid create_timestamp: Mon Dec 14 12:35:44 2020","date":"2023-01-14","objectID":"/posts/ceph/6.-ceph-rbd%E4%BD%BF%E7%94%A8/:3:6","tags":["分布式存储"],"title":"Ceph RBD （六）","uri":"/posts/ceph/6.-ceph-rbd%E4%BD%BF%E7%94%A8/"},{"categories":["Ceph"],"content":"6.4 配置客户端使用RBD 在centos客户端挂载RBD，并分别使用admin及普通用户挂载RBD并验证使用. ","date":"2023-01-14","objectID":"/posts/ceph/6.-ceph-rbd%E4%BD%BF%E7%94%A8/:4:0","tags":["分布式存储"],"title":"Ceph RBD （六）","uri":"/posts/ceph/6.-ceph-rbd%E4%BD%BF%E7%94%A8/"},{"categories":["Ceph"],"content":"6.4.1 客户端配置yum源 客户端要想挂载使用ceph RBD，需要安装ceph客户端组件ceph-common，但是ceph-common不在cenos的yum仓库，因此需要单独配置yum源。 #配置yum源: yum install epel-release yum install htps://mirrors.aliyun.com/ceph/rpm-octopus/el7/noarch/ceph-release-1-1.el7.noarch.rpm -y","date":"2023-01-14","objectID":"/posts/ceph/6.-ceph-rbd%E4%BD%BF%E7%94%A8/:4:1","tags":["分布式存储"],"title":"Ceph RBD （六）","uri":"/posts/ceph/6.-ceph-rbd%E4%BD%BF%E7%94%A8/"},{"categories":["Ceph"],"content":"6.4.2 客户端安装ceph-common: yum install ceph-common","date":"2023-01-14","objectID":"/posts/ceph/6.-ceph-rbd%E4%BD%BF%E7%94%A8/:4:2","tags":["分布式存储"],"title":"Ceph RBD （六）","uri":"/posts/ceph/6.-ceph-rbd%E4%BD%BF%E7%94%A8/"},{"categories":["Ceph"],"content":"6.4.3 客户端使用普通用户挂载并使用RBD 创建普通账户并授权: #创建普通账户 [ceph@ceph-deploy ceph-cluster]$ ceph auth add clientshjie mon 'allow r' osd 'allow rwx pool=rbd-data1' added key for client.lije #验证用户信息 [ceph@ceph-deploy ceph-cluster$ ceph auth get client.shijie exported keyring for client.shijie [client.shijie] key = AQDHE9hfhzPVCRAAIlRuUIkWQW8YXv/JiLizFuA== caps mon = \"allow r\" caps osd = \"allow rwx pool=rbd-data1\" #创建空的keyring文件 [ceph@ceph-deploy ceph-cluster]$ ceph-authtool --riate-keyring ceph.client.shijie.keyring creating ceph.clent.shije.keyring #导出用户keyring [ceph@ceph-deploy ceph-cluster]$ ceph auth get client.shijie -o ceph.client.shiie.keyring exported keyring for client.shjjie #验证指定用户的keyring文件 [ceph@ceph-deploy ceph-cluster]$ cat ceph.client.shiie.keyring拷贝配置文件与普通账户keyring文件到客户端 scp ceph.cpnf ceph.client.shiie.keyring root@172.31.6.201:/etc/ceph","date":"2023-01-14","objectID":"/posts/ceph/6.-ceph-rbd%E4%BD%BF%E7%94%A8/:4:3","tags":["分布式存储"],"title":"Ceph RBD （六）","uri":"/posts/ceph/6.-ceph-rbd%E4%BD%BF%E7%94%A8/"},{"categories":["Ceph"],"content":"6.4.4 在客户端验证权限 [root@ceph-client2 ~]# cd /etc/ceph/ root@ceph-client2 ceph]# ls ceph.client.shije.keyring ceph.conf rbdmap root@ceph-client2 ceph]# ceph --user shijie -s #默认使用admin账户 cluster: id: 23b0f9f2 -8db3-477f-99a7 -35a90eaf3dab health: HEALTH_ OK mgr: ceph-mgr1(active), standbys: ceph-mgr2 mds: mycephfs-1/1/1 up {0=ceph-mgr1=up:active} osd: 12 osds: 12 up, 12 in rgw.1 daemon active data: pools: 9 pools, 256 pgs objects: 400 objects, 455 MiB usage: 14 GiB used, 1.2 TiB / 1.2 TiB avail pgs: 256 active+clean","date":"2023-01-14","objectID":"/posts/ceph/6.-ceph-rbd%E4%BD%BF%E7%94%A8/:4:4","tags":["分布式存储"],"title":"Ceph RBD （六）","uri":"/posts/ceph/6.-ceph-rbd%E4%BD%BF%E7%94%A8/"},{"categories":["Ceph"],"content":"6.4.5 映射rbd 使用普通用户权限映射rbd #映射rbd root@ceph-client2 ceph]# rbd --user shijie -p rbd-data1 map data-img2 /dev/rbd0 #验证rbd root@ceph- client2 ceph]# fdisk - /dev/rbd0 Disk /dev/rbd0: 5368 MB, 5368709120 bytes, 10485760 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 4194304 bytes / 4194304 bytes ","date":"2023-01-14","objectID":"/posts/ceph/6.-ceph-rbd%E4%BD%BF%E7%94%A8/:4:5","tags":["分布式存储"],"title":"Ceph RBD （六）","uri":"/posts/ceph/6.-ceph-rbd%E4%BD%BF%E7%94%A8/"},{"categories":["Ceph"],"content":"6.4.6 格式化并使用rbd镜像 [root@ceph-client2 ceph]# mkfs.ext4 /dev/rbd0 [root@ceph-client2 ceph]# mkdir /data [root@ceph-client2 ceph]# mount /dev/rbd0 /data/ [root@ceph-client2 ceph]# cp /var/log/messages /data/ root@ceph-client2 ceph]# df -TH #管理端验证镜像状态 [ceph@ceph-deploy ceph-cluster]$ rbd ls -p rbd-data1 -l NAME SIZE PARENT FMT PROT LOCK data-img1 3 GiB 2 excl #施加锁文件，已经被客户端映射 data-img2 5 GiB 2","date":"2023-01-14","objectID":"/posts/ceph/6.-ceph-rbd%E4%BD%BF%E7%94%A8/:4:6","tags":["分布式存储"],"title":"Ceph RBD （六）","uri":"/posts/ceph/6.-ceph-rbd%E4%BD%BF%E7%94%A8/"},{"categories":["Ceph"],"content":"6.4.7 验证ceph内核模块 挂载rbd之后系统内核会自动加载 libceph.ko 模块 #centos lsmod | grep ceph modinfo libceph #ubuntu ","date":"2023-01-14","objectID":"/posts/ceph/6.-ceph-rbd%E4%BD%BF%E7%94%A8/:4:7","tags":["分布式存储"],"title":"Ceph RBD （六）","uri":"/posts/ceph/6.-ceph-rbd%E4%BD%BF%E7%94%A8/"},{"categories":["Ceph"],"content":"6.4.8 rbd镜像空间拉伸 可以扩展空间，不建议缩小空间 #当前rbd镜像空间大小 [ceph@ceph-deploy ceph-cluster$ rbd ls -p rbd-data1 -l NAME SIZE PARENT FMT PROT LOCK data-img1 3 GiB 2 excl data-ima2 5 GiB 2 #rbd镜像空间拉伸命令 [ceph@ceph-deploy ceph-cluster]$ rbd help resize usage: rbd resize [--pool \u003cpool\u003e] [--image \u003cimage\u003e] --size \u003csize\u003e [--allow-shrink] [--no-progress] \u003cimage-spec\u003e #拉伸rbd镜像空间 [ceph@ceph-deploy ceph-cluster]$ rbd resize --pool rbd-data1 --image data-img2 -size 8G Resizing image: 100% complete..done. [ceph@ceph-deploy ceph-cluster]$ rbd ls -p rbd-data1 -l NAME SIZE PARENT FMT PROT LOCK data-img1 3 GiB 2 excl data-img2 8 GiB 2 客户端验证镜像空间: [root@ceph-client2 ~]# fdisk -l /dev/rbd0 Disk /dev/rbd0: 8589 MB, 8589934592 bytes, 16777216 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes/ 512 bytes I/O size (minimum/optimal): 4194304 bytes / 4194304 bytes","date":"2023-01-14","objectID":"/posts/ceph/6.-ceph-rbd%E4%BD%BF%E7%94%A8/:4:8","tags":["分布式存储"],"title":"Ceph RBD （六）","uri":"/posts/ceph/6.-ceph-rbd%E4%BD%BF%E7%94%A8/"},{"categories":["Ceph"],"content":"6.4.9 开机自动挂载 [root@ceph-client2 ~]# cat /etc/rc.d/rc.local ripod --user shijie -p rbd-data1 map data-img1 mount /dev/rbd0 /data/ [root@ceph-client2 ~]# chmod a+x /etc/rc.d/rc.local [root@ceph-client2 ~]# reboot #查看映射 [root@ceph-client2 ~]# rbd showmapped id pool image snap device 0 rbd-data1 data-img2 - /dev/rbd0 #验证挂载 [root@ceph-client2 ~]# df -TH Filesystem Type Size Used Avail Use% Mounted on devtmpfs devtmpfs 942M 0 942M 0% /dev","date":"2023-01-14","objectID":"/posts/ceph/6.-ceph-rbd%E4%BD%BF%E7%94%A8/:4:9","tags":["分布式存储"],"title":"Ceph RBD （六）","uri":"/posts/ceph/6.-ceph-rbd%E4%BD%BF%E7%94%A8/"},{"categories":["Ceph"],"content":"6.4.10 卸载rbd镜像 [root@ceph-client2 ceph]# umount /data root@ceph-client2 ceph]# rbd --user shijie -p rbd-data1 unmap data-img2","date":"2023-01-14","objectID":"/posts/ceph/6.-ceph-rbd%E4%BD%BF%E7%94%A8/:4:10","tags":["分布式存储"],"title":"Ceph RBD （六）","uri":"/posts/ceph/6.-ceph-rbd%E4%BD%BF%E7%94%A8/"},{"categories":["Ceph"],"content":"6.4.11 删除rbd镜像 镜像删除后数据也会被删除而且是无法恢复，因此在执行删除操作的时候要慎重。 [ceph@ceph-deploy ceph-cluster]$ rbd help rm usage: rbd rm [--pool \u003cpool\u003e] [--image \u003cimage\u003e] [--no-progress] \u003cimage-spec\u003e","date":"2023-01-14","objectID":"/posts/ceph/6.-ceph-rbd%E4%BD%BF%E7%94%A8/:4:11","tags":["分布式存储"],"title":"Ceph RBD （六）","uri":"/posts/ceph/6.-ceph-rbd%E4%BD%BF%E7%94%A8/"},{"categories":["Ceph"],"content":"6.4.12 rbd镜像回收站机制 删除的镜像数据无法恢复,但是还有另外一种方法可以先把镜像移动到回收站，后期确认删除的时候再从回收站删除即可。 [ceph@ceph-deploy ceph-cluster$ rbd help trash status Show the status of this image. trash list (trash ls) List trash images. trash move (trash mv) Move an image to the trash. trash purge Remove all expired images from trash. trash remove (trash rm) Remove an image from trash. trash restore Restore an image from trash. #查看镜像状态: [ceph@ceph-deploy ceph-cluster]$ rbd status --pool rbd-data1 --image data-img2 Watchers: watcher=172.31.6.1 10:0/2342274731 client.54552 cookie=18446462598732840961 watcher=1 72.31.6.111:0/2165319040 client.54558 cookie=18446462598732840961 #将镜像移动到回收站: [ceph@ceph-deploy ceph-cluster]$ rbd trash move --pool rbd-data1 --image data-img2 #查看回收站的镜像: [ceph@ceph-deploy ceph-cluster$ rbd trash list --pool rbd-data1 d42b6b8b4567 data-img2 #从回收站删除镜像 如果镜像不再使用，可以直接使用trash remove将其从回收站删除 #还原镜像 [ceph@ceph-deploy ceph-cluster]$lybd trash restore --pool rbd-data1 --image data-img2 --image-id d42b6b8b4567 #验证镜像: [ceph@ceph-deploy ceph-cluster]$ rbd Is --pool rbd-data1 -1 NAME SIZE PARENT FMT PROT LOCK data-img2 8 GiB 2","date":"2023-01-14","objectID":"/posts/ceph/6.-ceph-rbd%E4%BD%BF%E7%94%A8/:4:12","tags":["分布式存储"],"title":"Ceph RBD （六）","uri":"/posts/ceph/6.-ceph-rbd%E4%BD%BF%E7%94%A8/"},{"categories":["Ceph"],"content":"6.5 镜像快照 [ceph@ceph-deploy ceph-cluster]$ rbd help snap snap create (snap add) #创建快照 snap limit clear #清除镜像的快照数量限制 snap limit set #设置一个镜像的快照上限 snap list (snap ls) #列出快照 snap protect #保护快 照被删除 snap purge #删除所有未保护的快照 snap remove (snap rm) #删除一个快照 snap rename #重命名快照 snap rollback (snap revert) #还原快照 snap unprotect #允许一个快照被删除(取消快照保护)","date":"2023-01-14","objectID":"/posts/ceph/6.-ceph-rbd%E4%BD%BF%E7%94%A8/:5:0","tags":["分布式存储"],"title":"Ceph RBD （六）","uri":"/posts/ceph/6.-ceph-rbd%E4%BD%BF%E7%94%A8/"},{"categories":["Ceph"],"content":"6.5.1 创建快照 (ceph@ceph-deploy ceph-cluster]$ rbd help snap create usage: rbd snap create [--pool \u003cpoob\u003e] [--image \u003cimage\u003e] [--snap \u003csnap\u003e] \u003csnap-spec\u003e #创建快照 $ rbd snap create --pool rbd-data1 --image data-img2 - snap img2-snap-20201215 #验证快照 $ rbd snap list --pool rbd-data1 --image data-img2 SNAPID NAME SIZE TIMESTAMP 4 img2-snap-20201215 8 GiB Tue Dec 15 15:26:20 2020","date":"2023-01-14","objectID":"/posts/ceph/6.-ceph-rbd%E4%BD%BF%E7%94%A8/:5:1","tags":["分布式存储"],"title":"Ceph RBD （六）","uri":"/posts/ceph/6.-ceph-rbd%E4%BD%BF%E7%94%A8/"},{"categories":["Ceph"],"content":"6.5.2 删除数据并还原快照 关闭服务\u003e取消挂载 \u003e 取消映射 #卸载rbd root@ceph-client2 ~]# umount /data root@ceph-client2 ~]# rbd unmap /dev/rbd0 #回滚命令: [ceph@ceph-deploy ceph-cluster]$ rbd help snap rollback usage: rbd snap rollback [--pool \u003cpool\u003e] [-image \u003cimage\u003e] [--snap \u003csnap\u003e] [--no-progress] \u003csnap-spec\u003e #回滚快照 [ceph@ceph-deploy ceph-clusterI$ rbd snap rollbaci --pool rbd-data1 --image data-img2 --snap img2-snap-20201215 Rolling back to snapshot: 100% complete..done.","date":"2023-01-14","objectID":"/posts/ceph/6.-ceph-rbd%E4%BD%BF%E7%94%A8/:5:2","tags":["分布式存储"],"title":"Ceph RBD （六）","uri":"/posts/ceph/6.-ceph-rbd%E4%BD%BF%E7%94%A8/"},{"categories":["Ceph"],"content":"6.5.3 删除快照 #删除指定快照 [ceph@ceph-deploy ceph-cluster]$I rbd snap remove --pool rbd-data1 --image data-img2 --snap img2-snap-20201215 Removing snap: 100% complet...done. #验证快照是否删除 [ceph@ceph-deploy ceph-cluster]$ rbd snap list --pool rbd-data1 --image data-img2","date":"2023-01-14","objectID":"/posts/ceph/6.-ceph-rbd%E4%BD%BF%E7%94%A8/:5:3","tags":["分布式存储"],"title":"Ceph RBD （六）","uri":"/posts/ceph/6.-ceph-rbd%E4%BD%BF%E7%94%A8/"},{"categories":["Ceph"],"content":"6.5.4 快照数量限制 #设置与修改快照数量限制 [ceph@ceph-deploy ceph-cluster]$ rbd snap limit set --pool rbd-data1 --image data-img2 --limit 30 [ceph@ceph-deploy ceph-cluster]$ rbd snap limit set --pool rbd-data1 --image data-img2 --limit 20 [ceph@ceph-deploy ceph-cluster]$ rbd snap limit set --pool rbd-data1 --image data-img2 --limit 15","date":"2023-01-14","objectID":"/posts/ceph/6.-ceph-rbd%E4%BD%BF%E7%94%A8/:5:4","tags":["分布式存储"],"title":"Ceph RBD （六）","uri":"/posts/ceph/6.-ceph-rbd%E4%BD%BF%E7%94%A8/"},{"categories":["Ceph"],"content":"Ceph使用cephx协议对客户端进行身份认证cephx用于对ceph保存的数据进行认证访问和授权，用于对访问ceph的请求进行认证和授权检测，与mon通信的请求都要经过ceph认证通过，但是也可以在mon节点关闭cephx认证，但是关闭认证之后任何访问都将被允许，因此无法保证数据的安全性, ","date":"2023-01-13","objectID":"/posts/ceph/5.-cephx%E8%AE%A4%E8%AF%81%E6%9C%BA%E5%88%B6/:0:0","tags":["分布式存储"],"title":"CephX 认证机制 （五）","uri":"/posts/ceph/5.-cephx%E8%AE%A4%E8%AF%81%E6%9C%BA%E5%88%B6/"},{"categories":["Ceph"],"content":"5.1 授权流程 每个mon节点都可以对客户端进行身份认证并分发秘钥，因此多个mon节点就不存在单点故障和认证性能瓶颈。mon节点会返回用于身份认证的数据结构，其中包含获取ceph服务时用到的session key,session key通过客户端秘钥进行加密，秘钥是在客户端提前配置好的，/etc/ceph/ceph.client.admin.keyring 客户端使用session key向mon请求所需要的服务，mon 向客户端提供一个tiket, 用于向实际处理数据的OSD等服务验证客户端身份，MON和OSD共享同一个secret,因此OSD会信任所有MON发放的tikettiket存在有效期 :::info 注意:CephX身份验证功能仅限制在Ceph的各组件之间，不能扩展到其他非ceph组件Ceph 只负责认证授权，不能解决数据传输的加密问题 ::: ","date":"2023-01-13","objectID":"/posts/ceph/5.-cephx%E8%AE%A4%E8%AF%81%E6%9C%BA%E5%88%B6/:0:1","tags":["分布式存储"],"title":"CephX 认证机制 （五）","uri":"/posts/ceph/5.-cephx%E8%AE%A4%E8%AF%81%E6%9C%BA%E5%88%B6/"},{"categories":["Ceph"],"content":"5.2 访问流程 无论ceph客户端是哪种类型，例如块设备、对象存储、文件系统，ceph 都会在存储池中将所有数据存储为对象: ceph用户需要拥有存储池访问权限，才能读取和写入数据ceph用户必须拥有执行权限才能使用ceph的管理命令 查看key cephaceph-dep Loy:~/ ceph-cluster$ cat ceph. C lient . admin. key ring [client. admin] key = AQA3dhdhMd/UABAAr 2SNpJ+hcK1dD5 L2Hj 5XMg== caps mds = \"allow *\" caps mgr = \"allow *\" caps mon = \"allow *\" caps osd = \"allow *\"","date":"2023-01-13","objectID":"/posts/ceph/5.-cephx%E8%AE%A4%E8%AF%81%E6%9C%BA%E5%88%B6/:1:0","tags":["分布式存储"],"title":"CephX 认证机制 （五）","uri":"/posts/ceph/5.-cephx%E8%AE%A4%E8%AF%81%E6%9C%BA%E5%88%B6/"},{"categories":["Ceph"],"content":"5.3 ceph用户 用户是指个人(ceph管理者)或系统参与者**(MON/OSD/MDS)**.通过创建用户，可以控制用户或哪个参与者能够访问ceph存储集群、以及可访问的存储池及存储池中的数据。ceph支持多种类型的用户，但可管理的用户都属于client类型区分用户类型的原因在于, MON/OSD/MDS 等系统组件特使用cephx协议,但是它们为非客户端。 通过点号来分割用户类型和用户名，格式为TYPE.ID,例如client. admin 通常容器和服务器使用client类型 cephaceph-dep Loy:~/ ceph-cluster$ cat ceph.Client.admin.key ring [client.admin] key = AQA3dhdhMd/UABAAr 2SNpJ+hcK1dD5 L2Hj 5XMg== caps mds = \"allow *\" caps mgr = \"allow *\" caps mon = \"allow *\" caps osd = \"allow *\"","date":"2023-01-13","objectID":"/posts/ceph/5.-cephx%E8%AE%A4%E8%AF%81%E6%9C%BA%E5%88%B6/:2:0","tags":["分布式存储"],"title":"CephX 认证机制 （五）","uri":"/posts/ceph/5.-cephx%E8%AE%A4%E8%AF%81%E6%9C%BA%E5%88%B6/"},{"categories":["Ceph"],"content":"5.3.1 列出指定用户信息: (ceph@ceph-deploy ceph-cluster]$ ceph auth get osd.10 exported keyring for osd.10 [osd.10] key = AQCKF6JfL aEpBRAAbY/P +cHPFPUtnkzljruyXw== caps mgr = \"allow profile osd\" caps mon = \"allow profile osd\" caps osd = \"allow [ceph@ceph-deploy ceph-cluter$ ceph auth get client.admin exported keyring for client.admin [client.admin] key = AQAGDKJfQk/dAxAA3Y +9xoE/p8in6QjoHeXmeg== caps mds = \"allow *\" caps mgr = \"allow *\" caps mon = \"allow *\" caps osd = \"allow *\"","date":"2023-01-13","objectID":"/posts/ceph/5.-cephx%E8%AE%A4%E8%AF%81%E6%9C%BA%E5%88%B6/:2:1","tags":["分布式存储"],"title":"CephX 认证机制 （五）","uri":"/posts/ceph/5.-cephx%E8%AE%A4%E8%AF%81%E6%9C%BA%E5%88%B6/"},{"categories":["Ceph"],"content":"5.4 ceph授权和使能 ceph基于使能/能力(Capabilities,简称caps )来描述用户可针对MON/OSD或MDS使用的授权范围或级别，通用的语法格式: daemon-type 'allow caps' [...] 能力一览表: r: 向用户授子读取权限，访问监视器(mon)以检索CRUSH运行图时需具有此能力。W: 向用户授子针对对象的写人权限。x: 授予用户调用类方法(包括读取和写人)的能力，以及在监视器中执行auth操作的能力。*: 授予用户对特定守护进程/存储池的读取、写入和执行权限，以及执行管理命令的能力 class-read:授子用户调用类读取方法的能力，属于是x 能力的子集.class-write:授子用户调用类写人方法的能力，属于是x 能力的子集。 集群组件权限profile osd: 授予用户以某个OSD身份连接到其他OSD或监视器的权限.授予OSD权限，使OSD 能够处理复制检测信号流量和状态报告(获取OSD的状态信息).profile mds: 授予用户以某个MDS身份连接到其他MDS或监视器的权限。profile bootstrap-osd: 授予用户引导OSD的权限(初始化OSD并将OSD加人ceph集群)，授权给部署工具，使其在引导OSD时有权添加密钥。profile bootstrap-mds: 授子用户引导元数据服务器的权限，授权部署工具权限，使其在引导元数据服务器时有权添加密钥. MON能力:包括r/w/x和allow profile cap(ceph的运行图)例如: mon 'allow rwx' mon 'allow profile osd'OSD能力:包括r、w、 x、class-read、 class-write(类读取) 和profile osd(类写入)，另外OSD能力还允许进行存储池和名称空间设置。 osd 'llow capability' [pool=poolname] [namespace =namespace-name]MDS能力:只需要allow 或空都表示允许. mds 'allow'","date":"2023-01-13","objectID":"/posts/ceph/5.-cephx%E8%AE%A4%E8%AF%81%E6%9C%BA%E5%88%B6/:3:0","tags":["分布式存储"],"title":"CephX 认证机制 （五）","uri":"/posts/ceph/5.-cephx%E8%AE%A4%E8%AF%81%E6%9C%BA%E5%88%B6/"},{"categories":["Ceph"],"content":"5.5 ceph用户管理 用户管理功能可让 Ceph集群管理员能够直接在 Ceph集群中创建更新和删除用户。在Ceph集群中创建或删除用户时，可能需要将密钥分发到客户端，以便将密钥添加到密钥环文件中/etc/ceph/ceph.client.admin.keyring，此文件中可以包含一个或 者多个用户认证信息，凡是拥有此文件的节点，将具备访问ceph的权限，而且可以使用其中任何一个账户的权限，此文件类似于linux系统的中的**/etc/passwd**文件. 注意: TYPEID 表示法针对用户采用TYPE.ID表示法 例如osd.0指定是osd类并且ID为0的用户(节点), client.admin是client类型的用户，其ID为admin, 另请注意，每个项包含一个 key=xxx项，以及一个或多个caps项。 可以结合使用-0文件名选项和ceph auth list 将输出保存到某个文件, [ceph@ceph-deploy ceph-cluster]$ ceph auth list -o 123.key 添加一个用户会创建用户名(TYPE.ID). 机密密钥，以及包含在命令中用于创建该用户的所有能力,用户可使用其密钥向Ceph 存储集群进行身份验证。用户的能力授予该用户在Ceph monitor (mon)、Ceph OSD (osd)或Ceph元数据服务器(mds) 上进行读取、写入或执行的能力,可以使用以下几个命令来添加用户: ","date":"2023-01-13","objectID":"/posts/ceph/5.-cephx%E8%AE%A4%E8%AF%81%E6%9C%BA%E5%88%B6/:4:0","tags":["分布式存储"],"title":"CephX 认证机制 （五）","uri":"/posts/ceph/5.-cephx%E8%AE%A4%E8%AF%81%E6%9C%BA%E5%88%B6/"},{"categories":["Ceph"],"content":"5.5.1 ceph auth add 此命令是添加用户的规范方法。它会创建用户、生成密钥，并添加所有指定的能力。 [ceph@ceph-deploy ceph-cluster]$ ceph auth -h auth add \u003centity\u003e {\u003ccaps\u003e[\u003ccaps\u003e...]} #添加认证key: [ceph@ceph-deploy ceph-cluster]$ ceph auth add client.tom mon 'allow r' osd 'allow rwx pool=mypool' added key for client.tom #tom用户只能在mypool存储池对mon有读权限、对osd有读写执行权限 #验证key [ceph@ceph-deploy ceph-cluster]$ ceph auth get client.tom exported keyring for client.tom [client.tom] key = AQCErsdftuumL BAADUiAfQUI42ZIX1e/4PjpdA== caps mon = \"allow r\" caps osd = \"allow rwx pool=mypool\" exported keyring for client.tom","date":"2023-01-13","objectID":"/posts/ceph/5.-cephx%E8%AE%A4%E8%AF%81%E6%9C%BA%E5%88%B6/:4:1","tags":["分布式存储"],"title":"CephX 认证机制 （五）","uri":"/posts/ceph/5.-cephx%E8%AE%A4%E8%AF%81%E6%9C%BA%E5%88%B6/"},{"categories":["Ceph"],"content":"5.5.2 ceph auth get-or-create ceph auth get-or-create此命令是创建用户较为常见的方式之一, 它会返回包含用户名(在方括号中)和密钥的密钥文，如果该用户已存在，此命令只以密钥文件格式返回用户名和密钥，还可以使用 -o 指定文件名选项将输出保存到某个文件。 #创建用户 [ceph@ceph-deploy ceph-cluster]$ ceph auth get-or-create client,jack mon 'allow r osd 'allow rwx pool=mypool' [client.jack] key = AQAtr8dfi37XMhAADbHWEZOshY1QZ5A8eBpeoQ== #验证用户 [ceph@ceph-deploy ceph-clusterI$ ceph auth get client.jack exported keyring for cliet,jack [client.jack] key = AQAtr8dfi37XMhAADbHWEZOshY1QZ5A8eBpeoQ== caps mon=\"allow r\" caps osd =\"allow rwx pool=mypool\" #再次创建就不会创建了会打印key信息 [ceph@ceph-deploy ceph-cluster]$ ceph auth get-or-create client.jack mon 'allow r osd 'allow rwx pool=mypool [client.jack] key = AQAtr8dfiI37XMhAADbHWEZ0shY1QZ5A8eBpeoQ==","date":"2023-01-13","objectID":"/posts/ceph/5.-cephx%E8%AE%A4%E8%AF%81%E6%9C%BA%E5%88%B6/:4:2","tags":["分布式存储"],"title":"CephX 认证机制 （五）","uri":"/posts/ceph/5.-cephx%E8%AE%A4%E8%AF%81%E6%9C%BA%E5%88%B6/"},{"categories":["Ceph"],"content":"5.5.3 ceph auth get-or-create-key 此命令是创建用户并仅返回用户密钥，对于只需要密钥的客户端(例如libvirt) ，此命令非常有用。如果该用户已存在，此命令只返回密钥。您可以使用-o文件名选项将输出保存到某个文件。 创建客户端用户时，可以创建不具有能力的用户.不具有能力的用户可以进行身份验证，但不能执行其他操作，此类客户端无法从监视器检索集群地图,但是,如果希望稍后再添加能力，可以使用ceph auth caps命令创建一个不具有能力的用户。典型的用户至少对Ceph monitor具有读取功能，并对Ceph OSD具有读取和写人功能.此外，用户的OSD 权限通常限制为只能访问特定的存储池。 [ceph@ceph-deploy ceph-cluster]$ ceph auth get-or-create-key client.jack mon 'allow r' osd 'allow rwx pool=mypool' AQAtr8dfi37XMhAADbHWEZ0shY1QZ5A8eBpeoQ== #用户有key就显示没有就创建","date":"2023-01-13","objectID":"/posts/ceph/5.-cephx%E8%AE%A4%E8%AF%81%E6%9C%BA%E5%88%B6/:4:3","tags":["分布式存储"],"title":"CephX 认证机制 （五）","uri":"/posts/ceph/5.-cephx%E8%AE%A4%E8%AF%81%E6%9C%BA%E5%88%B6/"},{"categories":["Ceph"],"content":"5.5.4 ceph auth print-key 只获取单个指定用户的key信息 [ceph@ceph-deploy ceph-cluster$ ceph auth print-key client.jack AQAtr8dfi37XMhAADbHWEZ0shY 1QZ5A8eBpeoQ==","date":"2023-01-13","objectID":"/posts/ceph/5.-cephx%E8%AE%A4%E8%AF%81%E6%9C%BA%E5%88%B6/:4:4","tags":["分布式存储"],"title":"CephX 认证机制 （五）","uri":"/posts/ceph/5.-cephx%E8%AE%A4%E8%AF%81%E6%9C%BA%E5%88%B6/"},{"categories":["Ceph"],"content":"5.5.5 修改用户能力 使用ceph auth caps命令可以指定用户以及更改该用户的能力，设置新能力会完全覆盖当前的能力，因此要加上之前的用户已经拥有的能和新的能力，如果看当前能力，可以运行ceph auth get USERTYPE.USERID,如果要添加能力，使用以下格式时还需要指定现有能力: 权限修改后立即生效 #查看用户当前权限 [ceph@ceph-deploy ceph-cluster]$ ceph auth get client.jack exported keyring for client.jack [client.jack] key = AQAtr8dfi37XMhAADbHWEZ0shY1QZ5A8eBpeoQ== caps mon = \"allow r\" caps osd = \"allw rwx pool=mypool\" #修改用户权限 [ceph@ceph-deploy ceph-cluster]$ ceph auth caps client.jack mon 'allow r' osd 'allow rw pool=mypool' updated caps for client.jack #再次验证权限 [ceph@ceph-deploy ceph-cluster]$ ceph auth get client.jack exported keyring for client.jack","date":"2023-01-13","objectID":"/posts/ceph/5.-cephx%E8%AE%A4%E8%AF%81%E6%9C%BA%E5%88%B6/:4:5","tags":["分布式存储"],"title":"CephX 认证机制 （五）","uri":"/posts/ceph/5.-cephx%E8%AE%A4%E8%AF%81%E6%9C%BA%E5%88%B6/"},{"categories":["Ceph"],"content":"5.5.6 删除用户 要删除用户使用ceph auth del TYPE.ID,其中TYPE是client. osd. mon 或mds之一,ID是用户名或守护进程的ID. [ceph@ceph-deploy ceph-cluster]$ ceph auth del client.tom updated","date":"2023-01-13","objectID":"/posts/ceph/5.-cephx%E8%AE%A4%E8%AF%81%E6%9C%BA%E5%88%B6/:4:6","tags":["分布式存储"],"title":"CephX 认证机制 （五）","uri":"/posts/ceph/5.-cephx%E8%AE%A4%E8%AF%81%E6%9C%BA%E5%88%B6/"},{"categories":["Ceph"],"content":"5.6 秘钥环管理 ceph的秘钥环是一个保存了secrets、keys 、certificates并且能够让客户端通认证访问ceph的keyring file(集合文件），一个keyring file可以保存一个或者多个认证信息，每一个 key都有一个实体名称加权限，类型为:{client、mon、mds、osd)}.name 当客户端访问ceph集群时，ceph 会按顺序依次使用以下四个密钥环文件预设置密钥环设置:集群名词 + 用户类型 +用户ID+固定后缀/etc/ceph/\u003c$cluster name\u003e.\u003cuser $type\u003e.\u003cuser $id\u003e.keyring 保存单个用户的keyring/etc/ceph/cluster.keyring 保存多个用户的keyring/etc/ceph/keyring 未定义集群名称的多个用户的keyring/etc/ceph/keyring.bin 编译后的二进制文件 ","date":"2023-01-13","objectID":"/posts/ceph/5.-cephx%E8%AE%A4%E8%AF%81%E6%9C%BA%E5%88%B6/:5:0","tags":["分布式存储"],"title":"CephX 认证机制 （五）","uri":"/posts/ceph/5.-cephx%E8%AE%A4%E8%AF%81%E6%9C%BA%E5%88%B6/"},{"categories":["Ceph"],"content":"5.6.1 通过秘钥环文件备份与恢复用户 如果误删除账号可以通过秘钥环恢复用户，主要是key不会变（k8s 几十几百个pod）使用ceph auth add等命令添加的用户还需要额外使用ceph-authtool命令为其创建用户秘钥环文件.先创建空 keyring文件命令格式: ceph-authtool --create-keyring FILE导出用户认证信息至keyring文件:将用户信息导出至keyring文件，对用户信息进行备份。 #创建用户: (ceph@ceph-deploy ceph cluster]$ ceph auth get-or-create client.user1 mon 'allow r' osd 'allow *pool=mypool' [client.user1] key =AQAUUchfjpMqGRAARV6h0ofdDEneuaRnxuHjoQ== #验证用户 [ceph@ceph-deploy ceph-clusterl$ ceph auth get client.user1 exported keyring for client.user1 [client.user1] key = AQAUUchfjpMqGRAARV6hOofdDEneuaRnxuHjoQ== caps mon = \"allow r\" caps osd = \"allow *pool=mypool\" #创建keyring文件: (ceph@ceph-deploy ceph-cluster]$ ceph-authtool --create-keyring ceph.client.user1.keyring #验证keyring文件: [ceph@ceph-deploy ceph-cluster]$ cat ceph.client.user1.keyring #是个空文件 [ceph@ceph-deploy ceph-cluster]$ file ceph.client.user1.keyring ceph.client.user1.keyring: empty #导出keyring至指定文件 [ceph@ceph-deploy ceph-cluster]$ceph auth get client.user1 -o ceph.client.user1.keyring exported keyring for client.user1 #验证指定用户的keyring文件: [ceph@ceph-deploy ceph-cluster]$ cat ceph.client.user1.keyring [client.user1] key = AQAUUchfjpMqGRAARV6hOofdDEneuaRnxuHjoQ== caps mon = \"allow r\" caps osd = \"allow * pool=mypool\"在创建包含单个用户的密钥环时，通常建议使用ceph集群名称、用户类型和用户名及keyring来命名，并将其保存在**/etc/ceph** 目录中，例如为client.user1用户创建ceph.client.user1.keyring ","date":"2023-01-13","objectID":"/posts/ceph/5.-cephx%E8%AE%A4%E8%AF%81%E6%9C%BA%E5%88%B6/:5:1","tags":["分布式存储"],"title":"CephX 认证机制 （五）","uri":"/posts/ceph/5.-cephx%E8%AE%A4%E8%AF%81%E6%9C%BA%E5%88%B6/"},{"categories":["Ceph"],"content":"5.6.2 从keyring文件恢复用户认证信息 可以使用ceph auth import +指定keyring文件并导人到ceph,其实就是起到用户备份和恢复的目的: [ceph@ceph-deploy ceph-cluster]$ ceph auth import -i ceph.client.user1.keyring #导入用户","date":"2023-01-13","objectID":"/posts/ceph/5.-cephx%E8%AE%A4%E8%AF%81%E6%9C%BA%E5%88%B6/:5:2","tags":["分布式存储"],"title":"CephX 认证机制 （五）","uri":"/posts/ceph/5.-cephx%E8%AE%A4%E8%AF%81%E6%9C%BA%E5%88%B6/"},{"categories":["Ceph"],"content":"5.6.3 秘钥环文件添加多用户 一个keyring文件中可以包含多个不同用户的认证文件。将多用户导出至秘钥环: #创建keyring文件: $ ceph-authtool --create-keyring ceph.client.user.keyring #创建空的keyring文件 creating ceph.client.user.keyring #把指定的admin用户的keyring文件内容导人到user用户的keyring文件: $ceph-authtool ./ceph.client.user.keyring --import-keyring ./ceph.client.admin.keyring importing contents of ./ceph.client.admin.keyring into ./ceph.client.user.keyring #验证keyring文件: [ceph@ceph-deploy ceph-cluster]$ ceph-authtool -I ./ceph.client.user.keyring [client.admin] key = AQAGDKJfQk/dAxAA3Y +9xoE/p8in6QjoHeXmeg== caps mds = \"allow *\" caps mgr = \"allow *\" caps mon = \"allow *\" caps osd = \"allow *\" #再导入一个其他用户的keyring: (ceph@ceph-deploy ceph-cluster]$ceph-authtool ./ceph.client.user.keyring --import-kevring ./ceph.client.user1.kevring #再次验证keyring文件是否包含多个用户的认证信息: [ceph@ceph-deploy ceph-cluster]$ ceph- authtool -I ./ceph.client.user.keyring [client.admin] key = AQAGDKJfQk/dAxAA3Y +9xoE/p8in6QjoHeXmeg== caps mds = \"allow *\" caps mgr = \"allow *\" caps mon = \"allow *\" caps osd = \"allow *\" [client.user1] key = AQAUUchfjpMqGRAARV6h0ofdDEneuaRnxuHjoQ== caps mon = \"allow r\" caps osd = \"allow * pool=mypool\"","date":"2023-01-13","objectID":"/posts/ceph/5.-cephx%E8%AE%A4%E8%AF%81%E6%9C%BA%E5%88%B6/:5:3","tags":["分布式存储"],"title":"CephX 认证机制 （五）","uri":"/posts/ceph/5.-cephx%E8%AE%A4%E8%AF%81%E6%9C%BA%E5%88%B6/"},{"categories":["Ceph"],"content":"ceph集群配置、部署与运维 http://docs.ceph.org.cn/rados/ ","date":"2023-01-12","objectID":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/:0:0","tags":["分布式存储"],"title":"Ceph 集群维护 （四）","uri":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/"},{"categories":["Ceph"],"content":"4.1:通过套接字进行单机管理 每个node节点上都有不同数量的OSD数量 启动osd进程会在 /var/run/ceph下生成soke文件 ls /var/run/ceph ceph-osd.0.asok= ceph-osd.1.asok= ceph-osd.2.asok= ceph-osd.3.asok= ceph-osd.4.asok=","date":"2023-01-12","objectID":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/:1:0","tags":["分布式存储"],"title":"Ceph 集群维护 （四）","uri":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/"},{"categories":["Ceph"],"content":"可在node节点或者mon节点通过ceph命令进行单机管理本机的mon或者osd服务 先将admin认证文件同步到mon或者node节点 ceph@ceph-deploy:/home/ceph/ceph-cluster$scp ceph.client.admin.keyring root@172.31.6.101:/etc/ceph #指定要管理的asok文件 [root@ceph-node1 ~]# ceph -- admin-socket /var/run/ceph/ceph-osd.0.asok --help -- admin-daemon 在 mon节点获取daemon服务帮助: #帮助信息: ceph-mon1~]#ceph --admin-daemon /var/run/ceph/ceph-mon.cephjmon1.asok help #mon状态: ceph-mon1~]# ceph --admin-daemon /var/run/ceph/ceph-mon.ceph-mon1.asok mon_ status #查看配置信息: ceph-mon1~]# ceph - admin-daemon /var/run/ceph/ceph-mon.ceph-mon1.asok config show","date":"2023-01-12","objectID":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/:1:1","tags":["分布式存储"],"title":"Ceph 集群维护 （四）","uri":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/"},{"categories":["Ceph"],"content":"4.2 ceph集群的停止或重启 重启之前按照正确的流程，要提前设置ceph集群不要将OSD标记为out,避免node节点关闭服务后被踢出ceph集群外 node节点每隔6s向mon节点汇报一次OSD状态，连续20秒后没有通告正常mon就会把OSD标记为OUT ，就会触发磁盘的高可用开始磁盘的选举和数据同步。 #关闭服务前设置noout [ceph@ceph-deploy ceph-cluster]$ ceph osd set noout noout is set #启动服务后取消noout [ceph@ceph-deploy ceph-cluster]$ ceph osd unset noout noout is unset","date":"2023-01-12","objectID":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/:2:0","tags":["分布式存储"],"title":"Ceph 集群维护 （四）","uri":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/"},{"categories":["Ceph"],"content":"4.2.1 关闭顺序 关闭服务前设置noout 关闭存储客户端停止读写数据 如果使用RGW，关闭RGW 关闭cephfs 元数据服务 关闭ceph OSD 关闭ceph manager 关闭 ceph monitor ","date":"2023-01-12","objectID":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/:2:1","tags":["分布式存储"],"title":"Ceph 集群维护 （四）","uri":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/"},{"categories":["Ceph"],"content":"4.2.2 启动顺序 启动 ceph monitor 启动 ceph manager 启动 ceph OSD 启动 ceph FS 元数据服务 启动RGW 启动存储客户端 启动服务后取消 noout ","date":"2023-01-12","objectID":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/:2:2","tags":["分布式存储"],"title":"Ceph 集群维护 （四）","uri":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/"},{"categories":["Ceph"],"content":"4.2.3 服务时间偏差 http://docs.ceph.org.cn/rados/configuration/mon-config-ref/ 重启发现： cluster: id:5ac860ab- 9a4e- 4edd- 9da2 e3de293a8d44 health: HEALTH WARN clock skew detected on mon. ceph-mon2, mon. ceph-mon3 noout flag(s) set 通常由于服务器重启后导致时间不太一致，因为服务器有时间同步计划任务同步周期还没到 可以设置监视器运行的时钟漂移量，默认为0.050秒即50毫秒 cat /ceph.conf #设置监视器运行的时钟漂移量 mon clock drift allowed =3 #时钟偏移警告的退避指數即连续多少次时间偏差后就出发警告 mon clock drift warn backoff= 10 #同步配置文件mon服务器 [ceph@ceph-deploy ceph-cluster]$ ceph-deploy --overwrite-conf config push stor01..3) #重启mon #拷贝方式 #ceph@ceph-deploy:~/ceph-cluster$ scp ceph.conf root@172.31.6.101: /etc/ceph/ #ceph@ceph-deploy:~/ceph-cluster$ scp ceph.conf root@172.31.6.102: /etc/ceph/ #ceph@ceph-deploy:~/ceph-cluster$ scp ceph.conf root@172.31.6.103: /etc/ceph/ [root@ceph-mon1 ~]# ntpdate timel.aliyun.com \u0026\u0026 hwclock -W root@ceph-mon1:~# systemctl restart ceph-mon@ceph-mon1.service","date":"2023-01-12","objectID":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/:2:3","tags":["分布式存储"],"title":"Ceph 集群维护 （四）","uri":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/"},{"categories":["Ceph"],"content":"4.3 ceph 配置文件 Ceph的主配置文件是/etc/ceph/ceph.conf ，ceph 服务在启动时会检查ceph.conf分号;和#在配置文件中都是注释，ceph.conf 主要由以下配置段组成: :::info [global] #全局配置[osd] #osd专用配置，可以使用osd.N, 来表示某一个OSD专用配置，N为osd的编号，如0、2、1等， [mon] #mon专用配置，也可以使用mon.A来为某一个monitor节点做专用配置，其中A为该节点的名称，ceph-monitor-2、 ceph-monitor-1 等，使用命令ceph mon dump可以获取节点的名称、 [client] #客户端专用配置. ::: ceph 文件的加載順序 $CEPH_CONF 环境变量 -c 指定配置文件位置 /etc/ceph/ceph.conf ~/.ceph/ceph.conf ./ceph.conf","date":"2023-01-12","objectID":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/:3:0","tags":["分布式存储"],"title":"Ceph 集群维护 （四）","uri":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/"},{"categories":["Ceph"],"content":"4.4 存储池、PG与CRUSH :::info 副本池:repicated,定义每个对象在集群中保存为多少个副本，默认为三个副本, 一主两备,实现高可用，副本池是ceph默认的存储池类型. ::: 在创建存储池的时候可以指定默认是三副本osd pool create pool --help [replicated] :::info 纠删码池(erasure code): ceph另一种数据可用性机制一定程度上实现数据高可用（使用的不多），存储机制类似于raid5 把一部分存储空间用于存放校验码实现数据恢复的目的，既可以提高磁盘空间利用率，又能实现一定程度上的数据高可用。和raid机制一样不能坏一定数量的磁盘所以高可用机制有限。 ::: 但是不是所有应用都支持纠删码池，RDB块存储只支持副本池而radosgw 可以支持纠删码池 一部分存数据、一部分存校验码 把各对象存储为N=K+M个块，其中K为数据块数量，M为编码快数量，因此存储池的尺寸为K+M. 即数据保存在K个数据块,并提供M个冗余块提供数据高可用，那么最多能故障的块就是M个,实际的磁盘占用就是K+M块，因此相比副本池机制比较节省存储资源。 一般采用8+4机制，即8个数据块+4个冗余块，那么也就是12个数据块有8个数据块保存数据,有4个实现数据冗余，即1/3的磁盘空间用于数据冗余，比默认副本池的三倍冗余节省空间,但是不能出现大于一定数据块故障。 但是不是所有的应用都支持纠删码池，RBD只支持副本池而Tjadosgw则可以支持纠删码池。 创建纠删码池 ceph osd pool create erasure-testpool 32 32 erasure写入数据 sudo rados put -p erasure-testpool testfile1 /var/log/syslog验证数据 ceph osd map erasure-testpool testfile1验证当前pg状态 ceph pg ls-by-pool erasure-testpool | awk '{print $1,$2,$15}'","date":"2023-01-12","objectID":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/:4:0","tags":["分布式存储"],"title":"Ceph 集群维护 （四）","uri":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/"},{"categories":["Ceph"],"content":"4.4.1 副本池 将一个数据对象存储为多个副本 在客户端写入操作时，ceph使用CRUSH算法计算出与对象相对应的PG ID和primary OSD 主OSD根据设置的副本数、对象名称、存储池名称和**集群运行图(cluster map)**计算出PG 的各辅助OSD，然后由OSD将数据再同步给辅助OSD. 读取数据: 1.客户端发送读请求，RADOS 将请求发送到主OSD. 2.主OSD从本地磁盘读取数据并返回数据，最终完成读请求。 写入数据: 客户端APP请求写入数据，RADOS发送数据到主OSD. 主OSD识别副本OSDs,并发送数据到各副本OSD. 副本OSDs写入数据，并发送写入完成信号给主OSD. 主OSD发送写人完成信号给客户端APP. ","date":"2023-01-12","objectID":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/:4:1","tags":["分布式存储"],"title":"Ceph 集群维护 （四）","uri":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/"},{"categories":["Ceph"],"content":"4.4.2 纠删码池 纠删码池降低了数据保存所需要的磁盘总空间数量，但是读写数据的计算成本要比副本池高RGW可以支持纠删码池，RBD 不支持纠删码池可以降低企业的前期TCO总拥有成本。 纠删码写:数据将在主OSD进行编码然后分发到相应的OSDs.上去。1.计算合适的数据块并进行编码2.对每个数据块进行编码并写入OSD 纠删码读:从相应的OSDs中获取数据后进行解码，如果此时有数据丢失，Ceph 会自动从存放校验码的OSD中读取数据进行解码。 ","date":"2023-01-12","objectID":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/:4:2","tags":["分布式存储"],"title":"Ceph 集群维护 （四）","uri":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/"},{"categories":["Ceph"],"content":"4.5 PG与PGP :::info PG = Placement Group 归置组PGP = Placement Group for Placement purpose 归置组的组合， pgp 相当于是pg对应osd的一种排列组合关系。 ::: **归置组(placement group)**是用于跨越多OSD将数据存储在每个存储池中的内部数据结构.归置组在OSD守护进程和ceph客户端之间生成了一个中间层，CRUSH 算法负责将每个对象动态映射到一个归置组，然后再将每个归置组动态映射到一个或多个OSD守护进程,从而能够支持在新的OSD设备上线时进行数据重新平衡。 相对于存储池来说，PG是一个虚拟组件，它是对象映射到存储池时使用的虚拟层。根据业务的数据量分配PG 一般 几百个G16和32就可以，TB级 64 到128。2的次方 想对于存储池来说，PG 是一个虚拟组件，它是对象映射到存储池时使用的虚拟层。可以自定义存储池中的归置组数量。 ceph出于规模伸缩及性能方面的考虑，ceph 将存储池细分为多个归置组，把每个单独的对象映射到归置组，并为归置组分配一个主OSD. 存储池由一系列的归置组组成，而CRUSH算法则根据集群运行图和集群状态，将个PG均匀、伪随机(基于hash映射，每次的计算结果够 样)的分布到集群中的OSD之上。如果某个OSD失败或需要对集群进行重新平衡，ceph 则移动或复制整个归置组而不需要单独对每个镜像进行寻址。 ","date":"2023-01-12","objectID":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/:5:0","tags":["分布式存储"],"title":"Ceph 集群维护 （四）","uri":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/"},{"categories":["Ceph"],"content":"4.6 PG与 OSD的关系 ceph基于crush算法将归置组PG分配至OSD当一个客户端存储对象的时候，CRUSH 算法映射每一个对象至归置组(PG) ","date":"2023-01-12","objectID":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/:6:0","tags":["分布式存储"],"title":"Ceph 集群维护 （四）","uri":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/"},{"categories":["Ceph"],"content":"4.7 PG分配计算 归置组(PG)的数量是由管理员在创建存储池的时候指定的，然后由CRUSH负责创建和使用，PG的数量是2的N次方的倍数,每个OSD的PG不要超出250个PG，官方是每个OSD100个左右 一个磁盘可能属于多个PG分别担任不同的角色，https://docs.ceph.com/en/mimic/rados/configuration/pool-pg-config-ref/ recommend approximately确保设置了合适的归置组大小，我们建议每个OSD大约100个，例如，osd 总数乘以100除以副本数量(即 osd池默认大小)，因此，对于10个osd、存储池为4个，我们建议每个存储池大约(100 * 10) /4= 250 先算磁盘数量是多少块，官方推荐每个OSD是100个PG左右，10块就是1000个PG PG的数量在集群分发数据和重新平衡时扮演者重要的角色 PG的数量过少，PG的数量在ceph同步数据时有短暂影响，一个OSD上保存的数据数据会相对加多，那么ceph同步数据的时候产生的网络负载将对集群的性能输出产生一定影响。 PG数量太少 数据量又大，那么必然同步是时间就长 PG过多的时候，ceph将会占用过多的CPU和内存资源用于记录PG的状态信息 至于一个pool应该使用多少个PG，可以通过下面的公式计算后，将pool的PG值四舍五人到最近的2的N次幂，如下先计算出ceph集群的总PG数: 磁盘总数x每个磁盘PG数/副本数=\u003e ceph集群总PG数(略大于2^n次方)单个pool的PG计算如下: :::info 有100个osd,3副本，5个poolTotal PGS =100*100/3-3333每个pool的PG=3333/5=512.那么创建pool的时候就指定pg为512 ::: 需要结合数据数量、磁盘数量及磁盘空间计算出PG数量，8、16、 32、64、128、 256等2的N次方。一个RADOS集群上会存在多个存储池，因此管理员还需要考虑所有存储池上的PG分布后每个OSD需要映射的PG数量. ","date":"2023-01-12","objectID":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/:7:0","tags":["分布式存储"],"title":"Ceph 集群维护 （四）","uri":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/"},{"categories":["Ceph"],"content":"4.8 PG的状态 ","date":"2023-01-12","objectID":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/:8:0","tags":["分布式存储"],"title":"Ceph 集群维护 （四）","uri":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/"},{"categories":["Ceph"],"content":"4.8.1:Peering 正在同步状态，同一个PG中的OSD需要将准备数据同步一致, 而Peering(对等)就是OSD同步过程中的状态。 ","date":"2023-01-12","objectID":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/:8:1","tags":["分布式存储"],"title":"Ceph 集群维护 （四）","uri":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/"},{"categories":["Ceph"],"content":"4.8.2:Activating Peering已经完成，PG 正在等待所有PG实例同步Peering的结果(info、Log等) ","date":"2023-01-12","objectID":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/:8:2","tags":["分布式存储"],"title":"Ceph 集群维护 （四）","uri":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/"},{"categories":["Ceph"],"content":"4.8.3 Clean 磁盘没有宕机 干净态,PG当前不存在待修复的对象，并且大小等于存储池的副本数，即PG的活动集(Acting Set)和上行集(Up Set)为同一组OSD且内容一致。 活动集(Acting Set):由PG当前主的OSD和其余处于活动状态的备用OSD组成，当前PG内的OSD负责处理用户的读写请求。 上行集(Up Set):在某一个OSD故障时，需要将故障的OSD更换为可用的OSD,并主PG内部的主OSD同步数据到新的OSD上，例如PG内有OSD1、OSD2、OSD3，当OSD3故障后需要用OSD4替换OSD3,那么OSD1. OSD2、OSD3就是上行集，替换后OSD1、OSD2、OSD4就是活动集，OSD 替换完成后活动集最终要替换上行集。 ","date":"2023-01-12","objectID":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/:8:3","tags":["分布式存储"],"title":"Ceph 集群维护 （四）","uri":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/"},{"categories":["Ceph"],"content":"4.8.4 Active 正常就绪状态或活跃状态，Active 表示主OSD和备OSD处于正常工作状态，此时的PG可以正常处理来自客户端的读写请求，正常的PG默认就是Active+Clean状态。 ","date":"2023-01-12","objectID":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/:8:4","tags":["分布式存储"],"title":"Ceph 集群维护 （四）","uri":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/"},{"categories":["Ceph"],"content":"4.8.5 Degraded 降级状态 一般出现在磁盘宕机后，并且一段时间没有恢复降级状态出现于OSD被标记为down以后，那么其他映射到此OSD的PG都会转换到降级状态。如果此OSD还能重新启动完成并完成Peering操作后,那么使用此OSD的PG将重新恢复为clean状态。如果此OSD被标记为down的时间超过5分钟还没有修复，那么此OSD将会被ceph踢出集群，然后ceph会对被降级的PG启动恢复操作，直到所有由于此OSD而被降级的PG重新恢复为clean状态。恢复数据会从PG内的主OSD恢复，如果是主OSD故障，那么会在剩下的两个备用OSD重新选择一个作为主OSD. ","date":"2023-01-12","objectID":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/:8:5","tags":["分布式存储"],"title":"Ceph 集群维护 （四）","uri":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/"},{"categories":["Ceph"],"content":"4.8.6 Stale:过期状态 发生在OSD主宕了，数据不是最新正常状态下，每个主OSD都要周期性的向RADOS集群中的监视器(Mon)报告其作为主OSD所持有的所有PG的最新统计数据，因任何原因导致某个OSD无法正常向监视器发送汇报信息的、或者由其他OSD报告某个OSD已经down的时候，则所有以此OSD为主PG则会立即被标记为stale 状态，即他们的主OSD已经不是最新的数据了，如果是备份的OSD发送down的时候，则ceph会执行修复而不会触发PG状态转换为stale状态不会切换主。 ","date":"2023-01-12","objectID":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/:8:6","tags":["分布式存储"],"title":"Ceph 集群维护 （四）","uri":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/"},{"categories":["Ceph"],"content":"4.8.7 undersized 一主两副本，备宕了 出现副本数太低了PG当前副本数小于其存储池定义的值的时候，PG会转换为undersixed状态，比如两个备份OSD都down了，那么此时PG中就只有一个主OSD了，不符合ceph最少要求一个主OSD加一个备OSD的要求，那么就会导致使用此OSD的PG转换为undersized状态，直到添加备份OSD添加完成，或者修复完成。 ","date":"2023-01-12","objectID":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/:8:7","tags":["分布式存储"],"title":"Ceph 集群维护 （四）","uri":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/"},{"categories":["Ceph"],"content":"4.8.8 Scrubbing 每天进行数据的浅清理（整理元数据），每周进行数据的深清理（整理元数据和数据本身）scrub是ceph对数据的清洗状态，用来保证数据完整性的机制, Ceph的OSD定期启动scrub线程来扫描部分对象，通过与其他副本比对来发现是否一致， 如果存在不一致,抛出异常提示用户手动解决。scrub 以PG为单位，对于每一个pg, ceph 分析该pg下所有的object,产生一个类似于元数据信息摘要的数据结构,如对象大小，属性等,叫scrubmap,比较主与副scrubmap,来保证是不是有object丢失或者不匹配，扫描分为轻量级扫描和深度扫描，轻量级扫描也叫做light scrubs或者shallow scrubs或者simply scrubs即轻量级扫描.Light scrub(daily)比较object size和属性，deep scrub (weekly)读取数据部分并通过checksum(CRC32算法)对比和数据的一致性,深度扫描过程中的PG会处scrubbing +deep状态. ","date":"2023-01-12","objectID":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/:8:8","tags":["分布式存储"],"title":"Ceph 集群维护 （四）","uri":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/"},{"categories":["Ceph"],"content":"4.8.9:Recovering: 正在恢复态，集群正在执行迁移或同步对象和他们的副本，这可能是由于添加了一个新的OSD到集群中或者某个OSD宕掉后，PG可能会被CRUSH算法重新分配不同的OSD,而由于OSD更换导致PG发生内部数据同步的过程中的PG会被标记为Recovering. ","date":"2023-01-12","objectID":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/:8:9","tags":["分布式存储"],"title":"Ceph 集群维护 （四）","uri":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/"},{"categories":["Ceph"],"content":"4.8.10 Backfilling 正在后台填充态,backfill是recovery的一种特殊场景, 指peering完成后，如果基于当前权威日志无法对Up Set (. 上行集)当中的某些PG实例实施增量同步(例如承载这些PG实例的OSD离线太久,或者是新的OSD加入集群导致的PG实例整体迁移)则通过完全拷贝当前Primary所有对象的方式进行全量同步，此过程中的PG会处于backilling. ","date":"2023-01-12","objectID":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/:8:10","tags":["分布式存储"],"title":"Ceph 集群维护 （四）","uri":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/"},{"categories":["Ceph"],"content":"4.8.11 Backfill-toofull 某个需要被Backfill的PG实例，其所在的OSD可用空间不足，Backfill 流程当前被挂起时PG给的状态。 ","date":"2023-01-12","objectID":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/:8:11","tags":["分布式存储"],"title":"Ceph 集群维护 （四）","uri":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/"},{"categories":["Ceph"],"content":"4.9 ceph存储池操作 存储池的管理通常保存创建、列出、重命名和删除等操作，管理工具使用ceph osd pool的子命令及参数，比如create/ls/rename/rm等。ceph官方运维手册http://docs.ceph.org.cn/rados/ ","date":"2023-01-12","objectID":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/:9:0","tags":["分布式存储"],"title":"Ceph 集群维护 （四）","uri":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/"},{"categories":["Ceph"],"content":"4.9.1 常用命令 创建存储池命令格式 $ceph osd pool create \u003cpoolname\u003e pg. num pgp_ num {replicatedlerasure} #列出存储池: [ceph@ceph-deploy ceph-cluster]$ ceph osd poolls [detail] #不带 pool ID mypool myrdb1 .rgw.root default.rgw.control default.rgw.meta default.rgw.log cephfs-metadata cephfs-data #带pool ID ceph osd poolls #查看详细 ceph osd pool ls detail #查看存储池的事件信息 ceph osd pool stats mypool #重命名存储池 ceph osd pool rename old-name new-name ceph osd pool rename myrbd1 myrbd2 #显示存储池用量 ceph df rados df ","date":"2023-01-12","objectID":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/:9:1","tags":["分布式存储"],"title":"Ceph 集群维护 （四）","uri":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/"},{"categories":["Ceph"],"content":"4.9.2 删除存储池 ceph为了防止误删除存储池设置了两个机制来防止误删除操作。 第一个机制是NODELETE 标志,需要设置为false 但是默认就是FALSE。 #创建一个测试pool ceph osd pool create mypool2 32 32 ceph osd pool get mypool2 nodelete nodelete：false #如果设置了为true就表示不能删除，可以使用set指令重新设置为false ceph osd pool set mypool2 nodelete true set pool 9 nodelete to true第二个机制是集群范围的配置参数mon allow pool delete,默认值为false,即监视器不允许删除存储池，可以在特定场合使用tell指令临时设置为(true)允许删除,在删除指定的pool之后再重新设置为false. $ ceph tell mon.* injectargs --mon-allow-pool-delete=true mon.ceph-mon1:injectargs:mon_allow_pool delete = 'true' mon.ceph-mon2:injectargs:mon_allow_pool delete = 'true' mon.ceph-mon3:injectargs:mon_allow_pool delete = 'true' $ ceph osd pool rm mypool2 mypool2 --yes-i-really-really-mean-it pool 'mypool2' removed","date":"2023-01-12","objectID":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/:9:2","tags":["分布式存储"],"title":"Ceph 集群维护 （四）","uri":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/"},{"categories":["Ceph"],"content":"4.9.3 存储池配额 存储池可以设置两个配对存储的对象进行限制，一个配额是最大空间(max_ bytes), 另外一个配额是对象最大数量(max_ objects)。 #查看存储池限制 $ ceph osd pool get-quota mypool quotas for pool 'mypool': max objects: N/A #默认不限制对象数量 max bytes : N/A #默认不限制空间大小 ---- $ ceph osd pool set-quota mypool max_objects 1000 #限制最大1000个对象 set-quota max_objects = 1000 for pool mypool [ceph@ceph-deploy ceph-cluster]$ ceph osd pool set-quota mypool max_bytes 10737418240 #限制最大10737418240字节 set-quota max_bytes = 10737418240 for pool mypool","date":"2023-01-12","objectID":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/:9:3","tags":["分布式存储"],"title":"Ceph 集群维护 （四）","uri":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/"},{"categories":["Ceph"],"content":"4.9.4 存储池可用参数 size: 存储池中的对象副本数，默认一主两个备3副本 min_size: 提供服务所需要的最小副本数，如果定义size为3, min_size 也为3,坏掉一个OSD,如果pool池中有副本在此块OSD上面，那么此pool将不提供服务，如果将min_size定义为2，那么还可以提供服务，如果提供为1.表示只要有一块副本都提供服务。 pg_num: 查看当前PG的数量crush_rule: 设置crush算法规则crush_ rule: 默认为副本池nodelete:控制是否可删除，默认可以nopgchange: 控制是否可更改存储池的pg num和pgp numnosizechange: 控制是否可以更改存储池的大小noscrub和nodeep-scrub:控制是否不进行轻量扫描或是否深层扫描存储池，可临时解决高l/0问题scrub_min_interval: 集群存储池的最小清理时间间隔，默认值没有设置，可以通过配置文件中的osd_scrub_min_interval 参数指定间隔时间. scrub_max_interval: 整理存储池的最大清理时间间隔，默认值没有设置，可以通过配置文件中的osd_scrub_max_interval 参数指定。 deep_scrub_interval: 深层整理存储池的时间间隔，默认值没有设置，可以通过配置文件中的osd_deep_scrub_interval 参数指定。 #查看副本数 ceph osd pool get mypool size size:3 #修改副本数为2 ceph osd pool get mypool size 2 #为2就是允许挂一个OSD ceph osd pool mypool min_size min_size:2 #pg_num:查看当前PG的数量 $ ceph osd pool get mypool pg_num pg num: 32 #crush_rule: 设置crush算法规则 $ ceph osd pool get mypool crush_rule crush_ rule: replicated_rule #默认为副本池 #nodelete:控制是否可删除，默认可以 $ ceph osd pool get mypool nodelete nodelete: false #nopgchange:控制是否可更改存储池的pg num和pgp num S cenh osd pool get mypool nopgchange #修改指定pool的pg数量 $ ceph osd pool set mypool pg_num 64 set pool1 pg_num to 64 ##修改指定pool的pgp数量 $ ceph osd pool set mypool pgp_num 64 #nosizechange:控制是否可以更改存储池的大小 $ ceph osd pool get mypool nosizechange nosizechange: false #默认允许修改存储池大小 $ ceph osd pool get-quota mypool quotas for pool 'mypool': max objects: 1 k objects max bytes : 10 GiB #限制存储池最大写入大小 ceph osd pool set-quota mypool max bytes 21474836480 #noscrub和nodeep-scrub:控制是否不进行轻量扫描或是否深层扫描存储池，可临时解决高l/0问题 #查看 当前是否关闭轻量扫描数据，默认为不关闭，即开启 $ ceph osd pool get mypool noscrub noscrub: false #可以修改某个指定的pool的轻量级扫描测量为true,即不执行轻量级扫描 $ ceph osd pool set mypool noscrub true set pool 1 noscrub to true #再次 查看就不进行轻量级扫描了 $ ceph osd pool get mypool noscrub noscrub: true #查看当前是否关闭深度扫描数据，默认为不关闭，即开启 $ ceph osd pool get mypool nodeep-scrub nodeep-scrub: false #再次查看就不执行深度扫描了 $ ceph osd pool get mypool nodeep-scrub nodeep-scrub: true #scrub_ min_ interval: 集群存储池的最小清理时间间隔，默认值没有设置，可以通过配置文件中的osd_scrub_min_interval 参数指定间隔时间. $ ceph osd pool get mypool scrub min interval Error ENOENT: option 'scrub_min_interval' is not set on pool 'mypool' #scrub_max_interval: 整理存储池的最大清理时间间隔，默认值没有设置，可以通过配置文件中的osd_scrub_max_interval 参数指定。 $ ceph osd pool get mypool scrub max interval Error ENOENT: option 'scrub_max_interval' is not set on pool 'mypool' #deep_scrub_interval: 深层整理存储池的时间间隔，默认值没有设置，可以通过配置文件中的osd_deep_scrub_interval 参数指定。 $ ceph osd pool get mypool deep_scrub_interval Error ENOENT: option 'deep_scrub_interval' is not set on pool 'mypool' #查看ceph node的默认配置: [root@ceph-node1 ~]# ll /var/run/ceph/ total 0 Srxr-Xr-x 1 ceph ceph 0 Nov 3 12:22 ceph-osd.3.asok SrwXr-Xr-x 1 ceph ceph 0 Nov 3 12:22 ceph-osd.6.asok SrWXr-Xr-x 1 ceph ceph 0 Nov 3 12:23 ceph-osd.9.asok [root@ceph-node1 ~]# ceph daemon osd.3 config show | grep scrub \"mds_max_scrub_ops_in_progress\": \"5\", \"mon_scrub_inject_crc_mismatch\": \"0.000000\", \"mon_scrub_inject_missing_keys\": \"0.000000\", \"mon_scrub_jinterval\": \"86400\"， \"mon_scrub_max_keys\": \"100\"， \"mon_scrub_timeout\": \"300\",， \"mon_warn_not_deep_scrubbed\": \"0\", \"mon_warn_not_scrubbed\": \"0\", \"osd_debug_deep_scrub_sleep\": \"0.000000\", \"osd_deep_scrub_jinterval\":_\"604800.00000\"，#定义深度清洗间隔，604800秒=7天 \"osd_deep_scrub_keys\": \"1024\"， \"osd_deep_scrub_Jarge_omap_object_key_threshold\": \"200000\", \"osd_deep_scrub_large_omap_object_value_sum_threshold\": \"1073741824\", \"osd_deep_scrub_randomize.ratio\": \"0.150000\", \"osd_deep_scrub.stride\": \"524288\", \"osd.deep_scrub.update_digest_min_age\": \"7200\"， \"osd.max_scrubs\": \"1\"， #定义一个ceph OSD daemon内能够同时进行scrubbing的操作数 （启用几个线程扫描 默认是一个） \"osd_op_queue_mclock_scrub_lim\": \"0.001000\", \"osd_op_queue_mclock_scrub_res\": \"0.000000\", \"osd_op_queue_mclock_scrub_wgt\": \"1.000000, \"osd_requested_scrub_priority\": \"120\"， \"osd_scrub_auto_repair\": \"false\", \"osd_scrub_auto_repair_num_errors\": \"5\"， \"osd_scrub_backoff_ratio\": \"0.660000\", \"osd_scrub_begin_hour\": \"0\"， \"osd_scrub_begin_week_day\": \"0\", \"osd_scrub_chunk_max\": \"25\", \"osd_scrub_chunk_","date":"2023-01-12","objectID":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/:9:4","tags":["分布式存储"],"title":"Ceph 集群维护 （四）","uri":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/"},{"categories":["Ceph"],"content":"4.10 存储池快照 快照用于读存储池中的数据进行备份与还原，创建快照需要占用的磁盘空间会比较大,取决于存储池中的数据大小，使用以下命令创建快照: ","date":"2023-01-12","objectID":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/:10:0","tags":["分布式存储"],"title":"Ceph 集群维护 （四）","uri":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/"},{"categories":["Ceph"],"content":"4.10.1 创建快照 $ ceph osd pool ls #命令1: ceph osd pool mksnap {pool-name} {snap-name} $ ceph osd pool mksnap mypool mypool-snap created pool mypool snap mypool-snap #命令2: rados -P {pool-name} mksnap {snap-name} $ rados -P mypool mksnap mypool-snap2 created pool mypool snap mypool-snap2","date":"2023-01-12","objectID":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/:10:1","tags":["分布式存储"],"title":"Ceph 集群维护 （四）","uri":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/"},{"categories":["Ceph"],"content":"4.10.2 验证快照 $ rados lssnap -p mypool 1 mypool-snap 2020.11.03 16:12:56 2 mypool-snap2 2020.11.03 16:13:40 2 snaps","date":"2023-01-12","objectID":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/:10:2","tags":["分布式存储"],"title":"Ceph 集群维护 （四）","uri":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/"},{"categories":["Ceph"],"content":"4.10.3 回滚快照 测试上传文件后创建快照，照后删除文件再还原文件,基于对象还原。rados rollback \u003cobj-name\u003e \u003csnap-name\u003e roll back object to snap \u003csnap-name\u003e #上传文件 [ceph@ceph-deploy ceph-cluster]$ rados -P mypool put testile /etc/hosts #验证文件 [ceph@ceph-deploy ceph-cluster]$ rados -P mypool ls msg1 testfile my.conf #创建快照 (ceph@ceph-deploy ceph-cluster]$ ceph pool mksnap mypool mypool-snapshot001 created pool mypool snap mypool-snapshot001 #验证快照 [ceph@ceph-deploy ceph-cluster]$ rados lssnap -p mypool 3 mypool-snap 2020.11.04 14:11:41 4 mypool-snap2 2020.11.0414:1 1:49 5 mypool-conf-bak 2020.11.04 14:18:41 6 mypool-snapshot001 2020.11.0414:38:50 4 snaps #删除文件 [ceph@ceph-deploy ceph-cluster]$ rados -P mypool rm testile #删除文件后，无法再次删除文件，提升文件不存在 [ceph@ceph-deploy ceph-cluster$ rados -P mypool rm testfile error removing mypool\u003etestfile: (2) No such file or directory #通过快照还原某个文件 [ceph@ceph-deploy ceph-cluster]$ rados rollback -P mypool testfile mypool-snapshot001 rolled back pool mypool to snapshot mypool-snapshot001 #再次执行删除就可以执行成功","date":"2023-01-12","objectID":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/:10:3","tags":["分布式存储"],"title":"Ceph 集群维护 （四）","uri":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/"},{"categories":["Ceph"],"content":"4.10.4 删除快照 ceph osd pool rmsnap \u003cpoolname\u003e \u003csnap\u003e [ceph@ceph-deploy ceph-cluster$ rados Issnap -P mypool 3 mypool-snap 2020.11.0414:11:41 4 mypool-snap2 2020.11.04 14:11:49 5 mypool-conf-bak 2020.11.04 14:18:41 6 mypool-snapshot001 2020.1 1.0414:38:50 4 snaps [ceph@ceph-deploy ceph-cluster]$ ceph osd pool rmsnap mypool mypool-snap removed pool mypool snap mypool-snap [ceph@ceph-deploy ceph-cluster$ rados Issnap -P mypool 4 mypool-snap2 2020.11.04 14:11:49 5 mypool-conf-bak 2020.11.04 14:18:41 6 mvoool-snanshot001 2020.11.04 14:38:50","date":"2023-01-12","objectID":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/:10:4","tags":["分布式存储"],"title":"Ceph 集群维护 （四）","uri":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/"},{"categories":["Ceph"],"content":"4.11 数据压缩 如果使用bulestore存储引擎，ceph 支持称为\"实时数据压缩”即边压缩边保存数据的功能，该功能有助于节省磁盘空间，可以在BlueStore OSD 上创建的每个Ceph池上启用或禁用压缩，以节约磁盘空间，默认没有开启压缩，需要后期配置并开启。 ","date":"2023-01-12","objectID":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/:11:0","tags":["分布式存储"],"title":"Ceph 集群维护 （四）","uri":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/"},{"categories":["Ceph"],"content":"4.11.1 启用压缩并指定压缩算法 压缩会导致CPU利用率偏高 ceph-cluster]$ ceph osd pool set \u003cpool name\u003e compression_algorithm snappy #默认算 法为snappy:::info snappy:该配置为指定压缩使用的算法默认为sanppy,还有none、zlib、 lz4、 zstd 和snappy等算法，zstd压缩比好，但消耗CPU, lz4 和snappy对CPU占用较低，不建议使用zlib. ::: ","date":"2023-01-12","objectID":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/:11:1","tags":["分布式存储"],"title":"Ceph 集群维护 （四）","uri":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/"},{"categories":["Ceph"],"content":"4.11.2 指定压缩模式 ceph-cluster]$ ceph osd pool set \u003cpool name\u003e compression_mode aggressiveaggressive: 压缩的模式，有none、aggressive 、passive 和force 默认none none: 从不压缩数据.passive: 除非写操作具有可压缩的提示集，否则不要压缩数据.aggressive: 压缩数据，除非写操作具有不可压缩的提示集。force: 无论如何都尝试压缩数据，即使客户端暗示数据不可压缩也会压缩，也就是在所有情况下都使用压缩。 存储池压缩设置参数:compression_algorithm 压缩算法compression_mode 压缩模式 compression_required_ratio #压缩后与压缩前的压缩比，默认为.875compression_max_blob_size: #大于此的块在被压缩之前被分解成更小的blob(块)，此设置将覆盖bluestore压缩max blob 的全局设置。compression_min_blob_size: #小于此的块不压缩，此设置将覆盖bluestore压缩min blob的全局设置， 全局压缩选项，这些可以配置到ceph.conf配置文件，作用于所有存储池: bluestore_compression_algorithm #压缩算法 bluestore_compression_mode #压缩模式 bluestore_compression_required_ratio #压缩后与压缩前的压缩比，默认为.875 bluestore_compression_min_blob_size #小于它的块不会被压缩,默认0 bluestore_compression_max_blob_size #大于它的块在压缩前会被拆成更小的块,默认0 bluestore_compression_min_blob_size_ssd #默认 8k bluestore_compression_max_blob_size_ssd #默认 64k bluestore_compression_min_blob_size_hdd #默认 128k bluestore_compression_max_blob_size_hdd #默认 512k 到node 节点验证 [root@ceph-node3 ~]# ceph daemon osd.11 config show | grep compression #修改压缩算法 [ceph@ceph-deploy ~]$ ceph osd pool set mypool compression algorithm snapy set pool 2 compression algorithm to snappy [ceph@ceph-deploy ~]$ ceph osd pool get mypool compression algorithm compression_algorithm:snappy #修改压缩模式: [ceph@ceph-deploy ~]$ ceph osd pool set mypool compression mode passive set pool 2 compression mode to passive [ceph@ceph-deploy ~]$ ceph osd pool get mypool compression_mode compression_mode: passive","date":"2023-01-12","objectID":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/:11:2","tags":["分布式存储"],"title":"Ceph 集群维护 （四）","uri":"/posts/ceph/4.ceph%E9%9B%86%E7%BE%A4%E7%BB%B4%E6%8A%A4/"},{"categories":["Kubernetes"],"content":"常用命令 ","date":"2023-01-12","objectID":"/posts/kubernetes/primary/kubernetes-4/:1:0","tags":["k8s进阶训练营"],"title":"kubernetesAPI资源对象 (四)","uri":"/posts/kubernetes/primary/kubernetes-4/"},{"categories":["Kubernetes"],"content":"K8S的几个重要概念 1.用什么和k8s打交道？ 通过k8s声明式API 调用K8S资源对象。2.怎么打交道？ 通过写yaml文件调用声明式API。3.怎么声明？ yaml中必需的字段： apiVersion - 创建该对象所使用的Kubernetes API的版本 kind- 想要创建的对象的类型 metadata- 帮助识别对象唯一性的数据， 包括一个name名称、可选的namespace spec 期望状态 status (Pod创建完成后k8s自动生成status状态) spec和status的区别:spec是期望状态status是实际状态 ","date":"2023-01-12","objectID":"/posts/kubernetes/primary/kubernetes-4/:2:0","tags":["k8s进阶训练营"],"title":"kubernetesAPI资源对象 (四)","uri":"/posts/kubernetes/primary/kubernetes-4/"},{"categories":["Kubernetes"],"content":"Pod 概述 pod是k8s中的最小单元。 一个pod中可以运行一个容器， 也可以运行多个容器。 运行多个容器的话，这些容器是一起被调度的。 Pod的生命周期是短暂的， 不会自愈， 是用完就销毁的实体。 一般我们是通过Controller来创建和管理pod的。 Pod生命周期 初始化容器、启动前操作、就绪探针、存活探针、删除pod操作 livenessProbe和readinessProbe 探针 livenessProbe：存活探针，检测应用发生故障时使用，不能提供服务、超时等检测失败重启pod readinessProbe：就绪探针，检测pod启动之后应用是否就绪，是否可以提供服务，检测成功，pod才开始接收流量。 Controller 控制器 Replication Controller 第一代pod副本控制器**ReplicaSet ** 第二代pod副本控制器Deployment 第三代pod控制器 Rc,Rs 和Deployment 区别： Replication Controller：副本控制器（selector = !=）https://kubernetes.io/zh-cn/docs/concepts/workloads/controllers/replicationcontroller/• https://kubernetes.io/zh/docs/concepts/overview/working-with-objects/labels/ ReplicaSet：副本控制集，和副本控制器的区别是：对选择器的支持（selector 还支持in notin）• https://kubernetes.io/zh/docs/concepts/workloads/controllers/replicaset/ Deployment：比rs更高一级的控制器，除了有rs的功能之外，还有很多高级功能,，比如说最重要的：滚动升级、回滚等• https://kubernetes.io/zh/docs/concepts/workloads/controllers/deployment/ 以下是 Deployments 的典型用例： :::tips 创建 Deployment 以将 ReplicaSet 上线。ReplicaSet 在后台创建 Pod。 检查 ReplicaSet 的上线状态，查看其是否成功。 通过更新 Deployment 的 PodTemplateSpec，声明 Pod 的新状态 。 新的 ReplicaSet 会被创建，Deployment 以受控速率将 Pod 从旧 ReplicaSet 迁移到新 ReplicaSet。 每个新的 ReplicaSet 都会更新 Deployment 的修订版本。 如果 Deployment 的当前状态不稳定，回滚到较早的 Deployment 版本。 每次回滚都会更新 Deployment 的修订版本。 扩大 Deployment 规模以承担更多负载。 暂停 Deployment 的上线 以应用对 PodTemplateSpec 所作的多项修改， 然后恢复其执行以启动新的上线版本。 使用 Deployment 状态来判定上线过程是否出现停滞。 清理较旧的不再需要的 ReplicaSet 。 ::: ","date":"2023-01-12","objectID":"/posts/kubernetes/primary/kubernetes-4/:2:1","tags":["k8s进阶训练营"],"title":"kubernetesAPI资源对象 (四)","uri":"/posts/kubernetes/primary/kubernetes-4/"},{"categories":["Kubernetes"],"content":"Service pod重启后ip地址就变了，如何保证pod间访问不受影响？ 通过声明一个service对象，服务和应用进行解耦。 一般常用的有两种： k8s集群内的service：selector指定pod，自动创建Endpoints k8s集群外的service：手动创建Endpoints，指定外部服务的ip，端口和协议 kube-proxy和service的关系 kube-proxy 监听着k8s-apiserver，一旦service资源发生变化（调用k8s-api修改service信息），kube-proxy就会生成对应的负载调度的调整，这样就保证service的最新状态。 kube-proxy有三种调度模型： userspace：k8s1.1之前 iptables：1.2-k8s1.11之前 ipvs：k8s 1.11之后，如果没有开启ipvs，则自动降级为iptables ","date":"2023-01-12","objectID":"/posts/kubernetes/primary/kubernetes-4/:2:2","tags":["k8s进阶训练营"],"title":"kubernetesAPI资源对象 (四)","uri":"/posts/kubernetes/primary/kubernetes-4/"},{"categories":["Kubernetes"],"content":"Volume k8s 抽象出的一个对象，用来保存数据，解耦数据和镜像（数据存镜像里面每次更新镜像会特别大），实现容器间数据共享。 常用的几种卷： emptyDir：本地临时卷 hostPath：本地卷 nfs等：共享卷 configmap: 配置文件 https://kubernetes.io/zh-cn/docs/concepts/storage/volumes/ emptyDir 当 Pod 被分配给节点时，首先创建 emptyDir 卷，并且只要该Pod 在该节点上运行，该卷就会存在。正如卷的名字所述，它最初是空的。Pod 中的容器可以读取和写入 emptyDir 卷中的相同文件，尽管该卷可以挂载到每个容器中的相同或不同路径上。当出于任何原因从节点中删除 Pod 时，emptyDir 中的数据将被永久删除。 hostPath hostPath 卷将主机节点的文件系统中的文件或目录挂载到集群中，pod删除的时候，卷不会被删除，但是pod可能调度到不同的node 数据会出现丢失。 nfs等共享存储 nfs 卷允许将现有的 NFS（网络文件系统）共享挂载到您的容器中。不像 emptyDir，当删除 Pod 时，nfs 卷的内容被保留，卷仅仅是被卸载。这意味着 NFS 卷可以预填充数据，并且可以在 pod 之间“切换”数据。 NFS可以被多个写入者同时挂载。 创建多个pod测试挂载同一个NFS apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: replicas: 1 selector: matchLabels: app: ng-deploy-80 template: metadata: labels: app: ng-deploy-80 spec: containers: - name: ng-deploy-80 image: nginx ports: - containerPort: 80 volumeMounts: - mountPath: /usr/share/nginx/html/mysite name: my-nfs-volume volumes: - name: my-nfs-volume nfs: server: 172.31.7.109 path: /data/magedu/n56 --- apiVersion: v1 kind: Service metadata: name: ng-deploy-80 spec: ports: - name: http port: 81 targetPort: 80 nodePort: 30016 protocol: TCP type: NodePort selector: app: ng-deploy-80创建多个pod测试每个pod挂载多个NFS apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: replicas: 1 selector: matchLabels: app: ng-deploy-80 template: metadata: labels: app: ng-deploy-80 spec: containers: - name: ng-deploy-80 image: nginx ports: - containerPort: 80 volumeMounts: - mountPath: /usr/share/nginx/html/mysite name: my-nfs-volume - mountPath: /usr/share/nginx/html/js name: my-nfs-js volumes: - name: my-nfs-volume nfs: server: 172.31.7.109 path: /data/magedu/n56 - name: my-nfs-js nfs: server: 172.31.7.109 path: /data/magedu/js --- apiVersion: v1 kind: Service metadata: name: ng-deploy-80 spec: ports: - name: http port: 81 targetPort: 80 nodePort: 30016 protocol: TCP type: NodePort selector: app: ng-deploy-80重点： service 访问流程： k8s内部服务对外发布一般有两种方式，nodeport和ingress ，如果使用nodeport方式就会在每台node节点都会监听一个端口通常是30000以上，nodeport为什么不直接转发给pod ？ 因为维护nodeport和pod绑定关系比较难，通过需要service进行转发，service相当于k8s内部的负载均衡器负责转发，基于label标签匹配和筛选那些具有标签的pod。 默认使用轮询调度方式 ","date":"2023-01-12","objectID":"/posts/kubernetes/primary/kubernetes-4/:2:4","tags":["k8s进阶训练营"],"title":"kubernetesAPI资源对象 (四)","uri":"/posts/kubernetes/primary/kubernetes-4/"},{"categories":["Kubernetes"],"content":"configmap 功能：将配置信息和镜像解耦将配置信息放到configmap对象中，然后在pod的对象中导入configmap对象，实现导入配置文件的操作。yaml声明一个ConfigMap的对象，作为Volume挂载到pod中 使用 Configmap 挂载nginx 配置文件 apiVersion: v1 kind: ConfigMap metadata: name: nginx-config data: default: | server { listen 80; server_name www.mysite.com; index index.html; location / { root /data/nginx/html; if (!-e $request_filename) { rewrite ^/(.*) /index.html last; } } } --- #apiVersion: extensions/v1beta1 apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: replicas: 1 selector: matchLabels: app: ng-deploy-80 template: metadata: labels: app: ng-deploy-80 spec: containers: - name: ng-deploy-80 image: nginx ports: - containerPort: 80 volumeMounts: - mountPath: /data/nginx/html name: nginx-static-dir - name: nginx-config mountPath: /etc/nginx/conf.d volumes: - name: nginx-static-dir hostPath: path: /data/nginx/linux39 - name: nginx-config configMap: name: nginx-config items: - key: default path: mysite.conf --- apiVersion: v1 kind: Service metadata: name: ng-deploy-80 spec: ports: - name: http port: 81 targetPort: 80 nodePort: 30019 protocol: TCP type: NodePort selector: app: ng-deploy-80使用 Configmap 挂载环境变量到pod apiVersion: v1 kind: ConfigMap metadata: name: nginx-config data: username: user1 --- #apiVersion: extensions/v1beta1 apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: replicas: 1 selector: matchLabels: app: ng-deploy-80 template: metadata: labels: app: ng-deploy-80 spec: containers: - name: ng-deploy-80 image: nginx env: - name: MY_PASSWD value: \"123123\" - name: MY_USERNAME valueFrom: configMapKeyRef: name: nginx-config key: username ports: - containerPort: 80","date":"2023-01-12","objectID":"/posts/kubernetes/primary/kubernetes-4/:2:5","tags":["k8s进阶训练营"],"title":"kubernetesAPI资源对象 (四)","uri":"/posts/kubernetes/primary/kubernetes-4/"},{"categories":["Kubernetes"],"content":"Statefulset 功能： 为了解决有状态服务的问题 无状态服务（有主从关系、集群部署 ） 它所管理的Pod拥有固定的Pod名称，主机名，启停顺序 和 Deployment 类似， StatefulSet 管理基于相同容器规约的一组 Pod。但和 Deployment 不同的是， StatefulSet 为它们的每个 Pod 维护了一个有粘性的 ID。这些 Pod 是基于相同的规约来创建的， 但是不能相互替换：无论怎么调度，每个 Pod 都有一个永久不变的 ID。 https://kubernetes.io/zh-cn/docs/concepts/workloads/controllers/statefulset/ ","date":"2023-01-12","objectID":"/posts/kubernetes/primary/kubernetes-4/:2:6","tags":["k8s进阶训练营"],"title":"kubernetesAPI资源对象 (四)","uri":"/posts/kubernetes/primary/kubernetes-4/"},{"categories":["Kubernetes"],"content":"DaemonSet ","date":"2023-01-12","objectID":"/posts/kubernetes/primary/kubernetes-4/:2:7","tags":["k8s进阶训练营"],"title":"kubernetesAPI资源对象 (四)","uri":"/posts/kubernetes/primary/kubernetes-4/"},{"categories":["Kubernetes"],"content":"PV/PVC 对存储抽象 不直接存储数据而是在k8s层面做了一个隔离，权限控制、业务隔离，把不同是pod数据分开 用nfs挂载 那么每个pod都能看到相同数据，如何做数据隔离？ 在存储和k8s直接封装一层 PV/PVC PV是对底层⽹络存储的抽象，即将⽹络存储定义为⼀种存储资源，将⼀个整体的存储资源拆分成多份后给不同的业务使用。 PV是由管理员添加的的⼀个存储的描述，是⼀个全局资源即不⾪属于任何namespace，包含存储的类型，存储的⼤⼩和访问模式等，它的⽣命周期独⽴于Pod，例如当使⽤它的Pod销毁时对PV没有影响。 **PersistentVolumeClaim（PVC）**是⽤户存储的请求，它类似于pod，Pod消耗节点资源，PVC消耗存储资源， 就像 pod可以请求特定级别的资源（CPU和内存），PVC是namespace中的资源，可以设置特定的空间大小和访问模式。 pod是通过PVC将数据保存⾄PV，PV在保存⾄存储。 PersistentVolume参数： 访问模式 accessModes PersistentVolume 卷可以用资源提供者所支持的任何方式挂载到宿主系统上。 如下表所示，提供者（驱动）的能力不同，每个 PV 卷的访问模式都会设置为对应卷所支持的模式值。 例如，NFS 可以支持多个读写客户，但是某个特定的 NFS PV 卷可能在服务器上以只读的方式导出。 每个 PV 卷都会获得自身的访问模式集合，描述的是特定 PV 卷的能力。 访问模式有：ReadWriteOnce卷可以被一个节点以读写方式挂载。 ReadWriteOnce 访问模式也允许运行在同一节点上的多个 Pod 访问卷。ReadOnlyMany卷可以被多个节点以只读方式挂载。ReadWriteMany卷可以被多个节点以读写方式挂载。ReadWriteOncePod # kubectl explain PersistentVolume Capacity： #当前PV空间⼤⼩，kubectl explain PersistentVolume.spec.capacity accessModes ：访问模式，#kubectl explain PersistentVolume.spec.accessModes ReadWriteOnce – PV只能被单个节点以读写权限挂载，RWO ReadOnlyMany – PV以可以被多个节点挂载但是权限是只读的,ROX ReadWriteMany – PV可以被多个节点是读写⽅式挂载使⽤,RWXhttps://kubernetes.io/zh-cn/docs/concepts/storage/persistent-volumes/#access-modes 官⽅提供的基于各后端存储创建的PV⽀持的访问模式 回收机制 persistentVolumeReclaimPolicy persistentVolumeReclaimPolicy #删除机制即删除存储卷卷时候，已经创建好的存储卷由以下删除操作： #kubectl explain PersistentVolume.spec.persistentVolumeReclaimPolicy Retain – 删除PV后保持原装，最后需要管理员⼿动删除 Recycle – 空间回收，及删除存储卷上的所有数据(包括⽬录和隐藏⽂件),⽬前仅⽀持NFS和hostPath Delete – ⾃动删除存储卷 mountOptions #附加的挂载选项列表，实现更精细的权限控制 ro #等https://kubernetes.io/zh-cn/docs/concepts/storage/persistent-volumes/#reclaim-policy 卷模式 针对 PV 持久卷，Kubernetes 支持两种卷模式（volumeModes）：Filesystem（文件系统） 和 Block（块）。 volumeMode 是一个可选的 API 参数。 如果该参数被省略，默认的卷模式是 Filesystem。volumeMode 属性设置为 Filesystem 的卷会被 Pod 挂载（Mount） 到某个目录。 如果卷的存储来自某块设备而该设备目前为空，Kuberneretes 会在第一次挂载卷之前在设备上创建文件系统。 volumeMode #卷类型 kubectl explain PersistentVolume.spec.volumeMode 定义存储卷使⽤的⽂件系统是块设备还是⽂件系统，默认为⽂件系统https://kubernetes.io/zh-cn/docs/concepts/storage/persistent-volumes/#volume-mode PersistentVolumeClaim 参数 #kubectl explain PersistentVolumeClaim accessModes ：PVC 访问模式，#kubectl explain PersistentVolumeClaim.spec.volumeMode ReadWriteOnce – PVC只能被单个节点以读写权限挂载，RWO ReadOnlyMany – PVC以可以被多个节点挂载但是权限是只读的,ROX ReadWriteMany – PVC可以被多个节点是读写⽅式挂载使⽤,RWX resources： #定义PVC创建存储卷的空间⼤⼩ selector： #标签选择器，选择要绑定的PV matchLabels #匹配标签名称 matchExpressions #基于正则表达式匹配 volumeName #要绑定的PV名称 volumeMode #卷类型 定义PVC使⽤的⽂件系统是块设备还是⽂件系统，默认为⽂件系统PV及PVC实战案例之zookeeper集群 基于PV和PVC作为后端存储，实现zookeeper集群 1.下载JDK 镜像 https://www.oracle.com/java/technologies/javase/javase8u211-later-archive-downloads.html#license-lightbox docker pull elevy/slim_java:8 docker tag elevy/slim_java:8 harbor.ceamg.com/baseimages/slim_java:8 docker push harbor.ceamg.com/baseimages/slim_java:82.构建zookeeper 镜像 chmod a+x *.sh chmod a+x bin/*.sh bash build-command.sh bash build-command.sh 2022-1-12_9_16_32#FROM harbor-linux38.local.com/linux38/slim_java:8 FROM harbor.ceamg.com/baseimages/slim_java:8 ENV ZK_VERSION 3.4.14 ADD repositories /etc/apk/repositories # Download Zookeeper COPY zookeeper-3.4.14.tar.gz /tmp/zk.tgz COPY zookeeper-3.4.14.tar.gz.asc /tmp/zk.tgz.asc COPY KEYS /tmp/KEYS RUN apk add --no-cache --virtual .build-deps \\ ca-certificates \\ gnupg \\ tar \\ wget \u0026\u0026 \\ # # Install dependencies apk add --no-cache \\ bash \u0026\u0026 \\ # # # Verify the signature export GNUPGHOME=\"$(mktemp -d)\" \u0026\u0026 \\ gpg -q --batch --import /tmp/KEYS \u0026\u0026 \\ gpg -q --batch --no-auto-key-retrieve --verify /tmp/zk.tgz.asc /tmp/zk.tgz \u0026\u0026 \\ # # Set up directories # mkdir -p /zookeeper/data /zookeeper/wal /zookeeper/log \u0026\u0026 \\ # # Install tar -x -C /zookeeper --strip-components=1 --no-same-owner -f /tmp/zk.tgz \u0026\u0026 \\ # # Slim down cd /zookeeper \u0026\u0026 \\ cp dist-maven/zookeeper-${ZK_VERSION}.jar . \u0026\u0026 \\ rm -rf \\ *.txt \\ *.xml \\ bin/README.txt \\ bin/*.cmd \\ conf/* \\ contrib \\ dist-maven \\ docs \\ lib/*.txt \\ lib/cobertura \\ lib/jdiff \\ recipes \\ src \\ zookeeper-*.asc \\ zookeeper-*.md5 \\ zookeeper-*.sha1 \u0026\u0026 \\ # # Clean up apk del .build-deps \u0026\u0026 \\ rm -rf /tmp/* \"$GNUPGHOME","date":"2023-01-12","objectID":"/posts/kubernetes/primary/kubernetes-4/:2:8","tags":["k8s进阶训练营"],"title":"kubernetesAPI资源对象 (四)","uri":"/posts/kubernetes/primary/kubernetes-4/"},{"categories":["Kubernetes"],"content":"1. yaml格式 人员名单: 张三: 年龄: 18 # 职业: Linux运维工程师 爱好: - 看书 - 学习 - 加班 李四: 年龄: 20 职业: Java开发工程师 # 这是职业 爱好: - 开源技术 - 微服务 - 分布式存储 大小写敏感 使用缩进表示层级关系 缩进时不允许使用Tal键，只允许使用空格 缩进的空格数目不重要，只要相同层级的元素左侧对齐即可 使用”#” 表示注释，从这个字符一直到行尾，都会被解析器忽略 比json更适用于配置文件","date":"2023-01-12","objectID":"/posts/kubernetes/primary/kubernetes-3/:1:0","tags":["k8s进阶训练营"],"title":"yaml文件语法基础 （三）","uri":"/posts/kubernetes/primary/kubernetes-3/"},{"categories":["Kubernetes"],"content":"yaml文件主要特性 k8s中的yaml文件以及其他场景的yaml文件， 大部分都是以下类型： 上下级关系列表 键值对(也称为maps， 即key：value格式的键值对数据)容器在运行时是基于宿主机的内核的namespace隔离环境 pid ，如果只有环境没有服务就退出了 ","date":"2023-01-12","objectID":"/posts/kubernetes/primary/kubernetes-3/:1:1","tags":["k8s进阶训练营"],"title":"yaml文件语法基础 （三）","uri":"/posts/kubernetes/primary/kubernetes-3/"},{"categories":["Kubernetes"],"content":"2.yaml与json对比 在线yaml与json编辑器：http://www.bejson.com/validators/yaml_editor/json格式 { '人员名单': { '张三': { '年龄': 18, '职业': 'Linux运维工程师', '爱好': [ '看书', '学习', '加班' ] }, '李四': { '年龄': 20, '职业': 'Java开发工程师', '爱好': [ '开源技术', '微服务', '分布式存 储' ] } } }","date":"2023-01-12","objectID":"/posts/kubernetes/primary/kubernetes-3/:2:0","tags":["k8s进阶训练营"],"title":"yaml文件语法基础 （三）","uri":"/posts/kubernetes/primary/kubernetes-3/"},{"categories":["Kubernetes"],"content":"json特点 json 不能注释json 可读性较差json 语法很严格比较适用于API 返回值，也可用于配置文件 ","date":"2023-01-12","objectID":"/posts/kubernetes/primary/kubernetes-3/:2:1","tags":["k8s进阶训练营"],"title":"yaml文件语法基础 （三）","uri":"/posts/kubernetes/primary/kubernetes-3/"},{"categories":["Kubernetes"],"content":"3. 实践-4创建namespace yaml文件 #cat namespaces.yaml apiVersion：v1 #API版本 kind：Namespace #类型为namespac metadata： #定义元数据 name：xin-k8s #namespace名称 -------------------------------------- root@master02:~# mkdir /xin-yaml root@master02:~# cd /xin-yaml/ root@master02:/xin-yaml# vim namespaces.yaml root@master02:/xin-yaml# kubectl apply -f namespaces.yaml namespace/xin-k8s created root@master02:/xin-yaml# root@master02:/xin-yaml# kubectl get ns NAME STATUS AGE default Active 2d20h kube-node-lease Active 2d20h kube-public Active 2d20h kube-system Active 2d20h kubernetes-dashboard Active 42h xin-k8s Active 7s","date":"2023-01-12","objectID":"/posts/kubernetes/primary/kubernetes-3/:3:0","tags":["k8s进阶训练营"],"title":"yaml文件语法基础 （三）","uri":"/posts/kubernetes/primary/kubernetes-3/"},{"categories":["Kubernetes"],"content":"4.nginx yaml文件详解 可以使用kubectl explain deployment 查看版本信息 root@master02:/xin-yaml# kubectl explain deploy KIND: Deployment VERSION: apps/v1 DESCRIPTION: Deployment enables declarative updates for Pods and ReplicaSets.# cat nginx.yaml kind: Deployment #类型，是deployment控制器，kubectl explain Deployment apiVersion: extensions/v1 #API版本，# kubectl explain Deployment.apiVersion metadata: #pod的元数据信息，kubectl explain Deployment.metadata labels: #自定义pod的标签，# kubectl explain Deployment.metadata.labels app: xin01-nginx-deployment-label #标签名称为app值为linux36-nginx-deployment-label，后面会用到此标签 name: xin01-nginx-deployment #pod的名称 namespace: xin-01 #pod的namespace，默认是defaule spec: #定义deployment中容器的详细信息，kubectl explain Deployment.spec replicas: 3 #创建出的pod的副本数，即多少个pod，默认值为1 selector: #定义标签选择器 matchLabels: #定义匹配的标签，必须要设置 app: xin01-nginx-deployment-label #匹配的目标标签， template: #定义模板，必须定义，模板是起到描述要创建的pod的作用 metadata: #定义模板元数据 labels: #定义模板label，Deployment.spec.template.metadata.labels app: xin01-nginx-deployment-label #定义标签，等于Deployment.spec.selector.matchLabels spec: #定义pod信息 containers: #定义pod中容器列表，可以多个至少一个，pod不能动态增减容器 - name: xin-nginx-container #容器名称 image: nginx:1.20.1 #镜像地址 #command: [\"/apps/tomcat/bin/run_tomcat.sh\"] #容器启动执行的命令或脚本 #imagePullPolicy: IfNotPresent imagePullPolicy: Always #拉取镜像策略 ports: #定义容器端口列表 - containerPort: 80 #定义一个端口 protocol: TCP #端口协议 name: http #端口名称 - containerPort: 443 #定义一个端口 protocol: TCP #端口协议 name: https #端口名称 env: #配置环境变量 - name: \"password\" #变量名称。必须要用引号引起来 value: \"123456\" #当前变量的值 - name: \"age\" #另一个变量名称 value: \"18\" #另一个变量的值 resources: #对资源的请求设置和限制设置 limits: #资源限制设置，上限 cpu: 500m #cpu的限制，单位为core数，可以写0.5或者500m等CPU压缩值 memory: 1Gi #内存限制，单位可以为Mib/Gib，将用于docker run --memory参数 requests: #资源请求的设置 cpu: 200m #cpu请求数，容器启动的初始可用数量,可以写0.5或者500m等CPU压缩值 memory: 512Mi #内存请求大小，容器启动的初始可用数量，用于调度pod时候使用 --- kind: Service #类型为service apiVersion: v1 #service API版本， service.apiVersion metadata: #定义service元数据，service.metadata labels: #自定义标签，service.metadata.labels app: xin01-nginx #定义service标签的内容 name: xin01-nginx-spec #定义service的名称，此名称会被DNS解析 namespace: xin-01 #该service隶属于的namespaces名称，即把service创建到哪个namespace里面 spec: #定义service的详细信息，service.spec type: NodePort #service的类型，定义服务的访问方式，默认为ClusterIP， service.spec.type ports: #定义访问端口， service.spec.ports - name: http #定义一个端口名称 port: 80 #service 80端口 protocol: TCP #协议类型 targetPort: 80 #目标pod的端口 nodePort: 30001 #node节点暴露的端口 - name: https #SSL 端口 port: 443 #service 443端口 protocol: TCP #端口协议 targetPort: 443 #目标pod端口 nodePort: 30043 #node节点暴露的SSL端口 selector: #service的标签选择器，定义要访问的目标pod app: xin01-nginx-deployment-label #将流量路到选择的pod上，须等于Deployment.spec.selector.matchLabels","date":"2023-01-12","objectID":"/posts/kubernetes/primary/kubernetes-3/:4:0","tags":["k8s进阶训练营"],"title":"yaml文件语法基础 （三）","uri":"/posts/kubernetes/primary/kubernetes-3/"},{"categories":["Kubernetes"],"content":"5.pod 资源清单详细解读 apiVersion: v1 #版本号，例如 v1 kind: Pod #资源类型，如 Pod metadata: #元数据 name: string # Pod 名字 namespace: string # Pod 所属的命名空间 labels: #自定义标签 - name: string #自定义标签名字 annotations: #自定义注释列表 - name: string spec: # Pod 中容器的详细定义 containers: # Pod 中容器列表 - name: string #容器名称 image: string #容器的镜像名称 imagePullPolicy: [Always | Never | IfNotPresent] #获取镜像的策略 Alawys 表示下载镜像 IfnotPresent 表示优先使用本地镜像，否则下载镜像，Nerver 表示仅使用本地镜像 command: [string] #容器的启动命令列表，如不指定，使用打包时使用的启动命令 args: [string] #容器的启动命令参数列表 workingDir: string #容器的工作目录 volumeMounts: #挂载到容器内部的存储卷配置 - name: string #引用 pod 定义的共享存储卷的名称，需用 volumes[]部分定义的的卷名 mountPath: string #存储卷在容器内 mount 的绝对路径，应少于 512 字符 readOnly: boolean #是否为只读模式 ports: #需要暴露的端口库号 - name: string #端口号名称 containerPort: int #容器需要监听的端口号 hostPort: int #容器所在主机需要监听的端口号，默认与 Container 相同 protocol: string #端口协议，支持 TCP 和 UDP，默认 TCP env: #容器运行前需设置的环境变量列表 - name: string #环境变量名称 value: string #环境变量的值 resources: #资源限制和请求的设置 limits: #资源限制的设置 cpu: string #cpu 的限制，单位为 core 数 memory: string #内存限制，单位可以为 Mib/Gib requests: #资源请求的设置 cpu: string #cpu 请求，容器启动的初始可用数量 memory: string #内存请求，容器启动的初始可用内存 livenessProbe: #对 Pod 内个容器健康检查的设置，当探测无响应几次后将自动重启该容器，检查方法有 exec、httpGet 和 tcpSocket，对一个容器只需设置其中一种方法即可 exec: #对 Pod 容器内检查方式设置为 exec 方式 command: [string] #exec 方式需要制定的命令或脚本 httpGet: #对 Pod 内个容器健康检查方法设置为 HttpGet，需要制定 Path、port path: string port: number host: string scheme: string HttpHeaders: - name: string value: string tcpSocket: #对 Pod 内个容器健康检查方式设置为 tcpSocket 方式 port: number initialDelaySeconds: 0 #容器启动完成后首次探测的时间，单位为秒 timeoutSeconds: 0 #对容器健康检查探测等待响应的超时时间，单位秒，默认 1 秒 periodSeconds: 0 #对容器监控检查的定期探测时间设置，单位秒，默认 10 秒一次 successThreshold: 0 failureThreshold: 0 securityContext: privileged:false restartPolicy: [Always | Never | OnFailure]#Pod 的重启策略，Always 表示一旦不管以何种方式终止运行，kubelet 都将重启，OnFailure 表示只有 Pod 以非 0 退出码退出才重启，Nerver 表示不再重启该 Pod nodeSelector: obeject #设置 NodeSelector 表示将该 Pod 调度到包含这个 label 的 node上，以 key：value 的格式指定 imagePullSecrets: #Pull 镜像时使用的 secret 名称，以 key：secretkey 格式指定 - name: string hostNetwork:false #是否使用主机网络模式，默认为 false，如果设置为 true，表示使用宿主机网络 volumes: #在该 pod 上定义共享存储卷列表 - name: string #共享存储卷名称 （volumes 类型有很多种） emptyDir: {} #类型为 emtyDir 的存储卷，与 Pod 同生命周期的一个临时目录。为空值 hostPath: string #类型为 hostPath 的存储卷，表示挂载 Pod 所在宿主机的目录 path: string #Pod 所在宿主机的目录，将被用于同期中 mount 的目录 secret: #类型为 secret 的存储卷，挂载集群与定义的 secre 对象到容器内部 scretname: string items: - key: string path: string configMap: #类型为 configMap 的存储卷，挂载预定义的 configMap 对象到容器内部 name: string items: - key: string path: string#test-pod apiVersion: v1 #指定api版本，此值必须在kubectl apiversion中 kind: Pod #指定创建资源的角色/类型 metadata: #资源的元数据/属性 name: test-pod #资源的名字，在同一个namespace中必须唯一 labels: #设定资源的标签 k8s-app: apache version: v1 kubernetes.io/cluster-service: \"true\" annotations: #自定义注解列表 - name: String #自定义注解名字 spec: #specification of the resource content 指定该资源的内容 restartPolicy: Always #表明该容器一直运行，默认k8s的策略，在此容器退出后，会立即创建一个相同的容器 nodeSelector: #节点选择，先给主机打标签kubectl label nodes kube-node1 zone=node1 zone: node1 containers: - name: test-pod #容器的名字 image: 10.192.21.18:5000/test/chat:latest #容器使用的镜像地址 imagePullPolicy: Never #三个选择Always、Never、IfNotPresent，每次启动时检查和更新（从registery）images的策略， # Always，每次都检查 # Never，每次都不检查（不管本地是否有） # IfNotPresent，如果本地有就不检查，如果没有就拉取 command: ['sh'] #启动容器的运行命令，将覆盖容器中的Entrypoint,对应Dockefile中的ENTRYPOINT args: [\"$(str)\"] #启动容器的命令参数，对应Dockerfile中CMD参数 env: #指定容器中的环境变量 - name: str #变量的名字 value: \"/etc/run.sh\" #变量的值 resources: #资源管理 requests: #容器运行时，最低资源需求，也就是说最少需要多少资源容器才能正常运行 cpu: 0.1 #CPU资源（核数），两种方式，浮点数或者是整数+m，0.1=100m，最少值为0.001核（1m） memory: 32Mi #内存使用量 limits: #资源限制 cpu: 0.5 memory: 1000Mi ports: - containerPort: 80 #容器开发对外的端口 name: httpd #名称 protocol: TCP livenessProbe: #pod内容器健康检查的设置 httpGet: #通过httpget检查健康，返回200-399之间，则认为容器正常 path: / #URI地址 port: 80 #host: 127.0.0.1 #主机地址 scheme: HTTP initialDelaySeconds: 180 #表明第一次检测在容器启动后多长时间后开始 timeoutSeconds: 5 #检测的超时时间 periodSeconds: 15 #检查间隔时间 #也可以用这种方法 #exec: 执行命令的方法进行监测，如果其退出码不为0，则认为容器正常 # command: # - cat # - /tmp/health #也可以用这种方法 #tcpSocket: //通过tcpSocket检查健康 # port: num","date":"2023-01-12","objectID":"/posts/kubernetes/primary/kubernetes-3/:5:0","tags":["k8s进阶训练营"],"title":"yaml文件语法基础 （三）","uri":"/posts/kubernetes/primary/kubernetes-3/"},{"categories":["Ceph"],"content":"https://docs.ceph.com/en/latest/cephfs/链接 Ceph FS即Ceph Filesy Stem,可以实现文件系统共享功能,客户端通过ceph协议挂载并使用ceph集群作为数据存储服务器。 Ceph FS在公司中使用常场景相对比较多，主要用于动静分离，多服务数据共享例如Nginx 。 Ceph被多个服务同时挂载，写入数据时能实时同步，类似NFS。 客户端通过ceph协议挂载Linux内核版本\u003e2.6.34 就内置Cpeh模块无需安装 MDS存储池用于存储Ceph FS上存储的文件相关的元数据Ceph FS需要运行Meta Data Services(MDS)服务，其守护进程为ceph-mds, ceph-mds进程管理与cephFS上存储的文件相关的元数据，并协调对ceph存储集群的访问。 mate data pool：用于存储Ceph FS上存储的文件相关的元数据，pool名称可以随意指定。**ceph data pool **：用来保存客户端上传到Ceph的数据。 ","date":"2023-01-11","objectID":"/posts/ceph/3.ceph-fs%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8/:0:0","tags":["分布式存储"],"title":"Ceph 集群维护 （三）","uri":"/posts/ceph/3.ceph-fs%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8/"},{"categories":["Ceph"],"content":"3.1 部署MDS服务 在指定的ceph-mds服务器,部署ceph-mds服务，可以和其它服务器混用(如ceph-mon.ceph-mgr) Ubuntu: #查看当前可用版本 root@ceph-mgr1:~# sudo su - root root@ceph-mgr1:~# apt-cache madison ceph-mds #选择版本安装 root@ceph-mgr1:~# apt install ceph-mds=16.2.5-1bionicCentOS: Centos: [root@ceph-mgr1 ~]# yum install ceph-mds -y部署： [ceph@ceph-deploy ceph-cluster]$ ceph-deploymds create ceph-mgr1","date":"2023-01-11","objectID":"/posts/ceph/3.ceph-fs%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8/:1:0","tags":["分布式存储"],"title":"Ceph 集群维护 （三）","uri":"/posts/ceph/3.ceph-fs%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8/"},{"categories":["Ceph"],"content":"3.2 验证 MDS 服务 MDS服务目前还无法正常使用，需要为MDS创建存储池用于保存MDS的数据. [ceph@ceph-deploy ceph-clusterI$ ceph mds stat 1 up:standby #当前为备用状态，需要分配pool才可以使用，","date":"2023-01-11","objectID":"/posts/ceph/3.ceph-fs%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8/:2:0","tags":["分布式存储"],"title":"Ceph 集群维护 （三）","uri":"/posts/ceph/3.ceph-fs%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8/"},{"categories":["Ceph"],"content":"3.3 创建CephFS metadata和data存储池: 使用CephFS之前需要事先于集群中创建一个文件系统，并为其分别指定元数据和数据相关的存储池，如下命令将创建名为mycephfs的文件系统，它使用cephfs-metadata作为元数据存储池，使用cephfs-data为数据存储池: #保存 metadata的pool [ceph@ceph-deploy ceph-cluster]$ ceph osd pool create cephfs-metadata 32 32 pool 'cephfs-metadata' created #生产环境下matedata数据的pg数量为16个就可以了（数据量小 10几个T的数量元数据才几个G） #保存客户端数据的pool [ceph@ceph-deploy ceph-cluster]$ ceph osd pool create cephfs-data 64 64 pool 'cephfs-data' created [ceph@ceph-deploy ceph-cluster]$ ceph -S #当前ceph状态 #查看当前存储池 ceph@ceph-dep Loy :~/ ceph-cluster$ ceph osd pool ls device health_ metrics 32 mypool myrbd1 .rgw.root default.rgw.Log default.rgw.control default.rgw.meta cephfs-metadata cepnfs-data","date":"2023-01-11","objectID":"/posts/ceph/3.ceph-fs%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8/:3:0","tags":["分布式存储"],"title":"Ceph 集群维护 （三）","uri":"/posts/ceph/3.ceph-fs%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8/"},{"categories":["Ceph"],"content":"3.4 创建Ceph FS 并验证 Ceph FS在早期版本中一个集群中只能创建一个，现在支持启用多个 ceph fs new fs_name metadata data #--allow-dangerous-metadata-overlay:允许非安全的元数据写入 [ceph@ceph-deploy ceph-cluster]$ ceph fs new mycephfs cephfs-metadata cephfs-data #查看Ceph FS状态 cephaceph-dep Loy:~/ ceph-cluster$ ceph -S cluster: id: 5ac860ab- 9a4e- 4edd 9da2 e3de293a8d44 health: HEALTH 0K se rvices : mon: 3 daemons, auorum ceph-mon1 , ceph-mon2 , ceph-mon3 (age 48m) mgr: ceph-mg r1(active, since 47m), standbys: ceph-mgr2 mds: 1/1 daemons up osd:20 osds: 20 up (since 47m), 20 in(since 6d) rgw: 1 daemon active (1 hosts, 1 zones ) data : voLumes: 1/1 healthy pools: 10 pools, 329 pgs objects: 253 objects, 89 MiB usage : 562 MiB used, 2.0 TiB / 2.0 TiB avail pgs: 329 act ive+c Lean io: Client : 1.3 KiB/S wr ,0 op/s rd, 4 op/s wr cephaceph-deploy :~/ceph-cluster$ ceph mds stat mycephfs:1 {0=ceph-mgr1=up: active} #active表示启用成功","date":"2023-01-11","objectID":"/posts/ceph/3.ceph-fs%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8/:4:0","tags":["分布式存储"],"title":"Ceph 集群维护 （三）","uri":"/posts/ceph/3.ceph-fs%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8/"},{"categories":["Ceph"],"content":"3.5 客户端挂载Ceph FS 在ceph的客户端测试CephFS的挂载，需要指定mon节点（负责提供认证）的6789端口: （6789就是CephFS对外提供挂载的端口） #使用admin权限挂载 [ceph@ceph-deploy ceph-clusterl$ cat ceph.client.admin.keyring [client.admin] key = AQCrVhZhof2zKxAATltgtgAdDteHSAGFEyE/nw== caps mds = \"allow *\" caps mgr = \"allow *\" caps mon = \"allow *\" caps osd = \"allow *\"ubuntu及centos client 挂载(内核版本2.6.34在3.6.34及以上) #使用key挂载 #ip写mon集群中的任意一个都行，也可以写三个提供高可用 #挂载到/mnt目录 root@ceph-client3-ubuntu1 804:-# mount -t ceph 172.31.6.101:6789:/ /mnt -o name=admin,secret=AQCrVhZhof2zKxAATltgtgAdDteHSAGFEyE/mw==查看挂载情况 在任何一个节点变更数据，会立即在其他客户端同步显示，非常适合多节点的web服务。 ","date":"2023-01-11","objectID":"/posts/ceph/3.ceph-fs%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8/:5:0","tags":["分布式存储"],"title":"Ceph 集群维护 （三）","uri":"/posts/ceph/3.ceph-fs%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8/"},{"categories":["Ceph"],"content":"模拟web多节点服务场景下，数据同步效果 #安装nginx yum install epel-release -y yum install nginx -y #创建数据目录 mkdir /data/nginx/statics #挂载 root@ceph-client3-ubuntu1804:~# mount一t ceph 172.31.6.101:6789:/ /data/nginx/statics -o name=admin ,secret=AQA3dhdhMd/UABAA2SNpJ+hcK1dD5L2Hj5XMg== vim /etc/ngxin.conf server { listen 80; listen [::] : 80; server_name _; root /data/nginx/statics; cd /data/nginx/statics #上传文件尽管内核中自带CephFS组件，因为性能关系还是建议使用最新的内核模块，安装Ceph ceph common （ceph 的公共组件） Ubuntu源： vim /etc/ deb https://mirrors.tuna.tsinghua.edu.cn/ceph/debian-pacific bionic main #导入key wget -q -O- 'https ://download.ceph.com/keys/release.asc' | sudo apt-key add - apt update #导入key wget -q -O- 'https://mirrors.tuna.tsinghua.edu.cn/ceph/keys/release.asc' | sudo apt-key add -CentOS 安装16版本之前的客户端 wget https://mirrors.tuna.tsinghua.edu.cn/ceph/rpm-15.2.15/el7/x86_64/ceph-common-15.2.15-0.el7.x86_64.rpm","date":"2023-01-11","objectID":"/posts/ceph/3.ceph-fs%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8/:5:1","tags":["分布式存储"],"title":"Ceph 集群维护 （三）","uri":"/posts/ceph/3.ceph-fs%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8/"},{"categories":["Ceph"],"content":"16版本的CephFS状态 ","date":"2023-01-11","objectID":"/posts/ceph/3.ceph-fs%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8/:5:2","tags":["分布式存储"],"title":"Ceph 集群维护 （三）","uri":"/posts/ceph/3.ceph-fs%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8/"},{"categories":["Ceph"],"content":"13版本的CephFS状态 压测工具：Jmeter ","date":"2023-01-11","objectID":"/posts/ceph/3.ceph-fs%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8/:5:3","tags":["分布式存储"],"title":"Ceph 集群维护 （三）","uri":"/posts/ceph/3.ceph-fs%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8/"},{"categories":["Ceph"],"content":"3.6 命令总结 列出存储池并显示id $ ceph osd lspools $ ceph osd Ispools 1 device_ health metrics 2 mypool 3 cephfs-metadata 4 cephfs-data查看pg状态 $ceph pg stat $ceph pg stat 129 pgs: 129 active+clean; 319 KIB data, 1.1 GiB used, 2.0 TiB / 2.0 TiB avail查看指定pool或所有的pool的状态 $ ceph osd pool stats mypool pool myrdb1 id 2 nothing is going on查看集群存储状态 $ ceph df detail查看osd的状态 ceph osd stat显示OSD底层详细信息 ceph osd dump显示OSD和node节点对于关系 ceph osd tree查看mon节点状态 ceph mon stat 查看mon详细信息 ceph mon dump","date":"2023-01-11","objectID":"/posts/ceph/3.ceph-fs%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8/:6:0","tags":["分布式存储"],"title":"Ceph 集群维护 （三）","uri":"/posts/ceph/3.ceph-fs%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8/"},{"categories":["Ceph"],"content":"推荐配置 CPU 16C 或 32C 双路内存 64G以上磁盘 NVME 企业级SSD 推荐 intel D7 P5520 数据中心企业级固态硬盘U.2 nvme协议服务器工作站SSD P5520 3.84TB intel 英特尔 S4510/S4520 数据中心企业级固态硬盘SATA3 S4520 3.84T mon 服务器 16C 32G 200GMgr 服务器 8C 16G 200GCeph-deploy 4c 8G 120G","date":"2023-01-10","objectID":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/:1:0","tags":["分布式存储"],"title":"Ceph 集群部署 （二）","uri":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"},{"categories":["Ceph"],"content":"一、部署方式 ceph-ansible：https://github.com/ceph/ceph-ansible #python ceph-salt：https://github.com/ceph/ceph-salt #python ceph-container：https://github.com/ceph/ceph-container #shell ceph-chef：https://github.com/ceph/ceph-chef #Ruby ceph-deploy：https://github.com/ceph/ceph-deploy #python ceph-deploy是一个 ceph 官方维护的基于 ceph-deploy 命令行部署 ceph 集群的工具，基于 ssh 执行可以 sudo 权限的 shell 命令以及一些 python 脚本 实现 ceph 集群的部署和管理维护。 Ceph-deploy 只用于部署和管理 ceph 集群，客户端需要访问 ceph，需要部署客户端工具。 ","date":"2023-01-10","objectID":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/:2:0","tags":["分布式存储"],"title":"Ceph 集群部署 （二）","uri":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"},{"categories":["Ceph"],"content":"二、服务器准备 硬件推荐：https://docs.ceph.com/en/latest/start/hardware-recommendations/# ","date":"2023-01-10","objectID":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/:3:0","tags":["分布式存储"],"title":"Ceph 集群部署 （二）","uri":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"},{"categories":["Ceph"],"content":"2.1 OSD服务器 三台服务器作为 ceph 集群 OSD 存储服务器，每台服务器支持两个网络，public 网络针对客户端访问，cluster 网络用于集群管理及数据同步，每台三块或以上的磁盘。 10.1.0.30/192.168.10.240\r10.0.0.31/192.168.10.241\r10.0.0.32/192.168.10.242\r三台存储服务器磁盘划分： /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf #200G ","date":"2023-01-10","objectID":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/:3:1","tags":["分布式存储"],"title":"Ceph 集群部署 （二）","uri":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"},{"categories":["Ceph"],"content":"2.2 Mon 监视服务器 三台服务器作为 ceph 集群 Mon 监视服务器，每台服务器可以和 ceph 集群的 cluster 网络通信。 10.1.0.33/192.168.10.243\r10.0.0.34/192.168.10.244\r10.0.0.35/192.168.10.245","date":"2023-01-10","objectID":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/:3:2","tags":["分布式存储"],"title":"Ceph 集群部署 （二）","uri":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"},{"categories":["Ceph"],"content":"2.3 ceph-mgr 管理服务器 两个 ceph-mgr 管理服务器，可以和 ceph 集群的 cluster 网络通信。 10.1.0.30/192.168.10.240\r10.0.0.31/192.168.10.241","date":"2023-01-10","objectID":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/:3:3","tags":["分布式存储"],"title":"Ceph 集群部署 （二）","uri":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"},{"categories":["Ceph"],"content":"2.4 Ceph-deploy 部署服务器 一个服务器用于部署 ceph 集群即安装 Ceph-deploy，也可以和 ceph-mgr 等复用。 10.1.0.31/192.168.10.248 ","date":"2023-01-10","objectID":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/:3:4","tags":["分布式存储"],"title":"Ceph 集群部署 （二）","uri":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"},{"categories":["Ceph"],"content":"三、 服务器环境准备 ","date":"2023-01-10","objectID":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/:4:0","tags":["分布式存储"],"title":"Ceph 集群部署 （二）","uri":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"},{"categories":["Ceph"],"content":"3.1 配置集群网络 #更改网卡名称为eth*: sudo vim /etc/default/grub GRUB_DEFAULT=0 GRUB_TIMEOUT_STYLE=hidden GRUB_TIMEOUT=0 GRUB_DISTRIBUTOR=`lsb_release -i -s 2\u003e /dev/null || echo Debian` GRUB_CMDLINE_LINUX_DEFAULT=\"maybe-ubiquity\" GRUB_CMDLINE_LINUX=\"net.ifnames=0 biosdevname=0\" ~$ sudo update-grub Sourcing file `/etc/default/grub' Generating grub configuration file ... Found linux image: /boot/vmlinuz-4.15.0-55-generic Found initrd image: /boot/initrd.img-4.15.0-55-generic done#配置cluster和public网络 vim /etc/apt/sources.list # This is the network config written by 'subiquity' network: ethernets: eth0: dhcp4: no dhcp6: no addresses: - 10.1.0.39/24 gateway4: 10.1.0.254 nameservers: addresses: - 223.5.5.5 eth1: dhcp4: no dhcp6: no addresses: [192.168.10.239/24] version: 2 #生效 netplan apply #验证两块网卡IP地址 2: eth0: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc mq state UP group default qlen 1000 link/ether fe:fc:fe:cf:34:9d brd ff:ff:ff:ff:ff:ff inet 10.1.0.39/24 brd 10.1.0.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::fcfc:feff:fecf:349d/64 scope link valid_lft forever preferred_lft forever 3: eth1: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc mq state UP group default qlen 1000 link/ether fe:fc:fe:79:6f:6e brd ff:ff:ff:ff:ff:ff inet 192.168.10.239/24 brd 192.168.10.255 scope global eth1 valid_lft forever preferred_lft forever inet6 fe80::fcfc:feff:fe79:6f6e/64 scope link valid_lft forever preferred_lft forever","date":"2023-01-10","objectID":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/:4:1","tags":["分布式存储"],"title":"Ceph 集群部署 （二）","uri":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"},{"categories":["Ceph"],"content":"3.2 配置主机名解析 vim /etc/hosts 10.1.0.39 ceph-node1.xx.local ceph-node1 10.1.0.40 ceph-node2.xx.local ceph-node2 10.1.0.41 ceph-node3.xx.local ceph-node3 10.1.0.39 ceph-mon1.xx.local ceph-mon1 10.1.0.40 ceph-mon2.xx.local ceph-mon2 10.1.0.41 ceph-mon3.xx.local ceph-mon3 10.1.0.40 ceph-mgr1.xx.local ceph-mgr1 10.1.0.41 ceph-mgr2.xx.local ceph-mgr2 10.1.0.39 ceph-deploy.xx.local ceph-deploy","date":"2023-01-10","objectID":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/:4:2","tags":["分布式存储"],"title":"Ceph 集群部署 （二）","uri":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"},{"categories":["Ceph"],"content":"3.3 配置apt源 https://download.ceph.com/ #Ceph官方源 https://mirrors.aliyun.com/ceph/ #阿里云镜像仓库 http://mirrors.163.com/ceph/ #网易镜像仓库 https://mirrors.tuna.tsinghua.edu.cn/ceph/ #清华大学镜像源所有节点添加ceph 源 wget -q -O- 'https://download.ceph.com/keys/release.asc' | sudo apt-key add - echo deb https://download.ceph.com/debian-pacific/ $(lsb_release -sc) main | sudo tee /etc/apt/sources.list.d/ceph.list sudo apt update","date":"2023-01-10","objectID":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/:4:3","tags":["分布式存储"],"title":"Ceph 集群部署 （二）","uri":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"},{"categories":["Ceph"],"content":"3.4 时间同步 #设置时区 timedatectl set-timezone Asia/Shanghai #安装chrony #三节点安装 apt install chrony -y ##服务端配置 vim /etc/chrony/chrony.conf # Welcome to the chrony configuration file. See chrony.conf(5) for more # information about usuable directives. # This will use (up to): # - 4 sources from ntp.ubuntu.com which some are ipv6 enabled # - 2 sources from 2.ubuntu.pool.ntp.org which is ipv6 enabled as well # - 1 source from [01].ubuntu.pool.ntp.org each (ipv4 only atm) # This means by default, up to 6 dual-stack and up to 2 additional IPv4-only # sources will be used. # At the same time it retains some protection against one of the entries being # down (compare to just using one of the lines). See (LP: #1754358) for the # discussion. # # About using servers from the NTP Pool Project in general see (LP: #104525). # Approved by Ubuntu Technical Board on 2011-02-08. # See http://www.pool.ntp.org/join.html for more information. # 因为想修改本地时间，不去和其他服务器同步，将下面这四个pool注释掉 pool ntp.ubuntu.com iburst maxsources 4 #pool 0.ubuntu.pool.ntp.org iburst maxsources 1 #pool 1.ubuntu.pool.ntp.org iburst maxsources 1 #pool 2.ubuntu.pool.ntp.org iburst maxsources 2 # 添加自己作为服务器 #server 192.168.1.1 iburst # 为了方便客户端连接权限设置为允许所有 allow all # 当无法和其他同步时，使用本地的时间去给客户端同步 #注释：值10可以被其他值取代（1~15），stratum 1表示计算机具有直接连接的真实时间的参考时间源，例如gps，原子钟都和真实时间很接近欸， #stratum 2表示计算机有一个stratum 1的计算机作为同步时间源，stratum 3表示该计算机有一个stratum 10的计算机作为同步时间源。 #选择stratum 10.这个值是比较大的，表示距离有真实时间的服务器比较远，它的时间不太可靠，因此，local命令选取stratum 10可以 #防止机器本身的时间与真实时间混淆，可以保证该机器不会将本身的时间授给那些可以连接同步到真实时间的ntp服务器的ntp客户端】 local stratum 10 # This directive specify the location of the file containing ID/key pairs for # NTP authentication. keyfile /etc/chrony/chrony.keys # This directive specify the file into which chronyd will store the rate # information. driftfile /var/lib/chrony/chrony.drift # Uncomment the following line to turn logging on. #log tracking measurements statistics # Log files location. logdir /var/log/chrony # Stop bad estimates upsetting machine clock. maxupdateskew 100.0 # This directive enables kernel synchronisation (every 11 minutes) of the # real-time clock. Note that it can’t be used along with the 'rtcfile' directive. rtcsync # Step the system clock instead of slewing it if the adjustment is larger than # one second, but only in the first three clock updates. makestep 1 3#客户端 # This will use (up to): # - 4 sources from ntp.ubuntu.com which some are ipv6 enabled # - 2 sources from 2.ubuntu.pool.ntp.org which is ipv6 enabled as well # - 1 source from [01].ubuntu.pool.ntp.org each (ipv4 only atm) # This means by default, up to 6 dual-stack and up to 2 additional IPv4-only # sources will be used. # At the same time it retains some protection against one of the entries being # down (compare to just using one of the lines). See (LP: #1754358) for the # discussion. # # About using servers from the NTP Pool Project in general see (LP: #104525). # Approved by Ubuntu Technical Board on 2011-02-08. # See http://www.pool.ntp.org/join.html for more information. pool 10.1.0.39 iburst maxsources 4 #pool 0.ubuntu.pool.ntp.org iburst maxsources 1 #pool 1.ubuntu.pool.ntp.org iburst maxsources 1 #pool 2.ubuntu.pool.ntp.org iburst maxsources 2 # This directive specify the location of the file containing ID/key pairs for # NTP authentication. keyfile /etc/chrony/chrony.keys # This directive specify the file into which chronyd will store the rate # information. driftfile /var/lib/chrony/chrony.drift # Uncomment the following line to turn logging on. #log tracking measurements statistics # Log files location. logdir /var/log/chrony # Stop bad estimates upsetting machine clock. maxupdateskew 100.0 # This directive enables kernel synchronisation (every 11 minutes) of the # real-time clock. Note that it can’t be used along with the 'rtcfile' directive. rtcsync # Step the system clock instead of slewing it if the adjustment is larger than # one second, but only in the first three clock updates. makestep 1 3#查看 ntp_servers 状态 chronyc source","date":"2023-01-10","objectID":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/:4:4","tags":["分布式存储"],"title":"Ceph 集群部署 （二）","uri":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"},{"categories":["Ceph"],"content":"3.5 创建普通用户 推荐使用指定的普通用户部署和运行 ceph 集群，普通用户只要能以非交互方式执行 sudo 命令执行一些特权命令即可，新版的 ceph-deploy 可以指定包含 root 的在内只要可以执行 sudo 命令的用户，不过仍然推荐使用普通用户，比如 ceph、cephuser、cephadmin 这样的用户去管理 ceph 集群。 在包含 ceph-deploy 节点的存储节点、mon 节点和 mgr 节点等创建 ceph 用户。 #创建用户 groupadd -r -g 20235 xceo \u0026\u0026 useradd -r -m -s /bin/bash -u 20235 -g 20235 xceo \u0026\u0026 echo xceo:ceamg.com | chpasswd ~ #:id xceo uid=20235(xceo) gid=20235(xceo) groups=20235(xceo) #允许ceph 用户以sudo执行特殊权限 echo \"ceph ALL=(ALL:ALL) NOPASSWD:ALL\" \u003e\u003e /etc/sudoers","date":"2023-01-10","objectID":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/:4:5","tags":["分布式存储"],"title":"Ceph 集群部署 （二）","uri":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"},{"categories":["Ceph"],"content":"3.6 配置免密登录 root@ceph-node1:~ su - xceo ceph@ceph-node1:~$ ceph@ceph-node1:~$ ssh-keygen ssh-copy-id ceph@10.1.0.40 ssh-copy-id ceph@10.1.0.41 ssh ceph-mon1.xx.local ssh ceph-mon2.xx.local ssh ceph-mon3.xx.local ssh ceph-mgr1.xx.local ssh ceph-mgr2.xx.local ssh ceph-node1.xx.local ssh ceph-node2.xx.local ssh ceph-node3.xx.local ssh ceph-deploy.xx.local","date":"2023-01-10","objectID":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/:4:6","tags":["分布式存储"],"title":"Ceph 集群部署 （二）","uri":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"},{"categories":["Ceph"],"content":"3.7 其他基本优化 ~$ vi /etc/sysctl.conf 添加： fs.file-max = 10000000000 fs.nr_open = 1000000000 ~$ vi /etc/security/limits.conf #root账户的资源软限制和硬限制 root soft core unlimited root hard core unlimited root soft nproc 1000000 root hard nproc 1000000 root soft nofile 1000000 root hard nofile 1000000 root soft memlock 32000 root hard memlock 32000 root soft msgqueue 8192000 root hard msgqueue 8192000 #其他账户的资源软限制和硬限制 * soft core unlimited * hard core unlimited * soft nproc 1000000 * hard nproc 1000000 * soft nofile 1000000 * hard nofile 1000000 * soft memlock 32000 * hard memlock 32000 * soft msgqueue 8192000 * hard msgqueue 8192000 # 将这两个修改过的文件拷贝到其他节点 ~$ scp /etc/sysctl.conf ceph-node2:/etc/ ~$ scp /etc/sysctl.conf ceph-node3:/etc/ ~$ scp /etc/security/limits.conf ceph-node2:/etc/security/ ~$ scp /etc/security/limits.conf ceph-node3:/etc/security/ # 在三台虚拟机上分别执行以下命令，让内核参数生效，并重启 ~$ sysctl -p ~$ reboot","date":"2023-01-10","objectID":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/:4:7","tags":["分布式存储"],"title":"Ceph 集群部署 （二）","uri":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"},{"categories":["Ceph"],"content":"四、部署 RADOS 集群 ","date":"2023-01-10","objectID":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/:5:0","tags":["分布式存储"],"title":"Ceph 集群部署 （二）","uri":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"},{"categories":["Ceph"],"content":"4.1 安装Ceph-Deploy 部署工具 #查看可用版本 root@ceph-node1[15:45:17]~ #:apt-cache madison ceph-deploy ceph-deploy | 2.0.1-0ubuntu1.1 | https://mirrors.tuna.tsinghua.edu.cn/ubuntu focal-updates/universe amd64 Packages ceph-deploy | 2.0.1-0ubuntu1 | https://mirrors.tuna.tsinghua.edu.cn/ubuntu focal/universe amd64 Packages root@ceph-node1[15:45:17]~ #:apt install ceph-deploy -y","date":"2023-01-10","objectID":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/:5:1","tags":["分布式存储"],"title":"Ceph 集群部署 （二）","uri":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"},{"categories":["Ceph"],"content":"4.2 安装 python 2.7 root@ceph-mon1[09:49:37]~ apt install python2.7 #所有机器设置软链接 ln -sv /usr/bin/python2.7 /usr/bin/python2 #安装好后 执行python2测试 如果可以执行，说明安装好了 root@ceph-mon1[09:49:37]~ #:python2 Python 2.7.18 (default, Jul 1 2022, 12:27:04) [GCC 9.4.0] on linux2 Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. \u003e\u003e\u003e ","date":"2023-01-10","objectID":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/:5:2","tags":["分布式存储"],"title":"Ceph 集群部署 （二）","uri":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"},{"categories":["Ceph"],"content":"4.3 初始化Ceph-Deploy #切换到xceo账户创建集群目录 ~$su - xceo ~$ pwd /home/xceo ~$ mkdir ceph-cluster ~$ cd ceph-cluster/ ~/ceph-cluster$ #查看的结果如下： xceo@ceph-mon1:~/ceph-cluster$ ceph-deploy --help usage: ceph-deploy [-h] [-v | -q] [--version] [--username USERNAME] [--overwrite-conf] [--ceph-conf CEPH_CONF] COMMAND ... Easy Ceph deployment -^- / \\ |O o| ceph-deploy v2.0.1 ).-.( '/|||\\` | '|` | '|` Full documentation can be found at: http://ceph.com/ceph-deploy/docsceph-deploy 使用帮助 ceph-deploy --help\rnew：开始部署一个新的 ceph 存储集群，并生成 CLUSTER.conf 集群配置文件和 keyring 认证文件。 install: 在远程主机上安装 ceph 相关的软件包, 可以通过--release 指定安装的版本。 rgw：管理 RGW 守护程序(RADOSGW,对象存储网关)。 mgr：管理 MGR 守护程序(ceph-mgr,Ceph Manager DaemonCeph 管理器守护程序)。 mds：管理 MDS 守护程序(Ceph Metadata Server，ceph 源数据服务器)。 mon：管理 MON 守护程序(ceph-mon,ceph 监视器)。 gatherkeys：从指定获取提供新节点的验证 keys，这些 keys 会在添加新的 MON/OSD/MDS 加入的时候使用。 disk：管理远程主机磁盘。 osd：在远程主机准备数据磁盘，即将指定远程主机的指定磁盘添加到 ceph 集群作为 osd 使用。\rrepo：远程主机仓库管理。 admin：推送 ceph 集群配置文件和 client.admin 认证文件到远程主机。 config：将 ceph.conf 配置文件推送到远程主机或从远程主机拷贝。 uninstall：从远端主机删除安装包。 purgedata：从/var/lib/ceph 删除 ceph 数据,会删除/etc/ceph 下的内容。 purge: 删除远端主机的安装包和所有数据。 forgetkeys：从本地主机删除所有的验证 keyring, 包括 client.admin, monitor, bootstrap 等认证文件。 pkg：管理远端主机的安装包。 calamari：安装并配置一个 calamari web 节点，calamari 是一个 web 监控平台。","date":"2023-01-10","objectID":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/:5:3","tags":["分布式存储"],"title":"Ceph 集群部署 （二）","uri":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"},{"categories":["Ceph"],"content":"4.4 生成mon配置文件 在管理节点初始化mon节点 xceo@ceph-node1:~/ceph-cluster$ ceph-deploy new --cluster-network 192.168.10.0/24 --public-network 10.1.0.0/24 ceph-mon1.xx.local [ceph_deploy.conf][DEBUG ] found configuration file at: /home/xceo/.cephdeploy.conf [ceph_deploy.cli][INFO ] Invoked (2.0.1): /usr/bin/ceph-deploy new --cluster-network 192.168.10.0/24 --public-network 10.1.0.0/24 ceph-mon1.xx.local [ceph_deploy.cli][INFO ] ceph-deploy options: [ceph_deploy.cli][INFO ] verbose : False [ceph_deploy.cli][INFO ] quiet : False [ceph_deploy.cli][INFO ] username : None [ceph_deploy.cli][INFO ] overwrite_conf : False [ceph_deploy.cli][INFO ] ceph_conf : None [ceph_deploy.cli][INFO ] cluster : ceph [ceph_deploy.cli][INFO ] mon : ['ceph-mon1.xx.local'] [ceph_deploy.cli][INFO ] ssh_copykey : True [ceph_deploy.cli][INFO ] fsid : None [ceph_deploy.cli][INFO ] cluster_network : 192.168.10.0/24 [ceph_deploy.cli][INFO ] public_network : 10.1.0.0/24 [ceph_deploy.cli][INFO ] cd_conf : \u003cceph_deploy.conf.cephdeploy.Conf object at 0x7fa0a0be30a0\u003e [ceph_deploy.cli][INFO ] default_release : False [ceph_deploy.cli][INFO ] func : \u003cfunction new at 0x7fa0a0bdaf70\u003e [ceph_deploy.new][DEBUG ] Creating new cluster named ceph [ceph_deploy.new][INFO ] making sure passwordless SSH succeeds [ceph-mon1.xx.local][DEBUG ] connected to host: ceph-node1 [ceph-mon1.xx.local][INFO ] Running command: ssh -CT -o BatchMode=yes ceph-mon1.xx.local [ceph-mon1.xx.local][DEBUG ] connection detected need for sudo [ceph-mon1.xx.local][DEBUG ] connected to host: ceph-mon1.xx.local [ceph-mon1.xx.local][INFO ] Running command: sudo /bin/ip link show [ceph-mon1.xx.local][INFO ] Running command: sudo /bin/ip addr show [ceph-mon1.xx.local][DEBUG ] IP addresses found: ['10.1.0.39', '192.168.10.239'] [ceph_deploy.new][DEBUG ] Resolving host ceph-mon1.xx.local [ceph_deploy.new][DEBUG ] Monitor ceph-mon1 at 10.1.0.39 [ceph_deploy.new][DEBUG ] Monitor initial members are ['ceph-mon1'] [ceph_deploy.new][DEBUG ] Monitor addrs are ['10.1.0.39'] [ceph_deploy.new][DEBUG ] Creating a random mon key... [ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring... [ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...，是否生成配置文件 xceo@ceph-node1:~/ceph-cluster$ ll total 24 drwxrwxr-x 2 xceo xceo 4096 May 26 16:29 ./ drwxr-xr-x 5 xceo xceo 4096 May 26 16:15 ../ -rw-rw-r-- 1 xceo xceo 259 May 26 16:29 ceph.conf -rw-rw-r-- 1 xceo xceo 7307 May 26 16:29 ceph-deploy-ceph.log -rw------- 1 xceo xceo 73 May 26 16:29 ceph.mon.keyring xceo@ceph-node1:~/ceph-cluster$ cat ceph.conf [global] fsid = 31fdd971-2963-459b-9d6f-588f1811993f public_network = 10.1.0.0/24 cluster_network = 192.168.10.0/24 mon_initial_members = ceph-mon1 mon_host = 10.1.0.39 auth_cluster_required = cephx auth_service_required = cephx auth_client_required = cephx在安装过程中会报如下错误： [ceph_deploy][ERROR ] RuntimeError: AttributeError: module ‘platform’ has no attribute ’linux_distribution’ 这是由于python3.7后不再支持platform.linux_distribution 修改方法： 修改/usr/lib/python3/dist-packages/ceph_deploy/hosts/remotes.py文件为如下所示 def platform_information(_linux_distribution=None): \"\"\" detect platform information from remote host \"\"\" \"\"\" linux_distribution = _linux_distribution or platform.linux_distribution distro, release, codename = linux_distribution() \"\"\" distro = release = codename = None try: linux_distribution = _linux_distribution or platform.linux_distribution distro, release, codename = linux_distribution() except AttributeError: pass验证初始化完成之后，会得到三个文件 如下： xceo@ceph-node1:~/ceph-cluster$ ls ceph.conf ceph-deploy-ceph.log ceph.mon.keyring ------------------------------------------------- ceph.conf # 配置文件 ceph-deploy-ceph.log #部署的日志 ceph.mon.keyring #mon.keyring","date":"2023-01-10","objectID":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/:5:4","tags":["分布式存储"],"title":"Ceph 集群部署 （二）","uri":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"},{"categories":["Ceph"],"content":"4.5 初始化node节点 #no-adjust-repos 是不指定源 之前有配置好 #使用普通账户下操作 （在ceph-cluster下操作会有报错） xceo@ceph-mon1:~/ceph-cluster$ cd ~ ceph-deploy install --no-adjust-repos --nogpgcheck ceph-node1 ceph-node2 ceph-node3 （不指定版本） ceph-deploy install --no-adjust-repos --nogpgcheck --release pacific ceph-node1 ceph-node2 ceph-node3（指定版本）xceo@ceph-node1:~/ceph-cluster$ ceph-deploy install --no-adjust-repos --nogpgcheck --release pacific ceph-node1 ceph-node2 ceph-node3","date":"2023-01-10","objectID":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/:5:5","tags":["分布式存储"],"title":"Ceph 集群部署 （二）","uri":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"},{"categories":["Ceph"],"content":"4.5 初始化mon节点 在三个 mon 节点安装 ceph-mon #注意安装mon时hostname要和Ceph.conf配置文件中mon主机名一样 root@ceph-mon1[10:40:52]~ #:apt-cache madison ceph-mon ceph-mon | 16.2.13-1focal | https://download.ceph.com/debian-pacific focal/main amd64 Packages ceph-mon | 15.2.17-0ubuntu0.20.04.4 | http://mirrors.tuna.tsinghua.edu.cn/ubuntu focal-updates/main amd64 Packages ceph-mon | 15.2.17-0ubuntu0.20.04.3 | http://mirrors.tuna.tsinghua.edu.cn/ubuntu focal-security/main amd64 Packages ceph-mon | 15.2.1-0ubuntu1 | http://mirrors.tuna.tsinghua.edu.cn/ubuntu focal/main amd64 Packages #指定版本 apt -y install 16.2.13-1focal xceo@ceph-mon1:~/ceph-cluster$ ceph-deploy mon create-initial [ceph_deploy.conf][DEBUG ] found configuration file at: /home/xceo/.cephdeploy.conf [ceph_deploy.cli][INFO ] Invoked (2.0.1): /usr/bin/ceph-deploy mon create-initial [ceph_deploy.cli][INFO ] ceph-deploy options: [ceph_deploy.cli][INFO ] verbose : False [ceph_deploy.cli][INFO ] quiet : False [ceph_deploy.cli][INFO ] username : None [ceph_deploy.cli][INFO ] overwrite_conf : False [ceph_deploy.cli][INFO ] ceph_conf : None [ceph_deploy.cli][INFO ] cluster : ceph [ceph_deploy.cli][INFO ] subcommand : create-initial [ceph_deploy.cli][INFO ] cd_conf : \u003cceph_deploy.conf.cephdeploy.Conf object at 0x7fb775ca7a60\u003e [ceph_deploy.cli][INFO ] default_release : False [ceph_deploy.cli][INFO ] func : \u003cfunction mon at 0x7fb775d12700\u003e [ceph_deploy.cli][INFO ] keyrings : None [ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts ceph-mon1 [ceph_deploy.mon][DEBUG ] detecting platform for host ceph-mon1 ... [ceph-mon1][DEBUG ] connection detected need for sudo [ceph-mon1][DEBUG ] connected to host: ceph-mon1 [ceph_deploy.mon][INFO ] distro info: ubuntu 20.04 focal [ceph-mon1][DEBUG ] determining if provided host has same hostname in remote [ceph-mon1][DEBUG ] deploying mon to ceph-mon1 [ceph-mon1][DEBUG ] remote hostname: ceph-mon1 [ceph-mon1][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-ceph-mon1/done [ceph-mon1][DEBUG ] done path does not exist: /var/lib/ceph/mon/ceph-ceph-mon1/done [ceph-mon1][INFO ] creating keyring file: /var/lib/ceph/tmp/ceph-ceph-mon1.mon.keyring [ceph-mon1][INFO ] Running command: sudo ceph-mon --cluster ceph --mkfs -i ceph-mon1 --keyring /var/lib/ceph/tmp/ceph-ceph-mon1.mon.keyring --setuser 64045 --setgroup 64045 [ceph-mon1][INFO ] unlinking keyring file /var/lib/ceph/tmp/ceph-ceph-mon1.mon.keyring [ceph-mon1][INFO ] Running command: sudo systemctl enable ceph.target [ceph-mon1][INFO ] Running command: sudo systemctl enable ceph-mon@ceph-mon1 [ceph-mon1][WARNIN] Created symlink /etc/systemd/system/ceph-mon.target.wants/ceph-mon@ceph-mon1.service → /lib/systemd/system/ceph-mon@.service. [ceph-mon1][INFO ] Running command: sudo systemctl start ceph-mon@ceph-mon1 [ceph-mon1][INFO ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.ceph-mon1.asok mon_status [ceph-mon1][DEBUG ] ******************************************************************************** [ceph-mon1][DEBUG ] status for monitor: mon.ceph-mon1 [ceph-mon1][DEBUG ] { [ceph-mon1][DEBUG ] \"election_epoch\": 3, [ceph-mon1][DEBUG ] \"extra_probe_peers\": [], [ceph-mon1][DEBUG ] \"feature_map\": { [ceph-mon1][DEBUG ] \"mon\": [ [ceph-mon1][DEBUG ] { [ceph-mon1][DEBUG ] \"features\": \"0x3f01cfbdfffdffff\", [ceph-mon1][DEBUG ] \"num\": 1, [ceph-mon1][DEBUG ] \"release\": \"luminous\" [ceph-mon1][DEBUG ] } [ceph-mon1][DEBUG ] ] [ceph-mon1][DEBUG ] }, [ceph-mon1][DEBUG ] \"features\": { [ceph-mon1][DEBUG ] \"quorum_con\": \"4540138314316775423\", [ceph-mon1][DEBUG ] \"quorum_mon\": [ [ceph-mon1][DEBUG ] \"kraken\", [ceph-mon1][DEBUG ] \"luminous\", [ceph-mon1][DEBUG ] \"mimic\", [ceph-mon1][DEBUG ] \"osdmap-prune\", [ceph-mon1][DEBUG ] \"nautilus\", [ceph-mon1][DEBUG ] \"octopus\", [ceph-mon1][DEBUG ] \"pacific\", [ceph-mon1][DEBUG ] \"elector-pinging\" [ceph-mon1][DEBUG ] ], [ceph-mon1][DEBUG ] \"required_con\": \"2449958747317026820\", [ceph-mon1][DEBUG ] \"required_mon\": [ [ceph-mon1][DEBUG ] \"kraken\", [ceph-mon1][DEBUG ] \"lum","date":"2023-01-10","objectID":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/:5:6","tags":["分布式存储"],"title":"Ceph 集群部署 （二）","uri":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"},{"categories":["Ceph"],"content":"4.6 分发admin密钥到node节点（非必须） 保存好生成的ceph.client.admin.keyring文件 在deploy和node节点上安装 ceph- common sudo apt install ceph-common -yxceo@ceph-mon1:~/ceph-cluster$ ceph-deploy --overwrite-conf admin ceph-node1 ceph-node2 ceph-node3 [ceph_deploy.conf][DEBUG ] found configuration file at: /home/xceo/.cephdeploy.conf [ceph_deploy.cli][INFO ] Invoked (2.0.1): /usr/bin/ceph-deploy --overwrite-conf admin ceph-node1 ceph-node2 ceph-node3 [ceph_deploy.cli][INFO ] ceph-deploy options: [ceph_deploy.cli][INFO ] verbose : False [ceph_deploy.cli][INFO ] quiet : False [ceph_deploy.cli][INFO ] username : None [ceph_deploy.cli][INFO ] overwrite_conf : True [ceph_deploy.cli][INFO ] ceph_conf : None [ceph_deploy.cli][INFO ] cluster : ceph [ceph_deploy.cli][INFO ] client : ['ceph-node1', 'ceph-node2', 'ceph-node3'] [ceph_deploy.cli][INFO ] cd_conf : \u003cceph_deploy.conf.cephdeploy.Conf object at 0x7f19678e51f0\u003e [ceph_deploy.cli][INFO ] default_release : False [ceph_deploy.cli][INFO ] func : \u003cfunction admin at 0x7f1967e2f040\u003e [ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph-node1 [ceph-node1][DEBUG ] connection detected need for sudo [ceph-node1][DEBUG ] connected to host: ceph-node1 [ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph-node2 [ceph-node2][DEBUG ] connection detected need for sudo [ceph-node2][DEBUG ] connected to host: ceph-node2 [ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph-node3 [ceph-node3][DEBUG ] connection detected need for sudo [ceph-node3][DEBUG ] connected to host: ceph-node3 认证文件的属主和属组为了安全考虑，默认设置为了root 用户和root组，如果需要ceph用户也能执行ceph 命令,那么就需要对ceph 用户进行授权 setfacl -m u:ceph:rw /etc/ceph/ceph.client.admin.keyring 在deploy主机执行 xceo@ceph-mon1:~$ sudo apt install acl root@ceph-mon1[14:11:31]~ #:setfacl -m u:xceo:rw /etc/ceph/ceph.client.admin.keyring xceo@ceph-mon1:~$ ceph -s cluster: id: 62be32df-9cb4-474f-8727-d5c4bbceaf97 health: HEALTH_WARN mon is allowing insecure global_id reclaim services: mon: 1 daemons, quorum ceph-mon1 (age 112m) mgr: no daemons active osd: 0 osds: 0 up, 0 in data: pools: 0 pools, 0 pgs objects: 0 objects, 0 B usage: 0 B used, 0 B / 0 B avail pgs: ","date":"2023-01-10","objectID":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/:5:7","tags":["分布式存储"],"title":"Ceph 集群部署 （二）","uri":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"},{"categories":["Ceph"],"content":"4.7 部署Ceph-Mgr节点 mgr节点需要读取ceph的配置文件，即/etc/ceph目录中的配置文件 #在mgr节点提前将mgr服务安装 apt install ceph-mgr -y xceo@ceph-mon1:~/ceph-cluster$ ceph-deploy mgr create ceph-mgr1 [ceph_deploy.conf][DEBUG ] found configuration file at: /home/xceo/.cephdeploy.conf [ceph_deploy.cli][INFO ] Invoked (2.0.1): /usr/bin/ceph-deploy mgr create ceph-mgr1 [ceph_deploy.cli][INFO ] ceph-deploy options: [ceph_deploy.cli][INFO ] verbose : False [ceph_deploy.cli][INFO ] quiet : False [ceph_deploy.cli][INFO ] username : None [ceph_deploy.cli][INFO ] overwrite_conf : False [ceph_deploy.cli][INFO ] ceph_conf : None [ceph_deploy.cli][INFO ] cluster : ceph [ceph_deploy.cli][INFO ] subcommand : create [ceph_deploy.cli][INFO ] cd_conf : \u003cceph_deploy.conf.cephdeploy.Conf object at 0x7fd8738734c0\u003e [ceph_deploy.cli][INFO ] default_release : False [ceph_deploy.cli][INFO ] func : \u003cfunction mgr at 0x7fd87392ec10\u003e [ceph_deploy.cli][INFO ] mgr : [('ceph-mgr1', 'ceph-mgr1')] [ceph_deploy.mgr][DEBUG ] Deploying mgr, cluster ceph hosts ceph-mgr1:ceph-mgr1 The authenticity of host 'ceph-mgr1 (10.1.0.40)' can't be established. ECDSA key fingerprint is SHA256:lhRjKQBhgEhjbqcfKBb6oyle8C9EIOzu48QUoaeISIE. Are you sure you want to continue connecting (yes/no/[fingerprint])? yes Warning: Permanently added 'ceph-mgr1' (ECDSA) to the list of known hosts. [ceph-mgr1][DEBUG ] connection detected need for sudo [ceph-mgr1][DEBUG ] connected to host: ceph-mgr1 [ceph_deploy.mgr][INFO ] Distro info: ubuntu 20.04 focal [ceph_deploy.mgr][DEBUG ] remote host will use systemd [ceph_deploy.mgr][DEBUG ] deploying mgr bootstrap to ceph-mgr1 [ceph-mgr1][WARNIN] mgr keyring does not exist yet, creating one [ceph-mgr1][INFO ] Running command: sudo ceph --cluster ceph --name client.bootstrap-mgr --keyring /var/lib/ceph/bootstrap-mgr/ceph.keyring auth get-or-create mgr.ceph-mgr1 mon allow profile mgr osd allow * mds allow * -o /var/lib/ceph/mgr/ceph-ceph-mgr1/keyring [ceph-mgr1][INFO ] Running command: sudo systemctl enable ceph-mgr@ceph-mgr1 [ceph-mgr1][WARNIN] Created symlink /etc/systemd/system/ceph-mgr.target.wants/ceph-mgr@ceph-mgr1.service → /lib/systemd/system/ceph-mgr@.service. [ceph-mgr1][INFO ] Running command: sudo systemctl start ceph-mgr@ceph-mgr1 [ceph-mgr1][INFO ] Running command: sudo systemctl enable ceph.target xceo@ceph-mon1:~/ceph-cluster$ ceph-deploy mgr create ceph-mgr2 [ceph_deploy.conf][DEBUG ] found configuration file at: /home/xceo/.cephdeploy.conf [ceph_deploy.cli][INFO ] Invoked (2.0.1): /usr/bin/ceph-deploy mgr create ceph-mgr2 [ceph_deploy.cli][INFO ] ceph-deploy options: [ceph_deploy.cli][INFO ] verbose : False [ceph_deploy.cli][INFO ] quiet : False [ceph_deploy.cli][INFO ] username : None [ceph_deploy.cli][INFO ] overwrite_conf : False [ceph_deploy.cli][INFO ] ceph_conf : None [ceph_deploy.cli][INFO ] cluster : ceph [ceph_deploy.cli][INFO ] subcommand : create [ceph_deploy.cli][INFO ] cd_conf : \u003cceph_deploy.conf.cephdeploy.Conf object at 0x7f5b7f96b4c0\u003e [ceph_deploy.cli][INFO ] default_release : False [ceph_deploy.cli][INFO ] func : \u003cfunction mgr at 0x7f5b7fa26c10\u003e [ceph_deploy.cli][INFO ] mgr : [('ceph-mgr2', 'ceph-mgr2')] [ceph_deploy.mgr][DEBUG ] Deploying mgr, cluster ceph hosts ceph-mgr2:ceph-mgr2 The authenticity of host 'ceph-mgr2 (10.1.0.41)' can't be established. ECDSA key fingerprint is SHA256:lhRjKQBhgEhjbqcfKBb6oyle8C9EIOzu48QUoaeISIE. Are you sure you want to continue connecting (yes/no/[fingerprint])? yes Warning: Permanently added 'ceph-mgr2' (ECDSA) to the list of known hosts. [ceph-mgr2][DEBUG ] connection detected need for sudo [ceph-mgr2][DEBUG ] connected to host: ceph-mgr2 [ceph_deploy.mgr][INFO ] Distro info: ubuntu 20.04 focal [ceph_deploy.mgr][DEBUG ] remote host will use systemd [ceph_deploy.mgr][DEBUG ] deploying mgr bootstrap to ceph-mgr2 [ceph-mgr2][WARNIN] mgr keyring does not exist yet, creating one [ceph-mgr2][INFO ] Running command: sudo ceph --cluster ceph --name client.bootstrap-mgr --keyring /var/lib/ceph/bootstrap-mgr/ceph.keyring auth ge","date":"2023-01-10","objectID":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/:5:8","tags":["分布式存储"],"title":"Ceph 集群部署 （二）","uri":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"},{"categories":["Ceph"],"content":"4.8 初始化 OSD 节点 #在deploy主机执行 ceph-deploy install --release pacific ceph-node1 ceph-node2 ceph-node3 #这步在初始化node节点已经做过 #擦除磁盘之前通过deploy节点对node节点执行安装ceph基本运行环境 报错 File \"/usr/lib/python3/dist-packages/ceph_deploy/util/decorators.py\", line 69, in newfunc #列出远端存储node节点的磁盘信息 xceo@ceph-mon1:~/ceph-cluster$ sudo ceph-deploy disk list ceph-node2 [ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf [ceph_deploy.cli][INFO ] Invoked (2.0.1): /usr/bin/ceph-deploy disk list ceph-node2 [ceph_deploy.cli][INFO ] ceph-deploy options: [ceph_deploy.cli][INFO ] verbose : False [ceph_deploy.cli][INFO ] quiet : False [ceph_deploy.cli][INFO ] username : None [ceph_deploy.cli][INFO ] overwrite_conf : False [ceph_deploy.cli][INFO ] ceph_conf : None [ceph_deploy.cli][INFO ] cluster : ceph [ceph_deploy.cli][INFO ] subcommand : list [ceph_deploy.cli][INFO ] cd_conf : \u003cceph_deploy.conf.cephdeploy.Conf object at 0x7ff31b4353a0\u003e [ceph_deploy.cli][INFO ] default_release : False [ceph_deploy.cli][INFO ] func : \u003cfunction disk at 0x7ff31b4d2a60\u003e [ceph_deploy.cli][INFO ] host : ['ceph-node2'] [ceph_deploy.cli][INFO ] debug : False The authenticity of host 'ceph-node2 (10.1.0.40)' can't be established. ECDSA key fingerprint is SHA256:lhRjKQBhgEhjbqcfKBb6oyle8C9EIOzu48QUoaeISIE. Are you sure you want to continue connecting (yes/no/[fingerprint])? ^C[ceph_deploy][ERROR ] KeyboardInterrupt xceo@ceph-mon1:~/ceph-cluster$ ceph-deploy disk list ceph-node2 [ceph_deploy.conf][DEBUG ] found configuration file at: /home/xceo/.cephdeploy.conf [ceph_deploy.cli][INFO ] Invoked (2.0.1): /usr/bin/ceph-deploy disk list ceph-node2 [ceph_deploy.cli][INFO ] ceph-deploy options: [ceph_deploy.cli][INFO ] verbose : False [ceph_deploy.cli][INFO ] quiet : False [ceph_deploy.cli][INFO ] username : None [ceph_deploy.cli][INFO ] overwrite_conf : False [ceph_deploy.cli][INFO ] ceph_conf : None [ceph_deploy.cli][INFO ] cluster : ceph [ceph_deploy.cli][INFO ] subcommand : list [ceph_deploy.cli][INFO ] cd_conf : \u003cceph_deploy.conf.cephdeploy.Conf object at 0x7f4c4cac53d0\u003e [ceph_deploy.cli][INFO ] default_release : False [ceph_deploy.cli][INFO ] func : \u003cfunction disk at 0x7f4c4cb62af0\u003e [ceph_deploy.cli][INFO ] host : ['ceph-node2'] [ceph_deploy.cli][INFO ] debug : False [ceph-node2][DEBUG ] connection detected need for sudo [ceph-node2][DEBUG ] connected to host: ceph-node2 [ceph-node2][INFO ] Running command: sudo fdisk -l [ceph_deploy][ERROR ] Traceback (most recent call last): [ceph_deploy][ERROR ] File \"/usr/lib/python3/dist-packages/ceph_deploy/util/decorators.py\", line 69, in newfunc [ceph_deploy][ERROR ] return f(*a, **kw) [ceph_deploy][ERROR ] File \"/usr/lib/python3/dist-packages/ceph_deploy/cli.py\", line 166, in _main [ceph_deploy][ERROR ] return args.func(args) [ceph_deploy][ERROR ] File \"/usr/lib/python3/dist-packages/ceph_deploy/osd.py\", line 434, in disk [ceph_deploy][ERROR ] disk_list(args, cfg) [ceph_deploy][ERROR ] File \"/usr/lib/python3/dist-packages/ceph_deploy/osd.py\", line 375, in disk_list [ceph_deploy][ERROR ] if line.startswith('Disk /'): [ceph_deploy][ERROR ] TypeError: startswith first arg must be bytes or a tuple of bytes, not str [ceph_deploy][ERROR ] 解决方法 sudo vim /usr/lib/python3/dist-packages/ceph_deploy/osd.py中 if line.startswith('Disk /'): #替换为 if line.startswith(b'Disk /'): ceph-deploy disk list ceph-node1 [ceph_deploy.conf][DEBUG ] found configuration file at: /home/xceo/.cephdeploy.conf [ceph_deploy.cli][INFO ] Invoked (2.0.1): /usr/bin/ceph-deploy disk list ceph-node1 [ceph_deploy.cli][INFO ] ceph-deploy options: [ceph_deploy.cli][INFO ] verbose : False [ceph_deploy.cli][INFO ] quiet : False [ceph_deploy.cli][INFO ] username : None [ceph_deploy.cli][INFO ] overwrite_conf : False [ceph_deploy.cli][INFO ] ceph_conf : None [ceph_deploy.cli][INFO ] cluster : ceph [ceph_deploy.cli][INFO ] subcommand : list [ceph_deploy.cli][INFO ] cd_conf : \u003cceph_deploy.conf.cephdeploy.Conf object at 0x7f90a9868310\u003e [ceph_deploy.cli][INFO ] default_release : ","date":"2023-01-10","objectID":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/:5:9","tags":["分布式存储"],"title":"Ceph 集群部署 （二）","uri":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"},{"categories":["Ceph"],"content":"4.9 添加OSD 节点 Data：即ceph保存的对象数据 block：rocks DB 数据即元数据 block-wal： 数据库的 wal 日志 添加OSD节点前的告警 #添加磁盘 xceo@ceph-mon1:~/ceph-cluster$ ceph-deploy osd create ceph-node1 --data /dev/vdb [ceph_deploy.conf][DEBUG ] found configuration file at: /home/xceo/.cephdeploy.conf [ceph_deploy.cli][INFO ] Invoked (2.0.1): /usr/bin/ceph-deploy osd create ceph-node1 --data /dev/vdb [ceph_deploy.cli][INFO ] ceph-deploy options: [ceph_deploy.cli][INFO ] verbose : False [ceph_deploy.cli][INFO ] quiet : False [ceph_deploy.cli][INFO ] username : None [ceph_deploy.cli][INFO ] overwrite_conf : False [ceph_deploy.cli][INFO ] ceph_conf : None [ceph_deploy.cli][INFO ] cluster : ceph [ceph_deploy.cli][INFO ] subcommand : create [ceph_deploy.cli][INFO ] cd_conf : \u003cceph_deploy.conf.cephdeploy.Conf object at 0x7fef2b8bbfa0\u003e [ceph_deploy.cli][INFO ] default_release : False [ceph_deploy.cli][INFO ] func : \u003cfunction osd at 0x7fef2b959a60\u003e [ceph_deploy.cli][INFO ] data : /dev/vdb [ceph_deploy.cli][INFO ] journal : None [ceph_deploy.cli][INFO ] zap_disk : False [ceph_deploy.cli][INFO ] fs_type : xfs [ceph_deploy.cli][INFO ] dmcrypt : False [ceph_deploy.cli][INFO ] dmcrypt_key_dir : /etc/ceph/dmcrypt-keys [ceph_deploy.cli][INFO ] filestore : None [ceph_deploy.cli][INFO ] bluestore : None [ceph_deploy.cli][INFO ] block_db : None [ceph_deploy.cli][INFO ] block_wal : None [ceph_deploy.cli][INFO ] host : ceph-node1 [ceph_deploy.cli][INFO ] debug : False [ceph_deploy.osd][DEBUG ] Creating OSD on cluster ceph with data device /dev/vdb [ceph-node1][DEBUG ] connection detected need for sudo [ceph-node1][DEBUG ] connected to host: ceph-node1 [ceph_deploy.osd][INFO ] Distro info: ubuntu 20.04 focal [ceph_deploy.osd][DEBUG ] Deploying osd to ceph-node1 [ceph-node1][INFO ] Running command: sudo /usr/sbin/ceph-volume --cluster ceph lvm create --bluestore --data /dev/vdb [ceph-node1][WARNIN] Running command: /usr/bin/ceph-authtool --gen-print-key [ceph-node1][WARNIN] Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring -i - osd new d9637751-9b7f-42d6-b9d8-2718039162b5 [ceph-node1][WARNIN] Running command: vgcreate --force --yes ceph-ac4ab09d-3655-4447-a0e3-d0d79f3b326a /dev/vdb [ceph-node1][WARNIN] stdout: Physical volume \"/dev/vdb\" successfully created. [ceph-node1][WARNIN] stdout: Volume group \"ceph-ac4ab09d-3655-4447-a0e3-d0d79f3b326a\" successfully created [ceph-node1][WARNIN] Running command: lvcreate --yes -l 30719 -n osd-block-d9637751-9b7f-42d6-b9d8-2718039162b5 ceph-ac4ab09d-3655-4447-a0e3-d0d79f3b326a [ceph-node1][WARNIN] stdout: Logical volume \"osd-block-d9637751-9b7f-42d6-b9d8-2718039162b5\" created. [ceph-node1][WARNIN] Running command: /usr/bin/ceph-authtool --gen-print-key [ceph-node1][WARNIN] Running command: /usr/bin/mount -t tmpfs tmpfs /var/lib/ceph/osd/ceph-0 [ceph-node1][WARNIN] --\u003e Executable selinuxenabled not in PATH: /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin [ceph-node1][WARNIN] Running command: /usr/bin/chown -h ceph:ceph /dev/ceph-ac4ab09d-3655-4447-a0e3-d0d79f3b326a/osd-block-d9637751-9b7f-42d6-b9d8-2718039162b5 [ceph-node1][WARNIN] Running command: /usr/bin/chown -R ceph:ceph /dev/dm-0 [ceph-node1][WARNIN] Running command: /usr/bin/ln -s /dev/ceph-ac4ab09d-3655-4447-a0e3-d0d79f3b326a/osd-block-d9637751-9b7f-42d6-b9d8-2718039162b5 /var/lib/ceph/osd/ceph-0/block [ceph-node1][WARNIN] Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring mon getmap -o /var/lib/ceph/osd/ceph-0/activate.monmap [ceph-node1][WARNIN] stderr: 2023-05-29T14:48:36.998+0800 7f3b7f4e2700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.bootstrap-osd.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory [ceph-node1][WARNIN] 2023-05-29T14:48:36.998+0800 7f3b7f4e2700 -1 AuthRegistry(0x7f3b7805bc18) no keyr","date":"2023-01-10","objectID":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/:5:10","tags":["分布式存储"],"title":"Ceph 集群部署 （二）","uri":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"},{"categories":["Ceph"],"content":"五、 验证 Ceph 集群 ","date":"2023-01-10","objectID":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/:6:0","tags":["分布式存储"],"title":"Ceph 集群部署 （二）","uri":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"},{"categories":["Ceph"],"content":"5.1 从RADOS 移除OSD 这个步骤主要做了解，一般不常用 Ceph集群中的一个OSD是一个node节点的服务进程且对应于一个物理磁盘设备，是一个专用的守护进程。 在某OSD设备出现故障，或管理员处于管理之需确实要移除特定的OSD设备时，需要先停止相关的守护进程，而后再进行移除操作，对于linux以及以后的版本来说，停止和移除命令的格式分别如下： 停用设备： ceph osd out {osd-num} 停止进程： sudo systemctl stop ceph-osd@{osd-num} 移除设备： ceph osd purge {id} --yes-i-really-mean-itOSD 的配置信息存在于 ceph.conf 配置文件中， 管理员在删除 OSD 之后手动将其删除 #于 CRUSH 运行图中移除设备 ceph osd crush remove {name} #移除 OSD 的认证 key： ceph auth del osd.{osd-num} #最后移除 OSD 设备： ceph osd rm {osd-num}","date":"2023-01-10","objectID":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/:6:1","tags":["分布式存储"],"title":"Ceph 集群部署 （二）","uri":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"},{"categories":["Ceph"],"content":"5.2 测试上传和下载数据 RADOS命令 存取数据时， 客户端必须首先连接至 RADOS 集群上某存储池， 然后根据对象名称由相关的CRUSH 规则完成数据对象寻址。 于是， 为了测试集群的数据存取功能， 这里首先创建一个用于测试的存储池 mypool， 并设定其 PG 数量为 32 个 创建pool root@ceph-mon1[20:03:36]~ #:ceph osd pool create mypool 32 32 #创建 pool 'mypool' created #或者rados lspools #验证 root@ceph-mon1[20:44:54]~ #:ceph osd pool ls device_health_metrics mypool ceph pg ls-by-pool mypool | awk '{print $1,$2,$15}' #验证PG 与 PGP 组合 root@ceph-mon1[20:45:22]~ #:ceph pg ls-by-pool mypool | awk '{print $1,$2,$15}' PG OBJECTS ACTING 2.0 0 [8,10,3]p8 2.1 0 [2,13,9]p2 2.2 0 [5,1,10]p5 2.3 0 [5,4,14]p5 2.4 0 [1,12,6]p1 2.5 0 [12,4,8]p12 2.6 0 [1,13,9]p1 2.7 0 [6,13,2]p6 2.8 0 [8,13,0]p8 2.9 0 [4,9,12]p4 2.a 0 [11,4,8]p11 2.b 0 [13,7,4]p13 2.c 0 [12,0,5]p12 2.d 0 [12,8,3]p12 2.e 0 [2,13,8]p2 2.f 0 [11,8,0]p11 2.10 0 [10,1,8]p10 2.11 0 [6,1,12]p6 2.12 0 [10,3,9]p10 2.13 0 [13,6,3]p13 2.14 0 [8,13,0]p8 2.15 0 [10,1,5]p10 2.16 0 [8,12,1]p8 2.17 0 [6,14,2]p6 2.18 0 [13,9,2]p13 2.19 0 [3,6,13]p3 2.1a 0 [6,14,2]p6 2.1b 0 [11,7,3]p11 2.1c 0 [10,7,1]p10 2.1d 0 [10,7,0]p10 2.1e 0 [3,13,5]p3 2.1f 0 [4,7,14]p4 * NOTE: afterwards上传文件 sudo rados put msg1 /var/log/syslog --pool=mypool #把消息文件上传到 mypool 并指定对象 id 为 msg1 rados ls --pool=mypool #列出文件 root@ceph-mon1[20:58:42]~ #:rados ls --pool=mypool msg1 #可以获取到存储池中数据对象的具体位置信息： root@ceph-mon1[21:00:22]~ #:ceph osd map mypool msg1 osdmap e86 pool 'mypool' (2) object 'msg1' -\u003e pg 2.c833d430 (2.10) -\u003e up ([10,1,8], p10) acting ([10,1,8], p10) ------------------------------------------------------------------------------- 表示文件存储了存储池 id 为 2 的 c833d430 的 PG 上,10 为当前 PG 的 id, 2.10 表示数据是在 id 为 2 的存储池中 id 为 10 的 PG 中存储，在线的 OSD 编号 10,1,8，主 OSD 为 10，活动 的OSD 10,1,8，三个OSD表示数据放一共3个副本，PG中的OSD是ceph的crush算法计算 算出三份数据保存在哪些OSD。 下载文件 #下载mypool里的msg1文件 放在本地目录的名字 sudo rados get msg1 --pool=mypool /opt/my.txt xceo@ceph-mon1:~/ceph-cluster$ sudo rados get msg1 --pool=mypool /opt/my.txt xceo@ceph-mon1:~/ceph-cluster$ ll -ls /opt/my.txt 860 -rw-r--r-- 1 root root 879881 May 30 09:10 /opt/my.txt修改文件 #将/etc/passwd 上传到mypool下的 msg1文件中 xceo@ceph-mon1:~/ceph-cluster$ sudo rados put msg1 /etc/passwd --pool=mypool #重新从mypool里下载msg1 并存放本地目录重命名为/opt/my3.txt xceo@ceph-mon1:~/ceph-cluster$ sudo rados get msg1 --pool=mypool /opt/my3.txt xceo@ceph-mon1:~/ceph-cluster$ head /opt/my3.txt root:x:0:0:root:/root:/bin/bash daemon:x:1:1:daemon:/usr/sbin:/usr/sbin/nologin bin:x:2:2:bin:/bin:/usr/sbin/nologin sys:x:3:3:sys:/dev:/usr/sbin/nologin sync:x:4:65534:sync:/bin:/bin/sync games:x:5:60:games:/usr/games:/usr/sbin/nologin man:x:6:12:man:/var/cache/man:/usr/sbin/nologin lp:x:7:7:lp:/var/spool/lpd:/usr/sbin/nologin mail:x:8:8:mail:/var/mail:/usr/sbin/nologin news:x:9:9:news:/var/spool/news:/usr/sbin/nologin删除文件 sudo rados rm msg1 --pool=mypool rados ls --pool=mypool","date":"2023-01-10","objectID":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/:6:2","tags":["分布式存储"],"title":"Ceph 集群部署 （二）","uri":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"},{"categories":["Ceph"],"content":"六、扩展 ceph 实现高可用 主要是扩展ceph集群的mon节点以及mgr节点 以实现集群高可用 ","date":"2023-01-10","objectID":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/:7:0","tags":["分布式存储"],"title":"Ceph 集群部署 （二）","uri":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"},{"categories":["Ceph"],"content":"6.1 扩展 ceph-mon 节点 在mon2 mon3 上安装ceph-mon Ceph-mon 是原生具备自噬以实现高可用机制的 ceph 服务，节点数量通常是奇数 #ubuntu上安装指定版本 apt install ceph-mon=16.2.13-1focal #在ceph-deploy操作将mon3添加 ceph-deploy mon add ceph-mon2 ceph-mon3 #添加ceph-mon2 xceo@ceph-mon1:~/ceph-cluster$ ceph-deploy mon add ceph-mon2 [ceph_deploy.conf][DEBUG ] found configuration file at: /home/xceo/.cephdeploy.conf [ceph_deploy.cli][INFO ] Invoked (2.0.1): /usr/bin/ceph-deploy mon add ceph-mon2 [ceph_deploy.cli][INFO ] ceph-deploy options: [ceph_deploy.cli][INFO ] verbose : False [ceph_deploy.cli][INFO ] quiet : False [ceph_deploy.cli][INFO ] username : None [ceph_deploy.cli][INFO ] overwrite_conf : False [ceph_deploy.cli][INFO ] ceph_conf : None [ceph_deploy.cli][INFO ] cluster : ceph [ceph_deploy.cli][INFO ] subcommand : add [ceph_deploy.cli][INFO ] cd_conf : \u003cceph_deploy.conf.cephdeploy.Conf object at 0x7f4eea93da90\u003e [ceph_deploy.cli][INFO ] default_release : False [ceph_deploy.cli][INFO ] func : \u003cfunction mon at 0x7f4eea9a8700\u003e [ceph_deploy.cli][INFO ] address : None [ceph_deploy.cli][INFO ] mon : ['ceph-mon2'] [ceph_deploy.mon][INFO ] ensuring configuration of new mon host: ceph-mon2 [ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph-mon2 [ceph-mon2][DEBUG ] connection detected need for sudo [ceph-mon2][DEBUG ] connected to host: ceph-mon2 [ceph_deploy.mon][DEBUG ] Adding mon to cluster ceph, host ceph-mon2 [ceph_deploy.mon][DEBUG ] using mon address by resolving host: 10.1.0.40 [ceph_deploy.mon][DEBUG ] detecting platform for host ceph-mon2 ... [ceph-mon2][DEBUG ] connection detected need for sudo [ceph-mon2][DEBUG ] connected to host: ceph-mon2 [ceph_deploy.mon][INFO ] distro info: ubuntu 20.04 focal [ceph-mon2][DEBUG ] determining if provided host has same hostname in remote [ceph-mon2][DEBUG ] adding mon to ceph-mon2 [ceph-mon2][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-ceph-mon2/done [ceph-mon2][INFO ] Running command: sudo systemctl enable ceph.target [ceph-mon2][INFO ] Running command: sudo systemctl enable ceph-mon@ceph-mon2 [ceph-mon2][INFO ] Running command: sudo systemctl start ceph-mon@ceph-mon2 [ceph-mon2][INFO ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.ceph-mon2.asok mon_status [ceph-mon2][WARNIN] ceph-mon2 is not defined in `mon initial members` [ceph-mon2][WARNIN] monitor ceph-mon2 does not exist in monmap [ceph-mon2][INFO ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.ceph-mon2.asok mon_status [ceph-mon2][DEBUG ] ******************************************************************************** [ceph-mon2][DEBUG ] status for monitor: mon.ceph-mon2 [ceph-mon2][DEBUG ] { [ceph-mon2][DEBUG ] \"election_epoch\": 0, [ceph-mon2][DEBUG ] \"extra_probe_peers\": [], [ceph-mon2][DEBUG ] \"feature_map\": { [ceph-mon2][DEBUG ] \"mon\": [ [ceph-mon2][DEBUG ] { [ceph-mon2][DEBUG ] \"features\": \"0x3f01cfbdfffdffff\", [ceph-mon2][DEBUG ] \"num\": 1, [ceph-mon2][DEBUG ] \"release\": \"luminous\" [ceph-mon2][DEBUG ] } [ceph-mon2][DEBUG ] ] [ceph-mon2][DEBUG ] }, [ceph-mon2][DEBUG ] \"features\": { [ceph-mon2][DEBUG ] \"quorum_con\": \"0\", [ceph-mon2][DEBUG ] \"quorum_mon\": [], [ceph-mon2][DEBUG ] \"required_con\": \"2449958197560098820\", [ceph-mon2][DEBUG ] \"required_mon\": [ [ceph-mon2][DEBUG ] \"kraken\", [ceph-mon2][DEBUG ] \"luminous\", [ceph-mon2][DEBUG ] \"mimic\", [ceph-mon2][DEBUG ] \"osdmap-prune\", [ceph-mon2][DEBUG ] \"nautilus\", [ceph-mon2][DEBUG ] \"octopus\", [ceph-mon2][DEBUG ] \"pacific\", [ceph-mon2][DEBUG ] \"elector-pinging\" [ceph-mon2][DEBUG ] ] [ceph-mon2][DEBUG ] }, [ceph-mon2][DEBUG ] \"monmap\": { [ceph-mon2][DEBUG ] \"created\": \"2023-05-29T04:19:43.502249Z\", [ceph-mon2][DEBUG ] \"disallowed_leaders: \": \"\", [ceph-mon2][DEBUG ] \"election_strategy\": 1, [ceph-mon2][DEBUG ] \"epoch\": 1, [ceph-mon2][DEBUG ] \"features\": { [ceph-mon2][DEBUG ] \"optional\": [], [ceph-mon2][DEBUG ] \"persistent\": [ [ceph-mon2][DEBUG ] \"kraken\", [ceph-mon2][DEBUG ] \"luminous\", [ceph-mon2][DEBUG ] \"mimic\", [ceph-mon2][DEBUG ] \"osdmap-pru","date":"2023-01-10","objectID":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/:7:1","tags":["分布式存储"],"title":"Ceph 集群部署 （二）","uri":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"},{"categories":["Ceph"],"content":"6.2 扩展Ceph-Mgr 节点 #在mgr主机安装mgr apt install ceph-mgr -y # 同步配 置文件到ceph-mg2 节点 ceph-deploy admin ceph-mgr2 ceph-deploy mgr create ceph-mgr2 ","date":"2023-01-10","objectID":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/:7:2","tags":["分布式存储"],"title":"Ceph 集群部署 （二）","uri":"/posts/ceph/2.ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"},{"categories":["Kubernetes"],"content":"前言 Kubernetes二进制部署相对于使用自动化工具（如kubeadm）而言，涉及更多的手动步骤和配置过程。然而，这种部署方式在一些情境下具有显著的优势。通过手动下载、配置和启动Kubernetes的各个组件，用户能够获得更高程度的定制性和精细控制权，以便根据特定需求进行调整。这种灵活性使用户能够选择所需的Kubernetes版本、特定的组件配置，以及自定义的网络和存储方案。 同时，通过深入参与每个部署步骤，用户可以更加深入地理解Kubernetes的内部工作机制和组件之间的关系。这种深入了解对于排查问题、优化性能以及实现定制的集群架构至关重要。此外，二进制部署还允许用户在没有网络连接的环境中进行部署，这在某些限制性网络环境下非常有用。对于特殊场景，如需要定制化的认证、授权和网络设置，手动部署能够更好地满足需求。 然而，需要注意的是，Kubernetes的二进制部署需要更多的时间、技术知识和资源投入。相较于自动化工具，它可能增加了出错的风险，需要更多的监控和维护工作。因此，在选择部署方式时，应该根据自身的技能水平、时间成本和项目需求来选择适合自己的部署方式。 k8s-实战案例_v1.21.x-部署.pdf ","date":"2023-01-06","objectID":"/posts/kubernetes/primary/kubernetes-1/:1:0","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署 (一)","uri":"/posts/kubernetes/primary/kubernetes-1/"},{"categories":["Kubernetes"],"content":"1.基础环境配置 ","date":"2023-01-06","objectID":"/posts/kubernetes/primary/kubernetes-1/:2:0","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署 (一)","uri":"/posts/kubernetes/primary/kubernetes-1/"},{"categories":["Kubernetes"],"content":"1.1 时间同步 ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime cat /etc/default/locale LANG=en_US.UTF-8 LC_TIME=en_DK.UTF-8 */5 * * * * /usr/sbin/ntpdate time1.aliyun.com \u0026\u003e /dev/null \u0026\u0026 hwclock -w/usr/sbin/ntpdate","date":"2023-01-06","objectID":"/posts/kubernetes/primary/kubernetes-1/:2:1","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署 (一)","uri":"/posts/kubernetes/primary/kubernetes-1/"},{"categories":["Kubernetes"],"content":"1.2 安裝docker root@master01:/home/ceamg# cd /apps/docker/ root@master01:/apps/docker# tar xvf docker-19.03.15-binary-install.tar.gz root@master01:/apps/docker# ll total 153128 drwxr-xr-x 2 root root 4096 Apr 11 2021 ./ drwxr-xr-x 3 root root 4096 Jan 2 03:52 ../ -rw-r--r-- 1 root root 647 Apr 11 2021 containerd.service -rw-r--r-- 1 root root 78156440 Jan 2 03:57 docker-19.03.15-binary-install.tar.gz -rw-r--r-- 1 root root 62436240 Feb 5 2021 docker-19.03.15.tgz -rwxr-xr-x 1 root root 16168192 Jun 24 2019 docker-compose-Linux-x86_64_1.24.1* -rwxr-xr-x 1 root root 2708 Apr 11 2021 docker-install.sh* -rw-r--r-- 1 root root 1683 Apr 11 2021 docker.service -rw-r--r-- 1 root root 197 Apr 11 2021 docker.socket -rw-r--r-- 1 root root 454 Apr 11 2021 limits.conf -rw-r--r-- 1 root root 257 Apr 11 2021 sysctl.conf#!/bin/bash DIR=`pwd` PACKAGE_NAME=\"docker-19.03.15.tgz\" DOCKER_FILE=${DIR}/${PACKAGE_NAME} centos_install_docker(){ grep \"Kernel\" /etc/issue \u0026\u003e /dev/null if [ $? -eq 0 ];then /bin/echo \"当前系统是`cat /etc/redhat-release`,即将开始系统初始化、配置docker-compose与安装docker\" \u0026\u0026 sleep 1 systemctl stop firewalld \u0026\u0026 systemctl disable firewalld \u0026\u0026 echo \"防火墙已关闭\" \u0026\u0026 sleep 1 systemctl stop NetworkManager \u0026\u0026 systemctl disable NetworkManager \u0026\u0026 echo \"NetworkManager\" \u0026\u0026 sleep 1 sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux \u0026\u0026 setenforce 0 \u0026\u0026 echo \"selinux 已关闭\" \u0026\u0026 sleep 1 \\cp ${DIR}/limits.conf /etc/security/limits.conf \\cp ${DIR}/sysctl.conf /etc/sysctl.conf /bin/tar xvf ${DOCKER_FILE} \\cp docker/* /usr/bin \\cp containerd.service /lib/systemd/system/containerd.service \\cp docker.service /lib/systemd/system/docker.service \\cp docker.socket /lib/systemd/system/docker.socket \\cp ${DIR}/docker-compose-Linux-x86_64_1.24.1 /usr/bin/docker-compose groupadd docker \u0026\u0026 useradd docker -g docker id -u magedu \u0026\u003e /dev/null if [ $? -ne 0 ];then useradd magedu usermod magedu -G docker fi systemctl enable containerd.service \u0026\u0026 systemctl restart containerd.service systemctl enable docker.service \u0026\u0026 systemctl restart docker.service systemctl enable docker.socket \u0026\u0026 systemctl restart docker.socket fi } ubuntu_install_docker(){ grep \"Ubuntu\" /etc/issue \u0026\u003e /dev/null if [ $? -eq 0 ];then /bin/echo \"当前系统是`cat /etc/issue`,即将开始系统初始化、配置docker-compose与安装docker\" \u0026\u0026 sleep 1 \\cp ${DIR}/limits.conf /etc/security/limits.conf \\cp ${DIR}/sysctl.conf /etc/sysctl.conf /bin/tar xvf ${DOCKER_FILE} \\cp docker/* /usr/bin \\cp containerd.service /lib/systemd/system/containerd.service \\cp docker.service /lib/systemd/system/docker.service \\cp docker.socket /lib/systemd/system/docker.socket \\cp ${DIR}/docker-compose-Linux-x86_64_1.24.1 /usr/bin/docker-compose ulimit -n 1000000 /bin/su -c - ceamg \"ulimit -n 1000000\" /bin/echo \"docker 安装完成!\" \u0026\u0026 sleep 1 id -u magedu \u0026\u003e /dev/null if [ $? -ne 0 ];then groupadd -r docker useradd -r -m -g docker docker fi systemctl enable containerd.service \u0026\u0026 systemctl restart containerd.service systemctl enable docker.service \u0026\u0026 systemctl restart docker.service systemctl enable docker.socket \u0026\u0026 systemctl restart docker.socket fi } main(){ centos_install_docker ubuntu_install_docker } mainsudo tee /etc/docker/daemon.json \u003c\u003c-'EOF' { \"registry-mirrors\": [\"https://lc2kkql3.mirror.aliyuncs.com\"], \"storage-driver\": \"overlay\", \"data-root\": \"/data/docker\" } EOF sudo systemctl daemon-reload sudo systemctl restart dockerroot@master01:~# cat /etc/sysctl.conf net.ipv4.ip_forward=1 vm.max_map_count=262144 kernel.pid_max=4194303 fs.file-max=1000000 net.ipv4.tcp_max_tw_buckets=6000 net.netfilter.nf_conntrack_max=2097152 net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 vm.swappiness=0root@master01:/apps/docker# bash docker-install.sh 当前系统是Ubuntu 20.04.3 LTS \\n \\l,即将开始系统初始化、配置docker-compose与安装docker docker/ docker/dockerd docker/docker-proxy docker/containerd-shim docker/docker-init docker/docker docker/runc docker/ctr docker/containerd su: user jack does not exist docker 安装完成! Created symlink /etc/systemd/system/multi-user.target.wants/co","date":"2023-01-06","objectID":"/posts/kubernetes/primary/kubernetes-1/:2:2","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署 (一)","uri":"/posts/kubernetes/primary/kubernetes-1/"},{"categories":["Kubernetes"],"content":"1.3 安装ansible #部署节点安装ansible root@master01:~# apt install python3-pip git root@master01:~# pip3 install ansible -i https://mirrors.aliyun.com/pypi/simple/ root@master01:~# ansible --version ansible [core 2.13.7] config file = None configured module search path = ['/root/.ansible/plugins/modules', '/usr/share/ansible/plugins/modules'] ansible python module location = /usr/local/lib/python3.8/dist-packages/ansible ansible collection location = /root/.ansible/collections:/usr/share/ansible/collections executable location = /usr/local/bin/ansible python version = 3.8.10 (default, Nov 14 2022, 12:59:47) [GCC 9.4.0] jinja version = 3.1.2 libyaml = True","date":"2023-01-06","objectID":"/posts/kubernetes/primary/kubernetes-1/:2:3","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署 (一)","uri":"/posts/kubernetes/primary/kubernetes-1/"},{"categories":["Kubernetes"],"content":"1.4 配置集群免秘钥登录 #⽣成密钥对 root@k8s-master1:~# ssh-keygen #安装sshpass命令⽤于同步公钥到各k8s服务器 # apt-get install sshpass #分发公钥脚本： root@k8s-master1:~# cat scp-key.sh #!/bin/bash #⽬标主机列表 IP=\" 10.1.0.32 10.1.0.33 10.1.0.34 10.1.0.35 10.1.0.38 \" for node in ${IP};do sshpass -p ceamg.com ssh-copy-id ${node} -o StrictHostKeyChecking=no if [ $? -eq 0 ];then echo \"${node} 秘钥copy完成\" else echo \"${node} 秘钥copy失败\" fi done","date":"2023-01-06","objectID":"/posts/kubernetes/primary/kubernetes-1/:2:4","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署 (一)","uri":"/posts/kubernetes/primary/kubernetes-1/"},{"categories":["Kubernetes"],"content":"1.5 部署节点下载kubeasz部署项⽬及组件 使⽤ **master01 **作为部署节点GitHub - easzlab/kubeasz: 使用Ansible脚本安装K8S集群，介绍组件交互原理，方便直接，不受国内网络环境影响 root@k8s-master1:~# export release=3.3.1 root@k8s-master1:~# curl -C- -fLO --retry 3 https://github.com/easzlab/kubeasz/releases/download/${release}/ezdownroot@master01:~# chmod a+x ezdown root@master01:~# ./ezdown -D 2023-01-02 13:28:24 INFO Action begin: download_all 2023-01-02 13:28:24 INFO downloading docker binaries, version 19.03.15 --2023-01-02 13:28:24-- https://mirrors.tuna.tsinghua.edu.cn/docker-ce/linux/static/stable/x86_64/docker-19.03.15.tgz Resolving mirrors.tuna.tsinghua.edu.cn (mirrors.tuna.tsinghua.edu.cn)... 101.6.15.130, 2402:f000:1:400::2 Connecting to mirrors.tuna.tsinghua.edu.cn (mirrors.tuna.tsinghua.edu.cn)|101.6.15.130|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 62436240 (60M) [application/octet-stream] Saving to: ‘docker-19.03.15.tgz’ docker-19.03.15.tgz 100%[========================================================================================================================\u003e] 59.54M 11.2MB/s in 5.5s 2023-01-02 13:28:29 (10.9 MB/s) - ‘docker-19.03.15.tgz’ saved [62436240/62436240] 2023-01-02 13:28:31 WARN docker is already running. 2023-01-02 13:28:31 INFO downloading kubeasz: 3.3.1 2023-01-02 13:28:31 DEBUG run a temporary container Unable to find image 'easzlab/kubeasz:3.3.1' locally 3.3.1: Pulling from easzlab/kubeasz Status: Image is up to date for easzlab/kubeasz:3.3.1 docker.io/easzlab/kubeasz:3.3.1 2023-01-02 13:41:44 INFO Action successed: download_all root@master01:~# cd /etc/kubeasz/ root@master01:/etc/kubeasz/down# ll total 1136932 drwxr-xr-x 2 root root 4096 Jan 2 13:41 ./ drwxrwxr-x 12 root root 4096 Jan 2 13:32 ../ -rw------- 1 root root 383673856 Jan 2 13:35 calico_v3.19.4.tar -rw------- 1 root root 48941568 Jan 2 13:36 coredns_1.9.3.tar -rw------- 1 root root 246784000 Jan 2 13:39 dashboard_v2.5.1.tar -rw-r--r-- 1 root root 62436240 Feb 1 2021 docker-19.03.15.tgz -rw------- 1 root root 106171392 Jan 2 13:37 k8s-dns-node-cache_1.21.1.tar -rw------- 1 root root 179129856 Jan 2 13:41 kubeasz_3.3.1.tar -rw------- 1 root root 43832320 Jan 2 13:40 metrics-scraper_v1.0.8.tar -rw------- 1 root root 65683968 Jan 2 13:41 metrics-server_v0.5.2.tar -rw------- 1 root root 721408 Jan 2 13:41 pause_3.7.tar -rw------- 1 root root 26815488 Jan 2 13:32 registry-2.tar上述脚本运行成功后，所有文件（kubeasz代码、二进制、离线镜像）均已整理好放入目录/etc/kubeasz ","date":"2023-01-06","objectID":"/posts/kubernetes/primary/kubernetes-1/:2:5","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署 (一)","uri":"/posts/kubernetes/primary/kubernetes-1/"},{"categories":["Kubernetes"],"content":"2.部署 harbor 镜像仓库 ","date":"2023-01-06","objectID":"/posts/kubernetes/primary/kubernetes-1/:3:0","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署 (一)","uri":"/posts/kubernetes/primary/kubernetes-1/"},{"categories":["Kubernetes"],"content":"2.1 创建自签ssl证书 #本地解析 root@harbor01:~# echo \"10.1.0.38 harbor.ceamg.com \u003e\u003e /etc/hosts\" mkdir -p /data/cert cd /data/cert #创建ca和harbor证书请求 openssl genrsa -out ca.key 4096 openssl req -x509 -new -nodes -sha512 -days 7300 -subj \"/C=CN/ST=Beijing/L=Beijing/O=example/OU=Personal/CN=harbor.ceamg.com\" -key ca.key -out ca.crt openssl genrsa -out harbor.ceamg.com.key 4096 openssl req -sha512 -new -subj \"/C=CN/ST=Beijing/L=Beijing/O=example/OU=Personal/CN=harbor.ceamg.com\" -key harbor.ceamg.com.key -out harbor.ceamg.com.csr #创建v3文件 cat \u003e v3.ext \u003c\u003c-EOF authorityKeyIdentifier=keyid,issuer basicConstraints=CA:FALSE keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment extendedKeyUsage = serverAuth subjectAltName = @alt_names [alt_names] DNS.1=harbor.ceamg.com DNS.2=harbor DNS.3=ks-allinone EOF #使用v3文件签发harbor证书 openssl x509 -req -sha512 -days 7300 -extfile v3.ext -CA ca.crt -CAkey ca.key -CAcreateserial -in harbor.ceamg.com.csr -out harbor.ceamg.com.crt #转换成cert openssl x509 -inform PEM -in harbor.ceamg.com.crt -out harbor.ceamg.com.cert #添加根证书让系统信任证书 root@harbor01:/data/cert# cp harbor.ceamg.com.crt /usr/local/share/ca-certificates/ root@harbor01:/data/cert# update-ca-certificates Updating certificates in /etc/ssl/certs... rehash: warning: skipping ca-certificates.crt,it does not contain exactly one certificate or CRL 1 added, 0 removed; done. Running hooks in /etc/ca-certificates/update.d... done. #update-ca-certificates命令将PEM格式的根证书内容附加到/etc/ssl/certs/ca-certificates.crt ，而/etc/ssl/certs/ca-certificates.crt 包含了系统自带的各种可信根证书.","date":"2023-01-06","objectID":"/posts/kubernetes/primary/kubernetes-1/:3:1","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署 (一)","uri":"/posts/kubernetes/primary/kubernetes-1/"},{"categories":["Kubernetes"],"content":"2.2 修改harbor配置 root@harbor01:/apps/harbor/harbor# cp harbor.yml.tmpl harbor.ymlroot@harbor01:/apps/harbor/harbor# grep -v \"#\" harbor.yml | grep -v \"^$\" hostname: harbor.ceamg.com http: port: 80 https: port: 443 certificate: /data/cert/harbor.ceamg.com.crt private_key: /apps/harbor/certs/harbor.ceamg.com.key harbor_admin_password: ceamg.com database: password: root123 max_idle_conns: 100 max_open_conns: 900 conn_max_lifetime: 5m conn_max_idle_time: 0 data_volume: /data trivy: ignore_unfixed: false skip_update: false offline_scan: false security_check: vuln insecure: false jobservice: max_job_workers: 10 notification: webhook_job_max_retry: 10 chart: absolute_url: disabled log: level: info local: rotate_count: 50 rotate_size: 200M location: /var/log/harbor _version: 2.7.0 proxy: http_proxy: https_proxy: no_proxy: components: - core - jobservice - trivy upload_purging: enabled: true age: 168h interval: 24h dryrun: false cache: enabled: false expire_hours: 24","date":"2023-01-06","objectID":"/posts/kubernetes/primary/kubernetes-1/:3:2","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署 (一)","uri":"/posts/kubernetes/primary/kubernetes-1/"},{"categories":["Kubernetes"],"content":"2.3 安装harbor Harbor – Reconfigure Harbor and Manage the Harbor Lifecycle 有扫描–with-trivy ,有认证–with-notary，有helm charts 模块退出–with-chartmuseum 其中–with-clair已弃用 #更新配置文件 root@harbor01:/apps/harbor/harbor# ./prepare root@harbor01:/apps/harbor/harbor# ./install.sh --with-notary --with-trivy --with-chartmuseum [Step 0]: checking if docker is installed ... Note: docker version: 19.03.15 [Step 1]: checking docker-compose is installed ... Note: docker-compose version: 1.24.1 [Step 2]: loading Harbor images ... 8991ee7e1c66: Loading layer [==================================================\u003e] 37.72MB/37.72MB caef0c5d2fe0: Loading layer [==================================================\u003e] 43.84MB/43.84MB d0ae0913849c: Loading layer [==================================================\u003e] 66.03MB/66.03MB d6c3137fc4e6: Loading layer [==================================================\u003e] 18.2MB/18.2MB db156fb6962c: Loading layer [==================================================\u003e] 65.54kB/65.54kB 578a990cf79f: Loading layer [==================================================\u003e] 2.56kB/2.56kB 9415b3c8b317: Loading layer [==================================================\u003e] 1.536kB/1.536kB bdb2dfba8b17: Loading layer [==================================================\u003e] 12.29kB/12.29kB 6a1b6c491cd2: Loading layer [==================================================\u003e] 2.613MB/2.613MB c35c2488b48b: Loading layer [==================================================\u003e] 407kB/407kB Loaded image: goharbor/prepare:v2.7.0 Loaded image: goharbor/harbor-db:v2.7.0 Loaded image: goharbor/harbor-core:v2.7.0 Loaded image: goharbor/harbor-log:v2.7.0 Loaded image: goharbor/harbor-exporter:v2.7.0 Loaded image: goharbor/nginx-photon:v2.7.0 Loaded image: goharbor/chartmuseum-photon:v2.7.0 Loaded image: goharbor/harbor-portal:v2.7.0 Loaded image: goharbor/harbor-jobservice:v2.7.0 Loaded image: goharbor/harbor-registryctl:v2.7.0 Loaded image: goharbor/registry-photon:v2.7.0 Loaded image: goharbor/notary-server-photon:v2.7.0 Loaded image: goharbor/redis-photon:v2.7.0 Loaded image: goharbor/notary-signer-photon:v2.7.0 Loaded image: goharbor/trivy-adapter-photon:v2.7.0 [Step 3]: preparing environment ... [Step 4]: preparing harbor configs ... prepare base dir is set to /apps/harbor/harbor Generated configuration file: /config/portal/nginx.conf Generated configuration file: /config/log/logrotate.conf Generated configuration file: /config/log/rsyslog_docker.conf Generated configuration file: /config/nginx/nginx.conf Generated configuration file: /config/core/env Generated configuration file: /config/core/app.conf Generated configuration file: /config/registry/config.yml Generated configuration file: /config/registryctl/env Generated configuration file: /config/registryctl/config.yml Generated configuration file: /config/db/env Generated configuration file: /config/jobservice/env Generated configuration file: /config/jobservice/config.yml Generated and saved secret to file: /data/secret/keys/secretkey Successfully called func: create_root_cert Generated configuration file: /config/trivy-adapter/env Generated configuration file: /compose_location/docker-compose.yml Clean up the input dir Note: stopping existing Harbor instance ... Removing network harbor_harbor WARNING: Network harbor_harbor not found. [Step 5]: starting Harbor ... Creating network \"harbor_harbor\" with the default driver Creating harbor-log ... done Creating redis ... done Creating harbor-portal ... done Creating registry ... done Creating harbor-db ... done Creating registryctl ... done Creating trivy-adapter ... done Creating harbor-core ... done Creating harbor-jobservice ... done Creating nginx ... done ✔ ----Harbor has been installed and started successfully.----","date":"2023-01-06","objectID":"/posts/kubernetes/primary/kubernetes-1/:3:3","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署 (一)","uri":"/posts/kubernetes/primary/kubernetes-1/"},{"categories":["Kubernetes"],"content":"2.4 harbor 调试 #关闭 harbor $~ sudo docker-compose down -v #更新配置 vim /apps/harbor.yml prepare #重新生成配置文件,增加上其他chart功能等 sudo prepare --with-notary --with-trivy --with-chartmuseum #启动 harbor $~ sudo docker-compose up -d","date":"2023-01-06","objectID":"/posts/kubernetes/primary/kubernetes-1/:3:4","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署 (一)","uri":"/posts/kubernetes/primary/kubernetes-1/"},{"categories":["Kubernetes"],"content":"3.创建集群配置实例 ","date":"2023-01-06","objectID":"/posts/kubernetes/primary/kubernetes-1/:4:0","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署 (一)","uri":"/posts/kubernetes/primary/kubernetes-1/"},{"categories":["Kubernetes"],"content":"3.1 ⽣成k8s集群 hosts⽂件 root@master01:/etc/kubeasz# ./ezctl new k8s-01 2023-01-03 04:52:33 DEBUG generate custom cluster files in /etc/kubeasz/clusters/k8s-01 2023-01-03 04:52:33 DEBUG set versions 2023-01-03 04:52:33 DEBUG cluster k8s-01: files successfully created. 2023-01-03 04:52:33 INFO next steps 1: to config '/etc/kubeasz/clusters/k8s-01/hosts' 2023-01-03 04:52:33 INFO next steps 2: to config '/etc/kubeasz/clusters/k8s-01/config.yml'# 'NEW_INSTALL': 'true' to install a harbor server; 'false' to integrate with existed one [harbor] 10.1.0.38 NEW_INSTALL=false # [optional] loadbalance for accessing k8s from outside [ex_lb] #192.168.1.6 LB_ROLE=backup EX_APISERVER_VIP=192.168.1.250 EX_APISERVER_PORT=8443 #192.168.1.7 LB_ROLE=master EX_APISERVER_VIP=192.168.1.250 EX_APISERVER_PORT=8443 # [optional] ntp server for the cluster [chrony] #192.168.1.1 [all:vars] # --------- Main Variables --------------- # Secure port for apiservers SECURE_PORT=\"6443\" # Cluster container-runtime supported: docker, containerd # if k8s version \u003e= 1.24, docker is not supported CONTAINER_RUNTIME=\"containerd\" # Network plugins supported: calico, flannel, kube-router, cilium, kube-ovn CLUSTER_NETWORK=\"calico\" # Service proxy mode of kube-proxy: 'iptables' or 'ipvs' PROXY_MODE=\"ipvs\" # K8S Service CIDR, not overlap with node(host) networking SERVICE_CIDR=\"10.10.0.0/16\" # Cluster CIDR (Pod CIDR), not overlap with node(host) networking CLUSTER_CIDR=\"10.20.0.0/16\" # NodePort Range NODE_PORT_RANGE=\"30000-65535\" # Cluster DNS Domain CLUSTER_DNS_DOMAIN=\"ceamg.local\" # -------- Additional Variables (don't change the default value right now) --- # Binaries Directory bin_dir=\"/usr/local/bin\" # Deploy Directory (kubeasz workspace) base_dir=\"/etc/kubeasz\" # Directory for a specific cluster cluster_dir=\"{{ base_dir }}/clusters/k8s-01\" # CA and other components cert/key Directory ca_dir=\"/etc/kubernetes/ssl\"","date":"2023-01-06","objectID":"/posts/kubernetes/primary/kubernetes-1/:4:1","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署 (一)","uri":"/posts/kubernetes/primary/kubernetes-1/"},{"categories":["Kubernetes"],"content":"3.1 ⽣成k8s集群 config ⽂件 root@master01:/etc/kubeasz# vim /etc/kubeasz/clusters/k8s-01/config.yml ############################ # prepare ############################ # 可选离线安装系统软件包 (offline|online) INSTALL_SOURCE: \"online\" # 可选进行系统安全加固 github.com/dev-sec/ansible-collection-hardening OS_HARDEN: false ############################ # role:deploy ############################ # default: ca will expire in 100 years # default: certs issued by the ca will expire in 50 years CA_EXPIRY: \"876000h\" CERT_EXPIRY: \"438000h\" # kubeconfig 配置参数 CLUSTER_NAME: \"cluster1\" CONTEXT_NAME: \"context-{{ CLUSTER_NAME }}\" # k8s version K8S_VER: \"1.24.2\" ############################ # role:etcd ############################ # 设置不同的wal目录，可以避免磁盘io竞争，提高性能 ETCD_DATA_DIR: \"/var/lib/etcd\" ETCD_WAL_DIR: \"\" ############################ # role:runtime [containerd,docker] ############################ # ------------------------------------------- containerd # [.]启用容器仓库镜像 ENABLE_MIRROR_REGISTRY: true # [containerd]基础容器镜像 SANDBOX_IMAGE: \"easzlab.io.local:5000/easzlab/pause:3.7\" # [containerd]容器持久化存储目录 CONTAINERD_STORAGE_DIR: \"/var/lib/containerd\" # ------------------------------------------- docker # [docker]容器存储目录 DOCKER_STORAGE_DIR: \"/var/lib/docker\" # [docker]开启Restful API ENABLE_REMOTE_API: false # [docker]信任的HTTP仓库 INSECURE_REG: '[\"http://easzlab.io.local:5000\"]' ############################ # role:kube-master ############################ # k8s 集群 master 节点证书配置，可以添加多个ip和域名（比如增加公网ip和域名） MASTER_CERT_HOSTS: - \"10.1.1.1\" - \"k8s.easzlab.io\" #- \"www.test.com\" # node 节点上 pod 网段掩码长度（决定每个节点最多能分配的pod ip地址） # 如果flannel 使用 --kube-subnet-mgr 参数，那么它将读取该设置为每个节点分配pod网段 # https://github.com/coreos/flannel/issues/847 NODE_CIDR_LEN: 24 ############################ # role:kube-node ############################ # Kubelet 根目录 KUBELET_ROOT_DIR: \"/var/lib/kubelet\" # node节点最大pod 数 MAX_PODS: 300 # 配置为kube组件（kubelet,kube-proxy,dockerd等）预留的资源量 # 数值设置详见templates/kubelet-config.yaml.j2 KUBE_RESERVED_ENABLED: \"yes\" # k8s 官方不建议草率开启 system-reserved, 除非你基于长期监控，了解系统的资源占用状况； # 并且随着系统运行时间，需要适当增加资源预留，数值设置详见templates/kubelet-config.yaml.j2 # 系统预留设置基于 4c/8g 虚机，最小化安装系统服务，如果使用高性能物理机可以适当增加预留 # 另外，集群安装时候apiserver等资源占用会短时较大，建议至少预留1g内存 SYS_RESERVED_ENABLED: \"no\" ############################ # role:network [flannel,calico,cilium,kube-ovn,kube-router] ############################ # ------------------------------------------- flannel # [flannel]设置flannel 后端\"host-gw\",\"vxlan\"等 FLANNEL_BACKEND: \"vxlan\" DIRECT_ROUTING: false # [flannel] flanneld_image: \"quay.io/coreos/flannel:v0.10.0-amd64\" flannelVer: \"v0.15.1\" flanneld_image: \"easzlab.io.local:5000/easzlab/flannel:{{ flannelVer }}\" # ------------------------------------------- calico # [calico]设置 CALICO_IPV4POOL_IPIP=“off”,可以提高网络性能，条件限制详见 docs/setup/calico.md CALICO_IPV4POOL_IPIP: \"Always\" # [calico]设置 calico-node使用的host IP，bgp邻居通过该地址建立，可手工指定也可以自动发现 IP_AUTODETECTION_METHOD: \"can-reach={{ groups['kube_master'][0] }}\" # [calico]设置calico 网络 backend: brid, vxlan, none CALICO_NETWORKING_BACKEND: \"brid\" # [calico]设置calico 是否使用route reflectors # 如果集群规模超过50个节点，建议启用该特性 CALICO_RR_ENABLED: false # CALICO_RR_NODES 配置route reflectors的节点，如果未设置默认使用集群master节点 # CALICO_RR_NODES: [\"192.168.1.1\", \"192.168.1.2\"] CALICO_RR_NODES: [] # [calico]更新支持calico 版本: [v3.3.x] [v3.4.x] [v3.8.x] [v3.15.x] calico_ver: \"v3.19.4\" # [calico]calico 主版本 calico_ver_main: \"{{ calico_ver.split('.')[0] }}.{{ calico_ver.split('.')[1] }}\" # ------------------------------------------- cilium # [cilium]镜像版本 cilium_ver: \"1.11.6\" cilium_connectivity_check: true cilium_hubble_enabled: false cilium_hubble_ui_enabled: false # ------------------------------------------- kube-ovn # [kube-ovn]选择 OVN DB and OVN Control Plane 节点，默认为第一个master节点 OVN_DB_NODE: \"{{ groups['kube_master'][0] }}\" # [kube-ovn]离线镜像tar包 kube_ovn_ver: \"v1.5.3\" # ------------------------------------------- kube-router # [kube-router]公有云上存在限制，一般需要始终开启 ipinip；自有环境可以设置为 \"subnet\" OVERLAY_TYPE: \"full\" # [kube-router]NetworkPolicy 支持开关 FIREWALL_ENABLE: true # [k","date":"2023-01-06","objectID":"/posts/kubernetes/primary/kubernetes-1/:4:2","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署 (一)","uri":"/posts/kubernetes/primary/kubernetes-1/"},{"categories":["Kubernetes"],"content":"4.步骤1-基础环境初始化 root@master01:/etc/kubeasz# ./ezctl help setup Usage: ezctl setup \u003ccluster\u003e \u003cstep\u003e available steps: 01 prepare to prepare CA/certs \u0026 kubeconfig \u0026 other system settings 02 etcd to setup the etcd cluster 03 container-runtime to setup the container runtime(docker or containerd) 04 kube-master to setup the master nodes 05 kube-node to setup the worker nodes 06 network to setup the network plugin 07 cluster-addon to setup other useful plugins 90 all to run 01~07 all at once 10 ex-lb to install external loadbalance for accessing k8s from outside 11 harbor to install a new harbor server or to integrate with an existed one examples: ./ezctl setup test-k8s 01 (or ./ezctl setup test-k8s prepare) ./ezctl setup test-k8s 02 (or ./ezctl setup test-k8s etcd) ./ezctl setup test-k8s all ./ezctl setup test-k8s 04 -t restart_master vim playbooks/01.prepare.yml #系统基础初始化主机配置 root@master01:/etc/kubeasz# ./ezctl setup k8s-01 01 #准备CA和基础系统设置","date":"2023-01-06","objectID":"/posts/kubernetes/primary/kubernetes-1/:5:0","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署 (一)","uri":"/posts/kubernetes/primary/kubernetes-1/"},{"categories":["Kubernetes"],"content":"5.步骤2-部署etcd集群 可更改启动脚本路径及版本等⾃定义配置 root@master01:/etc/kubeasz# ./ezctl setup k8s-01 02 ansible-playbook -i clusters/k8s-01/hosts -e @clusters/k8s-01/config.yml playbooks/02.etcd.yml 2023-01-03 13:39:13 INFO cluster:k8s-01 setup step:02 begins in 5s, press any key to abort健康检查 export NODE_IPS=\"10.1.0.34 10.1.0.35\" root@etcd01:~# for ip in ${NODE_IPS}; do ETCDCTL_API=3 /usr/local/bin/etcdctl --endpoints=https://${ip}:2379 --cacert=/etc/kubernetes/ssl/ca.pem --cert=/etc/kubernetes/ssl/etcd.pem --key=/etc/kubernetes/ssl/etcd-key.pem endpoint health; done https://10.1.0.34:2379 is healthy: successfully committed proposal: took = 14.95631ms https://10.1.0.35:2379 is healthy: successfully committed proposal: took = 15.037491ms 注：以上返回信息表示etcd集群运⾏正常，否则异常！部署containerd 同步docker证书脚本： #!/bin/bash #⽬标主机列表 IP=\" 10.1.0.32 10.1.0.33 10.1.0.34 10.1.0.35 \" for node in ${IP};do sshpass -p ceamg.com ssh-copy-id ${node} -o StrictHostKeyChecking=no if [ $? -eq 0 ];then echo \"${node} 秘钥copy完成\" echo \"${node} 秘钥copy完成,准备环境初始化.....\" ssh ${node} \"mkdir /etc/containerd/certs.d/harbor.ceamg.com -p\" echo \"Harbor 证书创建成功!\" scp /etc/containerd/certs.d/harbor.ceamg.com/harbor.ceamg.com.cert /etc/containerd/certs.d/harbor.ceamg.com/harbor.ceamg.com.crt /etc/containerd/certs.d/harbor.ceamg.com/harbor.ceamg.com.key /etc/containerd/certs.d/harbor.ceamg.com/ca.crt ${node}:/etc/containerd/certs.d/harbor.ceamg.com/ echo \"Harbor 证书拷贝成功!\" ssh ${node} \"echo \"10.1.0.38 harbor.ceamg.com\" \u003e\u003e /etc/hosts\" echo \"host 解析添加完成\" #scp -r /root/.docker ${node}:/root/ #echo \"Harbor 认证件拷完成!\" else echo \"${node} 秘钥copy失败\" fi done #执⾏脚本进⾏证书分发 root@k8s-master1:/etc/kubeasz# bash /root/scp-key.sh","date":"2023-01-06","objectID":"/posts/kubernetes/primary/kubernetes-1/:6:0","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署 (一)","uri":"/posts/kubernetes/primary/kubernetes-1/"},{"categories":["Kubernetes"],"content":"6.步骤3-部署运行时环境 项目根据k8s版本提供不同的默认容器运行时： k8s 版本 \u003c 1.24 时，支持docker containerd 可选 k8s 版本 \u003e= 1.24 时，仅支持 containerd ","date":"2023-01-06","objectID":"/posts/kubernetes/primary/kubernetes-1/:7:0","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署 (一)","uri":"/posts/kubernetes/primary/kubernetes-1/"},{"categories":["Kubernetes"],"content":"6.1 kubeasz 集成安装 containerd 注意：k8s 1.24以后，项目已经设置默认容器运行时为 containerd，无需手动修改 ./ezctl setup k8s-01 05","date":"2023-01-06","objectID":"/posts/kubernetes/primary/kubernetes-1/:7:1","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署 (一)","uri":"/posts/kubernetes/primary/kubernetes-1/"},{"categories":["Kubernetes"],"content":"6.2 配置containerd 对接私有harbor仓库 修改role模板文件 vim roles/containerd/templates/config.toml.j2主要修改如下： [plugins.\"io.containerd.grpc.v1.cri\".registry] [plugins.\"io.containerd.grpc.v1.cri\".registry.auths] [plugins.\"io.containerd.grpc.v1.cri\".registry.configs] [plugins.\"io.containerd.grpc.v1.cri\".registry.configs.\"https://harbor.ceamg.com\"] username = \"admin\" password = \"ceamg.com\" [plugins.\"io.containerd.grpc.v1.cri\".registry.configs.\"easzlab.io.local:5000\".tls] insecure_skip_verify = true [plugins.\"io.containerd.grpc.v1.cri\".registry.configs.\"harbor.ceamg.com\".tls] insecure_skip_verify = true ca_file = \"/etc/containerd/certs.d/harbor.ceamg.com/ca.crt\" cert_file = \"/etc/containerd/certs.d/harbor.ceamg.com/harbor.ceamg.com.cert\" key_file = \"/etc/containerd/certs.d/harbor.ceamg.com/harbor.ceamg.com.key\" [plugins.\"io.containerd.grpc.v1.cri\".registry.headers] [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors] [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors.\"easzlab.io.local:5000\"] endpoint = [\"http://easzlab.io.local:5000\"] [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors.\"harbor.ceamg.com\"] endpoint = [\"https://harbor.ceamg.com\"] [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors.\"docker.io\"] endpoint = [\"https://lc2kkql3.mirror.aliyuncs.com\",\"https://docker.mirrors.ustc.edu.cn\", \"http://hub-mirror.c.163.com\"] [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors.\"gcr.io\"] endpoint = [\"https://gcr.mirrors.ustc.edu.cn\"] [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors.\"k8s.gcr.io\"] endpoint = [\"https://gcr.mirrors.ustc.edu.cn/google-containers/\"] [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors.\"quay.io\"] endpoint = [\"https://quay.mirrors.ustc.edu.cn\"] [plugins.\"io.containerd.grpc.v1.cri\".registry.auths.\"https://harbor.ceamg.com\"] [plugins.\"io.containerd.grpc.v1.cri\".x509_key_pair_streaming] tls_cert_file = \"\" tls_key_file = \"\"","date":"2023-01-06","objectID":"/posts/kubernetes/primary/kubernetes-1/:7:2","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署 (一)","uri":"/posts/kubernetes/primary/kubernetes-1/"},{"categories":["Kubernetes"],"content":"6.3 containerd 使用证书对接harbor实现上传下载 6.3.1 使用脚本同步证书到客户端 #!/bin/bash #目标主机列表 IP=\" 10.1.0.32 10.1.0.33 10.1.0.34 10.1.0.35 \" for node in ${IP};do sshpass -p ceamg.com ssh-copy-id ${node} -o StrictHostKeyChecking=no if [ $? -eq 0 ];then echo \"${node} 秘钥copy完成\" echo \"${node} 秘钥copy完成,准备环境初始化.....\" ssh ${node} \"mkdir /etc/containerd/certs.d/harbor.ceamg.com -p\" echo \"Harbor 证书创建成功!\" scp /etc/containerd/certs.d/harbor.ceamg.com/harbor.ceamg.com.cert /etc/containerd/certs.d/harbor.ceamg.com/harbor.ceamg.com.crt /etc/containerd/certs.d/harbor.ceamg.com/harbor.ceamg.com.key /etc/containerd/certs.d/harbor.ceamg.com/ca.crt ${node}:/etc/containerd/certs.d/harbor.ceamg.com/ echo \"Harbor 证书拷贝成功!\" ssh ${node} \"echo \"10.1.0.38 harbor.ceamg.com\" \u003e\u003e /etc/hosts\" echo \"host 解析添加完成\" #scp -r /root/.docker ${node}:/root/ #echo \"Harbor 认证件拷完成!\" else echo \"${node} 秘钥copy失败\" fi done6.3.2 测试containerd 客户端使用证书登录harbor 推送镜像 nerdctl.pdf root@master01:/etc/containerd/certs.d/harbor.ceamg.com# ls ca.crt harbor.ceamg.com.cert harbor.ceamg.com.crt harbor.ceamg.com.key root@master01:/etc/containerd/certs.d/harbor.ceamg.com# nerdctl login harbor.ceamg.com WARNING: Your password will be stored unencrypted in /root/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded root@master01:/etc/containerd/certs.d/harbor.ceamg.com# nerdctl images REPOSITORY TAG IMAGE ID CREATED PLATFORM SIZE BLOB SIZE nginx latest 0047b729188a 4 hours ago linux/amd64 149.4 MiB 54.2 MiB harbor.ceamg.com/library/nginx latest 0047b729188a 3 hours ago linux/amd64 149.4 MiB 54.2 MiB root@master01:/etc/containerd/certs.d/harbor.ceamg.com# nerdctl push harbor.ceamg.com/library/nginx INFO[0000] pushing as a reduced-platform image (application/vnd.docker.distribution.manifest.list.v2+json, sha256:3f727bfae5cee62f35f014637b350dbc1d0b416bdd1717b61c5ce5b036771aa0) index-sha256:3f727bfae5cee62f35f014637b350dbc1d0b416bdd1717b61c5ce5b036771aa0: done |++++++++++++++++++++++++++++++++++++++| manifest-sha256:9a821cadb1b13cb782ec66445325045b2213459008a41c72d8d87cde94b33c8c: done |++++++++++++++++++++++++++++++++++++++| config-sha256:1403e55ab369cd1c8039c34e6b4d47ca40bbde39c371254c7cba14756f472f52: done |++++++++++++++++++++++++++++++++++++++| elapsed: 1.1 s total: 9.3 Ki (8.5 KiB/s) ","date":"2023-01-06","objectID":"/posts/kubernetes/primary/kubernetes-1/:7:3","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署 (一)","uri":"/posts/kubernetes/primary/kubernetes-1/"},{"categories":["Kubernetes"],"content":"7.步骤4-部署master cat playbooks/04.kube-master.yml - hosts: kube_master roles: - kube-lb # 四层负载均衡，监听在127.0.0.1:6443，转发到真实master节点apiserver服务 - kube-master # - kube-node # 因为网络、监控等daemonset组件，master节点也推荐安装kubelet和kube-proxy服务 ... root@master01:/etc/kubeasz# ./ezctl setup k8s-01 04 ansible-playbook -i clusters/k8s-01/hosts -e @clusters/k8s-01/config.yml playbooks/04.kube-master.yml 2023-01-03 14:07:04 INFO cluster:k8s-01 setup step:04 begins in 5s, press any key to abort:验证 master 集群 # 查看进程状态 systemctl status kube-apiserver systemctl status kube-controller-manager systemctl status kube-scheduler # 查看进程运行日志 journalctl -u kube-apiserver journalctl -u kube-controller-manager journalctl -u kube-scheduler执行 kubectl get componentstatus 可以看到 root@master01:/etc/kubeasz# kubectl get componentstatus Warning: v1 ComponentStatus is deprecated in v1.19+ NAME STATUS MESSAGE ERROR scheduler Healthy ok controller-manager Healthy ok etcd-1 Healthy {\"health\":\"true\",\"reason\":\"\"} etcd-0 Healthy {\"health\":\"true\",\"reason\":\"\"} ","date":"2023-01-06","objectID":"/posts/kubernetes/primary/kubernetes-1/:8:0","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署 (一)","uri":"/posts/kubernetes/primary/kubernetes-1/"},{"categories":["Kubernetes"],"content":"8.步骤5-部署node节点 kube_node 是集群中运行工作负载的节点，前置条件需要先部署好kube_master节点，它需要部署如下组件： cat playbooks/05.kube-node.yml - hosts: kube_node roles: - { role: kube-lb, when: \"inventory_hostname not in groups['kube_master']\" } - { role: kube-node, when: \"inventory_hostname not in groups['kube_master']\" } kube-lb：由nginx裁剪编译的四层负载均衡，用于将请求转发到主节点的 apiserver服务 kubelet：kube_node上最主要的组件 kube-proxy： 发布应用服务与负载均衡 root@master01:/etc/kubeasz# ./ezctl setup k8s-01 05 ansible-playbook -i clusters/k8s-01/hosts -e @clusters/k8s-01/config.yml playbooks/05.kube-node.yml 2023-01-04 09:06:25 INFO cluster:k8s-01 setup step:05 begins in 5s, press any key to abort:验证 node 状态 systemctl status kubelet # 查看状态 systemctl status kube-proxy journalctl -u kubelet # 查看日志 journalctl -u kube-proxy 运行 kubectl get node 可以看到类似 root@worker01:/etc/containerd/certs.d/harbor.ceamg.com# kubectl get nodes NAME STATUS ROLES AGE VERSION 10.1.0.31 Ready,SchedulingDisabled master 21h v1.24.2 10.1.0.32 Ready node 21h v1.24.2 10.1.0.33 Ready node 21h v1.24.2","date":"2023-01-06","objectID":"/posts/kubernetes/primary/kubernetes-1/:9:0","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署 (一)","uri":"/posts/kubernetes/primary/kubernetes-1/"},{"categories":["Kubernetes"],"content":"9.步骤6-部署网络组件 首先回顾下K8S网络设计原则，在配置集群网络插件或者实践K8S 应用/服务部署请牢记这些原则： 1.每个Pod都拥有一个独立IP地址，Pod内所有容器共享一个网络命名空间 2.集群内所有Pod都在一个直接连通的扁平网络中，可通过IP直接访问 所有容器之间无需NAT就可以直接互相访问 所有Node和所有容器之间无需NAT就可以直接互相访问 容器自己看到的IP跟其他容器看到的一样 3.Service cluster IP只可在集群内部访问，外部请求需要通过NodePort、LoadBalance或者Ingress来访问 calico 是k8s社区最流行的网络插件之一，也是k8s-conformance test 默认使用的网络插件，功能丰富，支持network policy；是当前kubeasz项目的默认网络插件。 如果需要安装calico，请在clusters/xxxx/hosts文件中设置变量 CLUSTER_NETWORK=\"calico\" ","date":"2023-01-06","objectID":"/posts/kubernetes/primary/kubernetes-1/:10:0","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署 (一)","uri":"/posts/kubernetes/primary/kubernetes-1/"},{"categories":["Kubernetes"],"content":"9.1 使⽤calico⽹络组件 vim clusters/k8s-01/config.yml # ------------------------------------------- calico # [calico]设置 CALICO_IPV4POOL_IPIP=“off”,可以提高网络性能，条件限制详见 docs/setup/calico.md CALICO_IPV4POOL_IPIP: \"Always\" # [calico]设置 calico-node使用的host IP，bgp邻居通过该地址建立，可手工指定也可以自动发现 IP_AUTODETECTION_METHOD: \"can-reach={{ groups['kube_master'][0] }}\" # [calico]设置calico 网络 backend: brid, vxlan, none CALICO_NETWORKING_BACKEND: \"brid\" # [calico]设置calico 是否使用route reflectors # 如果集群规模超过50个节点，建议启用该特性 CALICO_RR_ENABLED: false # CALICO_RR_NODES 配置route reflectors的节点，如果未设置默认使用集群master节点 # CALICO_RR_NODES: [\"192.168.1.1\", \"192.168.1.2\"] CALICO_RR_NODES: [] # [calico]更新支持calico 版本: [v3.3.x] [v3.4.x] [v3.8.x] [v3.15.x] calico_ver: \"v3.19.4\" # [calico]calico 主版本 calico_ver_main: \"{{ calico_ver.split('.')[0] }}.{{ calico_ver.split('.')[1] }}\"./ezctl setup k8s-01 06","date":"2023-01-06","objectID":"/posts/kubernetes/primary/kubernetes-1/:10:1","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署 (一)","uri":"/posts/kubernetes/primary/kubernetes-1/"},{"categories":["Kubernetes"],"content":"9.2 验证calico网络 执行calico安装成功后可以验证如下：(需要等待镜像下载完成，有时候即便上一步已经配置了docker国内加速，还是可能比较慢，请确认以下容器运行起来以后，再执行后续验证步骤) ","date":"2023-01-06","objectID":"/posts/kubernetes/primary/kubernetes-1/:10:2","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署 (一)","uri":"/posts/kubernetes/primary/kubernetes-1/"},{"categories":["Kubernetes"],"content":"9.3 查看所有calico节点状态 root@master01:/etc/kubeasz# kubectl get pod -A -o wide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES kube-system calico-kube-controllers-5c8bb696bb-hf2cp 1/1 Running 0 6m10s 10.1.0.33 10.1.0.33 \u003cnone\u003e \u003cnone\u003e kube-system calico-node-6nlt6 1/1 Running 0 6m10s 10.1.0.32 10.1.0.32 \u003cnone\u003e \u003cnone\u003e kube-system calico-node-fd6rj 1/1 Running 0 6m10s 10.1.0.33 10.1.0.33 \u003cnone\u003e \u003cnone\u003e kube-system calico-node-lhgh4 1/1 Running 0 6m10s 10.1.0.31 10.1.0.31 \u003cnone\u003e \u003cnone\u003eroot@master01:/etc/kubeasz# calicoctl node status Calico process is running. IPv4 BGP status +--------------+-------------------+-------+----------+-------------+ | PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO | +--------------+-------------------+-------+----------+-------------+ | 10.1.0.32 | node-to-node mesh | up | 04:16:44 | Established | | 10.1.0.33 | node-to-node mesh | up | 04:16:43 | Established | +--------------+-------------------+-------+----------+-------------+","date":"2023-01-06","objectID":"/posts/kubernetes/primary/kubernetes-1/:10:3","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署 (一)","uri":"/posts/kubernetes/primary/kubernetes-1/"},{"categories":["Kubernetes"],"content":"9.4 创建容器测试网络通信 root@master01:/etc/kubeasz# kubectl run net-test1 --image=harbor.ceamg.com/library/alpine sleep 360000 pod/net-test1 created root@master01:/etc/kubeasz# kubectl run net-test2 --image=harbor.ceamg.com/library/alpine sleep 360000 pod/net-test2 created root@master01:/etc/kubeasz# kubectl run net-test3 --image=harbor.ceamg.com/library/alpine sleep 360000 pod/net-test3 created root@master01:/etc/kubeasz# kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES net-test1 1/1 Running 0 19s 10.20.5.3 10.1.0.32 \u003cnone\u003e \u003cnone\u003e net-test2 1/1 Running 0 15s 10.20.30.67 10.1.0.33 \u003cnone\u003e \u003cnone\u003e net-test3 1/1 Running 0 12s 10.20.30.68 10.1.0.33 \u003cnone\u003e \u003cnone\u003e test 1/1 Running 0 16m 10.20.5.1 10.1.0.32 \u003cnone\u003e \u003cnone\u003eroot@master01:/etc/kubeasz# kubectl exec -it net-test1 -- sh / # ping 10.20.30.67 PING 10.20.30.67 (10.20.30.67): 56 data bytes 64 bytes from 10.20.30.67: seq=0 ttl=62 time=0.481 ms ^C --- 10.20.30.67 ping statistics --- 1 packets transmitted, 1 packets received, 0% packet loss round-trip min/avg/max = 0.481/0.481/0.481 ms / # ping 10.20.30.68 PING 10.20.30.68 (10.20.30.68): 56 data bytes 64 bytes from 10.20.30.68: seq=0 ttl=62 time=0.631 ms 64 bytes from 10.20.30.68: seq=1 ttl=62 time=1.360 ms 64 bytes from 10.20.30.68: seq=2 ttl=62 time=0.420 ms ^C --- 10.20.30.68 ping statistics --- 3 packets transmitted, 3 packets received, 0% packet loss round-trip min/avg/max = 0.420/0.803/1.360 ms / # ping 223.5.5.5 PING 223.5.5.5 (223.5.5.5): 56 data bytes 64 bytes from 223.5.5.5: seq=0 ttl=114 time=7.597 ms 64 bytes from 223.5.5.5: seq=1 ttl=114 time=7.072 ms 64 bytes from 223.5.5.5: seq=2 ttl=114 time=7.583 ms ^C --- 223.5.5.5 ping statistics --- 3 packets transmitted, 3 packets received, 0% packet loss round-trip min/avg/max = 7.072/7.417/7.597 ms","date":"2023-01-06","objectID":"/posts/kubernetes/primary/kubernetes-1/:10:4","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署 (一)","uri":"/posts/kubernetes/primary/kubernetes-1/"},{"categories":["Kubernetes"],"content":"10.步骤7-安装集群插件-coredns DNS 是 k8s 集群首要部署的组件，它为集群中的其他 pods 提供域名解析服务；主要可以解析 集群服务名 SVC 和 Pod hostname；目前建议部署 coredns。 ","date":"2023-01-06","objectID":"/posts/kubernetes/primary/kubernetes-1/:11:0","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署 (一)","uri":"/posts/kubernetes/primary/kubernetes-1/"},{"categories":["Kubernetes"],"content":"10.1 下载二进制包 kubernetes/CHANGELOG-1.24.md at master · kubernetes/kubernetes root@master01:/usr/local/src# ll total 489740 drwxr-xr-x 2 root root 4096 Jan 4 13:09 ./ drwxr-xr-x 13 root root 4096 Jan 1 13:20 ../ -rw-r--r-- 1 root root 30495559 Jan 4 13:09 kubernetes-client-linux-amd64.tar.gz -rw-r--r-- 1 root root 123361203 Jan 4 13:09 kubernetes-node-linux-amd64.tar.gz -rw-r--r-- 1 root root 347075448 Jan 4 13:09 kubernetes-server-linux-amd64.tar.gz -rw-r--r-- 1 root root 532769 Jan 4 13:09 kubernetes.tar.gz #解压后 root@master01:/usr/local/src/kubernetes# ll total 36996 drwxr-xr-x 10 root root 4096 Dec 8 18:31 ./ drwxr-xr-x 3 root root 4096 Jan 4 13:11 ../ drwxr-xr-x 2 root root 4096 Dec 8 18:26 addons/ drwxr-xr-x 3 root root 4096 Dec 8 18:31 client/ drwxr-xr-x 9 root root 4096 Dec 8 18:31 cluster/ drwxr-xr-x 2 root root 4096 Dec 8 18:31 docs/ drwxr-xr-x 3 root root 4096 Dec 8 18:31 hack/ -rw-r--r-- 1 root root 37826576 Dec 8 18:26 kubernetes-src.tar.gz drwxr-xr-x 4 root root 4096 Dec 8 18:31 LICENSES/ drwxr-xr-x 3 root root 4096 Dec 8 18:25 node/ -rw-r--r-- 1 root root 4443 Dec 8 18:31 README.md drwxr-xr-x 3 root root 4096 Dec 8 18:31 server/ -rw-r--r-- 1 root root 8 Dec 8 18:31 version #插件目录 root@master01:/usr/local/src/kubernetes/cluster/addons# ll total 80 drwxr-xr-x 18 root root 4096 Dec 8 18:31 ./ drwxr-xr-x 9 root root 4096 Dec 8 18:31 ../ drwxr-xr-x 2 root root 4096 Dec 8 18:31 addon-manager/ drwxr-xr-x 3 root root 4096 Dec 8 18:31 calico-policy-controller/ drwxr-xr-x 3 root root 4096 Dec 8 18:31 cluster-loadbalancing/ drwxr-xr-x 3 root root 4096 Dec 8 18:31 device-plugins/ drwxr-xr-x 5 root root 4096 Dec 8 18:31 dns/ drwxr-xr-x 2 root root 4096 Dec 8 18:31 dns-horizontal-autoscaler/ drwxr-xr-x 4 root root 4096 Dec 8 18:31 fluentd-gcp/ drwxr-xr-x 3 root root 4096 Dec 8 18:31 ip-masq-agent/ drwxr-xr-x 2 root root 4096 Dec 8 18:31 kube-proxy/ drwxr-xr-x 3 root root 4096 Dec 8 18:31 metadata-agent/ drwxr-xr-x 3 root root 4096 Dec 8 18:31 metadata-proxy/ drwxr-xr-x 2 root root 4096 Dec 8 18:31 metrics-server/ drwxr-xr-x 5 root root 4096 Dec 8 18:31 node-problem-detector/ -rw-r--r-- 1 root root 104 Dec 8 18:31 OWNERS drwxr-xr-x 8 root root 4096 Dec 8 18:31 rbac/ -rw-r--r-- 1 root root 1655 Dec 8 18:31 README.md drwxr-xr-x 8 root root 4096 Dec 8 18:31 storage-class/ drwxr-xr-x 4 root root 4096 Dec 8 18:31 volumesnapshots/ root@master01:/usr/local/src/kubernetes/cluster/addons/dns/coredns# ll total 44 drwxr-xr-x 2 root root 4096 Dec 8 18:31 ./ drwxr-xr-x 5 root root 4096 Dec 8 18:31 ../ -rw-r--r-- 1 root root 5060 Dec 8 18:31 coredns.yaml.base -rw-r--r-- 1 root root 5110 Dec 8 18:31 coredns.yaml.in -rw-r--r-- 1 root root 5112 Dec 8 18:31 coredns.yaml.sed -rw-r--r-- 1 root root 1075 Dec 8 18:31 Makefile -rw-r--r-- 1 root root 344 Dec 8 18:31 transforms2salt.sed -rw-r--r-- 1 root root 287 Dec 8 18:31 transforms2sed.sed cp coredns.yaml.base /root/ mv /root/coredns.yaml.base /root/coredns-ceamg.yaml vim /root/coredns-ceamg.yaml","date":"2023-01-06","objectID":"/posts/kubernetes/primary/kubernetes-1/:11:1","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署 (一)","uri":"/posts/kubernetes/primary/kubernetes-1/"},{"categories":["Kubernetes"],"content":"10.2 修改配置文件 主要配置参数： error: #错误⽇志输出到stdout。 health： #CoreDNS的运⾏状况报告为http://localhost:8080/health. cache： #启⽤coredns缓存。 reload：#配置⾃动重新加载配置⽂件，如果修改了ConfigMap的配置，会在两分钟后⽣效. loadbalance：#⼀个域名有多个记录会被轮询解析。 cache 30 #缓存时间 kubernetes：#CoreDNS将根据指定的service domain名称在Kubernetes SVC中进⾏域名解析。 forward： #不是Kubernetes集群域内的域名查询都进⾏转发指定的服务器（/etc/resolv.conf） prometheus：#CoreDNS的指标数据可以配置Prometheus 访问http://coredns svc:9153/metrics 进⾏收集。 ready：#当coredns 服务启动完成后会进⾏在状态监测，会有个URL 路径为/ready返回200状态码，否则返回报错。kubernetes __DNS__DOMAIN_是 clusters/k8s-01/hosts 中填写的内容CLUSTER_DNS_DOMAIN # Cluster DNS Domain CLUSTER_DNS_DOMAIN=\"ceamg.local\"212 clusterIP: __DNS__SERVER__是clusters/k8s-01/hosts 中填写的内容SERVICE_CIDR 第二个IP 也就是 10.10.0.2 # K8S Service CIDR, not overlap with node(host) networking SERVICE_CIDR=\"10.10.0.0/16 / # cat /etc/resolv.conf search default.svc.ceamg.local svc.ceamg.local ceamg.local nameserver 10.10.0.2修改如下行内容： 77 kubernetes ceamg.local in-addr.arpa ip6.arpa { 83 forward . 192.168.0.15 { 142 image: harbor.ceamg.com/baseimages/coredns:v1.8.6 145 limits: 146 memory: 2048Mi 147 requests: 148 cpu: 1000m 149 memory: 1024Mi 212 clusterIP: 10.10.0.2 209 spec: 210 type: NodePort 211 selector: 212 k8s-app: kube-dns 213 clusterIP: 10.10.0.2 214 ports: 215 - name: dns 216 port: 53 217 protocol: UDP 218 - name: dns-tcp 219 port: 53 220 protocol: TCP 221 - name: metrics 222 port: 9153 223 protocol: TCP 224 targetPort: 9153 225 nodePort: 30009 查看资源格式： kubectl explain ","date":"2023-01-06","objectID":"/posts/kubernetes/primary/kubernetes-1/:11:2","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署 (一)","uri":"/posts/kubernetes/primary/kubernetes-1/"},{"categories":["Kubernetes"],"content":"10.3 下载镜像并推送到harbor root@master01:/usr/local/src/kubernetes/cluster/addons/dns/coredns# nerdctl pull registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:v1.8.6 root@master01:/usr/local/src/kubernetes/cluster/addons/dns/coredns# nerdctl tag registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:v1.8.6 harbor.ceamg.com/baseimages/coredns:v1.8.6 root@master01:/usr/local/src/kubernetes/cluster/addons/dns/coredns# nerdctl push harbor.ceamg.com/baseimages/coredns:v1.8.6 INFO[0000] pushing as a reduced-platform image (application/vnd.docker.distribution.manifest.list.v2+json, sha256:53011ff05d62cd740ae785a98f646ace63374073b0e564a35d4cea008f040940) ","date":"2023-01-06","objectID":"/posts/kubernetes/primary/kubernetes-1/:11:3","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署 (一)","uri":"/posts/kubernetes/primary/kubernetes-1/"},{"categories":["Kubernetes"],"content":"10.4 安装coredns root@master01:/usr/local/src/kubernetes/cluster/addons/dns/coredns# kubectl apply -f /root/coredns-ceamg.yaml serviceaccount/coredns created clusterrole.rbac.authorization.k8s.io/system:coredns created clusterrolebinding.rbac.authorization.k8s.io/system:coredns created configmap/coredns created deployment.apps/coredns created service/kube-dns created root@master01:~# kubectl get pod -A NAMESPACE NAME READY STATUS RESTARTS AGE default net-test1 1/1 Running 0 25m default net-test2 1/1 Running 0 25m default net-test3 1/1 Running 0 25m default net-test4 1/1 Running 0 49m kube-system calico-kube-controllers-5c8bb696bb-hf2cp 1/1 Running 1 (57m ago) 151m kube-system calico-node-6nlt6 1/1 Running 0 151m kube-system calico-node-fd6rj 1/1 Running 0 151m kube-system calico-node-lhgh4 1/1 Running 0 151m kube-system coredns-6c496b89f6-hd8vf 1/1 Running 0 3s","date":"2023-01-06","objectID":"/posts/kubernetes/primary/kubernetes-1/:11:4","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署 (一)","uri":"/posts/kubernetes/primary/kubernetes-1/"},{"categories":["Kubernetes"],"content":"10.5 启动容器测试域名解析 root@master01:~# kubectl exec -it net-test1 error: you must specify at least one command for the container root@master01:~# kubectl exec -it net-test1 -- sh / # / # ping www.baidu.com PING www.baidu.com (110.242.68.3): 56 data bytes 64 bytes from 110.242.68.3: seq=0 ttl=49 time=9.778 ms --- www.baidu.com ping statistics --- 1 packets transmitted, 1 packets received, 0% packet loss round-trip min/avg/max = 9.778/9.778/9.778 ms / # / # cat /etc/resolv.conf search default.svc.ceamg.local svc.ceamg.local ceamg.local nameserver 10.10.0.2 options ndots:5","date":"2023-01-06","objectID":"/posts/kubernetes/primary/kubernetes-1/:11:5","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署 (一)","uri":"/posts/kubernetes/primary/kubernetes-1/"},{"categories":["Kubernetes"],"content":"10.6 测试 prometheus 监控项 http://10.1.0.31:30009/metrics ","date":"2023-01-06","objectID":"/posts/kubernetes/primary/kubernetes-1/:11:6","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署 (一)","uri":"/posts/kubernetes/primary/kubernetes-1/"},{"categories":["Kubernetes"],"content":"11. 步骤8-安装集群插件-dashboard https://github.com/kubernetes/dashboard ","date":"2023-01-06","objectID":"/posts/kubernetes/primary/kubernetes-1/:12:0","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署 (一)","uri":"/posts/kubernetes/primary/kubernetes-1/"},{"categories":["Kubernetes"],"content":"11.1 下载对应kubernetes版本的dashboard Compatibility Kubernetes version 1.21 1.22 1.23 1.24 Compatibility ? ? ? ✓ ✓ Fully supported version range. ? Due to breaking changes between Kubernetes API versions, some features might not work correctly in the Dashboard. # Copyright 2017 The Kubernetes Authors. # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. apiVersion: v1 kind: Namespace metadata: name: kubernetes-dashboard --- apiVersion: v1 kind: ServiceAccount metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboard --- kind: Service apiVersion: v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboard spec: ports: - port: 443 targetPort: 8443 selector: k8s-app: kubernetes-dashboard --- apiVersion: v1 kind: Secret metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-certs namespace: kubernetes-dashboard type: Opaque --- apiVersion: v1 kind: Secret metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-csrf namespace: kubernetes-dashboard type: Opaque data: csrf: \"\" --- apiVersion: v1 kind: Secret metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-key-holder namespace: kubernetes-dashboard type: Opaque --- kind: ConfigMap apiVersion: v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-settings namespace: kubernetes-dashboard --- kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboard rules: # Allow Dashboard to get, update and delete Dashboard exclusive secrets. - apiGroups: [\"\"] resources: [\"secrets\"] resourceNames: [\"kubernetes-dashboard-key-holder\", \"kubernetes-dashboard-certs\", \"kubernetes-dashboard-csrf\"] verbs: [\"get\", \"update\", \"delete\"] # Allow Dashboard to get and update 'kubernetes-dashboard-settings' config map. - apiGroups: [\"\"] resources: [\"configmaps\"] resourceNames: [\"kubernetes-dashboard-settings\"] verbs: [\"get\", \"update\"] # Allow Dashboard to get metrics. - apiGroups: [\"\"] resources: [\"services\"] resourceNames: [\"heapster\", \"dashboard-metrics-scraper\"] verbs: [\"proxy\"] - apiGroups: [\"\"] resources: [\"services/proxy\"] resourceNames: [\"heapster\", \"http:heapster:\", \"https:heapster:\", \"dashboard-metrics-scraper\", \"http:dashboard-metrics-scraper\"] verbs: [\"get\"] --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard rules: # Allow Metrics Scraper to get metrics from the Metrics server - apiGroups: [\"metrics.k8s.io\"] resources: [\"pods\", \"nodes\"] verbs: [\"get\", \"list\", \"watch\"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboard roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: kubernetes-dashboard subjects: - kind: ServiceAccount name: kubernetes-dashboard namespace: kubernetes-dashboard --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: kubernetes-dashboard roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: kubernetes-dashboard subjects: - kind: ServiceAccount name: kubernetes-dashboard namespace: kubernetes-dashboard --- kind: Deployment apiVersion: apps/v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboard spec: replicas: 1 revisionHis","date":"2023-01-06","objectID":"/posts/kubernetes/primary/kubernetes-1/:12:1","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署 (一)","uri":"/posts/kubernetes/primary/kubernetes-1/"},{"categories":["Kubernetes"],"content":"11.2 修改service暴露方式 32 kind: Service 33 apiVersion: v1 34 metadata: 35 labels: 36 k8s-app: kubernetes-dashboard 37 name: kubernetes-dashboard 38 namespace: kubernetes-dashboard 39 spec: 40 type: NodePort 41 ports: 42 - port: 443 43 targetPort: 8443 44 nodePort: 30010 45 selector: 46 k8s-app: kubernetes-dashboard","date":"2023-01-06","objectID":"/posts/kubernetes/primary/kubernetes-1/:12:2","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署 (一)","uri":"/posts/kubernetes/primary/kubernetes-1/"},{"categories":["Kubernetes"],"content":"11.3 下载镜像推送到harbor root@master01:~# cat k8s-dashboard-ceamg.yml | grep image image: kubernetesui/dashboard:v2.6.1 imagePullPolicy: Always image: kubernetesui/metrics-scraper:v1.0.8 root@master01:~#nerdctl pull kubernetesui/dashboard:v2.6.1 root@master01:~# nerdctl pull kubernetesui/metrics-scraper:v1.0.8 root@master01:~# nerdctl tag kubernetesui/dashboard:v2.6.1 harbor.ceamg.com/baseimages/dashboard:v2.6.1 root@master01:~# nerdctl push harbor.ceamg.com/baseimages/dashboard:v2.6.1 INFO[0000] pushing as a reduced-platform image (application/vnd.docker.distribution.manifest.list.v2+json, sha256:f12df071f8bad3e1965b5246095bd3f78df0eb76ceabcc0878d42849d33e4a10) index-sha256:f12df071f8bad3e1965b5246095bd3f78df0eb76ceabcc0878d42849d33e4a10: done |++++++++++++++++++++++++++++++++++++++| manifest-sha256:d95e1adbe846450bf9451f9c95ab33865115909cf3962960af5983bb916cf320: done |++++++++++++++++++++++++++++++++++++++| config-sha256:783e2b6d87ed93a9f9fee34e84c2b029b7a9572b2f41f761437e58af9c26827f: done |++++++++++++++++++++++++++++++++++++++| elapsed: 3.2 s total: 2.5 Ki (814.0 B/s) root@master01:~# root@master01:~# nerdctl tag kubernetesui/metrics-scraper:v1.0.8 harbor.ceamg.com/baseimages/metrics-scraper:v1.0.8 root@master01:~# nerdctl push harbor.ceamg.com/baseimages/metrics-scraper:v1.0.8 INFO[0000] pushing as a reduced-platform image (application/vnd.docker.distribution.manifest.list.v2+json, sha256:9fdef455b4f9a8ee315a0aa3bd71787cfd929e759da3b4d7e65aaa56510d747b) index-sha256:9fdef455b4f9a8ee315a0aa3bd71787cfd929e759da3b4d7e65aaa56510d747b: done |++++++++++++++++++++++++++++++++++++++| manifest-sha256:43227e8286fd379ee0415a5e2156a9439c4056807e3caa38e1dd413b0644807a: done |++++++++++++++++++++++++++++++++++++++| config-sha256:115053965e86b2df4d78af78d7951b8644839d20a03820c6df59a261103315f7: done |++++++++++++++++++++++++++++++++++++++| elapsed: 0.8 s total: 2.2 Ki (2.7 KiB/s) ","date":"2023-01-06","objectID":"/posts/kubernetes/primary/kubernetes-1/:12:3","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署 (一)","uri":"/posts/kubernetes/primary/kubernetes-1/"},{"categories":["Kubernetes"],"content":"11.4 修改镜像地址 195 image: harbor.ceamg.com/baseimages/dashboard:v2.6.1 280 image: harbor.ceamg.com/baseimages/metrics-scraper:v1.0.8","date":"2023-01-06","objectID":"/posts/kubernetes/primary/kubernetes-1/:12:4","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署 (一)","uri":"/posts/kubernetes/primary/kubernetes-1/"},{"categories":["Kubernetes"],"content":"11.5 安装dashboard组件 kubectl apply -f k8s-dashboard-ceamg.yml","date":"2023-01-06","objectID":"/posts/kubernetes/primary/kubernetes-1/:12:5","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署 (一)","uri":"/posts/kubernetes/primary/kubernetes-1/"},{"categories":["Kubernetes"],"content":"11.6 查看组件运行状态 root@master01:~# kubectl get pod -A NAMESPACE NAME READY STATUS RESTARTS AGE default net-test1 1/1 Running 0 99m default net-test2 1/1 Running 0 99m default net-test3 1/1 Running 0 99m default net-test4 1/1 Running 0 123m kube-system calico-kube-controllers-5c8bb696bb-hf2cp 1/1 Running 1 (131m ago) 3h45m kube-system calico-node-6nlt6 1/1 Running 0 3h45m kube-system calico-node-fd6rj 1/1 Running 0 3h45m kube-system calico-node-lhgh4 1/1 Running 0 3h45m kube-system coredns-6c496b89f6-hd8vf 1/1 Running 0 73m kubernetes-dashboard dashboard-metrics-scraper-8b9c56ffb-tjjc4 1/1 Running 0 14s kubernetes-dashboard kubernetes-dashboard-6f9f585c48-vv2pz 1/1 Running 0 14s","date":"2023-01-06","objectID":"/posts/kubernetes/primary/kubernetes-1/:12:6","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署 (一)","uri":"/posts/kubernetes/primary/kubernetes-1/"},{"categories":["Kubernetes"],"content":"11.7 获取登陆 token apiVersion: v1 kind: ServiceAccount metadata: name: admin-user namespace: kubernetes-dashboard --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin-user namespace: kubernetes-dashboard root@master01:~# kubectl apply -f admin-user.yml serviceaccount/admin-user created clusterrolebinding.rbac.authorization.k8s.io/admin-user created:::warning 注意：v1.24.0 更新之后进行创建 ServiceAccount 不会自动生成 Secret 需要对其手动创建 ::: # 创建token root@master01:~# kubectl -n kubernetes-dashboard create token admin-user --duration 604800s eyJhbGciOiJSUzI1NiIsImtpZCI6ImptTldRRDRZZVVSdXRhaU1RNUtyQmJUSmVTbW55VThqNHhLU1l6U3B4R28ifQ.eyJhdWQiOlsiYXBpIiwiaXN0aW8tY2EiXSwiZXhwIjoxNjczNDI1MDI3LCJpYXQiOjE2NzI4MjAyMjcsImlzcyI6Imh0dHBzOi8va3ViZXJuZXRlcy5kZWZhdWx0LnN2YyIsImt1YmVybmV0ZXMuaW8iOnsibmFtZXNwYWNlIjoia3ViZXJuZXRlcy1kYXNoYm9hcmQiLCJzZXJ2aWNlYWNjb3VudCI6eyJuYW1lIjoiYWRtaW4tdXNlciIsInVpZCI6IjdhNTAzN2E4LTQ2MGEtNGM3YS05NWQ5LTNjM2JkNGQ0YTUyZSJ9fSwibmJmIjoxNjcyODIwMjI3LCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZXJuZXRlcy1kYXNoYm9hcmQ6YWRtaW4tdXNlciJ9.ciX6c6hUe8NqPHWp7GteAecZ75L50sKL0l0jk6hETJM9xUVkE-knhm-wQWogOCq1vJMWtg_qeYqsyxfFAMbXdnGgXUXH3tuLVe0NcSHfGVa0BBfjUqODODoAcKdEWJJqdTO_QCfzHTTGkBDZoPgqjALBFzMVh_anlUdeSehRtTh6y2L0dsMRbWuEp1YI8phXumRGIbsrRDOenCycfyPh2AUEChMhD_uYS85z2tDVbno-1y4sSoSiPPn-awUEAxo-ly7zIOUz_b6ZiMhM6nGTuJ-7Jyxq4A8f2pj-iyXA_ve3g1Y4AaInd1aaZhCQ_82rOpmHP0Idyzg_lqEneltBaw方式二手动创建secrit root@master01:/zookeeper# kubectl apply -f secrit secret/admin-user created apiVersion: v1 kind: Secret type: kubernetes.io/service-account-token metadata: name: admin-user namespace: kubernetes-dashboard annotations: kubernetes.io/service-account.name: \"admin-user\" root@master01:/zookeeper# kubectl -n kubernetes-dashboard describe sa admin-user Name: admin-user Namespace: kubernetes-dashboard Labels: \u003cnone\u003e Annotations: \u003cnone\u003e Image pull secrets: \u003cnone\u003e Mountable secrets: \u003cnone\u003e Tokens: admin-user Events: \u003cnone\u003e root@master01:/zookeeper# kubectl -n kubernetes-dashboard describe secrets admin-user Name: admin-user Namespace: kubernetes-dashboard Labels: \u003cnone\u003e Annotations: kubernetes.io/service-account.name: admin-user kubernetes.io/service-account.uid: 7a5037a8-460a-4c7a-95d9-3c3bd4d4a52e Type: kubernetes.io/service-account-token Data ==== namespace: 20 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6ImptTldRRDRZZVVSdXRhaU1RNUtyQmJUSmVTbW55VThqNHhLU1l6U3B4R28ifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiI3YTUwMzdhOC00NjBhLTRjN2EtOTVkOS0zYzNiZDRkNGE1MmUiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZXJuZXRlcy1kYXNoYm9hcmQ6YWRtaW4tdXNlciJ9.YIZ1UepKs7WzebxKMOVIPkmz0KLkIyV59S7D0x4sBpefqseX6lSfV_YbhDjQv0dm6ne9HJ85dHzF1-qmSJEO_EW3m-aNOfmem7jkqr8XDUIgHceeKZimauTodKvApsWWD_Flsk7r2nin-MoNkOJ5mi6g5Pu3iQuKhQINl3G9Wwch5c-5FV0l-RBWR1rw9rVby6fh1jfkAhMWGL7lWKJeAA6fE2dTJVSJ-ZhW_bzwPTTDKNhIlpRsyKEnFXwWmK9Jqoxq8y5H0iJIhbvkYCxwUG2Gjjfi6jIWhJvWo20_kTq5Cy-7BNXafBI5D6VKmFwHFyOLBQcvkntN2IpVRNcfbA ca.crt: 1302 byteshttps://blog.csdn.net/qq_41619571/article/details/127217339 ","date":"2023-01-06","objectID":"/posts/kubernetes/primary/kubernetes-1/:12:7","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署 (一)","uri":"/posts/kubernetes/primary/kubernetes-1/"},{"categories":["Kubernetes"],"content":"11.8 登录测试 ","date":"2023-01-06","objectID":"/posts/kubernetes/primary/kubernetes-1/:12:8","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署 (一)","uri":"/posts/kubernetes/primary/kubernetes-1/"},{"categories":["Kubernetes"],"content":"12. 集群管理 集群管理主要是添加master、添加node、删除master与删除node等节点管理及监控 ","date":"2023-01-06","objectID":"/posts/kubernetes/primary/kubernetes-1/:13:0","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署 (一)","uri":"/posts/kubernetes/primary/kubernetes-1/"},{"categories":["Kubernetes"],"content":"12.1 添加node节点 ./ezctl add-node k8s-01 10.1.0.39","date":"2023-01-06","objectID":"/posts/kubernetes/primary/kubernetes-1/:13:1","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署 (一)","uri":"/posts/kubernetes/primary/kubernetes-1/"},{"categories":["Kubernetes"],"content":"12.2 添加master 节点 root@master01:/etc/kubeasz# ./ezctl add-master k8s-01 10.1.0.30master 节点添加后会向 node节点 /etc/kube-lb/conf/kube-lb.conf 中添加反向代理节点 user root; worker_processes 1; error_log /etc/kube-lb/logs/error.log warn; events { worker_connections 3000; } stream { upstream backend { server 10.1.0.30:6443 max_fails=2 fail_timeout=3s; server 10.1.0.31:6443 max_fails=2 fail_timeout=3s; } server { listen 127.0.0.1:6443; proxy_connect_timeout 1s; proxy_pass backend; } }","date":"2023-01-06","objectID":"/posts/kubernetes/primary/kubernetes-1/:13:2","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署 (一)","uri":"/posts/kubernetes/primary/kubernetes-1/"},{"categories":["Kubernetes"],"content":"12.3 验证当前节点 root@master02:~# kubectl get nodes NAME STATUS ROLES AGE VERSION 10.1.0.30 Ready,SchedulingDisabled master 15m v1.24.2 10.1.0.31 Ready,SchedulingDisabled master 44h v1.24.2 10.1.0.32 Ready node 44h v1.24.2 10.1.0.33 Ready node 44h v1.24.2 root@master02:~# calicoctl node status Calico process is running. IPv4 BGP status +--------------+-------------------+-------+----------+-------------+ | PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO | +--------------+-------------------+-------+----------+-------------+ | 10.1.0.31 | node-to-node mesh | up | 02:47:43 | Established | | 10.1.0.32 | node-to-node mesh | up | 02:47:12 | Established | | 10.1.0.33 | node-to-node mesh | up | 02:47:31 | Established | +--------------+-------------------+-------+----------+-------------+ IPv6 BGP status No IPv6 peers found.","date":"2023-01-06","objectID":"/posts/kubernetes/primary/kubernetes-1/:13:3","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署 (一)","uri":"/posts/kubernetes/primary/kubernetes-1/"},{"categories":["Kubernetes"],"content":"13. 集群升级 先升级master 保证集群中至少有一个master节点可用 ，在node节点nginx反向代理中注释掉要升级的master节点。 ","date":"2023-01-06","objectID":"/posts/kubernetes/primary/kubernetes-1/:14:0","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署 (一)","uri":"/posts/kubernetes/primary/kubernetes-1/"},{"categories":["Kubernetes"],"content":"13.1 升级master节点 在各个node节点反向代理配置中注释掉要升级的master节点 vim /etc/kube-lb/conf/kube-lb.conf user root; worker_processes 1; error_log /etc/kube-lb/logs/error.log warn; events { worker_connections 3000; } stream { upstream backend { #server 10.1.0.30:6443 max_fails=2 fail_timeout=3s; server 10.1.0.31:6443 max_fails=2 fail_timeout=3s; } server { listen 127.0.0.1:6443; proxy_connect_timeout 1s; proxy_pass backend; } } #重启服务 root@worker01:~# systemctl restart kube-lb.servicenode节点升级需要停服务，需要关闭kubelet 和 kube-proxy服务替换二进制文件 kube-apiserver kube-controller-manager kubectl kubelet kube-proxy kube-scheduler 去github找到想要升级的版本下载二进制文件： https://github.com/kubernetes/kubernetes/releases root@master01:/usr/local/src# ll total 489744 drwxr-xr-x 3 root root 4096 Jan 4 13:11 ./ drwxr-xr-x 13 root root 4096 Jan 1 13:20 ../ drwxr-xr-x 10 root root 4096 Dec 8 18:31 kubernetes/ -rw-r--r-- 1 root root 30495559 Jan 4 13:09 kubernetes-client-linux-amd64.tar.gz -rw-r--r-- 1 root root 123361203 Jan 4 13:09 kubernetes-node-linux-amd64.tar.gz -rw-r--r-- 1 root root 347075448 Jan 4 13:09 kubernetes-server-linux-amd64.tar.gz -rw-r--r-- 1 root root 532769 Jan 4 13:09 kubernetes.tar.gz二进制文件在/server/bin目录下面 root@master01:/usr/local/src/kubernetes/server/bin# ll -ls total 1090008 4 drwxr-xr-x 2 root root 4096 Dec 8 18:26 ./ 4 drwxr-xr-x 3 root root 4096 Dec 8 18:31 ../ 54176 -rwxr-xr-x 1 root root 55476224 Dec 8 18:26 apiextensions-apiserver* 43380 -rwxr-xr-x 1 root root 44421120 Dec 8 18:26 kubeadm* 48408 -rwxr-xr-x 1 root root 49569792 Dec 8 18:26 kube-aggregator* 123032 -rwxr-xr-x 1 root root 125980672 Dec 8 18:26 kube-apiserver* 4 -rw-r--r-- 1 root root 8 Dec 8 18:25 kube-apiserver.docker_tag 128092 -rw------- 1 root root 131165184 Dec 8 18:25 kube-apiserver.tar 112896 -rwxr-xr-x 1 root root 115605504 Dec 8 18:26 kube-controller-manager* 4 -rw-r--r-- 1 root root 8 Dec 8 18:25 kube-controller-manager.docker_tag 117960 -rw------- 1 root root 120790016 Dec 8 18:25 kube-controller-manager.tar 44680 -rwxr-xr-x 1 root root 45752320 Dec 8 18:26 kubectl* 53796 -rwxr-xr-x 1 root root 55085992 Dec 8 18:26 kubectl-convert* 113376 -rwxr-xr-x 1 root root 116095704 Dec 8 18:26 kubelet* 1452 -rwxr-xr-x 1 root root 1486848 Dec 8 18:26 kube-log-runner* 40820 -rwxr-xr-x 1 root root 41799680 Dec 8 18:26 kube-proxy* 4 -rw-r--r-- 1 root root 8 Dec 8 18:25 kube-proxy.docker_tag 109280 -rw------- 1 root root 111901184 Dec 8 18:25 kube-proxy.tar 46096 -rwxr-xr-x 1 root root 47202304 Dec 8 18:26 kube-scheduler* 4 -rw-r--r-- 1 root root 8 Dec 8 18:25 kube-scheduler.docker_tag 51160 -rw------- 1 root root 52386816 Dec 8 18:25 kube-scheduler.tar 1380 -rwxr-xr-x 1 root root 1413120 Dec 8 18:26 mounter*root@master01:/usr/local/src/kubernetes/server/bin# ./kube-apiserver --version Kubernetes v1.24.9 #当前版本 root@master01:/usr/local/src/kubernetes/server/bin# /etc/kubeasz/bin/kube-apiserver --version Kubernetes v1.24.2停止服务 systemctl stop kube-proxy kube-controller-manager kubelet kube-scheduler kube-apiserver替换二进制文件 root@master01:/usr/local/src/kubernetes/server/bin# scp kube-apiserver kube-controller-manager kubelet kube-scheduler kube-proxy 10.1.0.30:/usr/local/bin kube-apiserver 100% 120MB 129.5MB/s 00:00 kube-controller-manager 100% 110MB 128.8MB/s 00:00 kubelet 100% 111MB 137.1MB/s 00:00 kube-scheduler 100% 45MB 128.5MB/s 00:00 kube-proxy 100% 40MB 132.0MB/s 00:00启动服务 systemctl start kube-proxy kube-controller-manager kubelet kube-scheduler kube-apiserver验证版本 root@master02:~# kubectl get nodes NAME STATUS ROLES AGE VERSION 10.1.0.30 Ready,SchedulingDisabled master 136m v1.24.9 10.1.0.31 Ready,SchedulingDisabled master 46h v1.24.2 10.1.0.32 Ready node 46h v1.24.2 10.1.0.33 Ready node 46h v1.24.2在另外的master节点上重复以上操作 root@master01:/usr/local/src/kubernetes/server/bin# systemctl stop kube-proxy kube-controller-manager kubelet kube-scheduler kube-apiserver root@master01:/usr/local/src/kubernetes/server/bin# \\cp kube-apiserver kube-controller-manager kubelet kube-scheduler kube-proxy /usr/local/bin root@m","date":"2023-01-06","objectID":"/posts/kubernetes/primary/kubernetes-1/:14:1","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署 (一)","uri":"/posts/kubernetes/primary/kubernetes-1/"},{"categories":["Kubernetes"],"content":"13.2 升级node节点 node节点只需要替换kubelet 和 kube-proxy 两个 关闭服务 root@worker01:~# systemctl stop kubelet.service kube-proxy.service替换二进制文件 root@master01:/usr/local/src/kubernetes/server/bin# scp kubelet kube-proxy 10.1.0.32:/usr/local/bin kubelet 100% 111MB 134.8MB/s 00:00 kube-proxy 100% 40MB 139.3MB/s 00:00 root@master01:/usr/local/src/kubernetes/server/bin# scp kubelet kube-proxy 10.1.0.33:/usr/local/bin启动服务 root@worker01:~# systemctl start kubelet.service kube-proxy.service验证版本 root@master02:~# kubectl get nodes NAME STATUS ROLES AGE VERSION 10.1.0.30 Ready,SchedulingDisabled master 157m v1.24.9 10.1.0.31 Ready,SchedulingDisabled master 47h v1.24.9 10.1.0.32 Ready node 46h v1.24.9 10.1.0.33 Ready node 46h v1.24.9","date":"2023-01-06","objectID":"/posts/kubernetes/primary/kubernetes-1/:14:2","tags":["k8s进阶训练营"],"title":"Kubernetes 二进制部署 (一)","uri":"/posts/kubernetes/primary/kubernetes-1/"},{"categories":["Kubernetes"],"content":"etcd是CoreOS团队于2013年6月发起的开源项目，它的目标是构建一个高可用的分 布式键值(key-value)数据库。etcd内部采用raft协议作为-致性算法，etcd基于Go语言实现。 官方网站：https://etcd.iogithub地址: https://github.com/etcd-io/etcd官方硬件推荐：https://etcd.io/docs/v3.4/op-guide/hardware/ 为什么k8s使用etcd？ Etcd 特有优势 完全复制: 集群中的每个节点都可以使用完整的存档高可用性: Etcd可用于避免硬件的单点故障或网络问题一致性: 每次读取都会返回跨多主机的最新写入简单: 包括一个定义良好、面向用户的API (gRPC)安全: 实现了带有可选的客户端证书身份验证的自动化TLS快速: 每秒10000次写入的基准速度可靠: 使用Raft算法实现了存储的合理分布Etcd的工作原理 etcd 存储这k8s整个集群的数据，一定要做好定期备份 因为etcd数据存储在硬盘上，读写IO速度关系着集群中pod的运行，etcd集群最好使用固态硬盘并且内存要大一点。 中间件： 复制式集群 mysql 集群 zookeeper etcd redis 哨兵 分片式： redis cluster kafka elasticsearch启动脚本参数: root@k8s-etcd1:-# cat /etc/systemd/system/etcd. service Description=Etcd Server After=network.target After=network-online.target Wants=network-online.target Documentation=https://github.com/coreos [Service] Type=notify WorkingDirectory=/var/lib/etcd/ #数据保存目录ExecStart=/usr/local/bin/etcd \\ #二进制文件路径 --name=etcd1 \\ #当前node 名称 --cert-file=/etc/etcd/ssl/etcd.pem --key-file=/etc/etcd/ssl/etcd-key.pem --peer-cert-file=/etc/etcd/ssl/etcd.pem --peer-key-file=/etc/etcd/ssl/etcd-key.pem --trusted-ca-file=/etc/kubernetes/ssl/ca.pem --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem --initial-advertise-peer-urls=https://192.168.7.101:2380 \\ #通告自己的集群端口 --listen-peer-urls=https://192.168.7.101:2380 \\ #集群之间通讯端口 --listen-client-urls=https://192.168.7.101:2379,http://127.0.0.1:2379 \\ #客户端访问地址 --advertise-client-urls=https://192.168.7.101:2379 \\ #通告自己的客户端端口 --initial-cluster-token=etcd-cluster-0 \\ #创建集群使用的token，一个集群内的节点保持一致 --initial-cluster=etcd1=https://192.168.7.101:2380,etcd2=https://192.168.7.102:2380,etcd3=https://192.168.7.103:2380 \\ #集群所有的节点信息 --initial-cluster-state=new \\ #新建集群的时候的值为new,如果是已经存在的集群为existing。 --data-dir=/var/lib/etcd #数据目录路径 Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target","date":"2023-01-05","objectID":"/posts/kubernetes/primary/kubernetes-2/:0:0","tags":["k8s进阶训练营","Etcd"],"title":"etcd 客户端使用、数据备份与恢复 （二）","uri":"/posts/kubernetes/primary/kubernetes-2/"},{"categories":["Kubernetes"],"content":"验证当前etcd所有成员状态 1.心跳信息 #export NODE_IPS=\"172.31.7.101 172.31.7.102 172.31.7.103\" # for ip in ${NODE_IPS}; do ETCDCTL_API=3 /usr/local/bin/etcdctl --endpoints=https://${ip}:2379 --cacert=/etc/kubernetes/ssl/ca.pem --cert=/etc/kubernetes/ssl/etcd.pem --key=/etc/kubernetes/ssl/etcd-key.pem endpoint health; done2. 显示集群成员信息 ETCDCTL_API=3 /usr/local/bin/etcdctl --write-out=table member list --endpoints=https://172.31.7.101:2379 --cacert=/etc/kubernetes/ssl/ca.pem --cert=/etc/kubernetes/ssl/etcd.pem --key=/etc/kubernetes/ssl/etcd-key.pem3.以表格方式显示节点详细状态 export NODE_IPS=\"172.31.7.101 172.31.7.102 172.31.7.103\" for ip in ${NODE_IPS}; do ETCDCTL_API=3 /usr/local/bin/etcdctl --write-out=table endpoint status --endpoints=https://${ip}:2379 --cacert=/etc/kubernetes/ssl/ca.pem --cert=/etc/kubernetes/ssl/etcd.pem --key=/etc/kubernetes/ssl/etcd-key.pem; done","date":"2023-01-05","objectID":"/posts/kubernetes/primary/kubernetes-2/:0:1","tags":["k8s进阶训练营","Etcd"],"title":"etcd 客户端使用、数据备份与恢复 （二）","uri":"/posts/kubernetes/primary/kubernetes-2/"},{"categories":["Kubernetes"],"content":"查看etcd数据信息 查看etcd集群中保存的数据 ","date":"2023-01-05","objectID":"/posts/kubernetes/primary/kubernetes-2/:1:0","tags":["k8s进阶训练营","Etcd"],"title":"etcd 客户端使用、数据备份与恢复 （二）","uri":"/posts/kubernetes/primary/kubernetes-2/"},{"categories":["Kubernetes"],"content":"1.查看所有key ETCDCTL_API=3 etcdctl get / --prefix --keys-only #以路径的方式所有key信息 #pod信息 ETCDCTL_API=3 etcdctl get / --prefix --keys-only | grep pod #namespace信息 ETCDCTL_API=3 etcdctl get / --prefix --keys-only | grep namespaces #控制器信息 ETCDCTL_API=3 etcdctl get / --prefix --keys-only | grep deployment #calico组件信息 ETCDCTL_API=3 etcdctl get / --prefix --keys-only | grep calico","date":"2023-01-05","objectID":"/posts/kubernetes/primary/kubernetes-2/:1:1","tags":["k8s进阶训练营","Etcd"],"title":"etcd 客户端使用、数据备份与恢复 （二）","uri":"/posts/kubernetes/primary/kubernetes-2/"},{"categories":["Kubernetes"],"content":"2.查看指定key root@etcd01:~# ETCDCTL_API=3 etcdctl get /calico/ipam/v2/assignment/ipv4/block/10.20.241.64-26 /calico/ipam/v2/assignment/ipv4/block/10.20.241.64-26 {\"cidr\":\"10.20.241.64/26\",\"affinity\":\"host:master01\",\"allocations\":[0,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],\"unallocated\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63],\"attributes\":[{\"handle_id\":\"ipip-tunnel-addr-master01\",\"secondary\":{\"node\":\"master01\",\"type\":\"ipipTunnelAddress\"}}],\"deleted\":false}","date":"2023-01-05","objectID":"/posts/kubernetes/primary/kubernetes-2/:1:2","tags":["k8s进阶训练营","Etcd"],"title":"etcd 客户端使用、数据备份与恢复 （二）","uri":"/posts/kubernetes/primary/kubernetes-2/"},{"categories":["Kubernetes"],"content":"3.查看所有calico的数据 root@etcd01:~# ETCDCTL_API=3 etcdctl get /calico --keys-only --prefix | grep calico","date":"2023-01-05","objectID":"/posts/kubernetes/primary/kubernetes-2/:1:3","tags":["k8s进阶训练营","Etcd"],"title":"etcd 客户端使用、数据备份与恢复 （二）","uri":"/posts/kubernetes/primary/kubernetes-2/"},{"categories":["Kubernetes"],"content":"etcd增删改查数据 #添加数据 root@etcd01:~# etcdctl put /name \"xin\" #查询数据 root@etcd01:~# etcdctl get /name /name xin #直接覆盖就是更新数据 root@etcd01:~# etcdctl get /name /name xxx #删除数据 root@etcd01:~# etcdctl del /name 1 root@etcd01:~# etcdctl get /name #删除pod root@etcd01:~# etcdctl del /registry/pods/default/net-test1 1 root@master02:~# kubectl get pods -A |grep net-test1 default net-test1 1/1 Running 0 24h root@master02:~# kubectl get pods -A |grep net-test1","date":"2023-01-05","objectID":"/posts/kubernetes/primary/kubernetes-2/:1:4","tags":["k8s进阶训练营","Etcd"],"title":"etcd 客户端使用、数据备份与恢复 （二）","uri":"/posts/kubernetes/primary/kubernetes-2/"},{"categories":["Kubernetes"],"content":"etcd数据watch机制 基于不断监看数据，发生变化就主动触发通知客户端，Etcd v3 的watch机制支持watch某个固定的key，也支持watch一个范围。 在etcd node1上watch一个key ，在etcdnode2修改数据，验证etcdnode1是否能够发现数据变化 root@etcd02:~# etcdctl put /data/name xin123 OK root@etcd01:~# etcdctl watch /data/name PUT /data/name xin123","date":"2023-01-05","objectID":"/posts/kubernetes/primary/kubernetes-2/:2:0","tags":["k8s进阶训练营","Etcd"],"title":"etcd 客户端使用、数据备份与恢复 （二）","uri":"/posts/kubernetes/primary/kubernetes-2/"},{"categories":["Kubernetes"],"content":"etcd 数据手动备份与恢复 WAL是write ahead log的缩写，顾名思义，也就是在执行真正的写操作之前先写一个日志，预写日志。wal: 存放预写式日志,最大的作用是记录了整个数据变化的全部历程。在etcd中，所有数据的修改在提交前，都要先写入到WAL中。 ","date":"2023-01-05","objectID":"/posts/kubernetes/primary/kubernetes-2/:3:0","tags":["k8s进阶训练营","Etcd"],"title":"etcd 客户端使用、数据备份与恢复 （二）","uri":"/posts/kubernetes/primary/kubernetes-2/"},{"categories":["Kubernetes"],"content":" #V3版本备份数据 root@etcd01:~# ETCDCTL_API=3 etcdctl snapshot save etcd-xin-0105.db {\"level\":\"info\",\"ts\":\"2023-01-05T15:15:28.192+0800\",\"caller\":\"snapshot/v3_snapshot.go:65\",\"msg\":\"created temporary db file\",\"path\":\"etcd-xin-0105.db.part\"} {\"level\":\"info\",\"ts\":\"2023-01-05T15:15:28.195+0800\",\"logger\":\"client\",\"caller\":\"v3/maintenance.go:211\",\"msg\":\"opened snapshot stream; downloading\"} {\"level\":\"info\",\"ts\":\"2023-01-05T15:15:28.195+0800\",\"caller\":\"snapshot/v3_snapshot.go:73\",\"msg\":\"fetching snapshot\",\"endpoint\":\"127.0.0.1:2379\"} {\"level\":\"info\",\"ts\":\"2023-01-05T15:15:28.227+0800\",\"logger\":\"client\",\"caller\":\"v3/maintenance.go:219\",\"msg\":\"completed snapshot read; closing\"} {\"level\":\"info\",\"ts\":\"2023-01-05T15:15:28.245+0800\",\"caller\":\"snapshot/v3_snapshot.go:88\",\"msg\":\"fetched snapshot\",\"endpoint\":\"127.0.0.1:2379\",\"size\":\"3.0 MB\",\"took\":\"now\"} {\"level\":\"info\",\"ts\":\"2023-01-05T15:15:28.245+0800\",\"caller\":\"snapshot/v3_snapshot.go:97\",\"msg\":\"saved\",\"path\":\"etcd-xin-0105.db\"} Snapshot saved at etcd-xin-0105.db #V3版本数据恢复 --data-dir 数据存储目录 root@etcd01:~# ETCDCTL_API=3 etcdctl snapshot restore etcd-xin-0105.db --data-dir=/tmp/etcd root@etcd01:~# mkdir /data/etcd-backup-dir/ -p root@etcd01:~# cat script.sh #!/bin/bash source /etc/profile DATE=`date +%Y-%m-%d_%H-%M-%S` ETCDCTL_API=3 /usr/local/bin/etcdctl snapshot save /data/etcd-backup-dir/etcd-snapshot-${DATE}.db 0 */12 * * * /bin/bash /root/etcd-backup.sh \u0026\u003e /dev/null","date":"2023-01-05","objectID":"/posts/kubernetes/primary/kubernetes-2/:3:1","tags":["k8s进阶训练营","Etcd"],"title":"etcd 客户端使用、数据备份与恢复 （二）","uri":"/posts/kubernetes/primary/kubernetes-2/"},{"categories":["Kubernetes"],"content":"使用kubeasz 备份与恢复数据 恢复数据期间master 节点 kube-apiserver/scheduler/controller-manager 服务不可用 ./ezctl backup k8s-01 kubectl delete pod net-test1 ./ezctl restore k8s-01","date":"2023-01-05","objectID":"/posts/kubernetes/primary/kubernetes-2/:3:2","tags":["k8s进阶训练营","Etcd"],"title":"etcd 客户端使用、数据备份与恢复 （二）","uri":"/posts/kubernetes/primary/kubernetes-2/"},{"categories":["Kubernetes"],"content":"ETCD 数据恢复流程 当etcd集群宕机数量超过集群总节点数一半以上的时候(如总数为三台宕机两台)，就会导致整合集群宕机，后期需要重新恢复数据，则恢复流程如下: 恢复服务器系统 重新部署ETCD集群 停止kube-apiserver/controller-manager/scheduler/kubelet/kube-proxy 停止ETCD集群 各ETCD节点恢复同一份备份数据 启动各节点并验证ETCD集群 启动kube-apiserver/controller-manager/scheduler/kubelet/kube-proxy 验证k8s master状态及pod数据 ","date":"2023-01-05","objectID":"/posts/kubernetes/primary/kubernetes-2/:3:3","tags":["k8s进阶训练营","Etcd"],"title":"etcd 客户端使用、数据备份与恢复 （二）","uri":"/posts/kubernetes/primary/kubernetes-2/"},{"categories":["Ceph"],"content":"分布式存储简介 分布式存储的数据分为数据和元数据，元数据即是文件的属性信息(文件名、权限(属主、属组)、大小、时间戳等)，在分布式存储中当客户端或者应用程序产生的客户端数据被写入到分布式存储系统的时候,会有一个服务(Name Node)提供文件元数据的路由功能，告诉应用程序去哪个服务器去请求文件内容，然后再有(Data Node)提供数据的读写请求及数据的高可用功能。 ","date":"2023-01-02","objectID":"/posts/ceph/1.ceph%E7%90%86%E8%AE%BA%E8%AF%A6%E8%A7%A3/:1:0","tags":["分布式存储"],"title":"Ceph 理论详解 (一)","uri":"/posts/ceph/1.ceph%E7%90%86%E8%AE%BA%E8%AF%A6%E8%A7%A3/"},{"categories":["Ceph"],"content":"1. Ceph 概述 Ceph 是一个开源的分布式存储系统，同时支持对象存储、块设备、文件系统。 Ceph 是一个对象(object)式存储系统，它把每一个待管理的数据流(文件等数据)切分为一到多个固定大小(默认 4 兆)的对象数据，并以其为原子单元(原子是构成元素的最小单元)完成数据的读写。 对象数据的底层存储服务是由多个存储主机(host)组成的存储集群，该集群也被称之为RADOS(reliable automatic distributed obiect store)存储集群，即可靠的、自动化的、分布式的对象存储系统。 LibRADOS 是 RADOS 存储集群的 API，支持 C/C++/JAVA/python/ruby/php 等编程语言客户端。 如果每台服务器上有4块磁盘，那么就会自动启动四个OSD进程。并且一块磁盘可以用于多个OSD存储池。 为何要用Ceph? 高性能 : 摒弃了传统的集中式存储元数据寻址的方案，采用CRUSH算法，数据分布均衡，并行度高。 考虑了容灾的隔离，能够实现各类负载的副本放置规则，例如跨机房、机架感知等。 能够支持上千个存储节点的规模，支持TB到PB级的数据。 高可用 : 副本数可以灵活控制 支持故障域分隔，数据强一直性 多故障场景自动进行修复自愈 没有单点故障，自动管理，高可扩展性 去中心化 : 扩展灵活 随着节点增加而线性增长 特性丰富 : 支持三种存储接口 : 块存储、文件存储、对象存储 支持自定义接口，支持多种语言驱动。 ","date":"2023-01-02","objectID":"/posts/ceph/1.ceph%E7%90%86%E8%AE%BA%E8%AF%A6%E8%A7%A3/:2:0","tags":["分布式存储"],"title":"Ceph 理论详解 (一)","uri":"/posts/ceph/1.ceph%E7%90%86%E8%AE%BA%E8%AF%A6%E8%A7%A3/"},{"categories":["Ceph"],"content":"1.1 Ceph 的发展史 Ceph 项目起源于 于2003 年在加州大学圣克鲁兹分校攻读博士期间的研究课题 (Lustre 环境中的可扩展问题)。 Lustre 是一种平行分布式文件系统,早在 1999 年，由皮特·布拉姆(Peter Braam)创建的集群文件系统公司(Cluster File Systems inc)开始研发,并于2003 年发布 Lustre 1.0 版本。 2007 年 Sage Weil(塞奇·威尔)毕业后，Sage Weil 继续全职从事 Ceph 工作 , 2010 年3月19 日，Linus Torvalds 将 Ceph 客户端合并到 2010 年5月16 日发布的 Linux 内核版本 2.6.34, 2012年Sage Weil 创建了Inktank Storage 用于为 Ceph 提供专业服务和支持,2014年4月Redhat 以1.75亿美元收购inktank 公司并开源。 ","date":"2023-01-02","objectID":"/posts/ceph/1.ceph%E7%90%86%E8%AE%BA%E8%AF%A6%E8%A7%A3/:2:1","tags":["分布式存储"],"title":"Ceph 理论详解 (一)","uri":"/posts/ceph/1.ceph%E7%90%86%E8%AE%BA%E8%AF%A6%E8%A7%A3/"},{"categories":["Ceph"],"content":"1.2 Ceph 的设计思想 Ceph 的设计旨在实现以下目标: 每一组件皆可扩展。 无单点故障。 基于软件(而非专用设备)并且开源(无供应商锁定)在现有的廉价硬件上运行。 尽可能自动管理，减少用户干预。 ","date":"2023-01-02","objectID":"/posts/ceph/1.ceph%E7%90%86%E8%AE%BA%E8%AF%A6%E8%A7%A3/:2:2","tags":["分布式存储"],"title":"Ceph 理论详解 (一)","uri":"/posts/ceph/1.ceph%E7%90%86%E8%AE%BA%E8%AF%A6%E8%A7%A3/"},{"categories":["Ceph"],"content":"1.3 Ceph的版本历史 Ceph 的第一个版本是 0.1,发布目期为 2008 年1月,多年来 Ceph 的版本号一直采用递归更新的方式没变直到 2015 年4月 0.941(Hammer 的第一个修正版)发布后,为了避免0.99(以及0.100 或 1.00),后期的命名方式发生了改变: x.0.z- 开发版 (给早期测试者和勇士们) x.1.z - 候选版 (用于测试集群、高手们) x.2.z- 稳定、修正版 (给用户们) x将从9 算起它代表 Infernalis(首字母I是英文单词中的第九个字母),这样我们第九个发布周期的第一个开发版就是 9.0.0,后续的开发版依次是 9.0.0-\u003e9.0.1-\u003e9.0.2 等,测试版本就是9.1.0-\u003e9.1.1-\u003e9.1.2,稳定版本就是9.2.0-\u003e9.2.1-\u003e9.2.2。 ","date":"2023-01-02","objectID":"/posts/ceph/1.ceph%E7%90%86%E8%AE%BA%E8%AF%A6%E8%A7%A3/:2:3","tags":["分布式存储"],"title":"Ceph 理论详解 (一)","uri":"/posts/ceph/1.ceph%E7%90%86%E8%AE%BA%E8%AF%A6%E8%A7%A3/"},{"categories":["Ceph"],"content":"2. Ceph 集群角色定义 一个Ceph集群的组成部分： 若干的 Ceph OSD(对象存储守护程序) 至少需要一个 Ceph Monitors 监视器 (1,3,5,7…) 两个或以上的Ceph管理器managers,运行Ceph文件系统客户端时,还需要高可用的Ceph Metadata Server(文件系统元数据服务器) RADOS cluster:由多台host 存储服务器组成的Ceph 集群 OSD(Object Storage Daemon)：每台存储服务器的磁盘组成的存储空间 Mon(Monitor)：Ceph 的监视器,维护OSD 和PG 的集群状态，一个Ceph 集群至少要有一个mon，可以是一三五七等等这样的奇数个。 Mgr(Manager)：负责跟踪运行时指标和Ceph 集群的当前状态，包括存储利用率，当前性 能指标和系统负载等。 Ceph OSDs: Ceph OSD 守护进程 （Ceph OSD）的功能是存储数据，处理数据的复制、恢复、回填、在均衡，并通过查其OSD 守护进程的心跳来向Ceph Monitors 提供一些监控信息。当Ceph存储集群设定为2个副本时，至少需要2个OSD守护进程。这样集群才能达到 active+clean 状态(Ceph 默认有3个副本，但你可以调整副本数)。 Monitors: Ceph Monitor 维护着展示集群状态的各种图表、包括监视图、OSD图、归置组（PG）图、和CRUSH图。Ceph保存着发生在Monitors、OSD和PG上的每一次状态变更的历史记录信息（称为epoch）。 MDSs: Ceph元数据服务器（MDS）为Ceph文件系统存储元数据（也就是说，Ceph块设备和Ceph对象存储不使用MDS）。元数据服务器使得POSIX文件系统的用户们，可以在不对 Ceph 存储集群造成负担的前提下，执行诸如 ls、find 等基本命令。 Ceph把客户端数据保存为存储池内的对象。通过使用CRUSH算法，Ceph可以计算出哪个归置组（PG）应该持有指定的对象(Object)，然后进一步计算出哪个OSD守护进程持有该归置组。CRUSH算法使得Ceph存储集群能够动态地伸缩、再均衡和修复。 ","date":"2023-01-02","objectID":"/posts/ceph/1.ceph%E7%90%86%E8%AE%BA%E8%AF%A6%E8%A7%A3/:3:0","tags":["分布式存储"],"title":"Ceph 理论详解 (一)","uri":"/posts/ceph/1.ceph%E7%90%86%E8%AE%BA%E8%AF%A6%E8%A7%A3/"},{"categories":["Ceph"],"content":"2.1 Monitor(Ceph-mon) Ceph 监视器 在一个主机上运行的一个守护进程，用于维护集群状态映射(maintains maps of the cluster state)，比如Ceph 集群中有多少存储池、每个存储池有多少PG 以及存储池和PG的映射关系等， monitor map, manager map, the OSD map, the MDS map, and the CRUSH map，这些映射是Ceph 守护程序相互协调所需的关键群集状态，此外监视器还负责管理守护程序和客户端之间的身份验证(认证使用CephX 协议)。通常至少需要三个监视器才能实现冗余和高可用性。 监视器，维护集群状态的多种映射，同时提供认证和日志记录服务，包括有关monitor 节点端到端的信息，其中包括 Ceph 集群ID，监控主机名和IP以及端口。 并且存储当前版本信息以及最新更改信息，通过 “Ceph mon dump\"查看 monitor map。 Ceph osd unset noout #重启服务器不踢出磁盘，重启前设置 Ceph -s ","date":"2023-01-02","objectID":"/posts/ceph/1.ceph%E7%90%86%E8%AE%BA%E8%AF%A6%E8%A7%A3/:3:1","tags":["分布式存储"],"title":"Ceph 理论详解 (一)","uri":"/posts/ceph/1.ceph%E7%90%86%E8%AE%BA%E8%AF%A6%E8%A7%A3/"},{"categories":["Ceph"],"content":"2.2 Managers(Ceph-mgr)的功能： 在一个主机上运行的一个守护进程，Ceph Manager 守护程序（Ceph-mgr）负责跟踪运行时指标和Ceph 集群的当前状态，包括存储利用率，当前性能指标和系统负载。Ceph Manager 守护程序还托管基于python 的模块来管理和公开Ceph 集群信息，包括基于Web的Ceph 仪表板和REST API。 高可用性通常至少需要两个管理器。 ","date":"2023-01-02","objectID":"/posts/ceph/1.ceph%E7%90%86%E8%AE%BA%E8%AF%A6%E8%A7%A3/:3:2","tags":["分布式存储"],"title":"Ceph 理论详解 (一)","uri":"/posts/ceph/1.ceph%E7%90%86%E8%AE%BA%E8%AF%A6%E8%A7%A3/"},{"categories":["Ceph"],"content":"2.3 Ceph OSDs(对象存储守护程序Ceph-osd) 即对象存储守护程序，但是它并非针对对象存储。提供存储数据，操作系统上的一个磁盘就是一个OSD 守护程序。 是物理磁盘驱动器，将数据以对象的形式存储到集群中的每个节点的物理磁盘上。 OSD负责存储数据、处理数据复制、恢复、回（Backfilling）、再平衡。完成存储数据的工作绝大多数是由 OSD daemon 进程实现。 在构建 Ceph OSD的时候，建议采用SSD 磁盘以及xfs文件系统来格式化分区。 此外OSD还对其它OSD进行心跳检测，检测结果汇报给Monitor。 通常至少需要3 个Ceph OSD 才能实现冗余和高可用性。 ","date":"2023-01-02","objectID":"/posts/ceph/1.ceph%E7%90%86%E8%AE%BA%E8%AF%A6%E8%A7%A3/:3:3","tags":["分布式存储"],"title":"Ceph 理论详解 (一)","uri":"/posts/ceph/1.ceph%E7%90%86%E8%AE%BA%E8%AF%A6%E8%A7%A3/"},{"categories":["Ceph"],"content":"2.4 MDS(Ceph 元数据服务器Ceph-mds) Ceph 元数据，主要保存的是Ceph文件系统(NFS/CIFS)的元数据。 注意：Ceph的块存储和Ceph对象存储都不需要MDS。 ","date":"2023-01-02","objectID":"/posts/ceph/1.ceph%E7%90%86%E8%AE%BA%E8%AF%A6%E8%A7%A3/:3:4","tags":["分布式存储"],"title":"Ceph 理论详解 (一)","uri":"/posts/ceph/1.ceph%E7%90%86%E8%AE%BA%E8%AF%A6%E8%A7%A3/"},{"categories":["Ceph"],"content":"2.5 Ceph 的管理节点 1.Ceph 的常用管理接口是一组命令行工具程序，例如rados、Ceph、rbd 等命令，Ceph 管理员可以从某个特定的Ceph-mon 节点执行管理操作。 2.推荐使用部署专用的管理节点对Ceph 进行配置管理、升级与后期维护，方便后期权限管理，管理节点的权限只对管理人员开放，可以避免一些不必要的误操作的发生。 ","date":"2023-01-02","objectID":"/posts/ceph/1.ceph%E7%90%86%E8%AE%BA%E8%AF%A6%E8%A7%A3/:3:5","tags":["分布式存储"],"title":"Ceph 理论详解 (一)","uri":"/posts/ceph/1.ceph%E7%90%86%E8%AE%BA%E8%AF%A6%E8%A7%A3/"},{"categories":["Ceph"],"content":"3.Ceph的逻辑架构 pool 存储池、分区，存储池的大小取决于底层的存储空间,我们在创建pool存储池时需要指定pg的个数，来创建pg。创建pg需要用到crush算法，crush算法决定了pg与osd daemon的对应关系，所以说，在客户端往ceph中写入数据之前，pg与osd daemon的对应关系是已经确定的。虽然是确定的，但是pg与osd daemon的对应关系是动态的。 PG(placement group) pg是ceph中分配数据的最小单位，一个pg内包含多个osd daemon，一个pool 中有多少个PG 可以通过公式计算。 OSD(Object Storage Daemon,对象存储设备) : 每一块磁盘都是一个osd，一个主机由一个或多个osd 组成。 ","date":"2023-01-02","objectID":"/posts/ceph/1.ceph%E7%90%86%E8%AE%BA%E8%AF%A6%E8%A7%A3/:4:0","tags":["分布式存储"],"title":"Ceph 理论详解 (一)","uri":"/posts/ceph/1.ceph%E7%90%86%E8%AE%BA%E8%AF%A6%E8%A7%A3/"},{"categories":["Ceph"],"content":"3.1 pool存储池详解 Ceph的pool有四大属性 所有性和访问权限 对象副本数目，默认pool池中的一个pg只包含两个osd daemon，即一份数据交给pg后会存下2个副本，生产环境推荐设置为3个副本。 pg数目，pg是pool的存储单位，pool的存储空间就由pg组成 crush规则集合。 ","date":"2023-01-02","objectID":"/posts/ceph/1.ceph%E7%90%86%E8%AE%BA%E8%AF%A6%E8%A7%A3/:4:1","tags":["分布式存储"],"title":"Ceph 理论详解 (一)","uri":"/posts/ceph/1.ceph%E7%90%86%E8%AE%BA%E8%AF%A6%E8%A7%A3/"},{"categories":["Ceph"],"content":"3.2 pg 数与osd daemon之间对应关系的影响 创建pool时需要确定其pg的数目，在pool被创建后也可以调整该数字，但是增加池中的pg数是影响ceph集群的重大事件之一，在生产环境中应该避免这么做。因为pool中的pg的数目会影响到： 数据的均匀分布性 资源消耗：pg作为一个逻辑实体，它需要消耗一定的资源，包括内存，CPU和带宽，太多的pg的话，则占用资源会过多 清理时间：Ceph的清理工作是以pg为单位进行的。如果一个pg内的数据太多，则其清理时间会很长 数据的持久性：pool中的pg个数应该随着osd daemon的增多而增多，这样crush算法可以将pg和osd的对应关系尽量均匀一些，降低同一个osd属于很多个pg的几率，如果一个osd真的属于很多很多pg，这样可能会很糟糕，可能会出现如下情况： 假设我们pool副本的size为3，则表示每一个pg将数据存放在3个osd上。一旦某个osd daemon挂掉，因为一个osd daemon同时属于很多个pg，则此时会出现很多pg只有2个副本的情况，这个时候通过crush算法开始进行数据恢复。在数据恢复的过程中，因为数据量过大，又有一个osd daemon(也属于很多很多pg)扛不住压力也崩溃掉了，那么将会有一部分pg只有一个副本。这个时候通过crush算法再次开始进行数据恢复，情况继续恶化，如果再有第三个osd daemon挂掉，那么就可能会出现部分数据的丢失。由此可见，osd daemon上的pg组数目: 不能过小，过小则数据分布不均匀\r不能过大，过大则一个osd daemon挂掉影响范围会很广，这会增大数据丢失的风险\rosd daemon上的pg组数目应该是在合理范围内的，我们无法决定pg组与osd daemon之间的对应关系，这个是由crush算法决定的。但是我们可以在创建pool池时，可以指定pool池内所包含pg的数量，只要把pg的数量设置合理，crush算法自然会保证数据均匀。 ","date":"2023-01-02","objectID":"/posts/ceph/1.ceph%E7%90%86%E8%AE%BA%E8%AF%A6%E8%A7%A3/:4:2","tags":["分布式存储"],"title":"Ceph 理论详解 (一)","uri":"/posts/ceph/1.ceph%E7%90%86%E8%AE%BA%E8%AF%A6%E8%A7%A3/"},{"categories":["Ceph"],"content":"3.3 指定pool池中pg的数量 如何算出一个pool池内应该有多少个pg数？ Target PGs per OSD：crush算法为每个osd daemon分配的pg数(官网建议100或200个) OSD#：osd daemon的总数 %DATA：该存储池的空间占ceph集群整体存储空间的百分比 Size：pool池中的副本数 计算公式： (Target PGs per OSD)✖(OSD#)✖(%DATA)/Size如果如果ceph集群很长一段时间都不会拓展,我们osd daemon的总数为9,该存储池占用整个ceph集群整体存储空间的百分比为1%(10G/1000G),pool池中的副本数为3个,那么我们在pool池中设置pg的数量为多少合理? 100 * 9 * 0.01 / 3 = 3 (个)官网也给出了一些参考原则 osd daemon的总数少于5个，建议将pool中的pg数设为128\rosd daemon的总数5到10个，建议将pool中的pg数设为512\rosd daemon的总数10到50个，建议将pool中的pg数设为4093\rosd daemon的总数为50个以上，我们可以使用官网的工具进行计算，来确定pool池中pg的个数。ceph官网计算工具网址：https://ceph.com/pgcalc/ ","date":"2023-01-02","objectID":"/posts/ceph/1.ceph%E7%90%86%E8%AE%BA%E8%AF%A6%E8%A7%A3/:4:3","tags":["分布式存储"],"title":"Ceph 理论详解 (一)","uri":"/posts/ceph/1.ceph%E7%90%86%E8%AE%BA%E8%AF%A6%E8%A7%A3/"},{"categories":["Ceph"],"content":"3.4 Ceph中pool池的两种类型 Replicated pool(默认) 副本型pool，通过生产对象的多份拷贝 优点 : 保证了数据的安全 缺点 : 浪费空间，如果设置的pg对应三个副本，那么空间只能用到原来空间的三分之一 Erasure-coded pool 特点 : 没有副本，可以把空间百分之百利用起来，但是没有副本功能(无法保证数据的安全) 不支持ceph的压缩，不支持ceph垃圾回收的功能等 ","date":"2023-01-02","objectID":"/posts/ceph/1.ceph%E7%90%86%E8%AE%BA%E8%AF%A6%E8%A7%A3/:4:4","tags":["分布式存储"],"title":"Ceph 理论详解 (一)","uri":"/posts/ceph/1.ceph%E7%90%86%E8%AE%BA%E8%AF%A6%E8%A7%A3/"},{"categories":["Ceph"],"content":"3.5 Ceph网络划分 Ceph推荐主要使用两个网络，这么做，注意从性能(OSD节点之间会有大量的数据拷贝操作)和安全性(两网分离)考虑。 前端(南北)网络 : 连接客户端和集群 后端(东西)网络 : 连接ceph各个存储节点 ","date":"2023-01-02","objectID":"/posts/ceph/1.ceph%E7%90%86%E8%AE%BA%E8%AF%A6%E8%A7%A3/:4:5","tags":["分布式存储"],"title":"Ceph 理论详解 (一)","uri":"/posts/ceph/1.ceph%E7%90%86%E8%AE%BA%E8%AF%A6%E8%A7%A3/"},{"categories":["Ceph"],"content":"4. Ceph数据写入流程 Ceph 集群部署好之后,要先创建存储池才能向Ceph 写入数据，文件在向Ceph 保存之前要先进行一致性hash 计算，计算后会把文件保存在某个对应的PG 的，此文件一定属于某个pool 的一个PG，在通过PG 保存在OSD 上。数据对象在写到主OSD 之后再同步对从OSD 以实现数据的高可用。 第一步: 计算文件到对象的映射: File放到Ceph集群后，先把文件进行分割，分割为等大小的小块，小块叫object（默认为4M）\r计算文件到对象的映射,假如file 为客户端要读写的文件,得到oid(object id) = ino + ono\rino:inode number (INO)，File 的元数据序列号，File 的唯一id。\rono:object number (ONO)，File 切分产生的某个object 的序号，默认以4M 切分一个块大小。\r比如：一个文件FileID为A，它被切成了两个对象，一个对象编号0，另一个编号1，那么这两个文件的oid则为A0与A1。\r1）由Ceph集群指定的静态Hsah函数计算Object的oid，获取到其Hash值。\r2）将该Hash值与mask进行与操作，从而获得PG ID。第二步：通过hash 算法计算出文件对应的pool 中的PG: 小块跟据一定算法跟规律，算法是哈希算法，放置到PG组里。\r通过一致性HASH 计算Object 到PG， Object -\u003e PG 映射hash(oid) \u0026 mask-\u003e pgid第三步: 通过CRUSH 算法把对象映射到PG 中的OSD: 再把PG放到OSD里面。\r通过CRUSH 算法计算PG 到OSD，PG -\u003e OSD 映射：[CRUSH(pgid)-\u003e(osd1,osd2,osd3)]第四步：PG 中的主OSD 将对象写入到硬盘。 第五步: 主OSD 将数据同步给备份OSD,并等待备份OSD 返回确认。 第六步: 备份OSD返回确认后，主OSD 将写入完成返回给客户端。 Ceph中数据写入，会有三次映射 （1）File -\u003e object映射\r（2）Object -\u003e PG映射，hash(oid) \u0026 mask -\u003e pgid\r（3）PG -\u003e OSD映射，CRUSH算法 ","date":"2023-01-02","objectID":"/posts/ceph/1.ceph%E7%90%86%E8%AE%BA%E8%AF%A6%E8%A7%A3/:5:0","tags":["分布式存储"],"title":"Ceph 理论详解 (一)","uri":"/posts/ceph/1.ceph%E7%90%86%E8%AE%BA%E8%AF%A6%E8%A7%A3/"},{"categories":["Ceph"],"content":"5.元数据的保存方式 Ceph 对象数据的元数据信息放在哪里呢? 对象数据的元数据以 key-value 的形式存在， 在RADOS 中有两种实现：xattrs 和 omap： Ceph 可选后端支持多种存储引擎，比如 filestore，kvstore，memstore，目前是以 kvstore 的 形式存储对象数据的元数据信息。 ","date":"2023-01-02","objectID":"/posts/ceph/1.ceph%E7%90%86%E8%AE%BA%E8%AF%A6%E8%A7%A3/:6:0","tags":["分布式存储"],"title":"Ceph 理论详解 (一)","uri":"/posts/ceph/1.ceph%E7%90%86%E8%AE%BA%E8%AF%A6%E8%A7%A3/"},{"categories":["Ceph"],"content":"5.1 xattr 扩展属性 是将元数据保存在对象对应文件数据中并保存到系统磁盘上，这要求支持对象存储的 本地文件系统（一般是 XFS）支持扩展属性。元数据和数据放一起 ","date":"2023-01-02","objectID":"/posts/ceph/1.ceph%E7%90%86%E8%AE%BA%E8%AF%A6%E8%A7%A3/:6:1","tags":["分布式存储"],"title":"Ceph 理论详解 (一)","uri":"/posts/ceph/1.ceph%E7%90%86%E8%AE%BA%E8%AF%A6%E8%A7%A3/"},{"categories":["Ceph"],"content":"5.2 omap(object map 对象映射) omap是 object map 的简称，是将元数据保存在本地文件系统之外的独立 key-value 存储 系统中，在使用 filestore 时是 leveldb，在使用 bluestore 时是 rocksdb，由于 filestore 存在功能问题(需要将磁盘格式化为 XFS 格式)及元数据高可用问题等问题，因此在目前 Ceph 主要使用 bluestore。 ","date":"2023-01-02","objectID":"/posts/ceph/1.ceph%E7%90%86%E8%AE%BA%E8%AF%A6%E8%A7%A3/:6:2","tags":["分布式存储"],"title":"Ceph 理论详解 (一)","uri":"/posts/ceph/1.ceph%E7%90%86%E8%AE%BA%E8%AF%A6%E8%A7%A3/"},{"categories":["Ceph"],"content":"5.3 Filestore与LevelDB 存储系统 Ceph 早期基于 filestore 使用 google 的 levelDB 保存对象的元数据，LevelDb 是一个持久化存 储的 KV 系统，和 Redis 这种内存型的 KV 系统不同，LevelDb 不会像 Redis 一样将数据放在内存从而占用大量的内存空间，而是将大部分数据存储到磁盘上，但是需要把磁盘上的 levelDB 空间格式化为文件系统(XFS). FileStore 将数据保存到与 Posix 兼容的文件系统(例如 Btrfs、XFS、Ext4)。在 Ceph 后端使用传统的 Linux 文件系统尽管提供了一些好处，但也有代价，如性能、 对象属性与磁盘本地文 件系统属性匹配存在限制等。 Filestore 数据写入的过程 1、先把要写入的数据全部封装成一个事务，其整理作为一条日志，写入日志磁盘（一般把 日志放在 ssd 上提高性 能），这个过程叫日志的提交（journalsubmit）。\r2、把数据写入对象文件的磁盘中（也就是 OSD 的磁盘中），这个过程叫日志的应用(journal apply)。这个过程不一定写入磁盘，有可能缓存在本地文件系统的 page cache 中。 当系统在日志提交的过程中出错，系统重启后，直接丢弃不完整的日志条目，该条日志对应 的实际对象数据并没有修改，数据保证了一致性。当日志在应用过程中出错，由于日志已写 入到磁盘中，系统重启后，重放（replay）日志，这样保证新数据重新完整的写入，保证了 数据的一致性FileStore 日志的三个阶段 日志的提交（journal submit）：日志写入日志磁盘。 日志的应用(journal apply)：日志对应的修改更新到对象的磁盘文件中。这个过程不一定写入 磁盘，有可能缓存在本地文件系统的 page cache 中。 日志的同步（journal sync 或者 journal commit）：确定日志对应的修改操作已经刷到磁盘中","date":"2023-01-02","objectID":"/posts/ceph/1.ceph%E7%90%86%E8%AE%BA%E8%AF%A6%E8%A7%A3/:6:3","tags":["分布式存储"],"title":"Ceph 理论详解 (一)","uri":"/posts/ceph/1.ceph%E7%90%86%E8%AE%BA%E8%AF%A6%E8%A7%A3/"},{"categories":["Ceph"],"content":"5.4 BlueStore 与 RocksDB 存储系统 ​ 由于 levelDB 依然需要需要磁盘文件系统的支持，后期 Facebok 对 levelDB 进行改进为 RocksDB，RocksDB 将对象数据的元数据保存在 RocksDB，但是 RocksDB 的数据又放在哪里呢？ 放在内存怕丢失，放在本地磁盘但是解决不了高可用，Ceph 对象数据放在了每个 OSD 中，那么就在在当前 OSD 中划分出一部分空间，格式化为 BlueFS 文件系统用于保存 RocksDB 中的元数据信息(称为 BlueStore)，并实现元数据的高可用， BlueStore 最大的特点是构建在裸磁盘设备之上，并且对诸如 SSD 等新的存储设备做了很多优化工作。 对全 SSD 及全 NVMe SSD 闪存适配 绕过本地文件系统层，直接管理裸设备，缩短 IO 路径 严格分离元数据和数据，提高索引效率。 期望带来至少 2 倍的写性能提升和同等读性能 增加数据校验及数据压缩等功能。 解决日志“双写”问题。 使用 KV 索引，解决文件系统目录结构遍历效率低的问题 支持多种设备类型。 BlueStore 的逻辑架构如上图所示，模块的划分都还比较清晰，我们来看下各模块的作用: Allocator:负责裸设备的空间管理分配。 RocksDB： 是 facebook 基于 leveldb 开发的一款 kv 数据库，BlueStore 将元数据全部存放至 RocksDB 中，这些元数据包括存储预写式日志、数据对象元数据、Ceph 的 omap 数 据信息、以及分配器的元数据 。 RocksDB 通过中间层 BlueRocksDB 访问文件系统的接口。这个文件系统与传统的 Linux 文件系统(例如 Ext4 和 XFS)是不同的，它不是在 VFS 下面的通用文件系统，而是一个用 户态的逻辑。BlueFS 通过函数接口(API，非 POSIX)的方式为 BlueRocksDB 提供类似文件系统的能力。 BlueRocksEnv：这是 RocksDB 与 BlueFS 交互的接口；RocksDB 提供了文件操作的接口 EnvWrapper(Env 封装器)，可以通过继承实现该接口来自定义底层的读写操作，BlueRocksEnv 就是继承自 EnvWrapper 实现对 BlueFS 的读写。 BlueFS：BlueFS是BlueStore针对RocksDB开发的轻量级文件系统，用于存放RocksDB产生的.sst 和.log 等文件。 BlockDecive：BlueStore 抛弃了传统的 ext4、xfs 文件系统，使用直接管理裸盘的方式；BlueStore 支持同时使用多种不同类型的设备，在逻辑上 BlueStore 将存储空间划分为三层：慢速（Slow） 空间、高速（DB）空间、超高速（WAL）空间，不同的空间可以指定使用不同的设备类型， 当然也可使用同一块设备 BlueStore 的设计考虑了 FileStore 中存在的一些硬伤，抛弃了传统的文件系统直接管理裸设备，缩短了 IO 路径，同时采用 ROW 的方式，避免了日志双写的问题，在写入性能上有了极大的提高。 ","date":"2023-01-02","objectID":"/posts/ceph/1.ceph%E7%90%86%E8%AE%BA%E8%AF%A6%E8%A7%A3/:6:4","tags":["分布式存储"],"title":"Ceph 理论详解 (一)","uri":"/posts/ceph/1.ceph%E7%90%86%E8%AE%BA%E8%AF%A6%E8%A7%A3/"},{"categories":["Ceph"],"content":"6. CRUSH 算法简介 ​ Controllers replication under scalable hashing ：可控的、可复制的、可伸缩的一致性 hash 算法。 Ceph 使用 CURSH 算法来存放和管理数据，它是 Ceph 的智能数据分发机制。 Ceph 使用 CRUSH 算法来准确计算数据应该被保存到哪里，以及应该从哪里读取,和保存元数据不同,是CRUSH 按需计算出元数据，因此它就消除了对中心式的服务器／网关的需求,它使得 Ceph 客户端能够计算出元数据，该过程也称为CRUSH 查找，然后和 OSD 直接通信。 如果是把对象直接映射到 OSD 之上会导致对象与 OSD 的对应关系过于紧密和耦合，当 OSD 由于故障发生变更时将会对整个 Ceph 集群产生影响。 于是 Ceph 将一个对象映射到 RADOS 集群的时候分为两步走： 首先使用一致性 hash 算法将对象名称映射到 PG 然后将 PG ID 基于 CRUSH 算法映射到 OSD 即可查到对象。 以上两个过程都是以”实时计算”的方式完成，而没有使用传统的查询数据与块设备的对应表的方式，这样有效避免了组件的”中心化”问题，也解决了查询性能和冗余问题。这使得 Ceph 集群扩展不再受查询的性能限制，CRUSH 算法由Mon节点计算,根据使用量相应增加Mon节点数量。 这个实时计算操作使用的就是 CRUSH 算法 Controllers replication under scalable hashing 可控的、可复制的、可伸缩的一致性 hash 算法。CRUSH 是一种分布式算法，类似于一致性 hash 算法，用于为 RADOS 存储集群控制数据的分配， ","date":"2023-01-02","objectID":"/posts/ceph/1.ceph%E7%90%86%E8%AE%BA%E8%AF%A6%E8%A7%A3/:7:0","tags":["分布式存储"],"title":"Ceph 理论详解 (一)","uri":"/posts/ceph/1.ceph%E7%90%86%E8%AE%BA%E8%AF%A6%E8%A7%A3/"},{"categories":["小记"],"content":"Git常用命令与常见问题 ","date":"2022-06-02","objectID":"/posts/notes/git/:1:0","tags":["Git"],"title":"Git常用命令与常见问题","uri":"/posts/notes/git/"},{"categories":["小记"],"content":"git error: Your local changes to the following files would be overwritten by merge 解决方案 背景 团队其他成员修改了某文件并已提交入库，你在pull之前修改了本地该文件，等你修改完代码再pull时，这时会报错如下错误： error: Your local changes to the following files would be overwritten by merge 解决方案 根据是否要保存本地修改，有以下两种解决方案 保留修改 #执行以下三条命令 git stash #封存修改 git pull origin master git stash pop #把修改还原注： git stash：备份当前工作区内容，从最近的一次提交中读取相关内容，让工作区保证和上次提交的内容一致。同时，将当前工作区内容保存到Git栈中 git pull：拉取服务器上当前分支代码。 git stash pop：从Git栈中读取最近一次保存的内容，恢复工作区相关内容。同时，用户可能进行多次stash操作，需要保证后stash的最先被取到，所以用栈（先进后出）来管理；pop取栈顶的内容并恢复 git stash list：显示Git栈内的所有备份，可以利用这个列表来决定从那个地方恢复。 git stash clear：清空Git栈 2.废弃修改 核心思想就是版本回退，具体命令如下 git reset --hard git pull origin master注：不建议使用第二种。除非你再三确定不需要本地的修改了。 ","date":"2022-06-02","objectID":"/posts/notes/git/:1:1","tags":["Git"],"title":"Git常用命令与常见问题","uri":"/posts/notes/git/"},{"categories":["小记"],"content":"使用git时显示untracked files（未监控）解决办法 问题： git status 时除了显示自己修改的文件，还多了两个文件，显示如下： $ git status On branch main Your branch is up to date with 'origin/main'. Untracked files: (use \"git add \u003cfile\u003e...\" to include in what will be committed) .package-lock.json在编译git库拉下来的代码时，往往会产生一些中间文件，这些文件我们根本不需要，尤其是在成产环节做预编译，检查代码提交是否能编译通过这种case时，我们往往需要编译完成后不管正确与否，还原现场，以方便下次sync代码时不受上一次的编译影响。 解决办法：删除git库中untracked files（未监控）的文件： #1.删除 untracked files： git clean -f #2.连 untracked 的目录也一起删掉： git clean -fd #3.连 gitignore 的untrack 文件/目录也一起删掉 （慎用，一般这个是用来删掉编译出来的 .o之类的文件用的） git clean -xfd注意： 在用上述 git clean 前，强烈建议加上 -n 参数来先看看会删掉哪些文件，防止重要文件被误删： git clean -nxfd\rgit clean -nf\rgit clean -nfd","date":"2022-06-02","objectID":"/posts/notes/git/:1:2","tags":["Git"],"title":"Git常用命令与常见问题","uri":"/posts/notes/git/"},{"categories":["Kafka"],"content":"Kafka 集群实战与原理分析线上问题优化 ","date":"2021-12-04","objectID":"/posts/zookeeper/kafka01/:0:0","tags":["Kafka","Zookeeper","中间件"],"title":"Kafka 集群实战与原理分析线上问题优化","uri":"/posts/zookeeper/kafka01/"},{"categories":["Kafka"],"content":"一、为什么使用消息队列? ","date":"2021-12-04","objectID":"/posts/zookeeper/kafka01/:1:0","tags":["Kafka","Zookeeper","中间件"],"title":"Kafka 集群实战与原理分析线上问题优化","uri":"/posts/zookeeper/kafka01/"},{"categories":["Kafka"],"content":"1.Kafka知识点思维导图 以电商为业务背景 消息队列解决的具体问题是什么？ – 通信问题。 ","date":"2021-12-04","objectID":"/posts/zookeeper/kafka01/:1:1","tags":["Kafka","Zookeeper","中间件"],"title":"Kafka 集群实战与原理分析线上问题优化","uri":"/posts/zookeeper/kafka01/"},{"categories":["Kafka"],"content":"2.使用同步的通讯方式来解决多个服务之间的通讯 同步的通讯方式会存在性能和稳定性的问题。 ","date":"2021-12-04","objectID":"/posts/zookeeper/kafka01/:1:2","tags":["Kafka","Zookeeper","中间件"],"title":"Kafka 集群实战与原理分析线上问题优化","uri":"/posts/zookeeper/kafka01/"},{"categories":["Kafka"],"content":"3.使用异步通讯方式 在业务的上游与下游间加入 通讯模块 （消息队列 存储消息的队列） 最终一致性 针对同步的通讯方式来说，异步的方式，可以让上游快速成功，极大提高了系统的吞吐量。而且在分布式系统中，通过下游多个服务的分布式事务保障，也能保证业务执行之后的最终一致性。 ","date":"2021-12-04","objectID":"/posts/zookeeper/kafka01/:1:3","tags":["Kafka","Zookeeper","中间件"],"title":"Kafka 集群实战与原理分析线上问题优化","uri":"/posts/zookeeper/kafka01/"},{"categories":["Kafka"],"content":"二、消息队列的流派 ","date":"2021-12-04","objectID":"/posts/zookeeper/kafka01/:2:0","tags":["Kafka","Zookeeper","中间件"],"title":"Kafka 集群实战与原理分析线上问题优化","uri":"/posts/zookeeper/kafka01/"},{"categories":["Kafka"],"content":"1.什么是MQ ？ Message Queue（MQ），消息队列中间件。 很多人都说： MQ 通过将消息的发送和接收分离来实现应用程序的异步和解偶，这个给人的直觉是——MQ 是异步的，用来解耦的，但是这个只是 MQ 的效果而不是目的。 MQ 真正的目的是为了通讯，屏蔽底层复杂的通讯协议，定义了一套应用层的、更加简单的通讯协议。 一个分布式系统中两个模块之间通讯要么是 HTTP，要么是自己开发的 TCP，但是这两种协议其实都是原始的协议。 HTTP 协议很难实现两端通讯——模块 A 可以调用 B，B 也可以主动调用 A，如果要做到这个两端都要背上 WebServer，而且还不支持长连接（HTTP 2.0 的库根本找不到）。 TCP 就更加原始了，粘包、心跳、私有的协议，想一想头皮就发麻。 MQ 所要做的就是在这些协议之上构建一个简单的“协议”——生产者/消费者模型。 MQ 带给我的“协议”不是具体的通讯协议，而是更高层次通讯模型。 它定义了两个对象——发送数据的叫生产者，接收数据的叫消费者； 提供一个 SDK 让我们可以定义自己的生产者和消费者实现消息通讯而无视底层通讯协议。 ","date":"2021-12-04","objectID":"/posts/zookeeper/kafka01/:2:1","tags":["Kafka","Zookeeper","中间件"],"title":"Kafka 集群实战与原理分析线上问题优化","uri":"/posts/zookeeper/kafka01/"},{"categories":["Kafka"],"content":"2.中间件选型 目前消息队列的中间件选型有很多种： rabbit MQ：内部可玩性（功能性）是非常强的 rocket MQ :阿里内部大神根据Kafka的内部执行原理，手写的一个消息中间件。性能比肩kafka，除此之外，在功能上封装了更多的功能。（消息的逆序） kafka：全球消息处理性能最快的一款MQ（纯粹） zeroMQ 这些消息队列中间件有什么区别？ 我们把消息队列分为两种 MQ，分为有Broker的MQ，和没有Broker的MQ。 Broker，代理，经纪人的意思。 2.1有broker 有broker的MQ 这个流派通常有一台服务器作为Broker，所有的消息都通过它中转。生产者把消息发送给它就结束自己的任务了，Broker则把消息主动推送给消费者（或者消费者主动轮询）。 重topic：Kafka 、RocketMQ 、 ActiveMQ 整个broker，依据topic来进行消息的中转，在重topic的消息队列里必然需要topic的存在 轻topic：RabbitMQ topic只是其一种中转模式。 2.2无broker 在生产者和消费者之间没有使用broker，例如zeroMQ，直接使用socket进行通信 无broker的MQ代表是ZeroMQ，该作者非常睿智，他非常敏锐的意识到–MQ是更高级的Socket 它是解决通信问题的。所以ZeroMQ被设计成了一个“库”而不是一个中间件，这种实现也可以达到–没有Broker的目的。 ","date":"2021-12-04","objectID":"/posts/zookeeper/kafka01/:2:2","tags":["Kafka","Zookeeper","中间件"],"title":"Kafka 集群实战与原理分析线上问题优化","uri":"/posts/zookeeper/kafka01/"},{"categories":["Kafka"],"content":"三、Kafka的基本知识 Kafka是最初由Linkedin公司开发，是一个分布式、支持分区的（partition）、多副本的（replica），基于zookeeper协调的分布式消息系统. 它的最大的特性就是可以实时的处理大量数据以满足各种需求场景： 比如基于hadoop的批处理系统、低延迟的实时系统、storm/Spark流式处理引擎，web/nginx日志、访问日志，消息服务等等，用scala语言编写，Linkedin于2010年贡献给了Apache基金会并成为顶级开源项目。 ","date":"2021-12-04","objectID":"/posts/zookeeper/kafka01/:3:0","tags":["Kafka","Zookeeper","中间件"],"title":"Kafka 集群实战与原理分析线上问题优化","uri":"/posts/zookeeper/kafka01/"},{"categories":["Kafka"],"content":"3.1 Kafka的特性: 高吞吐量、低延迟：kafka每秒可以处理几十万条消息，它的延迟最低只有几毫秒，每个topic可以分多个partition, consumer group 对partition进行consume操作。 可扩展性：kafka集群支持热扩展 持久性、可靠性：消息被持久化到本地磁盘，并且支持数据备份防止数据丢失 容错性：允许集群中节点失败（若副本数量为n,则允许n-1个节点失败） 高并发：支持数千个客户端同时读写 ","date":"2021-12-04","objectID":"/posts/zookeeper/kafka01/:3:1","tags":["Kafka","Zookeeper","中间件"],"title":"Kafka 集群实战与原理分析线上问题优化","uri":"/posts/zookeeper/kafka01/"},{"categories":["Kafka"],"content":"3.2 Kafka的使用场景 日志收集：一个公司可以用Kafka可以收集各种服务的log，通过kafka以统一接口服务的方式开放给各种consumer，例如hadoop、Hbase、Solr等。 消息系统：解耦和生产者和消费者、缓存消息等。 用户活动跟踪：Kafka经常被用来记录web用户或者app用户的各种活动，如浏览网页、搜索、点击等活动，这些活动信息被各个服务器发布到kafka的topic中，然后订阅者通过订阅这些topic来做实时的监控分析，或者装载到hadoop、数据仓库中做离线分析和挖掘。 运营指标：Kafka也经常用来记录运营监控数据。包括收集各种分布式应用的数据，生产各种操作的集中反馈，比如报警和报告。 ","date":"2021-12-04","objectID":"/posts/zookeeper/kafka01/:3:2","tags":["Kafka","Zookeeper","中间件"],"title":"Kafka 集群实战与原理分析线上问题优化","uri":"/posts/zookeeper/kafka01/"},{"categories":["Kafka"],"content":"3.2 基本概念 kafka是一个分布式的，分区的消息(官方称之为commit log)服务。它提供一个消息系统应该具备的功能，但是确有着独特的设计。 首先，让我们来看一下基础的消息(Message)相关术语： kafka 中有这么些复杂的概念 名称 解释 Broker 消息中间件处理节点，一个kafka节点就是一个broker，一个或多个Broker可以组成一个kafka集群。 Topic kafka根据Topic对消息进行分类，发布到kafka集群的每条消息都需要指定一个Topic Producer 消息生产者，向Broker发送消息的客户端 Consumer 消息消费者，从Broker读取消息的客户端 ConsumerGroup 每个consumer属于一个特定的Consumer Group，一条消息可以被多个不同的Consumer消费，但是一个Consumer Group中只能有一个consumer能消费该消息 Partition 物理上的概念，一个topic可以分为多个partition，每个partition内部消息是有序的 ","date":"2021-12-04","objectID":"/posts/zookeeper/kafka01/:3:3","tags":["Kafka","Zookeeper","中间件"],"title":"Kafka 集群实战与原理分析线上问题优化","uri":"/posts/zookeeper/kafka01/"},{"categories":["Kafka"],"content":"3.3 创建主题topic topic kafka消息逻辑的划分 topic是什么概念? topic可以实现消息的分类，不同消费者订阅不同的topic。 执行以下命令创建名为\"test\"的topic，这个topic只有一个partition，并且备份因子也设置为1; ./kafka-topics.sh --create --zookeeper 172.16.253.35:2181 --replication-factor 1 --partitions 1 --topic test查看当前kafka内有哪些topic ./ kafka-topics.sh --list --zookeeper 172.16.253.35:2181 ","date":"2021-12-04","objectID":"/posts/zookeeper/kafka01/:3:4","tags":["Kafka","Zookeeper","中间件"],"title":"Kafka 集群实战与原理分析线上问题优化","uri":"/posts/zookeeper/kafka01/"},{"categories":["Kafka"],"content":"3.4 发送消息 kafka自带了一个producer命令客户端，可以从本地文件中读取内容，或者我们也可以以命令行中直接输入内容，并将这些内容以消息的形式发送到kafka集群中。在默认情况下，每一个行会被当做成一个独立的消息。 使用kafka的发送消息的客户端，指定发送到的kafka服务器地址和topic ./kafka-console-producer.sh --broker-list 10.31.167.10:9092 --topic test","date":"2021-12-04","objectID":"/posts/zookeeper/kafka01/:3:5","tags":["Kafka","Zookeeper","中间件"],"title":"Kafka 集群实战与原理分析线上问题优化","uri":"/posts/zookeeper/kafka01/"},{"categories":["Kafka"],"content":"3.5 消费消息 对于consumer, kafka同样也携带了一个命令行客户端，会将获取到内容在命令中进行输出，默认是消费最新的消息。使用kafka的消费者消息的客户端，从指定kafka服务器的指定topic中消费消息 方式一:从当前主题中最后一条消息的offset（偏移量）+1开始消费 ./kafka-console-consumer.sh --bootstrap-server 10.31.167.10:9092 --topic test 方式二∶从当前主题中的第一条消息开始消费 ./kafxa-console-consumer.sh --bootstrap-server 10.31.167.10:9092 --from-beginning --topic test","date":"2021-12-04","objectID":"/posts/zookeeper/kafka01/:3:6","tags":["Kafka","Zookeeper","中间件"],"title":"Kafka 集群实战与原理分析线上问题优化","uri":"/posts/zookeeper/kafka01/"},{"categories":["Kafka"],"content":"3.6 关于消息的细节 生产者将消息发送给broker，broker会将消息保存在本地的日志文件中 /usr/ local/kafka/data/kafka-logs/主题-分区/0000000o.log 消息的保存是有序的，通过offset偏移量来描述消息的有序性 消费者消费消息时也是通过offset来描述当前要消费的那条消息的位置 ","date":"2021-12-04","objectID":"/posts/zookeeper/kafka01/:3:7","tags":["Kafka","Zookeeper","中间件"],"title":"Kafka 集群实战与原理分析线上问题优化","uri":"/posts/zookeeper/kafka01/"},{"categories":["Kafka"],"content":"3.7单播消息 在一个kafka的topic中，启动两个消费者，一个生产者，问:生产者发送消息，这条消息是否同时会被两个消费者消费? 如果多个消费者在同一个消费组，那么只有一个消费者可以收到订阅的topic中的消息。换言之，同一个消费组中只能有一个消费者收到一个topic中的消息。 ./kafka-console-consumer.sh --bootstrap-server 172.16.253.38:9092--consumer-property group.id=testGroup --topic test ","date":"2021-12-04","objectID":"/posts/zookeeper/kafka01/:3:8","tags":["Kafka","Zookeeper","中间件"],"title":"Kafka 集群实战与原理分析线上问题优化","uri":"/posts/zookeeper/kafka01/"},{"categories":["Kafka"],"content":"3.8 多播消息 不同的消费组订阅同一个topic，那么不同的消费组中只有一个消费者能收到消息。实际上也是多个消费组中的多个消费者收到了同一个消息。 ./kafka-console-consumer.sh --bootstrap-server 172.16.253.38:9092--consumer-property group.id=testGroupl --topic test./kafka-console-consumer.sh --bootstrap-server 172.16.253.38:9092--consumer-property group.id=testGroup2 --topic test下图就是描述多播和单播消息的区别: ","date":"2021-12-04","objectID":"/posts/zookeeper/kafka01/:3:9","tags":["Kafka","Zookeeper","中间件"],"title":"Kafka 集群实战与原理分析线上问题优化","uri":"/posts/zookeeper/kafka01/"},{"categories":["Kafka"],"content":"3.9 查看消费组的详细信息 通过以下命令可以查看到消费组的相信信息︰ ./kafka-consumer-groups.sh --bootstrap-server 172.16.253.38:9092 --describe --group testGroup 重点关注以下几个信息∶ current-offset:最后被消费的消息的偏移量 Log-end-offset:消息总量(最后一条消息的偏移量) Lag:积压了多少条消息 ","date":"2021-12-04","objectID":"/posts/zookeeper/kafka01/:3:10","tags":["Kafka","Zookeeper","中间件"],"title":"Kafka 集群实战与原理分析线上问题优化","uri":"/posts/zookeeper/kafka01/"},{"categories":["Kafka"],"content":"四、Kafka 中主题和分区的概念 ","date":"2021-12-04","objectID":"/posts/zookeeper/kafka01/:4:0","tags":["Kafka","Zookeeper","中间件"],"title":"Kafka 集群实战与原理分析线上问题优化","uri":"/posts/zookeeper/kafka01/"},{"categories":["Kafka"],"content":"4.1 主题Topic 主题-topic在kafka中是一个逻辑的概念，kafka通过topic将消息进行分类。不同的topic会被订阅该topic的消费者消费。 但是有一个问题，如果说这个topic中的消息非常非常多，多到需要几T来存，因为消息是会被保存到log日志文件中的。为了解决这个文件过大的问题, kafka提出了Partition分区的概念 ","date":"2021-12-04","objectID":"/posts/zookeeper/kafka01/:4:1","tags":["Kafka","Zookeeper","中间件"],"title":"Kafka 集群实战与原理分析线上问题优化","uri":"/posts/zookeeper/kafka01/"},{"categories":["Kafka"],"content":"4.2 partition 分区 1)分区的概念 通过partition将一个topic中的消息分区来存储。 这样的好处有多个: 分区存储，可以解决统一存储文件过大的问题 提供了读写的吞吐量:读和写可以同时在多个分区中进行 2)创建多分区的主题 ./kafka-topics.sh --create --zokeeper 172.16.253.35:2181 --replication-factor l --partitions 2 --topic test1 ","date":"2021-12-04","objectID":"/posts/zookeeper/kafka01/:4:2","tags":["Kafka","Zookeeper","中间件"],"title":"Kafka 集群实战与原理分析线上问题优化","uri":"/posts/zookeeper/kafka01/"},{"categories":["Kafka"],"content":"4.3 kafka 中消息日志文件中保存的内容 . 00000.log:这个文件中保存的就是消息 _consumer_offsets-49 kafka内部自己创建了_consumer_offsets主题包含了50个分区。这个主题用来存放消费者消费某个主题的偏移量。 因为每个消费者都会自己维护着消费的主题的偏移量，也就是说每个消费者会把消费的主题的偏移量自主上报给kafka中的默认主题consumer_offsets。因此kafka为了提升这个主题的并发性，默认设置了50个分区。(可以通过offsets.topic.num.paritions设置)，这样可以通过加机器的方式抗大并发。 提交到哪个分区︰通过hash函数: hash(consumerGroupld) %_consumer_offsets主题的分区数 提交到该主题中的内容是: key是consumerGroupld+topic+分区号，value就是当前offset的值 文件中保存的消息，kafka会定期清理topic里的消息，最后就保留最新的那条数据默认保存7天。七天到后消息会被删除。 ","date":"2021-12-04","objectID":"/posts/zookeeper/kafka01/:4:3","tags":["Kafka","Zookeeper","中间件"],"title":"Kafka 集群实战与原理分析线上问题优化","uri":"/posts/zookeeper/kafka01/"},{"categories":["Kafka"],"content":"五、Kafka集群操作 ","date":"2021-12-04","objectID":"/posts/zookeeper/kafka01/:5:0","tags":["Kafka","Zookeeper","中间件"],"title":"Kafka 集群实战与原理分析线上问题优化","uri":"/posts/zookeeper/kafka01/"},{"categories":["Kafka"],"content":"5.1 搭建kafka集群（三个broker） 创建三个server.properties文件 #0 1 2 broker.id=2 #9092 9093 9094 listeners=PLAINTEXT ://192.168.65.60:9094 #kafka-logs kafka-logs-1 kafka-logs-2 log.dir=/usr/ local/ data/ kafka-logs-2 ./ kafka-server-start.sh -daemon ../config/server.properties ./ kafka-server-start.sh -daemon ../config/server1.properties ./ kafka-server-start.sh -daemon ../config/server2.properties 校验是否启动成功 进入到zk中查看/brokers/ids中过是否有三个znode (0,1,2) ","date":"2021-12-04","objectID":"/posts/zookeeper/kafka01/:5:1","tags":["Kafka","Zookeeper","中间件"],"title":"Kafka 集群实战与原理分析线上问题优化","uri":"/posts/zookeeper/kafka01/"},{"categories":["Kafka"],"content":"5.2 副本的概念 在创建主题时，除了指明了主题的分区数以外，还指明了副本数，那么副本是一个什么概念呢? 副本是为了为主题中的分区创建多个备份，多个副本在kafka集群的多个broker中，会有一个副本作为leader，其他是follower。 生产者与消费者只会与leader交互消息，而follower只会与leader保持同步以备不时之需。 leader: kafka的写和读的操作，都发生在leader上。leader负责把数据同步给follower。当leader挂了，经过主从选举，从多个follower中选举产生一个新的leader follower： 接收leader的同步的数据 isr: 可以同步和已同步的节点会被存入到isr集合中。这里有一个细节︰如果isr中的节点性能较差，会被提出isr集合。) 理解: 集群中有多个broker，创建主题时可以指明主题有多个分区(把消息拆分到不同的分区中存储)，可以为分区创建多个副本，不同的副本存放在不同的broker里。 ","date":"2021-12-04","objectID":"/posts/zookeeper/kafka01/:5:2","tags":["Kafka","Zookeeper","中间件"],"title":"Kafka 集群实战与原理分析线上问题优化","uri":"/posts/zookeeper/kafka01/"},{"categories":["Kafka"],"content":"5.3 关于集群消费 向集群发送消息∶ ./kafka-console-consumer . sh --bootstrap-server 172.16.253.38:9092,172.16.253.38:9093,172.16.253.38:9094 --from-beginning --consumer-property group.id=testGroupl --topic my-replicated-topic 从集群中消费消息 ./kafka-console-producer .sh --broker-list 172.16.253.38:9092,172.16.253.38:9093,172.16.253.38:9094 --topicmy-replicated-topic 指定消费组来消费消息 ./kafka-console-consumer .sh --bootstrap-server 172.16.253.38∶9092,172.16.253.38:9093,172.16.253.38:9094 --from-beginning --consumer-property group.id=testGroupl --topic my-replicated-topic ","date":"2021-12-04","objectID":"/posts/zookeeper/kafka01/:5:3","tags":["Kafka","Zookeeper","中间件"],"title":"Kafka 集群实战与原理分析线上问题优化","uri":"/posts/zookeeper/kafka01/"},{"categories":["Kafka"],"content":"5.4 分区分消费组的集群消费中的细节 一个partition只能被一个消费组中的一个消费者消费，目的是为了保证消费的顺序性，但是多个partion的多个消费者消费的总的顺序性是得不到保证的，那怎么做到消费的总顺序性呢? partition的数量决定了消费组中消费者的数量，建议同一个消费组中消费者的数量不要超过partition的数量，否则多的消费者消费不到消息 如果消费者挂了，那么会触发rebalance机制（后面介绍)，会让其他消费者来消费该分区 ","date":"2021-12-04","objectID":"/posts/zookeeper/kafka01/:5:4","tags":["Kafka","Zookeeper","中间件"],"title":"Kafka 集群实战与原理分析线上问题优化","uri":"/posts/zookeeper/kafka01/"},{"categories":["Kafka"],"content":"六、专题1 Kafka 集群Controller 、Rebalance 和HW ","date":"2021-12-04","objectID":"/posts/zookeeper/kafka01/:6:0","tags":["Kafka","Zookeeper","中间件"],"title":"Kafka 集群实战与原理分析线上问题优化","uri":"/posts/zookeeper/kafka01/"},{"categories":["Kafka"],"content":"1.controller 集群中谁来充当controller 每个broker启动时会向zk创建一个临时序号节点，获得的序号最小的那个broker将会作为集群中的controller，负责这么几件事: 当集群中有一个副本的leader挂掉，需要在集群中选举出一个新的leader，选举的规则是从isr集合中最左边获得。 当集群中有broker新增或减少，controller会同步信息给其他broker 当集群中有分区新增或减少，controller会同步信息给其他broker ","date":"2021-12-04","objectID":"/posts/zookeeper/kafka01/:6:1","tags":["Kafka","Zookeeper","中间件"],"title":"Kafka 集群实战与原理分析线上问题优化","uri":"/posts/zookeeper/kafka01/"},{"categories":["Kafka"],"content":"2.rebalance机制 前提:消费组中的消费者没有指明分区来消费 触发的条件:当消费组中的消费者和分区的关系发生变化的时候 分区分配的策略:在rebalance之前， 分区怎么分配会有这么三种策略 range: 根据公示计算得到每个消费消费哪几个分区:前面的消费者是分区总数/消费 者数量+1,之后的消费者是分区总数/消费者数量. 轮询:大家轮着来 sticky: 粘合策略，如果需要rebalance, 会在之前已分配的基础上调整，不会改变之前的分配情况。如果这个策略没有开，那么就要进行全部的重新分配。建议开启。 ","date":"2021-12-04","objectID":"/posts/zookeeper/kafka01/:6:2","tags":["Kafka","Zookeeper","中间件"],"title":"Kafka 集群实战与原理分析线上问题优化","uri":"/posts/zookeeper/kafka01/"},{"categories":["Kafka"],"content":"七、专题2 Kafka中的优化问题(面试问题) ","date":"2021-12-04","objectID":"/posts/zookeeper/kafka01/:7:0","tags":["Kafka","Zookeeper","中间件"],"title":"Kafka 集群实战与原理分析线上问题优化","uri":"/posts/zookeeper/kafka01/"},{"categories":["Kafka"],"content":"1.如何防止消息丢失 ⽣产者： 使⽤同步发送 把ack设成1（leader 成功写入）或者all(所有broker完成同步)，并且设置同步的分区数\u003e=2 消费者：把⾃动提交改成⼿动提交 ","date":"2021-12-04","objectID":"/posts/zookeeper/kafka01/:7:1","tags":["Kafka","Zookeeper","中间件"],"title":"Kafka 集群实战与原理分析线上问题优化","uri":"/posts/zookeeper/kafka01/"},{"categories":["Kafka"],"content":"2.如何防⽌重复消费 在防⽌消息丢失的⽅案中，如果⽣产者发送完消息后，因为⽹络抖动，没有收到ack，但实际 上broker已经收到了。 此时⽣产者会进⾏重试，于是broker就会收到多条相同的消息，⽽造成消费者的重复消费。 怎么解决： ⽣产者关闭重试：会造成丢消息（不建议） 消费者解决⾮幂等性消费问题： 所谓的幂等性：多次访问的结果是⼀样的。 对于rest的请求（get（幂等）、post（⾮幂 等）、put（幂等）、delete（幂等）） 幂等：多次访问的结果是一样的 解决⽅案： 在数据库中创建联合主键，防⽌相同的主键 创建出多条记录 使⽤分布式锁，以业务id为锁。保证只有⼀条记录能够创建成功 ","date":"2021-12-04","objectID":"/posts/zookeeper/kafka01/:7:2","tags":["Kafka","Zookeeper","中间件"],"title":"Kafka 集群实战与原理分析线上问题优化","uri":"/posts/zookeeper/kafka01/"},{"categories":["Kafka"],"content":"3.如何做到消息的顺序消费 ⽣产者：保证消息按顺序消费，且消息不丢失——使⽤同步的发送，ack设置成⾮0的 值。 消费者：主题只能设置⼀个分区，消费组中只能有⼀个消费者 kafka的顺序消费使⽤场景不多，因为牺牲掉了性能，但是⽐如rocketmq在这⼀块有专⻔的功能已设计好 ","date":"2021-12-04","objectID":"/posts/zookeeper/kafka01/:7:3","tags":["Kafka","Zookeeper","中间件"],"title":"Kafka 集群实战与原理分析线上问题优化","uri":"/posts/zookeeper/kafka01/"},{"categories":["Kafka"],"content":"4.如何解决消息积压问题 1）消息积压问题的出现 消息的消费者的消费速度远赶不上⽣产者的⽣产消息的速度，导致kafka中有⼤量的数据没有被消费。 随着没有被消费的数据堆积越多，消费者寻址的性能会越来越差，最后导致整个 kafka对外提供的服务的性能很差，从⽽造成其他服务也访问速度变慢，造成服务雪崩。 2）消息积压的解决⽅案 在这个消费者中，使⽤多线程，充分利⽤机器的性能进⾏消费消息。 通过业务的架构设计，提升业务层⾯消费的性能。 创建多个消费组，多个消费者，部署到其他机器上，⼀起消费，提⾼消费者的消费速度 创建⼀个消费者，该消费者在kafka另建⼀个主题，配上多个分区，多个分区再配上多个 消费者。该消费者将poll下来的消息，不进⾏消费，直接转发到新建的主题上。此时，新 的主题的多个分区的多个消费者就开始⼀起消费了。——不常⽤ ","date":"2021-12-04","objectID":"/posts/zookeeper/kafka01/:7:4","tags":["Kafka","Zookeeper","中间件"],"title":"Kafka 集群实战与原理分析线上问题优化","uri":"/posts/zookeeper/kafka01/"},{"categories":["Kafka"],"content":"5.实现延时队列的效果 1）应⽤场景 订单创建后，超过30分钟没有⽀付，则需要取消订单，这种场景可以通过延时队列来实现 2）具体⽅案 kafka中创建创建相应的主题 消费者消费该主题的消息(轮询) 消费者消费消息时判断消息的创建时间和当前时间是否超过30分钟(前提是订单没支付) 如果是:去数据库中修改订单状态为已取消 如果否:记录当前消息的offset,并不再继续消费之后的消息。等待1分钟后，再次向kafka拉取该offset及之后的消 息，继续进行判断，以此反复。 ","date":"2021-12-04","objectID":"/posts/zookeeper/kafka01/:7:5","tags":["Kafka","Zookeeper","中间件"],"title":"Kafka 集群实战与原理分析线上问题优化","uri":"/posts/zookeeper/kafka01/"},{"categories":["Kafka"],"content":"八、Kafka-eagle监控平台 ","date":"2021-12-04","objectID":"/posts/zookeeper/kafka01/:8:0","tags":["Kafka","Zookeeper","中间件"],"title":"Kafka 集群实战与原理分析线上问题优化","uri":"/posts/zookeeper/kafka01/"},{"categories":["Kafka"],"content":"1.搭建 去kafka-eagle官⽹下载压缩包 http://download.kafka-eagle.org/ 分配⼀台虚拟机 虚拟机中安装jdk 解压缩kafka-eagle的压缩包 给kafka-eagle配置环境变量 export KE_HOME=/usr/local/kafka-eagle export PATH=$PATH:$KE_HOME/bin 需要修改kafka-eagle内部的配置⽂件： vim system-config.properties 修改⾥⾯的zk的地址和mysql的地址 进⼊到bin中，通过命令来启动 ./ke.sh start ","date":"2021-12-04","objectID":"/posts/zookeeper/kafka01/:8:1","tags":["Kafka","Zookeeper","中间件"],"title":"Kafka 集群实战与原理分析线上问题优化","uri":"/posts/zookeeper/kafka01/"},{"categories":["Kafka"],"content":"1.使用 kafka-eagle 监控面板 kafka-监控查看节点信息 kafka-eagle查看消费组与消费主题信息 kafka-eagle查看消息积压情况 ","date":"2021-12-04","objectID":"/posts/zookeeper/kafka01/:8:2","tags":["Kafka","Zookeeper","中间件"],"title":"Kafka 集群实战与原理分析线上问题优化","uri":"/posts/zookeeper/kafka01/"},{"categories":["ElasticStack"],"content":"在等保2.0 测评单位对业务系统进行评测后，给出整改意见中提出： 应启用安全审计功能，审计覆盖到每个用户，对重要的用户行为和重要安全事件进行审计；建议对启用安全审计功能，对所有用户操作行为及系统安全事件进行审计记录。 应对审计记录进行保护，定期备份，避免受到未预期的删除、修改或覆盖等。建议定期对审计记录进行备份，保证审计记录不会受到未预期的删除、修改或覆盖。 搭建ELK平台收集各系统日志 ","date":"2021-10-21","objectID":"/posts/elk/elk-practice/:0:0","tags":["日志收集","等保2.0","运维实战"],"title":"等保2.0项目-日志收集实践","uri":"/posts/elk/elk-practice/"},{"categories":["ElasticStack"],"content":"应用场景 要收集的设备清单如下： 深信服防火墙设备 H3C 核心交换机 深信服SSL VPN 设备 业务通用服务器 ","date":"2021-10-21","objectID":"/posts/elk/elk-practice/:1:0","tags":["日志收集","等保2.0","运维实战"],"title":"等保2.0项目-日志收集实践","uri":"/posts/elk/elk-practice/"},{"categories":["ElasticStack"],"content":"思路 因为日志量不大所以使用四台服务器搭建ELK，部署情况如下： 192.168.10.106 部署ES + head 插件 用来处理日志数据\r192.168.10.107 部署Logstash 收集filebeat数据并将数据传送给ES 192.168.10.108 部署Kibana 将数据从ES中读取出进行可视化展示\r192.168.10.109 部署rsyslog + filebeat 收集汇总各系统日志文件并将日志文件传送给logstash\r系统版本为：Ubuntu 20.04.3 LTS ELK组件版本：7.14.x ","date":"2021-10-21","objectID":"/posts/elk/elk-practice/:1:1","tags":["日志收集","等保2.0","运维实战"],"title":"等保2.0项目-日志收集实践","uri":"/posts/elk/elk-practice/"},{"categories":["ElasticStack"],"content":"Rsyslog 部署 rsyslog用于收集对端系统的日志推送 #替换源 vim /etc/apt/sources.list deb http://mirrors.aliyun.com/ubuntu/ focal main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ focal main restricted universe multiverse deb http://mirrors.aliyun.com/ubuntu/ focal-security main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ focal-security main restricted universe multiverse deb http://mirrors.aliyun.com/ubuntu/ focal-updates main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ focal-updates main restricted universe multiverse deb http://mirrors.aliyun.com/ubuntu/ focal-proposed main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ focal-proposed main restricted universe multiverse deb http://mirrors.aliyun.com/ubuntu/ focal-backports main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ focal-backports main restricted universe multiverse #安装rsyslog sudo apt-get install rsyslog #编辑配置文件 vim /etc/rsyslog.conf #设置日志收集目录，以主机名+IP+时间的格式 并排除本机的日志 $template Remote,\"/var/log/attack-syslog/%hostname%_%fromhost-ip%/log_%$YEAR%-%$MONTH%-%$DAY%.log\" #定义模板，接受日志文件路径，区分了不同主机的日志 :fromhost-ip, !isequal, \"127.0.0.1\" ?Remote # 过滤server 本机的日志 #开启udp tcp 传输 $ModLoad imudp $UDPServerRun 514 $ModLoad imtcp $InputTCPServerRun 514 #然后,以root身份修改rsyslog启动配置文件(Ubuntu在/etc/default/rsyslog下) # Options to syslogd # -m 0 disables 'MARK' messages. # -r enables logging from remote machines # -x disables DNS lookups on messages recieved with -r //禁用掉dns记录项不够齐全或其他的日志中心的日志 # See syslogd(8) for more details # SYSLOGD_OPTIONS=\"-r\" #SYSLOGD_OPTIONS=\"-r -x -m 180\" # 加 -r 选项以允许接受外来日志消息 # 加 -x 禁用掉dns记录项不够齐全或其他的日志中心的日志# # 加 -m 修改syslog的内部mark消息写入间隔时间（0为关闭）。例如-m 180，表示每隔180分钟（每天8次）在日志文件里增加一行时间戳消息 # 加 -h 默认情况下，syslog不会发送从远端接受过来的消息到其他主机，而使用该选项，则把该开关打开，所有接受到的信息都可根据syslog.conf中定义的@主机转发过去。 # Options to klogd # -2 prints all kernel oops messages twice; once for klogd to decode, and # once for processing with 'ksymoops' # -x disables all klogd processing of oops messages entirely # See klogd(8) for more detailsKLOGD_OPTIONS=\"-x\" #SYSLOG_UMASK=077# set this to a umask value to use for all log files as in umask(1). # By default, all permissions are removed for \"group\" and \"other\". #重启rsyslog service rsyslog restart #查看其是否启动 netstat -nultp | grep 514 #验证 #在rsyslog server端,用tail动态查看 tail -f /var/log/host/'hostname'_'ip'/log_'y'_'m'_'d'.log ","date":"2021-10-21","objectID":"/posts/elk/elk-practice/:2:0","tags":["日志收集","等保2.0","运维实战"],"title":"等保2.0项目-日志收集实践","uri":"/posts/elk/elk-practice/"},{"categories":["ElasticStack"],"content":"对端深信服防火墙配置 版本号：AF 8.0.45 在监控—\u003e 日志—\u003e设置 中开启行为审计日志选项，配置对端syslog服务器地址和端口号。 ","date":"2021-10-21","objectID":"/posts/elk/elk-practice/:2:1","tags":["日志收集","等保2.0","运维实战"],"title":"等保2.0项目-日志收集实践","uri":"/posts/elk/elk-practice/"},{"categories":["ElasticStack"],"content":"对端深信服SSL VPN配置 版本号： SSL 7.6.9R1 在系统设置—\u003e 系统配置—\u003e 数据中心中启用Syslog 设置添加对端服务器地址 点击测试连通性测试对端服务器连接状况 设置日志输出类型，注意根据实际需求选择相应日志等级，避免不必要的资源浪费。 ","date":"2021-10-21","objectID":"/posts/elk/elk-practice/:2:2","tags":["日志收集","等保2.0","运维实战"],"title":"等保2.0项目-日志收集实践","uri":"/posts/elk/elk-practice/"},{"categories":["ElasticStack"],"content":"对端H3C核心交换机配置 设备型号：H3C S7503E-M 详情见交换机配置手册-15-信息中心配置-新华三集团-H3C [Intranet-CSW-S7503E]info-center enable #开启信息中心\r[Intranet-CSW-S7503E]info-center loghost 192.168.10.109 port 514 facility local5\r#配置发送日志信息到IP地址为192.168.10.109端口为514的日志主机，日志主机记录工具为local5。\r[Intranet-CSW-S7503E]info-center source default loghost deny\r# 关闭loghost方向所有模块日志信息的输出开关。\r[Intranet-CSW-S7503E]info-center source ftp loghost level notification\r# 配置输出规则：允许FTP模块的、等级高于等于notification的日志信息输出到日志主机（注意：允许输出信息的模块由产品决定）。 由于系统对各方向允许输出的日志信息的缺省情况不一样，所以配置前必须将所有模块的需求方向（本例为loghost）上日志信息的输出开关关闭，再根据当前的需求配置输出规则，以免输出太多不需要的信息。 完成对端设备日志的推送设置后，相关日志保存在109本地服务器对应目录下的。 可见 防火墙日志中记录了之前在服务器中指定收集的日志类型 至此rsyslog收集工作结束 ","date":"2021-10-21","objectID":"/posts/elk/elk-practice/:2:3","tags":["日志收集","等保2.0","运维实战"],"title":"等保2.0项目-日志收集实践","uri":"/posts/elk/elk-practice/"},{"categories":["ElasticStack"],"content":"Filebeat 组件配置 环境变量配置 vim /etc/profile.d/filebeat.sh #/bin/bash export PATH=/usr/share/filebeat/bin:$PATH经过配置rsyslog服务，已经收集到了各业务系统的日志文件，下一步工作就是收集到的文件通过ELK中的Filebeat组件对日志进行初步加工。 #编辑配置文件 vim /etc/filebeat/filebeat.yml #filebeat inputs 部分 设置日志的所在目录位置 #一定要注意格式 特别是插件前后顺序和-的位置。 #经过实践踩坑大部分问题都是因为格式导致的。 filebeat.inputs: - type: log # Change to true to enable this input configuration. enabled: true # Paths that should be crawled and fetched. Glob based paths. paths: - /var/log/attack-syslog/localhost_10.123.0.2/*.log fields: logtype1: \"sangfor-af\" #- c:\\programdata\\elasticsearch\\logs\\* - type: log enabled: true paths: - /var/log/attack-syslog/sslvpn_10.123.0.27/*.log fields: logtype1: \"sangfor-sslvpn\" - type: log enabled: true paths: - /var/log/attack-syslog/sslvpn_10.123.0.28/*.log fields: logtype1: \"sangfor-sslvpn\" - type: log enabled: true paths: - /var/log/attack-syslog/2021_192.168.10.254/*.log fields: logtype1: \"hc-nwhx\" #抓取数据并对日志进行打标签，后续通过标签建立独自的索引 #Outputs部分 #注释 Elasticsearch Output 配置Logstash Output output.logstash: # The Logstash hosts hosts: [\"192.168.10.107:5044\"] #可选项 # Optional SSL. By default is off. # List of root certificates for HTTPS server verifications #ssl.certificate_authorities: [\"/etc/pki/root/ca.pem\"] # Certificate for SSL client authentication #ssl.certificate: \"/etc/pki/client/cert.pem\" # Client Certificate Key #ssl.key: \"/etc/pki/client/cert.key\" #启动 systemctl start filebeat ","date":"2021-10-21","objectID":"/posts/elk/elk-practice/:3:0","tags":["日志收集","等保2.0","运维实战"],"title":"等保2.0项目-日志收集实践","uri":"/posts/elk/elk-practice/"},{"categories":["ElasticStack"],"content":"Filebeat 组件排错方法 在发生错误时，服务仍在运行状态时使用systemctl status filebeat 能看到的错误信息很少，通过前端手动指定运行日志的方式更易于问题的定位。 filebeat -c /etc/filebeat/filebeat.yml -path.home /usr/share/filebeat -path.config /etc/filebeat -path.data /var/lib/filebeat -path.logs /var/log/filebeat ","date":"2021-10-21","objectID":"/posts/elk/elk-practice/:3:1","tags":["日志收集","等保2.0","运维实战"],"title":"等保2.0项目-日志收集实践","uri":"/posts/elk/elk-practice/"},{"categories":["ElasticStack"],"content":"Logstash 组件配置 环境变量定制 echo 'export PATH=/usr/share/logstash/bin:$PATH' \u003e /etc/profile.d/logstash.sh source /etc/profile.d/logstash.sh生成配置文件 以命令行的方式来进行启动太繁琐，我们最好还是以配置文件的方式来进行服务的启动管理，对于 logstash来说，它提供好了一个专门用于生成配置文件的命令 system-install，我们只需要按照既定的 配置文件规则，定制应用配置，最后执行该命令，即可实现服务脚本的配置。 #进入应用目录 cd /etc/logstash #编辑启动参数文件 # vim startup.options ... # Arguments to pass to logstash LS_OPTS=\"--path.settings ${LS_SETTINGS_DIR} -f /etc/logstash/conf.d\" #注意： -f 指定的是 logstash的应用配置文件(比如 logstash.conf)存放到的目录#以root用户执行下面的命令 system-install #查看生成的服务配置文件 # ls /etc/systemd/system/logstash.service /etc/systemd/system/logstash.service #查看服务配置文件内容 # cat /etc/systemd/system/logstash.service [Unit] Description=logstash [Service] Type=simple User=logstash Group=logstash # Load env vars from /etc/default/ and /etc/sysconfig/ if they exist. # Prefixing the path with '-' makes it try to load, but if the file doesn't # exist, it continues onward. EnvironmentFile=-/etc/default/logstash EnvironmentFile=-/etc/sysconfig/logstash ExecStart=/usr/share/logstash/bin/logstash \"--path.settings\" \"/etc/logstash\" \"- f\" \"/etc/logstash/conf.d\" Restart=always WorkingDirectory=/ Nice=19 LimitNOFILE=16384 # When stopping, how long to wait before giving up and sending SIGKILL? # Keep in mind that SIGKILL on a process can cause data loss. TimeoutStopSec=infinity [Install] WantedBy=multi-user.target #注意： # 由于服务启动的时候，用户名和用户组都是 logstash ，所以，我们采集数据的文件必须是具备查看的 #权限日志在经过filebeat组件的初步改造，将各系统收集到的日志打上了type类型，Logstash根据类型创建不同的索引文件。这里通过grok插件对日志进行了字段自定义改造。以便后续在kibana中更好的绘图展示。详情见 grok插件用法 vim /etc/logstash/conf.d/logstash.conf #input 部分 #读取filebeat主机推送到5044端口的数据 input { beats { port =\u003e 5044 } } filter { grok { match =\u003e { \"message\" =\u003e \"%{TIMESTAMP_ISO8601:times} %{HOSTNAME:hosts} %{USERNAME:logtype}: message repeated %{INT:repetition_times} times: \\[ 日志类型:(?\u003cOperation_type\u003e(?\u003c=)(.{4})), (?\u003cOperation_typ1e\u003e(?\u003c=)(.{2})):%{USER:user}\\(%{HOSTNAME:connection_method}\\)\\(%{HOSTNAME:connection_method}\\), IP地址:%{IPV4:connection_ip}, 操作对象:%{GREEDYDATA:Action_log}, 操作类型:(?\u003cbehaviour_t\u003e(?\u003c=)(.{4})), 描述:(?\u003cBehavior_performance\u003e(?\u003c=)(.{4}))\\]\" } } } #output 部分 #注意if后需要加 [fields]并添加之前filebeat中定制的字段[logtype1]，经测试直接加logtype1不好使。 output { if [fields][logtype1] == \"sangfor-af\" { elasticsearch { hosts =\u003e [\"http://localhost:9200\"] index =\u003e \"sangfor-af01-%{+YYYY.MM.dd}\" } } if [fields][logtype1] == \"hc-nwhx\" { elasticsearch { hosts =\u003e [\"http://localhost:9200\"] index =\u003e \"hc-nwhx-%{+YYYY.MM.dd}\" } } if [fields][logtype1] == \"sangfor-sslvpn\" { elasticsearch { hosts =\u003e [\"http://localhost:9200\"] index =\u003e \"sangfor-sslvpn-%{+YYYY.MM.dd}\" } } } 启动服务 重载服务 systemctl daemon-reload 启动服务 systemctl start logstash.service systemctl status logstash.service 查看效果 # netstat -tnulp | egrep 'Add|java' Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp6 0 0 127.0.0.1:9600 :::* LISTEN 88794/java tcp6 0 0 :::9200 :::* LISTEN 87210/java tcp6 0 0 :::9300 :::* LISTEN 87210/java 结果显示： logstash的默认端口是 9600 ","date":"2021-10-21","objectID":"/posts/elk/elk-practice/:3:2","tags":["日志收集","等保2.0","运维实战"],"title":"等保2.0项目-日志收集实践","uri":"/posts/elk/elk-practice/"},{"categories":["ElasticStack"],"content":"logstash 排错 #查看日志： tail -f /var/log/logstash/logstash-plain.log #可以看到默认报错： [2021-08-15T18:44:08,643][WARN ] [filewatch.tailmode.handlers.createinitial][main] [cc34021140e2525e95d5755b6135b9801f3595239bcda82a1cca03a1d0f857d6] failed to open file {:path=\u003e\"/var/log/syslog\", :exception=\u003eErrno::EACCES, :message=\u003e\"Permission denied - /var/log/syslog\"} #临时增加一个 logstash 允许访问的权限 chown logstash.logstash /var/log/syslog通过head插件查看数据传递效果 ","date":"2021-10-21","objectID":"/posts/elk/elk-practice/:3:3","tags":["日志收集","等保2.0","运维实战"],"title":"等保2.0项目-日志收集实践","uri":"/posts/elk/elk-practice/"},{"categories":["ElasticStack"],"content":"一般系统或服务生成的日志都是一大长串。每个字段之间用空格隔开。logstash在获取日志是整个一串获取，如果把日志中每个字段代表的意思分割开来在传给elasticsearch。这样呈现出来的数据更加清晰，而且也能让kibana更方便的绘制图形。 Grok 是 Logstash 最重要的插件。它的主要作用就是将文本格式的字符串，转换成为具体的结构化的数据，配合正则表达式使用。 Grok 正则捕获 Grok 支持把预定义的 grok 表达式 写入到文件中，官方提供的预定义 grok 表达式见：https://github.com/logstash/logstash/tree/v1.4.2/patterns。 %{syntax:semantic} syntax代表的是正则表达式替代字段，semantic是代表这个表达式对应的字段名，你可以自由命名。这个命名尽量能简单易懂的表达出这个字段代表的意思。 logstash安装时就带有已经写好的正则表达式。路径如下： /usr/local/logstash-2.3.4/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-2.0.5/patterns 或者直接访问logstash-plugins/logstash-patterns-core · GitHub 上面IPORHOST，USER等都是在里面已经定义好的！当然还有其他的，基本能满足我们的需求。 ","date":"2021-10-21","objectID":"/posts/elk/grok-patterns/:0:0","tags":["日志收集"],"title":"日志处理-Grok正则捕获","uri":"/posts/elk/grok-patterns/"},{"categories":["ElasticStack"],"content":"grok-patterns USERNAME [a-zA-Z0-9._-]+ USER %{USERNAME} EMAILLOCALPART [a-zA-Z0-9!#$%\u0026'*+\\-/=?^_`{|}~]{1,64}(?:\\.[a-zA-Z0-9!#$%\u0026'*+\\-/=?^_`{|}~]{1,62}){0,63} EMAILADDRESS %{EMAILLOCALPART}@%{HOSTNAME} INT (?:[+-]?(?:[0-9]+)) BASE10NUM (?\u003c![0-9.+-])(?\u003e[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+))) NUMBER (?:%{BASE10NUM}) BASE16NUM (?\u003c![0-9A-Fa-f])(?:[+-]?(?:0x)?(?:[0-9A-Fa-f]+)) BASE16FLOAT \\b(?\u003c![0-9A-Fa-f.])(?:[+-]?(?:0x)?(?:(?:[0-9A-Fa-f]+(?:\\.[0-9A-Fa-f]*)?)|(?:\\.[0-9A-Fa-f]+)))\\b POSINT \\b(?:[1-9][0-9]*)\\b NONNEGINT \\b(?:[0-9]+)\\b WORD \\b\\w+\\b NOTSPACE \\S+ SPACE \\s* DATA .*? GREEDYDATA .* QUOTEDSTRING (?\u003e(?\u003c!\\\\)(?\u003e\"(?\u003e\\\\.|[^\\\\\"]+)+\"|\"\"|(?\u003e'(?\u003e\\\\.|[^\\\\']+)+')|''|(?\u003e`(?\u003e\\\\.|[^\\\\`]+)+`)|``)) UUID [A-Fa-f0-9]{8}-(?:[A-Fa-f0-9]{4}-){3}[A-Fa-f0-9]{12} # URN, allowing use of RFC 2141 section 2.3 reserved characters URN urn:[0-9A-Za-z][0-9A-Za-z-]{0,31}:(?:%[0-9a-fA-F]{2}|[0-9A-Za-z()+,.:=@;$_!*'/?#-])+ # Networking MAC (?:%{CISCOMAC}|%{WINDOWSMAC}|%{COMMONMAC}) CISCOMAC (?:(?:[A-Fa-f0-9]{4}\\.){2}[A-Fa-f0-9]{4}) WINDOWSMAC (?:(?:[A-Fa-f0-9]{2}-){5}[A-Fa-f0-9]{2}) COMMONMAC (?:(?:[A-Fa-f0-9]{2}:){5}[A-Fa-f0-9]{2}) IPV6 ((([0-9A-Fa-f]{1,4}:){7}([0-9A-Fa-f]{1,4}|:))|(([0-9A-Fa-f]{1,4}:){6}(:[0-9A-Fa-f]{1,4}|((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){5}(((:[0-9A-Fa-f]{1,4}){1,2})|:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){4}(((:[0-9A-Fa-f]{1,4}){1,3})|((:[0-9A-Fa-f]{1,4})?:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){3}(((:[0-9A-Fa-f]{1,4}){1,4})|((:[0-9A-Fa-f]{1,4}){0,2}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){2}(((:[0-9A-Fa-f]{1,4}){1,5})|((:[0-9A-Fa-f]{1,4}){0,3}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){1}(((:[0-9A-Fa-f]{1,4}){1,6})|((:[0-9A-Fa-f]{1,4}){0,4}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(:(((:[0-9A-Fa-f]{1,4}){1,7})|((:[0-9A-Fa-f]{1,4}){0,5}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:)))(%.+)? IPV4 (?\u003c![0-9])(?:(?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5])[.](?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5])[.](?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5])[.](?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5]))(?![0-9]) IP (?:%{IPV6}|%{IPV4}) HOSTNAME \\b(?:[0-9A-Za-z][0-9A-Za-z-]{0,62})(?:\\.(?:[0-9A-Za-z][0-9A-Za-z-]{0,62}))*(\\.?|\\b) IPORHOST (?:%{IP}|%{HOSTNAME}) HOSTPORT %{IPORHOST}:%{POSINT} # paths (only absolute paths are matched) PATH (?:%{UNIXPATH}|%{WINPATH}) UNIXPATH (/[[[:alnum:]]_%!$@:.,+~-]*)+ TTY (?:/dev/(pts|tty([pq])?)(\\w+)?/?(?:[0-9]+)) WINPATH (?\u003e[A-Za-z]+:|\\\\)(?:\\\\[^\\\\?*]*)+ URIPROTO [A-Za-z]([A-Za-z0-9+\\-.]+)+ URIHOST %{IPORHOST}(?::%{POSINT})? # uripath comes loosely from RFC1738, but mostly from what Firefox doesn't turn into %XX URIPATH (?:/[A-Za-z0-9$.+!*'(){},~:;=@#%\u0026_\\-]*)+ URIQUERY [A-Za-z0-9$.+!*'|(){},~@#%\u0026/=:;_?\\-\\[\\]\u003c\u003e]* # deprecated (kept due compatibility): URIPARAM \\?%{URIQUERY} URIPATHPARAM %{URIPATH}(?:\\?%{URIQUERY})? URI %{URIPROTO}://(?:%{USER}(?::[^@]*)?@)?(?:%{URIHOST})?(?:%{URIPATH}(?:\\?%{URIQUERY})?)? # Months: January, Feb, 3, 03, 12, December MONTH \\b(?:[Jj]an(?:uary|uar)?|[Ff]eb(?:ruary|ruar)?|[Mm](?:a|ä)?r(?:ch|z)?|[Aa]pr(?:il)?|[Mm]a(?:y|i)?|[Jj]un(?:e|i)?|[Jj]ul(?:y|i)?|[Aa]ug(?:ust)?|[Ss]ep(?:tember)?|[Oo](?:c|k)?t(?:ober)?|[Nn]ov(?:ember)?|[Dd]e(?:c|z)(?:ember)?)\\b MONTHNUM (?:0?[1-9]|1[0-2]) MONTHNUM2 (?:0[1-9]|1[0-2]) MONTHDAY (?:(?:0[1-9])|(?:[12][0-9])|(?:3[01])|[1-9]) # Days: Monday, Tue, Thu, etc... DAY (?:Mon(?:day)?|Tue(?:sday)?|Wed(?:nesday)?|Thu(?:rsday)?|Fri(?:day)?|Sat(?:urday)?|Sun(?:day)?) # Years? YEAR (?\u003e\\d\\d){1,2} HOUR (?:2[0123]|[01]?[0-9]) MINUTE (?:[0-5][0-9]) # '60' is a leap second in most time standards and thus is valid. SECOND (?:(?:[0-5]?[0-9]|60)(?:[:.,][0-9]+)?) TIME (?!\u003c[0-9])%{HOUR}:%{MINUTE}(?::%{SECOND})(?![","date":"2021-10-21","objectID":"/posts/elk/grok-patterns/:1:0","tags":["日志收集"],"title":"日志处理-Grok正则捕获","uri":"/posts/elk/grok-patterns/"},{"categories":["ElasticStack"],"content":"案例实践 例1：将下面的日志文件格式拆分为5段 2016-09-19T18:19:00 [8.8.8.8:prd] DEBUG this is an example log message 时间 IP地址 环境 等级 信息 使用Grok 默认提供的正则匹配后 %{TIMESTAMP_ISO8601:timestamp} \\[%{IPV4:ip};%{WORD:environment}\\] %{LOGLEVEL:log_level} %{GREEDYDATA:message} 这样就会生成结构化结果： { \"timestamp\": \"2016-09-19T18:19:00\", \"ip\": \"8.8.8.8\", \"environment\": \"prd\", \"log_level\": \"DEBUG\", \"message\": \"this is an example log message\" }TIMESTAMP_ISO8601用来匹配时间 IPV4匹配IPV4 IP地址 WORD匹配环境 LOGLEVEL匹配了日志等级 GREEDYDATA匹配后面的所有内容 例2： 220.181.108.96 - - [13/Jun/2015:21:14:28 +0000] \"GET /blog/geekery/xvfb-firefox.html HTTP/1.1\" 200 10975 \"-\" \"Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)\"转换后： %{IPORHOST:clientip} %{USER:ident} %{USER:auth} \\[%{HTTPDATE:timestamp}\\] \"%{WORD:verb} %{DATA:request} HTTP/%{NUMBER:httpversion}\" %{NUMBER:response:int} (?:-|%{NUMBER:bytes:int}) %{QS:referrer} %{QS:agent}例3： 220.181.108.96 - - [13/Jun/2015:21:14:28 +0000] \"GET /blog/geekery/xvfb-firefox.html HTTP/1.1\" 200 10975 \"-\" \"Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)\"转换后： %{IPORHOST:clientip} %{USER:ident} %{USER:auth} \\[%{HTTPDATE:timestamp}\\] \"%{WORD:verb} %{DATA:request} HTTP/%{NUMBER:httpversion}\" %{NUMBER:response:int} (?:-|%{NUMBER:bytes:int}) %{QS:referrer} %{QS:agent}例4：假设我们有三个使用“common_header：payload”格式的应用程序 Application 1: '8.8.8.8 process-name[666]: a b 1 2 a lot of text at the end' Application 2: '8.8.8.8 process-name[667]: a 1 2 3 a lot of text near the end;4' Application 3: '8.8.8.8 process-name[421]: a completely different format | 1111'转换后： grok { \"match\" =\u003e { \"message =\u003e [\r'%{IPORHOST:clientip} %{DATA:process_name}\\[%{NUMBER:process_id}\\]: %{WORD:word_1} %{WORD:word_2} %{NUMBER:number_1} %{NUMBER:number_2} %{DATA:data}',\r'%{IPORHOST:clientip} %{DATA:process_name}\\[%{NUMBER:process_id}\\]: %{WORD:word_1} %{NUMBER:number_1} %{NUMBER:number_2} %{NUMBER:number_3} %{DATA:data};%{NUMBER:number_4}',\r'%{IPORHOST:clientip} %{DATA:process_name}\\[%{NUMBER:process_id}\\]: %{DATA:data} | %{NUMBER:number}'\r] }\r}","date":"2021-10-21","objectID":"/posts/elk/grok-patterns/:2:0","tags":["日志收集"],"title":"日志处理-Grok正则捕获","uri":"/posts/elk/grok-patterns/"},{"categories":["ElasticStack"],"content":"下面针对Apache日志来分割处理 192.168.10.97 - - [19/Jul/2016:16:28:52 +0800] \"GET / HTTP/1.1\" 200 23 \"-\" \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.101 Safari/537.36\"日志中每个字段之间空格隔开，分别对应message中的字段。 如：%{IPORHOST:addre} –\u003e 192.168.10.97 但问题是IPORHOST又不是正则表达式，怎么能匹配IP地址呢？ 因为IPPRHOST是grok表达式，它代表的正则表达式如下： IPV6 ((([0-9A-Fa-f]{1,4}:){7}([0-9A-Fa-f]{1,4}|:))|(([0-9A-Fa-f]{1,4}:){6}(:[0-9A-Fa-f]{1,4}|((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){5}(((:[0-9A-Fa-f]{1,4}){1,2})|:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){4}(((:[0-9A-Fa-f]{1,4}){1,3})|((:[0-9A-Fa-f]{1,4})?:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){3}(((:[0-9A-Fa-f]{1,4}){1,4})|((:[0-9A-Fa-f]{1,4}){0,2}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){2}(((:[0-9A-Fa-f]{1,4}){1,5})|((:[0-9A-Fa-f]{1,4}){0,3}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){1}(((:[0-9A-Fa-f]{1,4}){1,6})|((:[0-9A-Fa-f]{1,4}){0,4}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(:(((:[0-9A-Fa-f]{1,4}){1,7})|((:[0-9A-Fa-f]{1,4}){0,5}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:)))(%.+)? IPV4 (?\u003c![0-9])(?:(?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5])[.](?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5])[.](?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5])[.](?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5]))(?![0-9]) IP (?:%{IPV6}|%{IPV4}) HOSTNAME \\b(?:[0-9A-Za-z][0-9A-Za-z-]{0,62})(?:\\.(?:[0-9A-Za-z][0-9A-Za-z-]{0,62}))*(\\.?|\\b) IPORHOST (?:%{IP}|%{HOSTNAME})IPORHOST代表的是ipv4或者ipv6或者HOSTNAME所匹配的grok表达式。\r上面的IPORHOST有点复杂，我们来看看简单点的，如USER\rUSERNAME [a-zA-Z0-9._-]+ #USERNAME是匹配由字母，数字，“.”, \"_\", \"-\"组成的任意字符\rUSER %{USERNAME}\r#USER代表USERNAME的正则表达式\r第一行，用普通的正则表达式来定义一个 grok 表达式；\r第二行，通过打印赋值格式，用前面定义好的 grok 表达式来定义另一个 grok 表达式。 filter { if [type] == \"apache\" { grok { match =\u003e [\"message\" =\u003e \"%{IPORHOST:addre} %{USER:ident} %{USER:auth} \\[%{HTTPDATE:timestamp}\\] \\\"%{WORD:http_method} %{NOTSPACE:request} HTTP/%{NUMBER:httpversion}\\\" %{NUMBER:status} (?:%{NUMBER:bytes}|-) \\\"(?:%{URI:http_referer}|-)\\\" \\\"%{GREEDYDATA:User_Agent}\\\"\"] remove_field =\u003e [\"message\"] } date { match =\u003e [ \"timestamp\", \"dd/MMM/YYYY:HH:mm:ss Z\" ] } } } ","date":"2021-10-21","objectID":"/posts/elk/grok-patterns/:2:1","tags":["日志收集"],"title":"日志处理-Grok正则捕获","uri":"/posts/elk/grok-patterns/"},{"categories":["ElasticStack"],"content":"Httpd HTTPDUSER %{EMAILADDRESS}|%{USER} HTTPDERROR_DATE %{DAY} %{MONTH} %{MONTHDAY} %{TIME} %{YEAR} # Log formats HTTPD_COMMONLOG %{IPORHOST:clientip} %{HTTPDUSER:ident} %{HTTPDUSER:auth} \\[%{HTTPDATE:timestamp}\\] \"(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})\" %{NUMBER:response} (?:%{NUMBER:bytes}|-) HTTPD_COMBINEDLOG %{HTTPD_COMMONLOG} %{QS:referrer} %{QS:agent} # Error logs HTTPD20_ERRORLOG \\[%{HTTPDERROR_DATE:timestamp}\\] \\[%{LOGLEVEL:loglevel}\\] (?:\\[client %{IPORHOST:clientip}\\] ){0,1}%{GREEDYDATA:message} HTTPD24_ERRORLOG \\[%{HTTPDERROR_DATE:timestamp}\\] \\[%{WORD:module}:%{LOGLEVEL:loglevel}\\] \\[pid %{POSINT:pid}(:tid %{NUMBER:tid})?\\]( \\(%{POSINT:proxy_errorcode}\\)%{DATA:proxy_message}:)?( \\[client %{IPORHOST:clientip}:%{POSINT:clientport}\\])?( %{DATA:errorcode}:)? %{GREEDYDATA:message} HTTPD_ERRORLOG %{HTTPD20_ERRORLOG}|%{HTTPD24_ERRORLOG} # Deprecated COMMONAPACHELOG %{HTTPD_COMMONLOG} COMBINEDAPACHELOG %{HTTPD_COMBINEDLOG} ","date":"2021-10-21","objectID":"/posts/elk/grok-patterns/:2:2","tags":["日志收集"],"title":"日志处理-Grok正则捕获","uri":"/posts/elk/grok-patterns/"},{"categories":["ElasticStack"],"content":"java JAVACLASS (?:[a-zA-Z$_][a-zA-Z$_0-9]*\\.)*[a-zA-Z$_][a-zA-Z$_0-9]* #Space is an allowed character to match special cases like 'Native Method' or 'Unknown Source' JAVAFILE (?:[A-Za-z0-9_. -]+) #Allow special \u003cinit\u003e, \u003cclinit\u003e methods JAVAMETHOD (?:(\u003c(?:cl)?init\u003e)|[a-zA-Z$_][a-zA-Z$_0-9]*) #Line number is optional in special cases 'Native method' or 'Unknown source' JAVASTACKTRACEPART %{SPACE}at %{JAVACLASS:class}\\.%{JAVAMETHOD:method}\\(%{JAVAFILE:file}(?::%{NUMBER:line})?\\) # Java Logs JAVATHREAD (?:[A-Z]{2}-Processor[\\d]+) JAVACLASS (?:[a-zA-Z0-9-]+\\.)+[A-Za-z0-9$]+ JAVAFILE (?:[A-Za-z0-9_.-]+) JAVALOGMESSAGE (.*) # MMM dd, yyyy HH:mm:ss eg: Jan 9, 2014 7:13:13 AM CATALINA_DATESTAMP %{MONTH} %{MONTHDAY}, 20%{YEAR} %{HOUR}:?%{MINUTE}(?::?%{SECOND}) (?:AM|PM) # yyyy-MM-dd HH:mm:ss,SSS ZZZ eg: 2014-01-09 17:32:25,527 -0800 TOMCAT_DATESTAMP 20%{YEAR}-%{MONTHNUM}-%{MONTHDAY} %{HOUR}:?%{MINUTE}(?::?%{SECOND}) %{ISO8601_TIMEZONE} CATALINALOG %{CATALINA_DATESTAMP:timestamp} %{JAVACLASS:class} %{JAVALOGMESSAGE:logmessage} # 2014-01-09 20:03:28,269 -0800 | ERROR | com.example.service.ExampleService - something compeletely unexpected happened... TOMCATLOG %{TOMCAT_DATESTAMP:timestamp} \\| %{LOGLEVEL:level} \\| %{JAVACLASS:class} - %{JAVALOGMESSAGE:logmessage} ","date":"2021-10-21","objectID":"/posts/elk/grok-patterns/:2:3","tags":["日志收集"],"title":"日志处理-Grok正则捕获","uri":"/posts/elk/grok-patterns/"},{"categories":["ElasticStack"],"content":"Grok Debugger 当我们拿到一段日志，按照上面的grok表达式一个个去匹配时，我们如何确定我们匹配的是否正确呢？ http://grokdebug.herokuapp.com/ 这个地址可以满足我们的测试需求。就拿上面apache的日志测试。 点击后就出现如下数据，你写的每个grok表达式都获取到值了。为了测试准确，可以多测试几条日志。 效果： kibana字段展示： 配置文件： # ---------------input 输入模块----------------------- input { beats { port =\u003e 5044 } } # ---------------filter 过滤模块----------------------- filter { grok { match =\u003e { \"message\" =\u003e \"%{TIMESTAMP_ISO8601:times} %{HOSTNAME:hosts} %{USERNAME:logtype}: message repeated %{INT:repetition_times} times: \\[ 日志类型:(?\u003cOperation_type\u003e(?\u003c=)(.{4})), (?\u003cOperation_typ1e\u003e(?\u003c=)(.{2})):%{USER:user}\\(%{HOSTNAME:connection_method}\\)\\(%{HOSTNAME:connection_method}\\), IP地址:%{IPV4:connection_ip}, 操作对象:%{GREEDYDATA:Action_log}, 操作类型:(?\u003cbehaviour_t\u003e(?\u003c=)(.{4})), 描述:(?\u003cBehavior_performance\u003e(?\u003c=)(.{4}))\\]\" } } } # ---------------output 输出模块----------------------- output { elasticsearch { hosts =\u003e [\"http://localhost:9200\"] index =\u003e \"sangfor-af-%{+YYYY.MM.dd}\" #user =\u003e \"elastic\" #password =\u003e \"changeme\" } } ","date":"2021-10-21","objectID":"/posts/elk/grok-patterns/:2:4","tags":["日志收集"],"title":"日志处理-Grok正则捕获","uri":"/posts/elk/grok-patterns/"},{"categories":["ElasticStack"],"content":"自定义grok表达式 grok主要有两部分：自定义正则表达式和系统预定义的模式表达式。 如果你感觉logstash自带的grok表达式不能满足需要，你也可以自己定义 如： filter { if [type] == \"apache\" { grok { patterns_dir =\u003e \"/usr/local/logstash-2.3.4/ownpatterns/patterns\" match =\u003e { \"message\" =\u003e \"%{APACHE_LOG}\" } remove_field =\u003e [\"message\"] } date { match =\u003e [ \"timestamp\", \"dd/MMM/YYYY:HH:mm:ss Z\" ] } } } #patterns_dir为自定义的grok表达式的路径。 #自定义的patterns中按照logstash自带的格式书写。 APACHE_LOG %{IPORHOST:addre} %{USER:ident} %{USER:auth} \\[%{HTTPDATE:timestamp}\\] \\\"%{WORD:http_method} %{NOTSPACE:request} HTTP/%{NUMBER:httpversion}\\\" %{NUMBER:status} (?:%{NUMBER:bytes}|-) \\\"(?:%{URI:http_referer}|-)\\\" \\\"%{GREEDYDATA:User_Agent}\\\" #我只是把apache日志匹配的grok表达式写入自定义文件中，简化conf文件。单个字段的正则表达式匹配你可以自己书写测试。 ","date":"2021-10-21","objectID":"/posts/elk/grok-patterns/:3:0","tags":["日志收集"],"title":"日志处理-Grok正则捕获","uri":"/posts/elk/grok-patterns/"},{"categories":["ElasticStack"],"content":"常用正则 (?\u003ctemMsg\u003e(.*)(?=Report)/?) 获取Report之前的字符\r(?\u003ctemMsg\u003e(?=Report)(.*)/?) 获取Report之后的字符\r(?\u003ctemMsg\u003e(?\u003c=report).*?(?=msg)) 截取report和msg之间的值 不包含report和msg本身\r(?\u003ctemMsg\u003e(report).*?(?=msg)) 截取包含report但不包含msg\r(?\u003ctemMsg\u003e(?\u003c=report).*?(msg)) 截取不包含report但包含msg\r(?\u003ctemMsg\u003e(report).*?(msg|request)) 输出以report开头,以msg或者以request结尾的所有包含头尾信息\r(?\u003ctemMsg\u003e(report).*?(?=(msg|request))) 输出以report开头,以msg或者以request结尾的不包含头尾信息 ","date":"2021-10-21","objectID":"/posts/elk/grok-patterns/:3:1","tags":["日志收集"],"title":"日志处理-Grok正则捕获","uri":"/posts/elk/grok-patterns/"},{"categories":["ElasticStack"],"content":"grok截取字符中指定长度的内容 要求利用grok截取日志消息中某一指定长度的内容。 Logstatsh需要两个必需参数input、output，以及一个可选参数filter。input用于输入数据的设置，output用于输出数据的设置。filter是实现数据过滤的设置。grok是在filter里面实现数据截取。 项目有一串协议消息如 7e8900000c040116432693324af0010180010005e98e0706000a7e，要求利用grok截取7e后面的四个字符，利用grok正则表达式即可实现。 实现代码如下： filter{ grok{ match =\u003e { \"message\" =\u003e \"(?\u003cmid\u003e(?\u003c=7e)(.{4}))\" } } }代码解释： message：即输入的数据信息。 mid：即输出结果的名称 (?\u003c=7e)：即表示获取7e后面的字符，但不包括7e (.{4})：即表示获取的字符长度为4个 引用文章： Grok 正则捕获 | Logstash 最佳实践 (yonyoucloud.com) logstash-patterns-core (github.com) Logstash 常用正则（grok-patterns）qianghong000_51CTO博客 https://blog.51cto.com/irow10/1828077 Logstash Grok详解_叱咤少帅的博客-CSDN博客 轻松掌握Logstash的grok匹配_全菜工程师小辉的博客-CSDN博客 logstash截取指定字符和grok的使用_cai750415222的博客-CSDN博客 ","date":"2021-10-21","objectID":"/posts/elk/grok-patterns/:3:2","tags":["日志收集"],"title":"日志处理-Grok正则捕获","uri":"/posts/elk/grok-patterns/"},{"categories":["Zookeeper"],"content":"企业面试真题（面试重点） ","date":"2021-10-09","objectID":"/posts/zookeeper/zookeeper-05/:0:0","tags":["Zookeeper","中间件"],"title":"zookeeper 企业面试真题（面试重点）","uri":"/posts/zookeeper/zookeeper-05/"},{"categories":["Zookeeper"],"content":"选举机制 半数机制，超过半数的投票通过，即通过。 （1）第一次启动选举规则： 投票过半数时，服务器 id 大的胜出 （2）第二次启动选举规则： ①EPOCH 大的直接胜出 ②EPOCH 相同，事务 id 大的胜出 ③事务 id 相同，服务器 id 大的胜出 ","date":"2021-10-09","objectID":"/posts/zookeeper/zookeeper-05/:1:0","tags":["Zookeeper","中间件"],"title":"zookeeper 企业面试真题（面试重点）","uri":"/posts/zookeeper/zookeeper-05/"},{"categories":["Zookeeper"],"content":"生产集群安装多少 zk 合适？ 安装奇数台。 生产经验： ⚫ 10 台服务器：3 台 zk； ⚫ 20 台服务器：5 台 zk； ⚫ 100 台服务器：11 台 zk； ⚫ 200 台服务器：11 台 zk 服务器台数多：好处，提高可靠性；坏处：提高通信延时 ","date":"2021-10-09","objectID":"/posts/zookeeper/zookeeper-05/:2:0","tags":["Zookeeper","中间件"],"title":"zookeeper 企业面试真题（面试重点）","uri":"/posts/zookeeper/zookeeper-05/"},{"categories":["Zookeeper"],"content":"常用命令 ls、get、create、delete ","date":"2021-10-09","objectID":"/posts/zookeeper/zookeeper-05/:3:0","tags":["Zookeeper","中间件"],"title":"zookeeper 企业面试真题（面试重点）","uri":"/posts/zookeeper/zookeeper-05/"},{"categories":["Zookeeper"],"content":"ZooKeeper 分布式锁案例 什么叫做分布式锁呢？ 比如说\"进程 1\"在使用该资源的时候，会先去获得锁，“进程 1\"获得锁以后会对该资源保持独占，这样其他进程就无法访问该资源，“进程 1\"用完该资源以后就将锁释放掉，让其 他进程来获得锁，那么通过这个锁机制，我们就能保证了分布式系统中多个进程能够有序的 访问该临界资源。那么我们把这个分布式环境下的这个锁叫作分布式锁。 ","date":"2021-10-08","objectID":"/posts/zookeeper/zookeeper-04/:0:0","tags":["Zookeeper","中间件"],"title":"zookeeper 分布式锁案例 （四）","uri":"/posts/zookeeper/zookeeper-04/"},{"categories":["Zookeeper"],"content":"Curator 框架实现分布式锁案例 ","date":"2021-10-08","objectID":"/posts/zookeeper/zookeeper-04/:1:0","tags":["Zookeeper","中间件"],"title":"zookeeper 分布式锁案例 （四）","uri":"/posts/zookeeper/zookeeper-04/"},{"categories":["Zookeeper"],"content":"原生的 Java API 开发存在的问题 会话连接是异步的，需要自己去处理。比如使用 CountDownLatch Watch 需要重复注册，不然就不能生效 开发的复杂性还是比较高的 不支持多节点删除和创建。需要自己去递归 Curator 是一个专门解决分布式锁的框架，解决了原生 JavaAPI 开发分布式遇到的问题。 详情请查看官方文档：https://curator.apache.org/index.html ","date":"2021-10-08","objectID":"/posts/zookeeper/zookeeper-04/:1:1","tags":["Zookeeper","中间件"],"title":"zookeeper 分布式锁案例 （四）","uri":"/posts/zookeeper/zookeeper-04/"},{"categories":["Zookeeper"],"content":"Curator 案例实操 1.添加依赖 \u003cdependency\u003e \u003cgroupId\u003eorg.apache.curator\u003c/groupId\u003e \u003cartifactId\u003ecurator-framework\u003c/artifactId\u003e \u003cversion\u003e4.3.0\u003c/version\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.apache.curator\u003c/groupId\u003e \u003cartifactId\u003ecurator-recipes\u003c/artifactId\u003e \u003cversion\u003e4.3.0\u003c/version\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.apache.curator\u003c/groupId\u003e \u003cartifactId\u003ecurator-client\u003c/artifactId\u003e \u003cversion\u003e4.3.0\u003c/version\u003e \u003c/dependency\u003e2.代码实现 package com.atguigu.lock; import org.apache.curator.RetryPolicy; import org.apache.curator.framework.CuratorFramework; import org.apache.curator.framework.CuratorFrameworkFactory; import org.apache.curator.framework.recipes.locks.InterProcessLock; import org.apache.curator.framework.recipes.locks.InterProcessMutex; import org.apache.curator.retry.ExponentialBackoffRetry; public class CuratorLockTest { private String rootNode = \"/locks\"; // zookeeper server 列表 private String connectString = \"hadoop102:2181,hadoop103:2181,hadoop104:2181\"; // connection 超时时间 private int connectionTimeout = 2000; // session 超时时间 private int sessionTimeout = 2000; public static void main(String[] args) { new CuratorLockTest().test(); } // 测试 private void test() { // 创建分布式锁 1 final InterProcessLock lock1 = new InterProcessMutex(getCuratorFramework(), rootNode); // 创建分布式锁 2 final InterProcessLock lock2 = new InterProcessMutex(getCuratorFramework(), rootNode); new Thread(new Runnable() { @Override public void run() { // 获取锁对象 try { lock1.acquire(); System.out.println(\"线程 1 获取锁\"); // 测试锁重入 lock1.acquire(); System.out.println(\"线程 1 再次获取锁\"); Thread.sleep(5 * 1000); lock1.release(); System.out.println(\"线程 1 释放锁\"); lock1.release(); System.out.println(\"线程 1 再次释放锁\"); } catch (Exception e) { e.printStackTrace(); } } }).start(); new Thread(new Runnable() { @Override public void run() { // 获取锁对象 try { lock2.acquire(); System.out.println(\"线程 2 获取锁\"); // 测试锁重入 lock2.acquire(); System.out.println(\"线程 2 再次获取锁\"); Thread.sleep(5 * 1000); lock2.release(); System.out.println(\"线程 2 释放锁\"); lock2.release(); System.out.println(\"线程 2 再次释放锁\"); } catch (Exception e) { e.printStackTrace(); } } }).start(); } // 分布式锁初始化 public CuratorFramework getCuratorFramework (){ //重试策略，初试时间 3 秒，重试 3 次 RetryPolicy policy = new ExponentialBackoffRetry(3000, 3); //通过工厂创建 Curator CuratorFramework client = CuratorFrameworkFactory.builder() .connectString(connectString) .connectionTimeoutMs(connectionTimeout) .sessionTimeoutMs(sessionTimeout) .retryPolicy(policy).build(); //开启连接 client.start(); System.out.println(\"zookeeper 初始化完成...\"); return client; } }","date":"2021-10-08","objectID":"/posts/zookeeper/zookeeper-04/:1:2","tags":["Zookeeper","中间件"],"title":"zookeeper 分布式锁案例 （四）","uri":"/posts/zookeeper/zookeeper-04/"},{"categories":["Zookeeper"],"content":"服务器动态上下线监听案例 某分布式系统中，主节点可以有多台，可以动态上下线，任意一台客户端都能实时感知 到主节点服务器的上下线。 ","date":"2021-10-07","objectID":"/posts/zookeeper/zookeeper-03/:0:0","tags":["Zookeeper","中间件"],"title":"zookeeper 服务器动态上下线监听案例 （三）","uri":"/posts/zookeeper/zookeeper-03/"},{"categories":["Zookeeper"],"content":"代码实现 ","date":"2021-10-07","objectID":"/posts/zookeeper/zookeeper-03/:1:0","tags":["Zookeeper","中间件"],"title":"zookeeper 服务器动态上下线监听案例 （三）","uri":"/posts/zookeeper/zookeeper-03/"},{"categories":["Zookeeper"],"content":"服务端代码 package com.atguigu.zkcase1; import java.io.IOException; import org.apache.zookeeper.CreateMode; import org.apache.zookeeper.WatchedEvent; import org.apache.zookeeper.Watcher; import org.apache.zookeeper.ZooKeeper; import org.apache.zookeeper.ZooDefs.Ids; public class DistributeServer { private static String connectString = \"hadoop102:2181,hadoop103:2181,hadoop104:2181\"; private static int sessionTimeout = 2000; private ZooKeeper zk = null; private String parentNode = \"/servers\"; // 创建到 zk 的客户端连接 public void getConnect() throws IOException{ zk = new ZooKeeper(connectString, sessionTimeout, new Watcher() { @Override public void process(WatchedEvent event) { } }); } // 注册服务器 public void registServer(String hostname) throws Exception{ String create = zk.create(parentNode + \"/server\", hostname.getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL); System.out.println(hostname +\" is online \"+ create); } // 业务功能 public void business(String hostname) throws Exception{ System.out.println(hostname + \" is working ...\"); Thread.sleep(Long.MAX_VALUE); } public static void main(String[] args) throws Exception { // 1 获取 zk 连接 DistributeServer server = new DistributeServer(); server.getConnect(); // 2 利用 zk 连接注册服务器信息 server.registServer(args[0]); // 3 启动业务功能 server.business(args[0]); } }","date":"2021-10-07","objectID":"/posts/zookeeper/zookeeper-03/:1:1","tags":["Zookeeper","中间件"],"title":"zookeeper 服务器动态上下线监听案例 （三）","uri":"/posts/zookeeper/zookeeper-03/"},{"categories":["Zookeeper"],"content":"客户端代码 package com.atguigu.zkcase1; import java.io.IOException; import java.util.ArrayList; import java.util.List; import org.apache.zookeeper.WatchedEvent; import org.apache.zookeeper.Watcher; import org.apache.zookeeper.ZooKeeper; public class DistributeClient { private static String connectString = \"hadoop102:2181,hadoop103:2181,hadoop104:2181\"; private static int sessionTimeout = 2000; private ZooKeeper zk = null; private String parentNode = \"/servers\"; // 创建到 zk 的客户端连接 public void getConnect() throws IOException { zk = new ZooKeeper(connectString, sessionTimeout, new Watcher() { @Override public void process(WatchedEvent event) { // 再次启动监听 try { getServerList(); } catch (Exception e) { e.printStackTrace(); } } }); } // 获取服务器列表信息 public void getServerList() throws Exception { // 1 获取服务器子节点信息，并且对父节点进行监听 List\u003cString\u003e children = zk.getChildren(parentNode, true); // 2 存储服务器信息列表 ArrayList\u003cString\u003e servers = new ArrayList\u003c\u003e(); // 3 遍历所有节点，获取节点中的主机名称信息 for (String child : children) { byte[] data = zk.getData(parentNode + \"/\" + child, false, null); servers.add(new String(data)); } // 4 打印服务器列表信息 System.out.println(servers); } // 业务功能 public void business() throws Exception{ System.out.println(\"client is working ...\"); Thread.sleep(Long.MAX_VALUE); } public static void main(String[] args) throws Exception { // 1 获取 zk 连接 DistributeClient client = new DistributeClient(); client.getConnect(); // 2 获取 servers 的子节点信息，从中获取服务器信息列表 client.getServerList(); //3 业务进程启动 client.business(); } }","date":"2021-10-07","objectID":"/posts/zookeeper/zookeeper-03/:1:2","tags":["Zookeeper","中间件"],"title":"zookeeper 服务器动态上下线监听案例 （三）","uri":"/posts/zookeeper/zookeeper-03/"},{"categories":["ElasticStack"],"content":"ELK 综合实践 ","date":"2021-10-06","objectID":"/posts/elk/elk-experiment/:0:0","tags":["日志收集","运维实战"],"title":"ELK 综合实践-收集Nignx的日志数据 （五）","uri":"/posts/elk/elk-experiment/"},{"categories":["ElasticStack"],"content":"实践案例 项目实现效果图 ","date":"2021-10-06","objectID":"/posts/elk/elk-experiment/:1:0","tags":["日志收集","运维实战"],"title":"ELK 综合实践-收集Nignx的日志数据 （五）","uri":"/posts/elk/elk-experiment/"},{"categories":["ElasticStack"],"content":"需求简介 在我们的项目中，日志信息会输出到定制的目录里面了，那么接下来，我们就以nignx的日志数据为对象，使用filebeat来获取这些日志，将其输入到logstash中，logstash接收到数据后，定制显示格式，将其输入到elasticsearch中，kibana从elasticsearch中获取数据，并展示到当前界面。 ","date":"2021-10-06","objectID":"/posts/elk/elk-experiment/:1:1","tags":["日志收集","运维实战"],"title":"ELK 综合实践-收集Nignx的日志数据 （五）","uri":"/posts/elk/elk-experiment/"},{"categories":["ElasticStack"],"content":"流程分析 确定nginx的日志文件 filebeat 读取本机的nginx日志，并传输到 logstash logstash 接收到数据后，定制输出格式，将数据转交给 elasticsearch kibana 根据定制的索引名称，从 elasticsearch中获取数据。 关键点分析 准备工作： nginx 日志文件径在/var/log/nginx/access.log，我们来获取.log格式文件数据 filebeat数据收集 基于默认的 input 方式确定数据文件，output 方式选择 logstash 注意： input 内部的 nabled 必须开启 logstash传输 基于 input 属性获取 filebeat 的内容，基于output属性将数据发送给es kibana展示 基于索引名称到 elasticsearch 获取数据，然后在discover中确认数据 实践步骤 环境还原 清空filebeat程序 关闭kibana程序 清空elasticsearch索引 定制filebeat 编写配置文件 启动filebeat 定制logstash 定制logstash文件 启动logstash 定制kibana 配置查询索引 验证效果 ","date":"2021-10-06","objectID":"/posts/elk/elk-experiment/:1:2","tags":["日志收集","运维实战"],"title":"ELK 综合实践-收集Nignx的日志数据 （五）","uri":"/posts/elk/elk-experiment/"},{"categories":["ElasticStack"],"content":"项目实践 环境还原 清除所有的index for index in $(curl -s http://192.168.8.12:9200/_cat/indices | awk '{print $3}') do curl -XDELETE 192.168.8.12:9200/$index done filebeat主机安装nginx apt install -y nginx 关闭所有服务 systemctl stop logstash systemctl stop filebeat systemctl stop kibana 编写 filebeat 配置文件 定制配置文件 # cd /etc/filebeat/ # cat filebeat.yaml filebeat.inputs: - type: log paths: - /var/log/nginx/*.log output.logstash: hosts: [\"192.168.8.13:5044\"] 启动filebeat systemctl start filebeat 编写 logstash 配置文件 # cd /etc/logstash/conf.d # vim logstash.conf input { beats { port =\u003e 5044 } } output{ elasticsearch { hosts =\u003e [\"192.168.8.12:9200\"] index =\u003e \"nginx-%{+YYYY.MM.dd}\" } } 重启logstash systemctl start logstash 检查效果 curl 192.168.8.12:9200/_cat/indices 查看日志 tail -f /var/log/logstash/logstash-plain.log 启动 kibana systemctl start kibana netstat -tnulp 浏览器登录到 192.168.8.14:5601，点击左上角的logo图标，进入到home页面 通过以下方式进入： 选择 左边栏的 Stack Management 点击 kibana 栏的 索引模式 点击 创建索引模式 在索引模式中，输入正则表达式，看是否能够匹配现有的日志，匹配到的话，点击下一步 时间字段选择 默认的 @timestamp 字段，然后点击右下角的 创建索引模式 我们收集到的数据中，包含58个字段，当我们点击某些属性的时候，还会显示简单的排序，到此为止，我们的kibana从elasticsearch中获取数据就配置完毕了 点击左边栏的第一个\"Discover\"按钮,点击\"Add ﬁlter\"的下拉框，选择nginx-*索引名，在\"Refresh\"右侧选择日志的时间范围，就可以实时的查看到说有数据的获取效果 界面解析 Filters 部分的规则，其实就是日志中的键名是否包含某些关键信息，等同于 KQL的示例 message is 200。 点开每条记录旁边的\"\u003e“表示查看该条日志的具体信息 ","date":"2021-10-06","objectID":"/posts/elk/elk-experiment/:1:3","tags":["日志收集","运维实战"],"title":"ELK 综合实践-收集Nignx的日志数据 （五）","uri":"/posts/elk/elk-experiment/"},{"categories":["Zookeeper"],"content":"集群部署 Zookeeper为了更好的实现生产的业务场景，一般都会采用分布式的集群架构。集群通常由2n+1台Server节点组成，每个Server都知道彼此的存在。每个server都维护的内存状态镜像以及持久化存储的事务日志和快照。 对于2n+1台server，只要有\u003e=(n+1)台server节点可用，整个Zookeeper系统保持可用。 为了维护集群内部所有主机信息的一致性，他们自己参考Paxos协议自己设计了一个更加轻量级的协议:Zab(Zookeeper Atomic Broadcast)来解决集群数据一致性的问题。 ","date":"2021-10-06","objectID":"/posts/zookeeper/zookeeper-02/:0:0","tags":["Zookeeper","中间件"],"title":"zookeeper 集群部署 （二）","uri":"/posts/zookeeper/zookeeper-02/"},{"categories":["Zookeeper"],"content":"集群流程 Leader恢复：当集群Leader主机服务重启或者崩溃后，当Zookeeper集群中所有Server主机基于 Zab协议选举新的Leader者，接着就进入日常操作阶段。然后其他Server主机和新的Leader主机进行数据 信息同步，当状态同步完成以后， 日常操作阶段： 日常操作阶段主要有两种场景：主机间心跳监测和数据操作。 主机间心跳监测： 当Leader选举完毕后，就进入日常操作阶段，第一步就是所有集群节点都互相保持通信， 然后Leader和Follower节点间进行数据同步，确保所有主机节点都是相同的状态， 当所有Follower节点和新的Leader主机完成数据信息同步以后，就开始进行日常的数据操作。 ","date":"2021-10-06","objectID":"/posts/zookeeper/zookeeper-02/:1:0","tags":["Zookeeper","中间件"],"title":"zookeeper 集群部署 （二）","uri":"/posts/zookeeper/zookeeper-02/"},{"categories":["Zookeeper"],"content":"通信机制 对于Zookeeper集群来说，我们要考虑的内容主要有三大块：客户端连接、主机通信、选举Leader。 客户端连接： 客户端连接服务端功能，进行相关请求操作 主机通信： 集群各服务节点进行信息交流的功能 选举Leader： 集群中各服务节点共同选举主节点的功能 格式： server.\u003cmyid\u003e=\u003cserver_ip\u003e:\u003cLF_Port\u003e:\u003cL_Port\u003e 客户端操作： 2181 主机通信： 2182 选举Leader： 2183 注意： 这三端口都是自定义的，在生产中，因为每台主机都有独立的ip，所以三个端口一般都设置一样。 ","date":"2021-10-06","objectID":"/posts/zookeeper/zookeeper-02/:2:0","tags":["Zookeeper","中间件"],"title":"zookeeper 集群部署 （二）","uri":"/posts/zookeeper/zookeeper-02/"},{"categories":["Zookeeper"],"content":"集群部署 Zookeeper集群使用多个独立的主机，每个主机上都部署同样环境的Zookeeper环境，基于内部的Zab协议达到数据的一致性，然后统一对外提供服务。客户端连接任意一节点，效果都一样。 节点 主机IP 通信端口 心跳端口 选举端口 软件存放目录 myid zk1 192.168.10.126 2181 2182 2183 /data/server/zk1/{data,logs} 1 zk2 192.168.10.127 2181 2182 2183 /data/server/zk2/{data,logs} 2 zk3 192.168.10.128 2181 2182 2183 /data/server/zk3/{data,logs} 3 ","date":"2021-10-06","objectID":"/posts/zookeeper/zookeeper-02/:3:0","tags":["Zookeeper","中间件"],"title":"zookeeper 集群部署 （二）","uri":"/posts/zookeeper/zookeeper-02/"},{"categories":["Zookeeper"],"content":"软件安装 分别在三个不同的Zookeeper目录中执行软件安装 安装三个节点 tar xf /data/softs/apache-zookeeper-3.7.0.tar.gz -C /data/server/ mv /data/server/apache-zookeeper-3.7.0 /data/server/zk1 mkdir /data/softs/zk/{data,log} -p注意： 三个节点执行同样的操作，唯一的区别是数字不一致，分别是1-2-3 ","date":"2021-10-06","objectID":"/posts/zookeeper/zookeeper-02/:3:1","tags":["Zookeeper","中间件"],"title":"zookeeper 集群部署 （二）","uri":"/posts/zookeeper/zookeeper-02/"},{"categories":["Zookeeper"],"content":"配置管理 准备配置文件 cd /data/server/ mv zk1/conf/zoo_sample.cfg zk1/conf/zoo.cfg 修改配置文件 # grep -ni '^[a-Z]' zk1/conf/zoo.cfg 2:tickTime=2000 5:initLimit=10 8:syncLimit=5 12:dataDir=/data/server/zk1/data 13:dataLogDir=/data/server/zk1/log 15:clientPort=2181 30:server.1=192.168.10.126:2182:2183 31:server.2=192.168.10.127:2282:2283 32:server.3=192.168.10.128:2382:2383 设置myid文件 echo 1 \u003e zk1/data/myid 注意： 三个节点执行同样的操作，唯一的区别是绿色背景的字体不一致，分别是1-2-3 ","date":"2021-10-06","objectID":"/posts/zookeeper/zookeeper-02/:3:2","tags":["Zookeeper","中间件"],"title":"zookeeper 集群部署 （二）","uri":"/posts/zookeeper/zookeeper-02/"},{"categories":["Zookeeper"],"content":"启动服务 以上三个节点内容都配置完毕后，我们就可以启动集群了。 与单机模式的启动方法一致，只需一次启动所有Zookeeper节点即可启动整个集群。我们还可以一个一\r个的手工启动，当然了我们还可以使用脚本方式一次启动所有Zookeeper主机服务。\r启动服务\r/data/server/zk1/bin/zkServer.sh start\r/data/server/zk2/bin/zkServer.sh start\r/data/server/zk3/bin/zkServer.sh start服务启动停止脚本 #!/bin/bash case $1 in \"start\"){ for i in zk1 zk2 zk3\rdo echo ---------- zookeeper $i 启动 ------------\rssh $i \"/opt/module/zookeeper-3.5.7/bin/zkServer.sh start\" done };; \"stop\"){ for i in zk1 zk2 zk3\rdo echo ---------- zookeeper $i 停止 ------------\rssh $i \"/opt/module/zookeeper-3.5.7/bin/zkServer.sh stop\" done };; \"status\"){ for i in zk1 zk2 zk3\rdo echo ---------- zookeeper $i 状态 ------------\rssh $i \"/opt/module/zookeeper-3.5.7/bin/zkServer.sh status\" done };; esac #增加脚本执行权限\r$ chmod u+x zk.sh\r#执行 Zookeeper 集群启动脚本\r$ zk.sh start\rZookeeper 集群停止脚本\r$ zk.sh stop ","date":"2021-10-06","objectID":"/posts/zookeeper/zookeeper-02/:3:3","tags":["Zookeeper","中间件"],"title":"zookeeper 集群部署 （二）","uri":"/posts/zookeeper/zookeeper-02/"},{"categories":["Zookeeper"],"content":"服务检查 检查集群服务一般有两类方法：检查端口和检查集群服务状态 检查端口\rnetstat -tnulp | grep 218\r查看集群服务状态\r[root@controller ~]# /data/server/zk1/bin/zkServer.sh status\rZooKeeper JMX enabled by default\rUsing config: /data/server/zk1/bin/../conf/zoo.cfg\rMode: follower\r[root@controller ~]# /data/server/zk2/bin/zkServer.sh status\rZooKeeper JMX enabled by default\rUsing config: /data/server/zk2/bin/../conf/zoo.cfg\rMode: leader\r[root@controller ~]# /data/server/zk3/bin/zkServer.sh status\rZooKeeper JMX enabled by default\rUsing config: /data/server/zk3/bin/../conf/zoo.cfg\rMode: follower\r结果显示：\r查看集群状态，关键就是看Mode:的值，我们可以看到，目前Zookeeper三节点集群中，处于leader的\r是zk2节点，其他两个节点是follower角色。\r同时连接多个server的方法\rbin/zkCli -server \u003czk1_ip\u003e:\u003czk1_port\u003e,\u003czk2_ip\u003e:\u003czk2_port\u003e,\u003czk3_ip\u003e:\u003czk3_port\u003e\r注意：\r同时连接多个server节点的时候，彼此间使用逗号隔开\r使用任意一个zkCli.sh连接三个Zookeeper节点\rcd /data/server/zk2/bin/\r./zkCli.sh -server 192.168.10.126:2181,192.168.10.127:2281,192.168.10.128:2381 ","date":"2021-10-06","objectID":"/posts/zookeeper/zookeeper-02/:3:4","tags":["Zookeeper","中间件"],"title":"zookeeper 集群部署 （二）","uri":"/posts/zookeeper/zookeeper-02/"},{"categories":["Zookeeper"],"content":"专用检测 因为使用telnet方法来检查集群的节点状态信息比较繁琐，而且经常中断，所以生产中我们一般使用nc软件 来检查Zookeeper集群状态. nc全称NetCat，在网络工具中有“瑞士军刀”美誉，支持Windows和Linux。因为它短小精悍(不过25k)、功 能实用，被设计为一个简单、可靠的网络工具，可通过TCP或UDP协议传输读写数据。同时，它还是一个网络应用Debug分析器，因为它可以根据需要创建各种不同类型的网络连接。 安装软件\rapt-get -y install netcat-traditional\r使用方式\recho \"命令\" | nc \u003cserver_ip\u003e \u003cserver_port\u003e\r检查集群状态\recho stat | nc 192.168.10.126 2181\recho stat | nc 192.168.10.127 2281\recho stat | nc 192.168.10.128 2381 ","date":"2021-10-06","objectID":"/posts/zookeeper/zookeeper-02/:3:5","tags":["Zookeeper","中间件"],"title":"zookeeper 集群部署 （二）","uri":"/posts/zookeeper/zookeeper-02/"},{"categories":["Zookeeper"],"content":"集群操作命令 ","date":"2021-10-06","objectID":"/posts/zookeeper/zookeeper-02/:4:0","tags":["Zookeeper","中间件"],"title":"zookeeper 集群部署 （二）","uri":"/posts/zookeeper/zookeeper-02/"},{"categories":["Zookeeper"],"content":"常见命令 命令 内容 conf 输出相关服务配置的详细信息 cons 列出所有连接到服务器的客户端的完全的连接/会话的详细信息 envi 输出关于服务环境的详细信息 dump 列出未经处理的会话和临时节点 stat 查看哪个节点被选择作为 Follower 或者 Leader ruok 测试是否启动了该 Server，若回复 imok 表示已经启动 mntr 输出一些运行时信息 reqs 列出未经处理的请求 wchs 列出服务器 watch 的简要信息 wchc 通过 session 列出服务器 watch 的详细信息 wchp 通过路径列出服务器 watch 的详细信息 srvr 输出服务的所有信息 srst 重置服务器统计信息 kill 关掉 Server isro 查看该服务的节点权限信息 ZooKeeper 支持某些特定的四字命令字母与其的交互。它们大多是查询命令，用来获取 ZooKeeper 服务的当前状态及相关信息。用户在客户端可以通过 telnet 或 nc 向 ZooKeeper 提交相应的命令。 默认情况下，这些4字命令有可能会被拒绝，发送如下报错 xxx is not executed because it is not in the whitelist. 解决办法：向 zoo.cfg 文件中添加如下配置 4lw.commands.whitelist=* ","date":"2021-10-06","objectID":"/posts/zookeeper/zookeeper-02/:4:1","tags":["Zookeeper","中间件"],"title":"zookeeper 集群部署 （二）","uri":"/posts/zookeeper/zookeeper-02/"},{"categories":["Zookeeper"],"content":"命令实践 查看节点服务状态\recho stat | nc 127.0.0.1 2281\r查看节点服务配置\recho conf | nc 127.0.0.1 2281\r查看节点服务环境\recho envi | nc 127.0.0.1 2281\r查看节点服务会话\recho cons | nc 127.0.0.1 2281\recho dump | nc 127.0.0.1 2281基本安全 在这么多的服务状态查看命令中有很多存在隐患的命令，所以为了避免生产中因为这些命令的安全隐患，所以\r我们要对这些命令进行一些安全限制，只需要编辑服务的zoo.cfg文件即可\r# vim /data/server/zk1/conf/zoo.cfg\r4lw.commands.whitelist=stat, ruok, conf, isro\r重启服务后\r/data/server/zk1/bin/zkServer.sh restart\r查看允许通过的命令效果\recho isro | nc 127.0.0.1 2181\recho conf | nc 127.0.0.1 2181\recho stat | nc 127.0.0.1 2181\r检查不允许通过的命令\r[root@controller bin]# echo dump | nc 127.0.0.1 2181\rdump is not executed because it is not in the whitelist.\r测试没有设置命令过滤的节点\r[root@controller bin]# echo dump | nc 127.0.0.1 2281\rSessionTracker dump:\rSession Sets (0):\rephemeral nodes dump:\rSessions with Ephemerals (0):\r所以生产中，我们一定要把不知道或者不想用的命令全部过滤掉，这样才能保证基本的安全。 ","date":"2021-10-06","objectID":"/posts/zookeeper/zookeeper-02/:4:2","tags":["Zookeeper","中间件"],"title":"zookeeper 集群部署 （二）","uri":"/posts/zookeeper/zookeeper-02/"},{"categories":["Zookeeper"],"content":"集群管理 状态监控 在Zookeeper服务端的操作中，有一个命令非常有用就是mntr，可以查看节点服务的所有运行时信息，这些 信息就是我们平常要监控到的内容。 命令效果 # echo mntr | nc 127.0.0.1 2281 zk_version 3.7.0-39d3a4f269333c922ed3db283be479f9deacaa0f, built on 03/23/2021 10:13 GMT zk_avg_latency 0 zk_max_latency 0 zk_min_latency 0 zk_packets_received 8 zk_packets_sent 7 zk_num_alive_connections 1 zk_outstanding_requests 0 zk_server_state leader zk_znode_count 4 zk_watch_count 0 zk_ephemerals_count 0 zk_approximate_data_size 27 zk_open_file_descriptor_count 36 zk_max_file_descriptor_count 4096 zk_followers 2 zk_synced_followers 2 zk_pending_syncs 0指标分类 网络响应延迟信息 zk_avg_latency、zk_max_latency、zk_min_latency 网络请求(数据包和连接状态数量) 数据包相关：zk_packets_received、zk_packets_sent 连接状态相关：zk_num_alive_connections(活跃连接)、zk_outstanding_requests 节点数量信息： zk_znode_count、zk_watch_count、zk_ephemerals_count(临时节点数) 服务状态 zk_server_state、zk_open_file_descriptor_count、zk_max_file_descriptor_count Leader特有： zk_followers、zk_synced_followers(同步数量)、zk_pending_syncs(阻塞数量)集群优化 文件隔离 生产中Zookeeper的dataDir 和 dataLogDir 应该分开部署，因为事务日志非常重要而且内容比较多，\r所以在配置的时候，dataLogDir所在的目录，要保证目录空间足够大，并挂载到单独的磁盘上，如果可以的\r话，磁盘应该开启实时刷新功能。日志滚动 默认情况下，一般日志是放在一个文件中，为了更好的查看日志效果，我们一般会将日志进行切割，接下来我\r们配置一下日志的切割功能。\rZookeeper的默认日志切割配置文件是 项目目录的conf/log4j.properties,和切割配置主要相关的是：\rlog4j.appender.ROLLINGFILE=org.apache.log4j.RollingFileAppender\r如果想按天进行日志切割的话，可以修改为 DaliyRollingFileAppender\rZookeeper使用日志切割功能\r# vim /data/server/zk1/bin/zkServer.sh\r...\r30 # 增加 ZOO_LOG_DIR 配置\r31 ZOO_LOG_DIR=\"$ZOOBINDIR/../log4j\"\r...\r# vim /data/server/zk1/bin/zkEnv.sh\r59 if [ \"x${ZOO_LOG4J_PROP}\" = \"x\" ]\r60 then\r61 ZOO_LOG4J_PROP=\"INFO,ROLLINGFILE\" # 注意：原CONSOLE 修改为\rROLLINGFILE\r62 fi日志清理 自动清理：自从Zookeeper 3.4.0版本之后，配置文件中多了两个和日志自动清理相关的配置\rautopurge.purgeInterval：指定清理频率，单位为小时(默认是0，表示不开启自动清理)\rautopurge.snapRetainCount：和purgeInterval配合使用，指定需要保留的文件数目\r注意：\rZookeeper 重启会自动清除 zookeeper-root-server-python-auto.out 日志，如果有排错需要，则应先备份好日志文件\r配置效果：\r# vim /data/server/zk1/conf/zoo.cfg\r...\rautopurge.purgeInterval=1\rautopurge.snapRetainCount=3\r手工清理：\r如果发现单事务日志量过大，导致定时清理无法及时处理，我们可以基于自定义脚本或者 zookeeper提供的 zkCleanup.sh 进行 结合 定时任务来实现自动清理的任务。\r#!/bin/bash\r# 定制日志目录\rzookeeperDir='/data/server/zookeeper'\rdataDir=\"$zookeeperDir/data/version-2\"\rdataLogDir=$zookeeperDir/logs/version-2\r# 保留文件60\rcount=60\rcount=$[$count+1] # 从61行开始删除\rls -t $dataLogDir/log.* | tail -n +$count | xargs rm -f\rls -t $dataDir/snapshot.* | tail -n +$count | xargs rm -f\r注意：\rls -t 是顺序排列，\rtail -n +5 是从第 5 个至最新文件节点扩展 在Zookeeper集群中有一个角色是observer，它主要的作用仅仅是增加额外的接收客户端请求的扩展 节点，将接收到的请求，转交给Leader处理，不会影响集群的其他任何操作。 我们只需要在Observe节点的zoo.cfg配置文件中添加如下配置即可 peerType=observer server.n:localhost:2181:3181:observer 修改配置文件 Zk1节点： vim /data/server/zk1/conf/zoo.cfg # 修改如下配置 server.3=192.168.8.14:2382:2383:observer Zk2节点： vim /data/server/zk2/conf/zoo.cfg # 修改如下配置 server.3=192.168.8.14:2382:2383:observer Zk3节点： vim /data/server/zk3/conf/zoo.cfg # 增加如下配置 peerType=observer # 修改如下配置 server.3=192.168.8.14:2382:2383:observer 重启相关服务 /data/server/zk1/bin/zkServer.sh restart /data/server/zk2/bin/zkServer.sh restart /data/server/zk3/bin/zkServer.sh restart 再次查看集群状态 /data/server/zk3/bin/zkServer.sh status /data/server/zk2/bin/zkServer.sh status /data/server/zk1/bin/zkServer.sh status 可以看到： zk3的集群角色就变成了观察者 验证observer是否参与选举 /data/server/zk2/bin/zkServer.sh stop 查看集群状态 /data/server/zk1/bin/zkServer.sh status 可以看到： 集群节点有三个，zk3是观察者，真正提供服务的是两个，我们关闭了一个，集群服务就崩溃了，所以 observer没 有参与集群的选举工作。","date":"2021-10-06","objectID":"/posts/zookeeper/zookeeper-02/:5:0","tags":["Zookeeper","中间件"],"title":"zookeeper 集群部署 （二）","uri":"/posts/zookeeper/zookeeper-02/"},{"categories":["ElasticStack"],"content":"一张图片胜过千万行日志，Kibana 让您能够自由地选择如何呈现自己的数据。Kibana 是一个免费且开放的用户界面，能够让您对 Elasticsearch 数据进行可视化，并让您在 Elastic Stack 中进行导航。您可以进行各种操作，从跟踪查询负载，到理解请求如何流经您的整个应用，都能轻松完成。 ","date":"2021-10-05","objectID":"/posts/elk/elk-kibana/:0:0","tags":["日志收集"],"title":"ELK组件-Kibana （四）","uri":"/posts/elk/elk-kibana/"},{"categories":["ElasticStack"],"content":"基础知识 ","date":"2021-10-05","objectID":"/posts/elk/elk-kibana/:1:0","tags":["日志收集"],"title":"ELK组件-Kibana （四）","uri":"/posts/elk/elk-kibana/"},{"categories":["ElasticStack"],"content":"功能简介 Kibana 是一个开源的分析和可视化平台，设计用于和Elasticsearch一起工作。Kibana来搜索，查看，并和存储在Elasticsearch索引中的数据进行交互。可以轻松地执行高级数据分析，并且以各种图标、表格和地图的形式可视化数据。Kibana使得理解大量数据变得很容易。它简单的、基于浏览器的界面使你能够快速创建和共享动态仪表板，实时显示Elasticsearch查询的变化。 运行环境 安装java8环境\rapt install openjdk-8-jdk\r检查效果\rjava -version ","date":"2021-10-05","objectID":"/posts/elk/elk-kibana/:1:1","tags":["日志收集"],"title":"ELK组件-Kibana （四）","uri":"/posts/elk/elk-kibana/"},{"categories":["ElasticStack"],"content":"软件安装 apt源码方式 获取软件源 wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add - apt install apt-transport-https echo \"deb https://artifacts.elastic.co/packages/7.x/apt stable main\" | sudo tee –a /etc/apt/sources.list.d/elastic-7.x.list apt update 安装软件 apt install kibana 软件包安装 wget https://artifacts.elastic.co/downloads/kibana/kibana-7.14.0-amd64.deb wget https://artifacts.elastic.co/downloads/kibana/kibana-7.14.0- amd64.deb.sha512 shasum -a 512 -c kibana-7.14.0-amd64.deb.sha512 dpkg -i kibana-7.14.0-amd64.deb 配置查看 # dpkg -L kibana /. /etc /etc/default /etc/default/kibana /etc/init.d /etc/init.d/kibana /etc/kibana kibana家目录 /etc/kibana/kibana.yml /etc/kibana/node.options /etc/systemd /etc/systemd/system /etc/systemd/system/kibana.service 服务启动文件 /usr /usr/share /usr/share/kibana ... /usr/share/kibana/bin 执行命令目录文件 /usr/share/kibana/bin/kibana-encryption-keys /usr/share/kibana/bin/kibana-plugin /usr/share/kibana/bin/kibana /usr/share/kibana/bin/kibana-keystore 定制环境变量 echo 'export PATH=/usr/share/kibana/bin:$PATH' \u003e /etc/profile.d/kibana.sh source /etc/profile.d/kibana.sh ","date":"2021-10-05","objectID":"/posts/elk/elk-kibana/:1:2","tags":["日志收集"],"title":"ELK组件-Kibana （四）","uri":"/posts/elk/elk-kibana/"},{"categories":["ElasticStack"],"content":"简单实践 命令格式 修改配置文件 # vim /etc/kibana/kibana.yml # 设定kibana对外开放的通信端口 server.port: 5601 # 设定可以访问kibana的主机地址 server.host: \"0.0.0.0\" # 设定elasticsearch的主机地址 elasticsearch.hosts: [\"http://192.168.8.12:9200\"] # 设定kibana的数据索引 kibana.index: \".kibana\" # 设定中文显示格式 i18n.locale: \"zh-CN\" 启动服务 启动服务 systemctl start kibana.service systemctl status kibana.service 查看端口 # netstat -tnulp | egrep 'Add|node' Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:5601 0.0.0.0:* LISTEN 31992/node 结果显示： kibana默认端口是 5601 浏览器查看效果 http://192.168.10.108:5601 kibana默认帮我们提供了非常多的示例数据 小结： 定位 数据的可视化平台 部署 安装软件 配置文件 启动查看效果 注意： 默认的地图虽然支持中文，但是国家地图有问题， 核心点： 1 数据采集 2 数据可视化 ","date":"2021-10-05","objectID":"/posts/elk/elk-kibana/:1:3","tags":["日志收集"],"title":"ELK组件-Kibana （四）","uri":"/posts/elk/elk-kibana/"},{"categories":["Zookeeper"],"content":"ZooKeeper 基础与安装 ","date":"2021-10-05","objectID":"/posts/zookeeper/zookeeper-01/:0:0","tags":["Zookeeper","中间件"],"title":"zookeeper 基础与安装 （一）","uri":"/posts/zookeeper/zookeeper-01/"},{"categories":["Zookeeper"],"content":"基础知识 ","date":"2021-10-05","objectID":"/posts/zookeeper/zookeeper-01/:1:0","tags":["Zookeeper","中间件"],"title":"zookeeper 基础与安装 （一）","uri":"/posts/zookeeper/zookeeper-01/"},{"categories":["Zookeeper"],"content":"开发框架 ORM - 一台主机承载所有的业务应用 MVC - 多台主机分别承载业务应用的不同功能，通过简单的网络通信实现业务的正常访问 RPC - 应用业务拆分、多应用共用功能、核心业务功能 独立部署，基于远程过程调用技术(RPC)的分布式服 务框架 提高业务功能复用及项目的整合 SOA - 粗放型的RPC分布式实现了大量的资源浪费，提高机器利用率的 资源调度和治理中心(SOA) ，基于 现有资源的高效利用，进一步提高服务的能力 微服务 - 随着互联网的发展、各种技术的平台工具出现、编程语言的升级、开发规范的标准化等因素，中小 型企业也有了相应的能力来发展更轻量级的SOA模式。 在微服务架构的场景中，有一个组件服务Service Registry,它是整个\"微服务架构\"中的核心，主要提供了 四个功能：服务注册和服务发现、下线处理、健康检测等。 服务注册：当服务启动后，将当前服务的相关配置信息都注册到一个公共的组件 – Service Registry中。 服务发现：当客户端调用操作某些已注册服务 或者 服务的新增或删除等，通过从Service Registry中读取这些 服务配置的过程。 目前，Service Registry的最佳解决方案就是Zookeeper。这就是我们要学习Zookeeper的目的之一。 ","date":"2021-10-05","objectID":"/posts/zookeeper/zookeeper-01/:1:1","tags":["Zookeeper","中间件"],"title":"zookeeper 基础与安装 （一）","uri":"/posts/zookeeper/zookeeper-01/"},{"categories":["Zookeeper"],"content":"分布式特性 目前来说，随着互联网的发展，各种软件技术，尤其是设备计算能力的提升，所以很多企业在项目的开启就应 用了 分布式架构。在分布式系统中各个节点之间的协作是通过高效网络进行消息数据传递，实现业务内部多 个服务的通信和协调，基于服务本地设备的性能实现资源的高效利用。 分布式系统的设计目标通常包括几个方面： 可用性：可用性是分布式系统的核心需求，衡量了一个分布式系统持续对外提供服务的能力。 可扩展性：增加及其后不会改变或者极少改变系统行为，并且获得相似的线性的性能提升 容错性：系统发生错误时，具有对错误进行规避以及从错误中恢复的能力 性能：对外服务的响应延时和吞吐率要满足用户的需求 ","date":"2021-10-05","objectID":"/posts/zookeeper/zookeeper-01/:1:2","tags":["Zookeeper","中间件"],"title":"zookeeper 基础与安装 （一）","uri":"/posts/zookeeper/zookeeper-01/"},{"categories":["Zookeeper"],"content":"一致性协议 我们为了满足分布式的各种场景需求，先后提出了 ACID、CAP、BASE等理论，其目的就是 在项目架构正常的运行过程中，即时出现各种问题，也能够保证业务保持基本可用的目标。 那么，我们在 项目架构在运行过程中 为了保证 业务保持基本可用 过程中定制的各种规约或者通信格 式，都可以将其称为 一致性协议。 一般情况下，我们会基于 集群的方式实现分布式的 可用性、可扩展性、容错性等的目标，那么我们如 何保证集群中的数据的一致性呢？ 一般情况下，我们会基于 集群的方式实现分布式的 可用性、可扩展性、容错性等的目标，这个时候， 集群中各个主机之间的通信信息是否一致的就非常重要了。所谓的一致性是集群内部各个主机系统对外呈现的状态是否一致，即时业务出现问题的时候，这是所有的节点也要达成一个错误的共识。如果各个主机之间通信的数据不一致，就会导致各种分布式的场景问题。 在一个集群系统中，为了保证所有的主机系统能够处于一种相对的平衡状态，我们一般会基于传递数据本 身和主机角色的方式来实现，所以我们可以从两个方面来进行分析： 数据本身：将所有的更新数据，同步到整个集群系统，保证数据的最终一致性。 主机角色：client向多个server主机系统发起访问(包括并行访问)请求时，如何获取相同的更新后数据。 分类 解析 状态复制机(StateMachineReplication) 一个服务端集群，有多个server主机组成，每个server主机的更新都在本地实现。每个服务端都有一个一致性模块来接收客户端请求，没接收一次用户请求，一致性模块的状态就发生改变，通过 状态机系统 对所有的一致性模块的状态进行管控，只要所有的模块状态是一样的，那么server主机本地执行后的最终数据值就是一样的，从而实现服务的容错效果。GFS、HDFS、Chubby、ZooKeeper和etcd等分布式系统都是基于复制状态机模型实现的。 拜占庭将军问题(Byzantine Failures) 一个服务端集群，有多个server主机组成,每个server主机接收到client请求后，根据自己本身的特性进行分析并给出执行的策略，多个server主机通过专用的通讯方式来进行协商，并达成最终的共识结果(少数服从多数)，然后按照最终的结果进行操作执行，从而实现服务的容错效果。 FLP定理(Fischer,Lynch ,Patterson) 三位科学家在1985年发表的分布式理论，最小化异步网络通信场景下，因为消息通信是延迟的，所以可能会出现 只有一个节点故障(没被其他节点发现)时，其他节点不能达成一致。这证明了在异步场景中永远无法避免的一种现象。比如：三台主机ABC异步方式通信，在正常协商之间，因为C主机突然网络故障，导致无法实现剩余两台的少数服从多数，从而导致业务终止执行。 ","date":"2021-10-05","objectID":"/posts/zookeeper/zookeeper-01/:1:3","tags":["Zookeeper","中间件"],"title":"zookeeper 基础与安装 （一）","uri":"/posts/zookeeper/zookeeper-01/"},{"categories":["Zookeeper"],"content":"ZooKeeper 简介 Zookeeper，英文字面意思就是\"动物管理员\"，因为动物园里面的所有动物的特殊性，需要管理员必须具备 观察动物状态和管理动物行为等方面的协调的能力，为动物们建立友好生存的生活环境。Zookeeper就是纷乱 的软件服务世界中的一名管理者，为繁杂的软件服务环境提供统一的协调管理服务。 可以想象为 Pig hive hadoop HAMA 等框架的logo都是动物的象形,zookeeper 相当于铲屎官，帮他们解决大小便 Zookeeper是Yahoo基于 Google的 Chubby 论文实现的一款解决分布式数据一致性问题的开源实现，它 是使用Java语言开发的，目前是Hadoop项目中的一个子项目。它在Hadoop、HBase、Kafka、Dubbo等技 术中充当了非常重要的核心组件角色。 官方网站：https://zookeeper.apache.org/ 最新版本：3.7.0 ","date":"2021-10-05","objectID":"/posts/zookeeper/zookeeper-01/:2:0","tags":["Zookeeper","中间件"],"title":"zookeeper 基础与安装 （一）","uri":"/posts/zookeeper/zookeeper-01/"},{"categories":["Zookeeper"],"content":"环境安装 搭建java环境 zookeeper 是依赖于java环境的，所以我们需要提前定制java环境 创建目录 mkdir /data/{softs,server} -p cd /data/softs 下载java或者上传java ls /data/softs 安装java tar xf jdk-8u121-linux-x64.tar.gz -C /data/server cd /data/server/ ln -s jdk1.8.0_121 java 配置java环境变量 echo 'export JAVA_HOME=/data/server/java' \u003e\u003e /etc/profile echo 'export JRE_HOME=$JAVA_HOME/jre' \u003e\u003e /etc/profile echo 'export CLASSPATH=$JAVA_HOME/lib/tools.jar:$JAVA_HOME/lib/dt.jar' \u003e\u003e /etc/profile echo 'export PATH=$JAVA_HOME/bin:$JRE_HOME/bin:$PATH' \u003e\u003e /etc/profile source /etc/profile 检查效果 java -version 检查java目录效果 tree -L 1 /data/server/java/安装软件 软件准备\rcd /data/softs\rwget http://archive.apache.org/dist/zookeeper/zookeeper-3.7.0/apache-zookeeper-3.7.0-bin.tar.gz\rwget http://archive.apache.org/dist/zookeeper/zookeeper-3.7.0/apache-zookeeper-3.7.0-bin.tar.gz.asc\r校验软件\rgpg --verify apache-zookeeper-3.7.0-bin.tar.gz.asc\r对比 MD5 码一致后进行解压安装\rtar zxvf apache-zookeeper-3.7.0-bin.tar.gz -C /data/server\rcd /data/server\rln -s apache-zookeeper-3.7.0-bin zookeeper\recho 'export PATH=/data/server/zookeeper/bin:$PATH' \u003e /etc/profile.d/zk.sh\rsource /etc/profile.d/zk.sh修改配置文件 查看配置模板文件\rcat zookeeper/conf/zoo_sample.cfg\rgrep -ni '^[a-Z]' zookeeper/conf/zoo_sample.cfg\r设置配置文件\rcp conf/zoo_sample.cfg conf/zoo.cfg\r默认读取配置文件名 zoo.cfg配置文件常用参数 tickTime ：“滴答时间”，用于配置 Zookeeper中最小的时间单元长度，单位毫秒，是其他时间配置的基础 initLimit：初始化时间，包含启动和数据同步，其值是tickTime的倍数 syncLimit ：正常工作，心跳监测的时间间隔，其值是tickTime的倍数 dataDir ：配置Zookeeper服务存储数据的目录 clientPort：配置当前Zookeeper服务对外暴露的端口，用户客户端和服务端建立连接会话 启动服务 在Zookeeper的bin目录下有很多执行文件，其中zkServer.sh是启动服务的脚本文件 ls bin/ 查看帮助信息 bin/zkServer.sh 命令参数功能详解 start：用于后台启动Zookeeper服务器 start-foreground：用于前台启动Zookeeper服务器，常用来排查失败原因 stop：用于关闭Zookeeper服务器 restart：用于重启Zookeeper服务器 status：用于查看Zookeeper服务器状态 upgrade：用于升级Zookeeper服务器 print-cmd：用于打印Zookeeper程序命令行及其相关启动参数 启动服务 bin/zkServer.sh start检查服务状态 Zookeeper的检查有很多种方式，主要有以下四种：端口、服务、进程、连接\r端口检查\rnetstat -tnulp | grep 2181\r服务检查\rbin/zkServer.sh status\r进程检查\rps aux | grep zoo\r连接检查\rbin/zkCli.sh进阶实践 在生产中，我们一般会讲Zookeeper的数据目录和日志目录都放在一个专用的路径下，而我们刚才实践 的效果是数据目录在临时文件夹/tmp下，而且没有设置日志文件配置信息，那么接下来我们就按照生产环境的 部署方法先来做一个单机版的Zookeeper环境。 关闭刚才的服务 bin/zkServer.sh stop 创建专用的数据和日志目录 cd /data/server/zookeeper mkdir {data,logs} 在默认的配置文件中，没有日志的配置项，日志的配置项是dataLogDir # vim conf/zoo.cfg # grep -ni '^[a-Z]' conf/zoo.cfg 2:tickTime=2000 5:initLimit=10 8:syncLimit=5 12:dataDir=/data/server/zookeeper/data 13:dataLogDir=/data/server/zookeeper/logs 15:clientPort=2181 启动之前注意权限 ll chown 1000.1000 -R /data/server/zookeeper* 启动当前Zookeeper的服务 bin/zkServer.sh start 三种方式查看不同的关注点 bin/zkServer.sh status ps aux | grep zoo bin/zkCli.sh 查看产生的数据 ls /data/server/zookeeper/data/ ls /data/server/zookeeper/logs/本地连接服务 当Zookeeper服务器正常启动后，我们就可以使用Zookeeper自带的zkCli.sh脚本，以命令行的方式 连接到Zookeeper。使用方法非常简单： bin/zkCli.sh 如果出现下面信息，就表示命令行客户端已经成功连入到Zookeeper WATCHER:: WatchedEvent state:SyncConnected type:None path:null [zk: localhost:2181(CONNECTED) 0]远程连接服务 zkCli.sh 脚本还提供了远程连接非本地的Zookeeper服务器的参数 -server，使用这个参数就可以连接 到远程的Zookeeper服务主机 zkCli.sh 脚本还提供了远程连接非本地的Zookeeper服务器的参数 -server，使用这个参数就可以连接\r到远程的Zookeeper服务主机\r命令格式：\rbin/zkCli.sh -server \u003czk_ip\u003e:\u003czk_port\u003e\r远程连接\rbin/zkCli.sh -server 192.168.8.14:2181命令帮助 当客户端成功的连接到Zookeeper服务后，我们可以输入任意非法的命令都可以获取Zookeeper客户端 相关的命令使用方法。 连接到Zookeeper服务后，输入help查看相关命令\rZooKeeper -server host:port cmd args 宿主机命令行执行Zookeeper客户端命令\rstat path [watch] 查看节点状态或者判断结点是否存在\rset path data [version] 设置节点数据\rls path [watch] 列出节点信息\rdelquota [-n|-b] path 删除节点个数(-n)或数据长度(-b)配额\rls2 path [watch] ls命令的加强版，列出更多信息\rsetAcl path acl 设置节点的权限信息\rsetquota -n|-b val path 设置节点个数(-n)或数据长度(-b)的配额\rhistory 列出最近的命令历史，可以和redo配合使用\rredo cmdid 再次执行某个命令，结合history使用\rprintwatches on|off 设置和显示监视状态\rdelete path [version] 删除节点，不可删除有子节点的节点\rsync path 强制数据同步\rlistquota path 显示节点资源配额信息\rrmr path 强制删除节点\rget path [watch] 获取节点数据\rcreate [-s] [-e] path data acl 创建顺序(-s)或临时(-e)结点\raddauth scheme auth 配置节点认证信息\rquit 退出连接\rgetAcl path 获取节点的权限信息\rclose 断开当前Zookeeper连接\rconnect host:port 连接Zookeeper服务端\r使用close命令可以关闭当前的连接\r使用quit命令可以退出Zookeeper服务\r使用connect host:port命令可以重新连接Zookeeper服务 connect 192.16","date":"2021-10-05","objectID":"/posts/zookeeper/zookeeper-01/:2:1","tags":["Zookeeper","中间件"],"title":"zookeeper 基础与安装 （一）","uri":"/posts/zookeeper/zookeeper-01/"},{"categories":["Zookeeper"],"content":"Zookeeper 工作机制 Zookeeper 从设计模式的角度理解：是一个基于观察者模式设计的分布式服务管理框架（监工），它负责存储和管理大家都关心的数据，然后接受观察者的注册，一旦这些数据的状态发生变化，Zookeeper就将负责通知已经在Zookeeper上注册的哪些观察者做出相应的反应。 Zookeeper作为一个典型的分布式数据一致性解决方案，依赖Zookeeper的分布式应用程序，可以基于Zookeeper实现数据发布/订阅、负载均衡、命名服务、服务注册与发现、分布式协调/事件通知、集群管理、Leader 选举、 分布式锁和队列 等功能。 ","date":"2021-10-05","objectID":"/posts/zookeeper/zookeeper-01/:2:2","tags":["Zookeeper","中间件"],"title":"zookeeper 基础与安装 （一）","uri":"/posts/zookeeper/zookeeper-01/"},{"categories":["Zookeeper"],"content":"特点 Zookeeper：一个领导者（Leader），多个跟随者（Follower）组成的集群。 集群中只要有半数以上节点存活，Zookeeper集群就能正常服务。所 以Zookeeper适合安装奇数台服务器。 全局数据一致：每个Server保存一份相同的数据副本，Client无论连接到哪个Server，数据都是一致的。 更新请求顺序执行，来自同一个Client的更新请求按其发送顺序依次执行。 数据更新原子性，一次数据更新要么成功，要么失败。 实时性，在一定时间范围内，Client能读到最新数据。 ","date":"2021-10-05","objectID":"/posts/zookeeper/zookeeper-01/:2:3","tags":["Zookeeper","中间件"],"title":"zookeeper 基础与安装 （一）","uri":"/posts/zookeeper/zookeeper-01/"},{"categories":["Zookeeper"],"content":"角色 基本上所有的集群模式中的主机都有自己的角色，最为典型的集群模式就是 M/S 主备模式。在这种模式下， 我们把处于主要地位(处理写操作)的主机称为 Master 节点，处于次要地位(处理读操作)的主机称为 Slave 节点，生产中读取的方式一般是以异步复制方式来实现的。 Zookeeper集群就是这种M/S的模型，集群通常由2n+1台Server节点组成，每个Server都知道彼此的存 在。对于2n+1台server，只要有\u003e=(n+1)台server节点可用，整个Zookeeper系统保持可用。 角色 描述 领导者 (Leader) 领导者不接受client读请求，负责进行投票发起和决议，更新系统状态 跟随者 (Follower) 接收客户请求并向客户端返回结果，在选Leader过程中参与投票 观察者 (Observer) 转交客户端写请求给leader节点，和同步leader状态，不参与选主投票 学习者(Learner) 和leader进行状态同步的节点统称Learner，Follower和Observer 都是 客户端(client) 请求发起方 Zookeeper集群系统启动时，集群中的主机会选举出一台主机为Leader，其它的就作为Learner(包括 Follower和Observer)。接着由follower来服务client的请求，对于不改变系统一致性状态的读操作， 由follower的本地内存数据库直接给client返回结果；对于会改变Zookeeper系统状态的更新操作，则交 由Leader进行提议投票，超过半数通过后返回将结果给client。 ","date":"2021-10-05","objectID":"/posts/zookeeper/zookeeper-01/:2:4","tags":["Zookeeper","中间件"],"title":"zookeeper 基础与安装 （一）","uri":"/posts/zookeeper/zookeeper-01/"},{"categories":["Zookeeper"],"content":"ZAB协议 （Zookeeper Atomic Broadcast，Zookeeper原子广播协议）来保证主从节点数据一致性的，ZAB协议支持「崩溃恢复和消息广播」两种模式，很好解决了这两个问题： 崩溃恢复： Leader挂了，进入该模式，选一个新的leader出来,接着，新的Leader服务器与集群中Follower服务进行数据同步，当集群中超过半数机器与该 Leader服务器完成数据同步之后，退出恢复模式进入消息广播模式。 消息广播： 把更新的数据，从Leader同步到所有Follower Leader 服务器开始接收客户端的事务请求生成事务Proposal进行事务请求处理。 ","date":"2021-10-05","objectID":"/posts/zookeeper/zookeeper-01/:2:5","tags":["Zookeeper","中间件"],"title":"zookeeper 基础与安装 （一）","uri":"/posts/zookeeper/zookeeper-01/"},{"categories":["Zookeeper"],"content":"事务id 所谓的事务id – zxid。ZooKeeper的在选举时通过比较各结点的zxid和机器ID选出新的主结点的。 zxid由Leader节点生成，有新写入事件时，Leader生成新zxid并随提案一起广播，每个结点本地都保存了 当前最近一次事务的zxid，zxid是递增的，所以谁的zxid越大，就表示谁的数据是最新的。 ZXID有两部分组成： 任期：完成本次选举后，直到下次选举前，由同一Leader负责协调写入； 事务计数器：单调递增，每生效一次写入，计数器加一。 –同一任期内，ZXID是连续的，每个结点又都保存着自身最新生效的ZXID，通过对比新提案的ZXID与 自身最新ZXID是否相差“1”，来保证事务严格按照顺序生效的。 ","date":"2021-10-05","objectID":"/posts/zookeeper/zookeeper-01/:2:6","tags":["Zookeeper","中间件"],"title":"zookeeper 基础与安装 （一）","uri":"/posts/zookeeper/zookeeper-01/"},{"categories":["Zookeeper"],"content":"数据结构 ZooKeeper 数据模型的结构与 Unix 文件系统很类似，整体上可以看作是一棵树，每个 节点称做一个 ZNode。每一个 ZNode 默认能够存储 1MB 的数据，每个 ZNode 都可以通过 其路径唯一标识。 Zookeeper使用这个基于内存的树状模型来存储分布式数据，正因为所有数据都存放在内存中，所以 才能实现高性能的目的，提高数据的吞吐率。特别是在集群主机节点间的数据同步。 Znode包含了 存储数据(data)、访问权限(acl)、子节点引用(child)、节点状态(stat)信息等信息 注意： 为了保证高吞吐和低延迟，以及数据的一致性，znode只适合存储非常小的数据，不能超过1M，最好都 小于1K ","date":"2021-10-05","objectID":"/posts/zookeeper/zookeeper-01/:2:7","tags":["Zookeeper","中间件"],"title":"zookeeper 基础与安装 （一）","uri":"/posts/zookeeper/zookeeper-01/"},{"categories":["Zookeeper"],"content":"节点类型解析 虽然ZNode的样式跟Linux文件系统类似，根据节点的生命周期，在Zookeeper中的ZNode有四种独有的特 性,有时候页称为四种类型： 基本节点： Persistent(持久节点)：会话断开后，除非主动进行移除操作，否则该节点一直存在 Ephemeral(临时节点)：会话断开后，该节点被删除 序列节点： Persistent Sequential:按顺序编号的持久节点该节点被创建的时候，Zookeeper 会自动在其子节点名上，加一个由父节点维护的、自增整数的后缀。 Ephemeral Sequential：按顺序编号的临时节点该节点被创建的时候，Zookeeper 会自动在其子节点名上，加一个由父节点维护的、自增整数的后缀 注意： 只有持久性节点(持久节点和顺序持久节点)才有资格创建子节点 自增后缀格式： 10位10进制数的序号 有序和无序区别： 多个客户端同时创建同一无序ZNode节点时，只有一个可创建成功，其它匀失败。并且创建出的节点名称 与创建时指定的节点名完全一样。 多个客户端同时创建同一有序ZNode节点时，都能创建成功，只是序号不同。 ","date":"2021-10-05","objectID":"/posts/zookeeper/zookeeper-01/:2:8","tags":["Zookeeper","中间件"],"title":"zookeeper 基础与安装 （一）","uri":"/posts/zookeeper/zookeeper-01/"},{"categories":["Zookeeper"],"content":"Stat 数据结构 Zookeeper 的 ZNode 上都会存储数据，对应于每个 ZNode，Zookeeper 都会为其维护一个叫做 Stat 的数据结构。 Stat 中记录了这个 ZNode 的三个数据版本： dataversion 当前 ZNode 数据内容的版本 cversion 当前 ZNode 子节点的版本 aversion 当前 ZNode 的 ACL 变更版本。 这里的版本起到了控制 Zookeeper 操作原子性的作用，基于这些功能，才能更好实现了分布式锁的功能。 ","date":"2021-10-05","objectID":"/posts/zookeeper/zookeeper-01/:2:9","tags":["Zookeeper","中间件"],"title":"zookeeper 基础与安装 （一）","uri":"/posts/zookeeper/zookeeper-01/"},{"categories":["Zookeeper"],"content":"节点实践 节点创建 使用create命令可以来创建一个节点，命令格式如下：\rcreate [-s] [-e] [-c] [-t ttl] path [data] [acl]\r注意：\r-s 表示创建的节点是顺序节点。\r-e 表示创建的节点是临时节点，这个是create的默认参数。\racl 用于权限控制，Zookeeper的权限控制很强大，默认不使用。分别创建2个普通节点（永久节点 + 不带序号） [zk: localhost:2181(CONNECTED) 3] create /sanguo \"diaochan\"\rCreated /sanguo\r[zk: localhost:2181(CONNECTED) 4] create /sanguo/shuguo\r\"liubei\"\rCreated /sanguo/shuguo\r注意：创建节点时，要赋值获得节点的值 [zk: localhost:2181(CONNECTED) 5] get -s /sanguo diaochan cZxid = 0x100000003 ctime = Wed Aug 29 00:03:23 CST 2018 mZxid = 0x100000003 mtime = Wed Aug 29 00:03:23 CST 2018 pZxid = 0x100000004 cversion = 1 dataVersion = 0 aclVersion = 0 ephemeralOwner = 0x0 dataLength = 7 numChildren = 1 [zk: localhost:2181(CONNECTED) 6] get -s /sanguo/shuguo liubei cZxid = 0x100000004 ctime = Wed Aug 29 00:04:35 CST 2018 mZxid = 0x100000004 mtime = Wed Aug 29 00:04:35 CST 2018 pZxid = 0x100000004 cversion = 0 dataVersion = 0 aclVersion = 0 ephemeralOwner = 0x0 dataLength = 6 numChildren = 0 创建带序号的节点（永久节点 + 带序号） （1）先创建一个普通的根节点/sanguo/weiguo\r[zk: localhost:2181(CONNECTED) 1] create/sanguo/weiguo \"caocao\" Created /sanguo/weiguo （2）创建带序号的节点 [zk: localhost:2181(CONNECTED) 2] create -s /sanguo/weiguo/zhangliao \"zhangliao\" Created /sanguo/weiguo/zhangliao0000000000 [zk: localhost:2181(CONNECTED) 3] create -s /sanguo/weiguo/zhangliao \"zhangliao\" Created /sanguo/weiguo/zhangliao0000000001 [zk: localhost:2181(CONNECTED) 4] create -s /sanguo/weiguo/xuchu \"xuchu\" Created /sanguo/weiguo/xuchu0000000002 如果原来没有序号节点，序号从0 开始依次递增。如果原节点下已有2 个节点，则再排序时从2 开始，以此类推。 创建短暂节点（短暂节点 + 不带序号 or 带序号） （1）创建短暂的不带序号的节点 [zk: localhost:2181(CONNECTED) 7] create -e /sanguo/wuguo \"zhouyu\" Created /sanguo/wuguo （2）创建短暂的带序号的节点 [zk: localhost:2181(CONNECTED) 2] create -e -s /sanguo/wuguo \"zhouyu\" Created /sanguo/wuguo0000000001 （3）在当前客户端是能查看到的 [zk: localhost:2181(CONNECTED) 3] ls /sanguo [wuguo, wuguo0000000001, shuguo] 退出当前客户端然后再重启客户端 [zk: localhost:2181(CONNECTED) 12] quit [atguigu@hadoop104 zookeeper-3.5.7]$ bin/zkCli.sh （5）再次查看根目录下短暂节点已经删除 [zk: localhost:2181(CONNECTED) 0] ls /sanguo [shuguo] 修改节点数据值 [zk: localhost:2181(CONNECTED) 6] set /sanguo/weiguo \"simayi\"查看状态 查看三者的状态信息 [zk: 127.0.0.1:2181(CONNECTED) 19] stat /sswang ... ephemeralOwner = 0x0 ... [zk: 127.0.0.1:2181(CONNECTED) 20] stat /sswang2 ... ephemeralOwner = 0x16454e2c6580007 # 这是临时节点的特点 ... [zk: 127.0.0.1:2181(CONNECTED) 21] stat /sswang10000000001 ... ephemeralOwner = 0x0 ...节点删除与查看 1） 删除节点\r[zk: localhost:2181(CONNECTED) 4] delete /sanguo/jin\r2） 递归删除节点\r[zk: localhost:2181(CONNECTED) 15] deleteall /sanguo/shuguo\r3） 查看节点状态\r[zk: localhost:2181(CONNECTED) 17] stat /sanguo\rcZxid = 0x100000003\rctime = Wed Aug 29 00:03:23 CST 2018 mZxid = 0x100000011 资源配额 查看资源配额 使用listquotat命令可以获取节点数据，命令格式如下： listquota path 注意： 注意事项同ls 查看指定结点的资源 [zk: localhost:2181(CONNECTED) 28] create /sswang sswang Created /sswang [zk: localhost:2181(CONNECTED) 29] listquota /sswang absolute path is /zookeeper/quota/sswang/zookeeper_limits quota for /sswang does not exist. 可以看到： 默认新创建的结点资源，是没有资源配额的。设置资源配额 使用listquotat命令可以获取节点资源配额数据，命令格式如下：\rsetquota -n|-b|-N|-B val path\r注意：\r-n 设置节点的节点个数\r-b 设置节点的数据长度\r如果超出了配置限制，不会停止行为操作，只是ZooKeeper将会在log日志中打印WARN日志。\r其他注意事项同ls\r设置节点的数量资源\r[zk: localhost:2181(CONNECTED) 34] setquota -n 2 /sswang\rComment: the parts are option -n val 2 path /sswang\r[zk: localhost:2181(CONNECTED) 35] listquota /sswang\rabsolute path is /zookeeper/quota/sswang/zookeeper_limits\rOutput quota for /sswang count=2,bytes=-1\rOutput stat for /sswang count=1,bytes=6\r可以看到：\rOutput stat 后面的 count 表示的是总数量，bytes指定的是数据总长度，包括的子节点的数据长度\r指定数量的话，数据长度默认没有限制\r测试效果：\r在/sswang节点下创建多个子节点\rcreate /sswang/child1 1\rcreate /sswang/child2 1\rcreate /sswang/child3 1\rcreate /sswang/child4 1\r查看zookeeper-root-server-python-auto.out文件信息，会有WARN提示信息\r2021-07-01 20:07:30,649 [myid:] - WARN [SyncThread:0:DataTree@301] - Quota\rexceeded: /sswang count=3 limit=2\r2021-07-01 20:08:06,715 [myid:] - WARN [SyncThread:0:DataTree@301] - Quota\rexceeded: /sswang count=4 limit=2\r设置节点的数据长度资源\r[zk: localhost:2181(CONNECTED) 54] create","date":"2021-10-05","objectID":"/posts/zookeeper/zookeeper-01/:2:10","tags":["Zookeeper","中间件"],"title":"zookeeper 基础与安装 （一）","uri":"/posts/zookeeper/zookeeper-01/"},{"categories":["Zookeeper"],"content":"应用场景 Zookeeper 提供的服务包括：统一命名服务、统一配置管理、统一集群管理、服务器节点动态上下 线、软负载均衡等 ","date":"2021-10-05","objectID":"/posts/zookeeper/zookeeper-01/:2:11","tags":["Zookeeper","中间件"],"title":"zookeeper 基础与安装 （一）","uri":"/posts/zookeeper/zookeeper-01/"},{"categories":["Zookeeper"],"content":"Watcher 事件监听器 Watcher(事件监听器)是 Zookeeper 提供的一种 发布/订阅的机制。 Zookeeper 允许客户端在指定的集群数据节点上注册一些 Watcher，当发生ZNode存储数据的修改， 子节点目录的变化等情况的时候，Zookeeper 服务端会将事件通知给监听的客户端，然后客户端根据 Watcher通知状态和事件类型做出业务上的改变。 客户端注册监听它关心的目录节点，当目录节点发生变化（数据改变、节点删除、子目 录节点增加删除）时，ZooKeeper 会通知客户端。监听机制保证 ZooKeeper 保存的任何的数 据的任何改变都能快速的响应到监听了该节点的应用程序。 该机制是 Zookeeper 实现分布式协调的重要特性，也是Zookeeper的核心特性,Zookeeper的很多 功能都是基于这个特性实现的。这个过程是异步的。 Zookeeper的监听机制主要有三个关键点：整体触发、发送方式、局部触发。 整体触发: 当设置监视的数据发生改变时，该监视事件会被发送到客户端，常见的就是监控ZNode中的数据或子目录发生变化。 发送方式： Zookeeper服务端被触发的时候，会基于会话给客户端发送信息，但是由于网络的原因，经常会出现网络延迟的因素，造成客户端接收的结构不一致，而Zookeeper有一个很重要的特点就是:一致性，为了达到这个目标，Zookeeper的监听机制在信息发送的方式上，就有了一个发送特点：所有的监视事件被触发后，不会立即发送至客户端，而是以异步的方式发送至监视者的，而且Zookeeper 本身提供了顺序保证。效果就是：客户端首先看到监视事件，然后才会感知到它所设置监视的znode发生了变化。这样就达到了，虽然不同的客户端在不同的时刻感知到了监视事件，但是客户端所看到的效果都是真实一致的。 局部触发 当客户端监视的Zookeeper节点ZNode内部有比较多的子目录数据的时候，这种场景下，我们只需要监视其中的一个小部分重要的数据，其他的数据是一些无关紧要的，所以就没有必要监视全部的ZNode数据变化，这意味着znode节点本身就应该具有不同的触发事件方式：即支持对ZNode数据事件的局部触发。 ","date":"2021-10-05","objectID":"/posts/zookeeper/zookeeper-01/:2:12","tags":["Zookeeper","中间件"],"title":"zookeeper 基础与安装 （一）","uri":"/posts/zookeeper/zookeeper-01/"},{"categories":["Zookeeper"],"content":"监听器实践 1.节点的值变化监听 在hadoop104 主机上注册监听/sanguo 节点数据变化 [zk: localhost:2181(CONNECTED) 26] get -w /sanguo在hadoop103 主机上修改/sanguo 节点的数据 [zk: localhost:2181(CONNECTED) 1] set /sanguo \"xisi\"观察hadoop104 主机收到数据变化的监听 WATCHER:: WatchedEvent state:SyncConnected type:NodeDataChanged path:/sanguo注意：在hadoop103再多次修改/sanguo的值，hadoop104上不会再收到监听。因为注册 一次，只能监听一次。想再次监听，需要再次注册。 2.节点的子节点变化监听（路径变化） 在hadoop104 主机上注册监听/sanguo 节点的子节点变化 [zk: localhost:2181(CONNECTED) 1] ls -w /sanguo [shuguo, weiguo]在hadoop103 主机/sanguo 节点上创建子节点 [zk: localhost:2181(CONNECTED) 2] create /sanguo/jin \"simayi\" Created /sanguo/jin 观察hadoop104 主机收到子节点变化的监听 WATCHER:: WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/sanguo 注意：节点的路径变化，也是注册一次，生效一次。想多次生效，就需要多次注册。 日志实践 zookeeper服务器会产生三类日志：事务日志、快照日志和系统日志。\r我们可以在zookeeper的配置文件zoo.cfg中 通过\rdataDir 设定数据快照日志的存储位置\rdataLogDir 设定事务日志的存储位置，如果不设置该项，这默认保存在 dataDir目录下\r注意：\r事务日志和快照日志都会保存在 指定目录的 version-2 子目录下。\r我们倾向于 dataLogDir 和 dataLog 单独配置，因为zookeeper集群频繁读写操作，\r可能会产生大龄日志，有可能影响系统性能，可以根据日志的特性，使用不同的存储介质\rzookeeper的系统运行日志是可以通过三个位置来进行设置\r1 在log4j.properties文件中 通过\rzookeeper.log.dir=. 来设置，这里的'.'指的是zkServer.sh坐在的目录\r2 在 zkEnv.sh 文件中通过\rZOO_LOG_DIR=\"$ZOOKEEPER_PREFIX/logs\" 来设置\r3 在 zkServer.sh 文件中 通过\rZOO_LOG_FILE=zookeeper-$USER-server-$HOSTNAME.log\r_ZOO_DAEMON_OUT=\"$ZOO_LOG_DIR/zookeeper-$USER-server-$HOSTNAME.out\"\r来指定事务日志 日志简介 事务日志是指 Zookeeper 系统在正常运行过程中，针对所有的更新操作，在返回客户端 更新成功 的 响应前，Zookeeper 会保证已经将本次更新操作的事务日志写到磁盘上，只有这样，整个更新操作才会生 效。 日志查看 zookeeper的事务日志为二进制文件，不能通过vim等工具直接访问,可以通过zookeeper自带的功能 文件读取事务日志文件。 对于3.5.5 之前的zookeeper需要借助于大量的jar包来实现日志的查看，比如 java -cp .:zookeeper-3.7.0.jar:slf4j-api-1.7.30.jar org.apache.zookeeper.server.LogFormatter log.1 zookeeper 从 3.5.5 版本之后，就取消的LogFormatter ，使用了一个更好的 TxnLogToolkit 工具,这个工具放置在了 bin/ 目录下，文件名是 zkTxnLogToolkit.sh 使用方式： zkTxnLogToolkit.sh log_file快照日志 日志简介\rzookeeper的数据在内存中是以树形结构进行存储的，而快照就是每隔一段时间就会把整个DataTree\r的数据序列化后存储在磁盘中。\r日志查看\r同样在 3.5.5 版本之后, 我们可以基于 zkSnapShotToolkit.sh 命令来进行快照日志的查看查看日志实践 日志文件查看 # ls logs/version-2/ -lh\r总用量 24K\r-rw-r--r-- 1 root root 65M 8月 10 16:38 log.100000005\r-rw-r--r-- 1 root root 65M 8月 10 16:06 log.800000001\r-rw-r--r-- 1 root root 65M 8月 10 16:23 log.b00000001\r结果显示：\r每个日志文件大小是 65M，文件名规则 'log.9个字符',这9个字符指的是事务id\r日志内容查看\r# ./bin/zkTxnLogToolkit.sh logs/version-2/log.b00000001\rZooKeeper Transactional Log File with dbid 0 txnlog format version 2\r# 这是每个事务日志文件都有的日志头，输出了 dbid 还有 version等信息\r2021-08-10 17:07:23,272 [myid:] - INFO [main:ZookeeperBanner@42] -\r...\r2021-08-10 17:07:23,289 [myid:] - INFO [main:ZooKeeperServer@260] -\rzookeeper.intBufferStartingSizeBytes = 1024\r21-8-10 下午04时23分21秒 session 0x100009bc0f50000 cxid 0x0 zxid 0xb00000001\rcloseSession v{}\r# 这是xx时候，sessionid 请求类型为 closeSession，表示关闭了会话\rEOF reached after 1 txns.快照日志查看 日志文件查看\r# ls data/version-2/ -lh\r总用量 20K\r-rw-r--r-- 1 root root 2 8月 10 16:45 acceptedEpoch\r-rw-r--r-- 1 root root 2 8月 10 16:45 currentEpoch\r-rw-r--r-- 1 root root 2.1K 8月 10 16:05 snapshot.600000000\r-rw-r--r-- 1 root root 2.1K 8月 10 16:06 snapshot.700000000\r-rw-r--r-- 1 root root 2.0K 8月 10 16:15 snapshot.800000001\r结果显示：\r快照日志的命名规则为'snapshot.9个字符',\r这9个字符表示zookeeper触发快照的那个瞬间，提交的最后一个事务的ID。\r日志内容查看\r#\r./bin/zkSnapShotToolkit.sh data/version-2/snapshot.800000001\r2021-08-10 17:08:25,876 [myid:] - INFO [main:SnapStream@61] -\rzookeeper.snapshot.compression.method = CHECKED\r...\r----\r/zookeeper/quota/sswang2/zookeeper_stats 路径\rcZxid = 0x00000000000051 创建节点时的 Zxid\rctime = Tue Aug 10 14:44:11 CST 2021 创建节点的时间\rmZxid = 0x00000000000051 节点最近一次更新对应的 Zxid\rmtime = Tue Aug 10 14:44:11 CST 2021 节点最近一次更新的时间\rpZxid = 0x00000000000051 父节点的 Zxid\rcversion = 0 子节点更新次数\rdataVersion = 0 数据更新次数\raclVersion = 0 节点 acl 更新次数\rephemeralOwner = 0x00000000000000 节点的 sessionid值\rdataLength = 16 存储的数据长度\r----\r这里表达的是当前抓取快照日志文件的时间记录\rSession Details (sid, timeout, ephemeralCount):\r0x1000048d7820002, 30000, 0\r0x100009bc0f50000, 30000, 0\r----\rLast zxid: 0x800000001系统日志查看 日志查看\r# ls bin/zook*\rbin/zookeeper_audit.log\r# ls logs/zook*\rlogs/zookeeper_audit.log logs/zookeeper-root-server-python-auto.out\r查看集群运行日志\r# cat logs/","date":"2021-10-05","objectID":"/posts/zookeeper/zookeeper-01/:2:13","tags":["Zookeeper","中间件"],"title":"zookeeper 基础与安装 （一）","uri":"/posts/zookeeper/zookeeper-01/"},{"categories":["Zookeeper"],"content":"基础配置 配置 解析 tickTime（SS / CS） 用来指示 服务器之间或客户端与服务器之间维护心跳机制的 最小时间单元，Session 最小过期时间默认为两倍的 tickTime（default：2000ms） initLimit（LF） 集群中的 Leader 节点和 Follower 节点之间初始连接时能容忍的最多心跳数（default：10 tickTime） syncLimit（LF） 集群中的 Leader 节点和 Follower 节点之间请求和应答时能容忍的最多心跳数（default：5 tickTime） dataDir Zookeeper 保存服务器存储快照文件的目录，默认情况，Zookeeper将 写数据的日志文件也保存在这个目录里（default：/tmp/zookeeper） clientPort 客户端连接 Zookeeper 服务器的端口，Zookeeper 会监听这个端口，接受客户端的访问请求（default：2181） dataLogDir 用来存储服务器事务日志 minSessionTimeout\u0026maxSessionTimeout 默认分别是 2 * tickTime ~ 20 * tickTime，来用控制 客户端设置的Session 超时时间。如果超出或者小于，将自动被服务端强制设置为最大或者最小 ","date":"2021-10-05","objectID":"/posts/zookeeper/zookeeper-01/:2:14","tags":["Zookeeper","中间件"],"title":"zookeeper 基础与安装 （一）","uri":"/posts/zookeeper/zookeeper-01/"},{"categories":["Zookeeper"],"content":"集群配置 为了配置 Zookeeper 集群，会在配置文件末尾增加如下格式的服务器节点配置\r格式：server.\u003cmyid\u003e=\u003cserver_ip\u003e:\u003cLF_Port\u003e:\u003cL_Port\u003e格式解析 \u003cmyid\u003e\r表示节点编号，是该节点在集群中唯一的编号，取值范围是1~255之间的整数，而且我们必须在\rdataDir目录下创建一个myid的文件，将节点对应的\u003cmyid\u003e值输入到该节点的myid文件。\r\u003cserver_ip\u003e\r表示集群中的节点ip地址，可以使用主机名或ip来表示，生产中如果配置好内部dns的话，推荐使用主\r机名，本机地址的表示方法是：127.0.0.1或者localhost\r\u003cLF_Port\u003e\r表示Leader节点和Follower节点进行心跳检测与数据同步所使用的端口。\r\u003cL_Port\u003e\r表示进行领导选举过程中，用于投票通信的端口。\r注意：\r这些端口可以随机自己定义。\r真正的生产环境中，不同主机上的clientPort、LF_Port、L_Port三个端口一般可以配置成一样，因\r为生产集群中每个server主机都分布在不同的主机上，都有独立的ip地址，不会造成端口冲突","date":"2021-10-05","objectID":"/posts/zookeeper/zookeeper-01/:2:15","tags":["Zookeeper","中间件"],"title":"zookeeper 基础与安装 （一）","uri":"/posts/zookeeper/zookeeper-01/"},{"categories":["ElasticStack"],"content":"Logstash 能够动态地采集、转换和传输数据，不受格式或复杂度的影响。Logstash 采用可插拔框架，拥有 200 多个插件。您可以将不同的输入选择、过滤器和输出选择混合搭配、精心安排，让它们在管道中和谐地运行。利用 Grok 从非结构化数据中派生出结构，从 IP 地址解码出地理坐标，匿名化或排除敏感字段，并简化整体处理过程。Logstash 提供众多输出选择，您可以将数据发送到您要指定的地方，并且能够灵活地解锁众多下游用例。 ","date":"2021-10-03","objectID":"/posts/elk/elk-logstash/:0:0","tags":["日志收集"],"title":"ELK组件-Logstash （三）","uri":"/posts/elk/elk-logstash/"},{"categories":["ElasticStack"],"content":"基础知识 ","date":"2021-10-03","objectID":"/posts/elk/elk-logstash/:1:0","tags":["日志收集"],"title":"ELK组件-Logstash （三）","uri":"/posts/elk/elk-logstash/"},{"categories":["ElasticStack"],"content":"功能简介 logstash 就是借助于大量的功能插件，实现从数据源获取数据，然后将数据传输到elasticsearch。 在图中我们可以明显看到，logstash组件至少包含两个插件：input和output，这两个主要用于信息的接入和输出。 注意： logstash 软件本身无序安装，它仅仅是一个软件运行命令程序，但是该软件的运行依赖于java环境 运行环境 安装java8环境\rapt install openjdk-8-jdk\r检查效果\rjava -version ","date":"2021-10-03","objectID":"/posts/elk/elk-logstash/:1:1","tags":["日志收集"],"title":"ELK组件-Logstash （三）","uri":"/posts/elk/elk-logstash/"},{"categories":["ElasticStack"],"content":"软件安装 apt源码方式 获取软件源 wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add - apt install apt-transport-https echo \"deb https://artifacts.elastic.co/packages/7.x/apt stable main\" | sudo tee –a /etc/apt/sources.list.d/elastic-7.x.list apt update 安装软件 apt install logstash 软件包安装 wget https://artifacts.elastic.co/downloads/logstash/logstash-7.14.0-amd64.deb wget https://artifacts.elastic.co/downloads/logstash/logstash-7.14.0- amd64.deb.sha512 shasum -a 512 -c logstash-7.14.0-amd64.deb.sha512 dpkg -i logstash-7.14.0-amd64.deb 配置查看 dpkg -L logstash /. /usr /usr/share /usr/share/logstash ... /etc /etc/logstash /etc/logstash/conf.d /etc/logstash/log4j2.properties /etc/logstash/startup.options 服务启动环境变量文件 /etc/logstash/jvm.options jvm相关配置 /etc/logstash/logstash.yml 服务配置文件 /etc/logstash/logstash-sample.conf 应用配置文件模板 /etc/logstash/pipelines.yml ... /usr/share/logstash/bin /usr/share/logstash/bin/benchmark.bat /usr/share/logstash/bin/benchmark.sh /usr/share/logstash/bin/cpdump /usr/share/logstash/bin/dependencies-report /usr/share/logstash/bin/ingest-convert.bat /usr/share/logstash/bin/ingest-convert.sh /usr/share/logstash/bin/logstash /usr/share/logstash/bin/logstash-keystore /usr/share/logstash/bin/logstash-keystore.bat /usr/share/logstash/bin/logstash-plugin /usr/share/logstash/bin/logstash-plugin.bat /usr/share/logstash/bin/logstash.bat /usr/share/logstash/bin/logstash.lib.sh /usr/share/logstash/bin/pqcheck /usr/share/logstash/bin/pqcheck.bat /usr/share/logstash/bin/pqrepair /usr/share/logstash/bin/pqrepair.bat /usr/share/logstash/bin/ruby /usr/share/logstash/bin/setup.bat /usr/share/logstash/bin/system-install 生成系统管理配置文件 ... 定制环境变量 echo 'export PATH=/usr/share/logstash/bin:$PATH' \u003e /etc/profile.d/logstash.sh source /etc/profile.d/logstash.sh ","date":"2021-10-03","objectID":"/posts/elk/elk-logstash/:1:2","tags":["日志收集"],"title":"ELK组件-Logstash （三）","uri":"/posts/elk/elk-logstash/"},{"categories":["ElasticStack"],"content":"简单实践 命令格式 logstash -e '启动参数' 启动参数： input { stdin {} } output { stdout {} } 参数解析： input {} 用于接受信息的输入 output {} 用于对内部的数据输出 stdin {} 表示屏幕上的标准输入 stdout {} 表示屏幕的标准输出 实践1 - 简单的输入输出测试 # logstash -e 'input { stdin { } } output { stdout {} }' ... [INFO ] 2021-08-15 18:03:54.011 [Api Webserver] agent - Successfully started Logstash API endpoint {:port=\u003e9600} # 看到上面准备好的信息后，接下来我们在屏幕上随便输入一段信息 nihao logstash # 信息输入完毕后，他会自动格式化的输出一些内容 { \"host\" =\u003e \"ubuntu\", # 当前的主机信息 \"@timestamp\" =\u003e 2021-08-15T10:04:32.873Z, # 该条信息的时间戳 \"message\" =\u003e \"nihao logstash\", # 我们输入的内容 \"@version\" =\u003e \"1\" # 版本信息 } 结果展示： 信息展示出来的内容，其实包含两部分： index(搜索数据时候的索引)和value(具体的数据内容) 实践2 - 信息传递到es 配置logstash将信息输出到es中\r# logstash -e 'input { stdin{} } output { elasticsearch { hosts =\u003e\r[\"192.168.10.106:9200\"] index =\u003e \"message\" } }'\r...\r[2021-08-15 18:07:51.678][INFO ][logstash.agent ] Successfully started\rLogstash API endpoint {:port=\u003e9600}\r# 看到上面准备好的信息后，接下来我们在屏幕上随便输入一段信息\rhello elasticsearch\r结果展示：\r因为我们将信息输入到es中了，所以这里看不到信息输入\res检查查看效果\r# curl 192.168.10.106:9200/_cat/indices\rgreen open message x3JlhXwBQsmGYITwOnlWEw 1 1 1 0 10.8kb 5.3kb\r# curl 192.168.10.106:9200/message?pretty\r{\r\"message\" : {\r...,\r\"number_of_shards\" : \"1\",\r\"provided_name\" : \"message\",\r\"creation_date\" : \"1629022162762\",\r\"number_of_replicas\" : \"1\",\r\"uuid\" : \"x3JlhXwBQsmGYITwOnlWEw\",\r\"version\" : {\r\"created\" : \"7140099\"\r}\r}\r}\r}\r}\r查看内容信息\r# curl 192.168.10.106:9200/message/_search?pretty\r{\r\"took\" : 88,\r\"timed_out\" : false,\r\"_shards\" : {\r\"total\" : 1,\r\"successful\" : 1,\r\"skipped\" : 0,\r\"failed\" : 0\r},\r\"hits\" : {\r\"total\" : {\r\"value\" : 1,\r\"relation\" : \"eq\"\r},\r\"max_score\" : 1.0,\r\"hits\" : [\r{\r\"_index\" : \"message\",\r\"_type\" : \"_doc\",\r\"_id\" : \"3UZJSXsBvfqpwa_-kJ8V\",\r\"_score\" : 1.0,\r\"_source\" : {\r\"host\" : \"python-auto\",\r\"@timestamp\" : \"2021-08-15T10:09:22.359Z\",\r\"message\" : \"hello elasticsearch\",\r\"@version\" : \"1\"\r}\r}\r]\r}\r} 实践3 - 读取日志文件到es 模块简介\rlogstash的信息采集模块支持file模块，可以通过指定日志文件，直接从文件中读取相关信息。\r参考资料：\rhttps://www.elastic.co/guide/en/logstash/7.14/plugins-inputs-file.html\r基本属性：\rpath 指定文件路径\rstart_position 设定从文件的那个位置开始读取，可选值 -- beginning, end(默认)\rtype 传递信息的时候，增加一个额外的属性字段\r配置示例：\rfile {\rpath =\u003e \"/var/log/syslog\"\rstart_position =\u003e \"beginning\"\rtype =\u003e \"elasticsearch\"\r}\r从系统日志文件中读取信息，输出到es中\r# logstash -e 'input { file{path =\u003e \"/var/log/syslog\" start_position =\u003e\r\"beginning\" type =\u003e \"elasticsearch\"} } output { elasticsearch { hosts =\u003e\r[\"192.168.10.106\"] index =\u003e \"message\" } }'\r...\r[INFO ] 2021-08-15 18:32:50.789 [[main]-pipeline-manager] elasticsearch - New\rElasticsearch output {:class=\u003e\"LogStash::Outputs::ElasticSearch\", :hosts=\u003e\r[\"//192.168.8.12:9200\"]}\r...\r[INFO ] 2021-08-15 18:32:54.992 [[main]-pipeline-manager] file - No sincedb_path\rset, generating one based on the \"path\" setting\r{:sincedb_path=\u003e\"/usr/share/logstash/data/plugins/inputs/file/.sincedb_f5fdf6ea0\rea92860c6a6b2b354bfcbbc\", :path=\u003e[\"/var/log/syslog\"]}\r在head插件中查看日志效果 ","date":"2021-10-03","objectID":"/posts/elk/elk-logstash/:1:3","tags":["日志收集"],"title":"ELK组件-Logstash （三）","uri":"/posts/elk/elk-logstash/"},{"categories":["ElasticStack"],"content":"服务文件方式 生成配置文件 ​ 以命令行的方式来进行启动太繁琐，我们最好还是以配置文件的方式来进行服务的启动管理，对于 logstash来说，它提供好了一个专门用于生成配置文件的命令 system-install，我们只需要按照既定的 配置文件规则，定制应用配置，最后执行该命令，即可实现服务脚本的配置。 服务启动参数 进入应用目录\rcd /etc/logstash\r编辑启动参数文件\r# vim startup.options\r...\r# Arguments to pass to logstash\rLS_OPTS=\"--path.settings ${LS_SETTINGS_DIR} -f /etc/logstash/conf.d\"\r注意： -f 指定的是 logstash的应用配置文件(比如 logstash.conf)存放到的目录 定制配置文件信息输入到es的配置文件 生成配置文件 cp logstash-sample.conf conf.d/logstash.conf 修改配置文件 conf.d/logstash.conf # Sample Logstash configuration for creating a simple # Beats -\u003e Logstash -\u003e Elasticsearch pipeline. #输入部分 input { # beats { # port =\u003e 5044 # } #插件 file { path =\u003e [\"/var/log/syslog\"] start_position =\u003e \"beginning\" type =\u003e \"elasticsearch\" } } #输出部分 output { elasticsearch { hosts =\u003e [\"http://192.168.8.12:9200\"] index =\u003e \"logstash-test-%{+YYYY.MM.dd}\" } } 生成配置文件 以root用户执行下面的命令 system-install 查看生成的服务配置文件 # ls /etc/systemd/system/logstash.service /etc/systemd/system/logstash.service 查看服务配置文件内容 # cat /etc/systemd/system/logstash.service [Unit] Description=logstash [Service] Type=simple User=logstash Group=logstash # Load env vars from /etc/default/ and /etc/sysconfig/ if they exist. # Prefixing the path with '-' makes it try to load, but if the file doesn't # exist, it continues onward. EnvironmentFile=-/etc/default/logstash EnvironmentFile=-/etc/sysconfig/logstash ExecStart=/usr/share/logstash/bin/logstash \"--path.settings\" \"/etc/logstash\" \"- f\" \"/etc/logstash/conf.d\" Restart=always WorkingDirectory=/ Nice=19 LimitNOFILE=16384 # When stopping, how long to wait before giving up and sending SIGKILL? # Keep in mind that SIGKILL on a process can cause data loss. TimeoutStopSec=infinity [Install] WantedBy=multi-user.target 注意： 由于服务启动的时候，用户名和用户组都是 logstash ，所以，我们采集数据的文件必须是具备查看的权限 启动服务 重载服务 systemctl daemon-reload 启动服务 systemctl start logstash.service systemctl status logstash.service 查看效果 # netstat -tnulp | egrep 'Add|java' Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp6 0 0 127.0.0.1:9600 :::* LISTEN 88794/java tcp6 0 0 :::9200 :::* LISTEN 87210/java tcp6 0 0 :::9300 :::* LISTEN 87210/java 结果显示： logstash的默认端口是 9600 查看日志 tail -f /var/log/logstash/logstash-plain.log 可以看到默认报错： [2021-08-15T18:44:08,643][WARN ] [filewatch.tailmode.handlers.createinitial][main] [cc34021140e2525e95d5755b6135b9801f3595239bcda82a1cca03a1d0f857d6] failed to open file {:path=\u003e\"/var/log/syslog\", :exception=\u003eErrno::EACCES, :message=\u003e\"Permission denied - /var/log/syslog\"} 临时增加一个 logstash 允许访问的权限 chown logstash.logstash /var/log/syslog 通过head插件查看数据传递效果 小结： 定位 数据的采集和传递 组成： 核心：input - filter - output 辅助：codec 特点： 每个部分都有对应业务场景的插件来实现功能 守护进程方式运行logstash 定制服务的启动参数 system-install 生成服务启动文件 应用服务启动文件 临时测试文件的方式 logstash -f xxx.conf ","date":"2021-10-03","objectID":"/posts/elk/elk-logstash/:1:4","tags":["日志收集"],"title":"ELK组件-Logstash （三）","uri":"/posts/elk/elk-logstash/"},{"categories":["ElasticStack"],"content":"两三年前ELK还是一套日志分析平台的解决方案 ，它是由Elastic公司开发，管理和维护的三款开源产品Elasticsearch，Logstash和Kibana的首字母缩写，随着该套解决方案的软件生态逐渐壮大，其组件和功能也渐渐多了起来，尤其是Beats组件的引入，逐渐形成了这个系列的四大支柱，然后公司将这套解决方案重新命名为：Elastic Stack，最新版本7.X。 ELK日志收集系统 ","date":"2021-10-02","objectID":"/posts/elk/elk-elasticsearch/:0:0","tags":["日志收集"],"title":"ELK日志收集系统 （一）","uri":"/posts/elk/elk-elasticsearch/"},{"categories":["ElasticStack"],"content":"基础知识 简介 所谓的 搜索引擎(Search Engine)，通常指的是收集了 几千万到几十亿个条数据，为了方便后续精\r确查询，我们为这些数据的多个词(关键词)进行索引，建立索引数据库的全文搜索引擎。\r这样，当用户查找某个关键词的时候，所有在页面内容中包含了该关键词的网页都将作为搜索结果被搜出来。再经过复杂的算法进行排序(或者包含商业化的竞价排名、商业推广或者广告)后，这些结果将按照与搜索关键词的相关度高低（或与相关度毫无关系），依次排列。 应用场景 搜索与推荐:\r搜索 - 用户在网上，主动根据关键字去搜索相关的数据\r推荐 - 网站程序，根据用户日常的搜索习惯，通过分析用户的行为数据，主动推送一些用户感兴趣的事情。\r搜索领域：\r门户网站 - 提供各种用户感兴趣的网站入口，便于用户快速获取相关信息，一般是爬虫爬取的数据。\r定向搜索 - 搜索特定领域的内容，一般是网站自身根据产品设计的样式，结构化存储的数据。\r搜索功能：\r智能搜索 - 根据用户搜索的历史或者关键字，自动弹出或者扩展类似的关键字，提高用户搜索的效率。\r普通搜索 - 用户输入数据然后正常的搜索。\r相关搜索 - 默认提供的其他关联网站的入口。 ","date":"2021-10-02","objectID":"/posts/elk/elk-elasticsearch/:1:0","tags":["日志收集"],"title":"ELK日志收集系统 （一）","uri":"/posts/elk/elk-elasticsearch/"},{"categories":["ElasticStack"],"content":"数据流程 数据采集 物理层：文件、设备等\r代理层：nginx、haproxy等\rweb层：nginx、apache、tomcat等\r数据库层：mysql、mongodb、redis等\r存储层：ceph、k8s、docker等\r...","date":"2021-10-02","objectID":"/posts/elk/elk-elasticsearch/:2:0","tags":["日志收集"],"title":"ELK日志收集系统 （一）","uri":"/posts/elk/elk-elasticsearch/"},{"categories":["ElasticStack"],"content":"流程解析 要完成一次完整的数据采集与处理至少需要有以下几方面来组成： ① 数据采集：根据业务的特性，采取多种方式，进行对一些针对性的数据进行采集\r② 数据整理：对上报后的数据源进行收集、清洗、整理\r③ 实时分析：对某些重要的核心的业务数据，进行实时分析\r④ 离线分析：对普通的数据、非紧急的业务数据进行存储，后续进行相应的分析\r⑤ 结果输出：将实时分析和离线分析后的数据结果展现出来，供决策参考\r⑥ 问题决策：根据当前业务情况，人工或者自动方式对输出的结构进行分析，并判定下一步的行动(警告或修\r复)，\r同时将其决策记录保存下来，以便为后序决策提供依据\r也就是说：采集、传输、存储、分析、警告这几部分是非常必要的。 ","date":"2021-10-02","objectID":"/posts/elk/elk-elasticsearch/:2:1","tags":["日志收集"],"title":"ELK日志收集系统 （一）","uri":"/posts/elk/elk-elasticsearch/"},{"categories":["ElasticStack"],"content":"方案梳理 在这个流程图中涉及到两种场景：实时分析与离线分析，如果需要使用市面 上的开源解决方案来实现的话，有两套方案比较有优势： 方案一：ELK + Kafka + 分布式存储 中数据量场景下，实时的数据多一些 方案二：Spark + Flume + Kafka + Hadoop(Hive + HBase) 大数据量场景下，离线的数据多一些 ","date":"2021-10-02","objectID":"/posts/elk/elk-elasticsearch/:2:2","tags":["日志收集"],"title":"ELK日志收集系统 （一）","uri":"/posts/elk/elk-elasticsearch/"},{"categories":["ElasticStack"],"content":"快速入门 ","date":"2021-10-02","objectID":"/posts/elk/elk-elasticsearch/:3:0","tags":["日志收集"],"title":"ELK日志收集系统 （一）","uri":"/posts/elk/elk-elasticsearch/"},{"categories":["ElasticStack"],"content":"ELK简介 两三年前ELK还是一套日志分析平台的解决方案 ，它是由Elastic公司开发，管理和维护的三款开源产品\rElasticsearch，Logstash和Kibana的首字母缩写，随着该套解决方案的软件生态逐渐壮大，其组件和功\r能也渐渐多了起来，尤其是Beats组件的引入，逐渐形成了这个系列的四大支柱，然后公司将这套解决方案重\r新命名为：Elastic Stack，最新版本7.X。 官方介绍 The products in the Elastic Stack are designed to be used together and releases\rare\rsynchronized to simplify the installation and upgrade process. The full stack\rconsists\rof:\rBeats 7.14 APM Server 7.14 Elasticsearch Hadoop 7.14\rKibana 7.14 Elasticsearch 7.14 Logstash 7.14 开发语言 java：Elasticsearch、Logstash、Kibana\rgo：FileBeats 组件发展 相关网站 官方网站：https://www.elastic.co\rgithub：https://github.com/elastic\r示例网站：https://demo.elastic.co ","date":"2021-10-02","objectID":"/posts/elk/elk-elasticsearch/:3:1","tags":["日志收集"],"title":"ELK日志收集系统 （一）","uri":"/posts/elk/elk-elasticsearch/"},{"categories":["ElasticStack"],"content":"架构组件 四个组件：\rBeats是安装在边缘主机上的轻型代理，可收集不同类型的数据以转发到堆栈中\rLogstash是一个日志聚合器，可从各种输入源收集数据，执行不同的转换和增强功能，然后将数据发送\r到各种受支持的输出目标\rElasticsearch是基于Apache Lucene搜索引擎的开源全文搜索和分析引擎\rKibana是在Elasticsearch之上工作的可视化层，为用户提供了分析和可视化数据的能力\r这四个组件在官方的介绍主要分成了两组势力：数据收集和展示\r- 数据收集和处理：Beats和Logstash负责数据收集和处理。\r- 数据搜索和展示：Elasticsearch索引并存储数据，Kibana提供了用于查询数据和可视化数据的用户界面\r这些组件在设计的时候，组件间的交流就非常简单，所以我们只需要简单的几条配置，就可以将不同的组件组\r合起来实现一个简单而强大的解决方案。而且还可以基于应用场景的不同随意组合，比如日志管理、日志分\r析、应用监控、故障排除和保护IT环境等。 对于小型的应用项目开发环境，ELK的四个组件可以实现一个非常经典的组合： 对于中大型场景，ELK基于丰富灵活的信息接口将非常多的功能整合到经典组合中，以提高其架构弹性和安全性： ","date":"2021-10-02","objectID":"/posts/elk/elk-elasticsearch/:3:2","tags":["日志收集"],"title":"ELK日志收集系统 （一）","uri":"/posts/elk/elk-elasticsearch/"},{"categories":["ElasticStack"],"content":"安装部署 要点 1 组件间如何联通\r2 组件如何在不同场景中使用对应插件\relasticsearch 实践术语及分片实践\rlogstash 组件(插件)实现数据的转移及转移过程中对数据的封装\rfilebeat 采集数据集转移\rkibana 可视化的流程及大量可视化实践\r","date":"2021-10-02","objectID":"/posts/elk/elk-elasticsearch/:4:0","tags":["日志收集"],"title":"ELK日志收集系统 （一）","uri":"/posts/elk/elk-elasticsearch/"},{"categories":["ElasticStack"],"content":"环境 前提：\rjava环境很重要，但是很可惜，elasticsearch软件内部已经有了java环境，\r默认情况下，elasticsearch 软件包里面已经包含了java环境，而且是最新版的 JDK-16.0.1\r环境布局\r10.0.0.12 elasticsearch elasticsearch_head\r10.0.0.13 elasticsearch logstash\r10.0.0.14 kibana\r10.0.0.15 filebeat + 项目代码定位 日志分析平台解决方案 - 大量小方案的整合\rStore, Search, and Analyze\r组成\rElasticsearch - 数据的存储和分析\rLogstash - 数据采集和传输(*)\rKibana - 数据的可视化(二次处理) KQL\rBeats - 数据采集(*)和传输\r中小场景方案\rBeats - Logstash - elasticsearch - kibana\r中大场景方案\rBeats - 消息队列 - Logstash - 消息队列 - elasticsearch - kibana(Nginx) 基本环境 安装java8环境\rapt install openjdk-8-jdk\r检查效果\rjava -version\r注意:\r默认情况下，elasticsearch 软件包里面已经包含了java环境，而且是最新版的 JDK-16.0.1 ","date":"2021-10-02","objectID":"/posts/elk/elk-elasticsearch/:4:1","tags":["日志收集"],"title":"ELK日志收集系统 （一）","uri":"/posts/elk/elk-elasticsearch/"},{"categories":["ElasticStack"],"content":"软件安装 apt 源码安装方式 获取软件源 wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add - apt install apt-transport-https echo \"deb https://artifacts.elastic.co/packages/7.x/apt stable main\" | sudo tee –a /etc/apt/sources.list.d/elastic-7.x.list apt update 安装软件 apt install elasticsearch 软件安装方法2 - 包安装 wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.14.0- amd64.deb wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.14.0- amd64.deb.sha512 shasum -a 512 -c elasticsearch-7.14.0-amd64.deb.sha512 dpkg -i elasticsearch-7.14.0-amd64.deb 注意： 这里推荐 两台服务器来部署 elasticsearch 基本配置 查看配置结构 #dpkg -L elasticsearch /usr /usr/share /usr/share/elasticsearch /usr/share/elasticsearch/bin /usr/share/elasticsearch/bin/elasticsearch-geoip ... /etc /etc/elasticsearch /etc/elasticsearch/elasticsearch.yml 核心配置文件 /etc/elasticsearch/log4j2.properties 日志相关的配置 /etc/elasticsearch/roles.yml /etc/elasticsearch/jvm.options jvm相关的配置 /etc/elasticsearch/role_mapping.yml /etc/elasticsearch/users_roles /etc/elasticsearch/users /etc/default /etc/default/elasticsearch 环境变量配置文件 /usr/lib /usr/lib/tmpfiles.d /usr/lib/tmpfiles.d/elasticsearch.conf /usr/lib/systemd /usr/lib/systemd/system /usr/lib/systemd/system/elasticsearch.service 服务启动文件 ... 环境变量定制 echo 'export PATH=/usr/share/elasticsearch/bin:$PATH' \u003e /etc/profile.d/elasticsearch.sh source /etc/profile.d/elasticsearch.sh ","date":"2021-10-02","objectID":"/posts/elk/elk-elasticsearch/:4:2","tags":["日志收集"],"title":"ELK日志收集系统 （一）","uri":"/posts/elk/elk-elasticsearch/"},{"categories":["ElasticStack"],"content":"简单实践 修改配置文件 vim /etc/elasticsearch/elasticsearch.yml # 设定elasticsearch集群的名称 cluster.name: elastic.example.com # 设定集群master界面的名称 node.name: 192.168.8.12 # 设定elasticsearch的存储目录，包括数据目录和日志目录 path.data: /var/lib/elasticsearch path.logs: /var/log/elasticsearch # 允许所有主机都能访问我们的elasticsearch network.host: 0.0.0.0 # 设置elasticsearch对外的访问端口 http.port:9200 # 设定主机发现 discovery.seed_hosts: [\"192.168.10.106\",\"192.168.10.107\"] cluster.initial_master_nodes: [\"192.168.10.106\"] # 开启跨域访问支持，默认为false http.cors.enabled: true # 跨域访问允许的域名地址，(允许所有域名)以上使用正则 http.cors.allow-origin: \"*\" 注意： 对于 node 主机只需要更改一处 node.name 即可 如果是重新还原es集群，启动前将 path.data 和 path.logs 目录下的数据清空，避免数据不一致 重启服务 重启es服务 systemctl start elasticsearch 检查效果 # netstat -tnulp | egrep 'Add|java' Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp6 0 0 :::9300 :::* LISTEN 12004/java tcp6 0 0 :::9200 :::* LISTEN 12004/java 结果显示 默认elasticsearch开启了两个端口， 9200 elasticsearch对外提供服务的接口 9300 集群节点间的通信端口 先启动 9300，然后再开启9200 常用地址 查看集群状态\rcurl -XGET 192.168.10.106:9200/_cat/health\rcurl -XGET 192.168.10.107:9200/_cat/health?v\r注意：在所有的地址后面，增加 ? 和 ?v ，会逐渐显示更多的详细内容\r查看集群节点\rcurl -XGET 192.168.10.106:9200/_cat/nodes\r查看索引\rcurl -XGET 192.168.10.106:9200/_cat/indices\r创建索引\rcurl -XPUT 192.168.10.106:9200/index_test\rcurl -XGET 192.168.10.106:9200/_cat/indices\r格式化展示\rcurl 192.168.8.12:9200/index_test?pretty\r删除索引\rcurl -XDELETE 192.168.10.106:9200/index_test\rcurl -XGET 192.168.10.106:9200/_cat/indices\r批量删除\rcurl -s http://192.168.10.106:9200/_cat/indices | awk '{print $3}'\rfor index in $(curl -s http://192.168.10.106:9200/_cat/indices | awk '{print\r$3}')\rdo\rcurl -XDELETE 192.168.10.106:9200/$index\rdone\r修改切片属性\rcurl -X PUT 192.168.10.106:9200/index_test -H 'Content-Type:application/json' -d'\r{\r\"settings\" : {\r\"number_of_shards\" : 3,\r\"number_of_replicas\" : 1\r}\r}'\rcurl 192.168.10.107:9200/index_test?pretty\r注意：\r在设置切片属性的时候，一定要注意在历史数据中，最好不要存在同名的索引，否则报错 ","date":"2021-10-02","objectID":"/posts/elk/elk-elasticsearch/:4:3","tags":["日志收集"],"title":"ELK日志收集系统 （一）","uri":"/posts/elk/elk-elasticsearch/"},{"categories":["ElasticStack"],"content":"功能插件 插件管理 elasticsearch最擅长的场景就是索引的操作，而索引的使用场景在不同的公司非常不同，所以\relasticsearch的索引场景可以基于不同的插件来实现对应的功能，而插件也是ELK非常重要的属性。\relasticsearch提供了两种插件的管理方式：\r- 专用的插件管理可以使用命令 elasticsearch-plugin，它可以对默认的插件进行管理；\r- 定制的插件管理可以基于离线方式安装插件。\r常见插件：\r分词插件：analysis-icu、analysis-ik、等\r管理插件：head、kopf、bigdest、等\r注意：\r随着elasticsearch的版本更新，很多之前可以直接安装的插件，目前无法直接安装了，\r需要采取自定义的方式来安装\r旧的插件地址\rhttps://www.elastic.co/guide/en/elasticsearch/plugins/2.4/management.html\r官方地址\rhttps://www.elastic.co/guide/en/elasticsearch/plugins/current/index.html 安装插件命令 在线方式：\relasticsearch-plugin install plugin_name/version\r离线方式：\r方法1：elasticsearch-plugin install file:///path/to/plugin.zip\r方法2：将下载的插件解压到elasticsearch的plugins目录即可\r查看已安装插件\relasticsearch-plugin list\r删除插件\relasticsearch-plugin remove [plugin_name]\r注意：\r删除插件的时候，推荐先关闭结点，然后再关闭。安装默认的插件 安装中文语法分析后重启服务\relasticsearch-plugin install analysis-smartcn\relasticsearch-plugin install analysis-icu\rsystemctl restart elasticsearch.service\r检查效果\r# elasticsearch-plugin list\ranalysis-icu\ranalysis-smartcn\r# ls /usr/share/elasticsearch/\rbin jdk lib modules NOTICE.txt plugins README.asciidoc\r# ls /usr/share/elasticsearch/plugins/\ranalysis-icu analysis-smartcn\r简单测试\r# curl -X POST 'http://192.168.10.106_analyze?pretty=true' -H 'content-type:\rapplication/json' -d '{\r\"analyzer\": \"icu_analyzer\",\r\"text\": \"中华人民共和国国歌\"\r}'\r---- 下面是显示的内容\r{\r\"tokens\" : [\r{\r\"token\" : \"中华\",\r\"start_offset\" : 0,\r\"end_offset\" : 2,\r\"type\" : \"\u003cIDEOGRAPHIC\u003e\",\r\"position\" : 0\r},\r{\r\"token\" : \"人民\",\r\"start_offset\" : 2,\r\"end_offset\" : 4,\r\"type\" : \"\u003cIDEOGRAPHIC\u003e\",\r\"position\" : 1\r},\r{\r\"token\" : \"共和国\",\r\"start_offset\" : 4,\rhead插件安装\r\"end_offset\" : 7,\r\"type\" : \"\u003cIDEOGRAPHIC\u003e\",\r\"position\" : 2\r},\r{\r\"token\" : \"国歌\",\r\"start_offset\" : 7,\r\"end_offset\" : 9,\r\"type\" : \"\u003cIDEOGRAPHIC\u003e\",\r\"position\" : 3\r}\r]\r}\r# curl -X POST 'http://192.168.10.106:9200/_analyze?pretty=true' -H 'content-type:\rapplication/json' -d '{\r\"analyzer\": \"smartcn\",\r\"text\": \"中华人民共和国国歌\"\r}'\r---- 下面是显示的内容\r{\r\"tokens\" : [\r{\r\"token\" : \"中华人民共和国\",\r\"start_offset\" : 0,\r\"end_offset\" : 7,\r\"type\" : \"word\",\r\"position\" : 0\r},\r{\r\"token\" : \"国歌\",\r\"start_offset\" : 7,\r\"end_offset\" : 9,\r\"type\" : \"word\",\r\"position\" : 1\r}\r]\r} ","date":"2021-10-02","objectID":"/posts/elk/elk-elasticsearch/:4:4","tags":["日志收集"],"title":"ELK日志收集系统 （一）","uri":"/posts/elk/elk-elasticsearch/"},{"categories":["ElasticStack"],"content":"head插件安装 简介\rhead插件，在elasticsearch中，应用的还算可以，但是自动2.x之后的版本，就不再支持了，需要我们自己来进行独立的部署。\r代码资料：https://github.com/mobz/elasticsearch-head\r准备工作\rapt install npm git -y\r安装插件\rmkdir /data/server/elasticsearch/plugins -p\rcd /data/server/elasticsearch/plugins\rgit clone git://github.com/mobz/elasticsearch-head.git\rcd elasticsearch-head\rnpm config set registry https://registry.npm.taobao.org\rnpm install --force\r配置软件的访问地址\r# vim Gruntfile.js\r...\rconnect: {\rserver: {\roptions: {\rhostname: '*', # 增加这一行\rport: 9100,\rbase: '.',\rkeepalive: true\r}\r}\r}\r...\r配置插件访问es\r# vim _site/app.js\r...\r4372 this._super();\r4373 this.prefs = services.Preferences.instance();\r4374 this.base_uri = this.config.base_uri ||\rthis.prefs.get(\"app-base_uri\") || \" http://192.168.10.106:9200\"; # 修改连接es的连接地址\r...\r启动服务\rnpm run start \u003e\u003e/dev/null 2\u003e\u00261 \u0026\r检查效果\r# netstat -tnulp | egrep 'Add|grunt'\rProto Recv-Q Send-Q Local Address Foreign Address State\rPID/Program name\rtcp6 0 0 :::9100 :::* LISTEN\r19818/grunt head插件图 属性解析：\r一定要先保证连接按钮左侧的es地址是正确的，然后再来点击\"连接\"\r* 代表索引es的主节点，黑色加粗的方框0表示，索引的主副本。\r黄色代表有从分片丢失，但是没有主分片数据丢失，即当前没有数据丢失。\r红色代表主分配丢失，即有数据丢失；绿色代表所有主分片和从分片的数据正常 服务启动脚本 启动脚本 # cat /data/scripts/elasticsearch_head.sh\r#!/bin/bash\r# 启动elasticsearch 脚本\rcd /data/server/elasticsearch/plugins/elasticsearch-head\rnpm run start \u003e\u003e/dev/null 2\u003e\u00261\r# vim /lib/systemd/system/elasticsearch_head.service\r[Unit]\rDescription= elasticsearch head server project\r[Service]\rUser=root\rExecStart=/bin/bash /data/scripts/elasticsearch_head.sh\rTimeoutStopSec=10\rRestart=on-failure\rRestartSec=5\r[Install]\rWantedBy=multi-user.target\r启动服务\rsystemctl daemon-reload\rsystemctl start elasticsearch_head.service\rsystemctl status elasticsearch_head.service 小结 elasticsearch 几乎所有的特色功能都是以插件的样式来整合的\relasticsearch-plugin install | remove | list\r默认情况下，这些插件安装在了 /usr/share/elasticsearch/plugins\r这些插件安装完毕后，需要重新启动环境，才会生效\r关键插件\rhead插件，是需要独立部署的，可以不安装到 plugins","date":"2021-10-02","objectID":"/posts/elk/elk-elasticsearch/:4:5","tags":["日志收集"],"title":"ELK日志收集系统 （一）","uri":"/posts/elk/elk-elasticsearch/"},{"categories":["ElasticStack"],"content":"轻量型数据采集器 Beats 是一个免费且开放的平台，集合了多种单一用途数据采集器。它们从成百上千或成千上万台机器和系统向 Logstash 或 Elasticsearch 发送数据。Beats 是数据采集的得力工具。将 Beats 和您的容器一起置于服务器上，或者将 Beats 作为功能加以部署，然后便可在 Elasticsearch 中集中处理数据。Beats 能够采集符合 Elastic Common Schema (ECS) 要求的数据，如果您希望拥有更加强大的处理能力，Beats 能够将数据转发至 Logstash 进行转换和解析。 ","date":"2021-10-02","objectID":"/posts/elk/elk-filebeat/:0:0","tags":["日志收集"],"title":"ELK组件-beats （二）","uri":"/posts/elk/elk-filebeat/"},{"categories":["ElasticStack"],"content":"基础知识 ","date":"2021-10-02","objectID":"/posts/elk/elk-filebeat/:1:0","tags":["日志收集"],"title":"ELK组件-beats （二）","uri":"/posts/elk/elk-filebeat/"},{"categories":["ElasticStack"],"content":"功能简介 根据我们对ELK的经典架构的了解，他的数据收集和处理流程是：beats - logstash - elasticsearch - kibana。Beats 默认提供了很多中场景的组件，最常见的就是FileBeat 运行环境 安装java8环境\rapt install openjdk-8-jdk\r检查效果\rjava -version ","date":"2021-10-02","objectID":"/posts/elk/elk-filebeat/:1:1","tags":["日志收集"],"title":"ELK组件-beats （二）","uri":"/posts/elk/elk-filebeat/"},{"categories":["ElasticStack"],"content":"软件安装 apt源码方式 获取软件源 wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add - apt install apt-transport-https echo \"deb https://artifacts.elastic.co/packages/7.x/apt stable main\" | sudo tee –a /etc/apt/sources.list.d/elastic-7.x.list apt update 安装软件 apt install filebeat 软件包安装 wget https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.14.0- amd64.deb wget https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.14.0- amd64.deb.sha512 shasum -a 512 -c filebeat-7.14.0-amd64.deb.sha512 dpkg -i filebeat-7.14.0-amd64.deb 配置查看 查看配置文件 # dpkg -L filebeat /. /etc /etc/init.d /etc/init.d/filebeat /etc/filebeat /etc/filebeat/filebeat.yml 核心配置文件 ... /etc/filebeat/filebeat.reference.yml /etc/filebeat/fields.yml ... /usr/share/filebeat/bin /usr/share/filebeat/bin/filebeat /usr/share/filebeat/bin/filebeat-god /usr/share/doc /usr/share/doc/filebeat /usr/share/doc/filebeat/changelog.gz /usr/bin /usr/bin/filebeat /lib /lib/systemd /lib/systemd/system /lib/systemd/system/filebeat.service 服务启动文件查看配置文件 # grep -Env '#|^$' /etc/filebeat/filebeat.yml 15:filebeat.inputs: 数据的采集 21:- type: log 24: enabled: false 默认该功能没有开启 27: paths: 28: - /var/log/*.log 66:- type: filestream 69: enabled: false 72: paths: 73: - /var/log/*.log 96:filebeat.config.modules: 98: path: ${path.config}/modules.d/*.yml 101: reload.enabled: false 108:setup.template.settings: 109: index.number_of_shards: 1 默认的数据分片个数是 1 145:setup.kibana: 176:output.elasticsearch: 数据的输出 178: hosts: [\"localhost:9200\"] 204:processors: 205: - add_host_metadata: 206: when.not.contains.tag: forwarded 207: - add_cloud_metadata: ~ 208: - add_docker_metadata: ~ 209: - add_kubernetes_metadata: ~ 结果显示： filebeat.yml 这就是filebeat的配置文件，里面有12部分的配置，而我们重点关心的就 是\"Filebeat inputs\" 和 \"Outputs\", 定制环境变量 echo 'export PATH=/usr/share/kibana/bin:$PATH' \u003e /etc/profile.d/kibana.sh source /etc/profile.d/kibana.sh ","date":"2021-10-02","objectID":"/posts/elk/elk-filebeat/:1:2","tags":["日志收集"],"title":"ELK组件-beats （二）","uri":"/posts/elk/elk-filebeat/"},{"categories":["ElasticStack"],"content":"简单实践 定制配置文件 备份配置文件 cd /etc/filebeat/ cp filebeat.yml filebeat.yml-bak 定制配置文件 filebeat.inputs: - type: log paths: - /var/log/syslog setup.template.settings: index.number_of_shards: 5 output.elasticsearch: hosts: [\"192.168.8.12:9200\"] template.name: \"filebeat\" 属性解析： enabled: true 表示启用这条配置 template.name 在将数据传入到elasticsearch的时候，自动添加一个索引，名称是filebeat 启动服务 启动服务 systemctl start filebeat.service systemctl status filebeat.service 查看效果 # curl 192.168.8.12:9200/_cat/indices green open filebeat-7.14.0-2021.08.15-000001 yTq8KQtGSpOyGohS4kcLhQ 5 1 730 0 348.5kb 587b 结果显示： 在elasticsearch中多了好几条索引数据 索引命名格式：\"自定义索引名-版本号-日期-6位编号\" 小结： 核心的配置 input output modules elasticsearch …. 实践 只获取指定文件内部包含 404 的文件内容 输出的时候，设定索引名称 要点 如果需要filebeat 定制es的索引名称的话，需要自己设定模板 ","date":"2021-10-02","objectID":"/posts/elk/elk-filebeat/:1:3","tags":["日志收集"],"title":"ELK组件-beats （二）","uri":"/posts/elk/elk-filebeat/"},{"categories":["ElasticStack"],"content":"核心交换机数据引流 ","date":"2021-09-29","objectID":"/posts/elk/h3c-mirroring-port/:0:0","tags":["日志收集","H3C"],"title":"H3C-7506E核心交换机数据引流","uri":"/posts/elk/h3c-mirroring-port/"},{"categories":["ElasticStack"],"content":"镜像源 镜像源是指被监控的对象，该对象可以是端口或单板上的 CPU，我们将之依次称为源端口和源 CPU。 经由被监控的对象收发的报文会被复制一份到与数据监测设备相连的端口，用户就可以对这些报文 （称为镜像报文）进行监控和分析了。镜像源所在的设备就称为源设备。 ","date":"2021-09-29","objectID":"/posts/elk/h3c-mirroring-port/:0:1","tags":["日志收集","H3C"],"title":"H3C-7506E核心交换机数据引流","uri":"/posts/elk/h3c-mirroring-port/"},{"categories":["ElasticStack"],"content":"镜像目的 镜像目的是指镜像报文所要到达的目的地，即与数据监测设备相连的那个端口，我们称之为目的端口，目的端口所在的设备就称为目的设备。目的端口会将镜像报文转发给与之相连的数据监测设备。 由于一个目的端口可以同时监控多个镜像源，因此在某些组网环境下，目的端口可能收到对同一报 文的多份拷贝。例如，目的端口 Port 1 同时监控同一台设备上的源端口 Port 2 和 Port 3 收发的报文， 如果某报文从 Port 2 进入该设备后又从 Port 3 发送出去，那么该报文将被复制两次给 Port 1。 ","date":"2021-09-29","objectID":"/posts/elk/h3c-mirroring-port/:0:2","tags":["日志收集","H3C"],"title":"H3C-7506E核心交换机数据引流","uri":"/posts/elk/h3c-mirroring-port/"},{"categories":["ElasticStack"],"content":"镜像方向 镜像方向是指在镜像源上可复制哪些方向的报文： 入方向：是指仅复制镜像源收到的报文。 出方向：是指仅复制镜像源发出的报文。 双向：是指对镜像源收到和发出的报文都进行复制。 ","date":"2021-09-29","objectID":"/posts/elk/h3c-mirroring-port/:0:3","tags":["日志收集","H3C"],"title":"H3C-7506E核心交换机数据引流","uri":"/posts/elk/h3c-mirroring-port/"},{"categories":["ElasticStack"],"content":"镜像组 镜像组是一个逻辑上的概念，镜像源和镜像目的都要属于某一个镜像组。根据具体的实现方式不同， 镜像组可分为本地镜像组、远程源镜像组和远程目的镜像组三类。 ","date":"2021-09-29","objectID":"/posts/elk/h3c-mirroring-port/:0:4","tags":["日志收集","H3C"],"title":"H3C-7506E核心交换机数据引流","uri":"/posts/elk/h3c-mirroring-port/"},{"categories":["ElasticStack"],"content":"反射端口、出端口和远程镜像VLAN 反射端口、出端口和远程镜像VLAN都是在二层远程端口镜像的实现过程中用到的概念。远程镜像 VLAN是将镜像报文从源设备传送至目的设备的专用VLAN；反射端口和出端口都位于源设备上，都 用来将镜像报文发送到远程镜像VLAN中。 ","date":"2021-09-29","objectID":"/posts/elk/h3c-mirroring-port/:0:5","tags":["日志收集","H3C"],"title":"H3C-7506E核心交换机数据引流","uri":"/posts/elk/h3c-mirroring-port/"},{"categories":["ElasticStack"],"content":"端口镜像的分类和实现方式 根据镜像源与镜像目的是否位于同一台设备上，可以将端口镜像分为本地端口镜像和远程端口镜像 两大类。 本地端口镜像 当源设备与数据监测设备直接相连时，源设备可以同时作为目的设备，即由本设备将镜像报文转发 至数据检测设备，这种方式实现的端口镜像称为本地端口镜像。对于本地端口镜像，镜像源和镜像 目的属于同一台设备上的同一个镜像组，该镜像组称为本地镜像组。 远程端口镜像 当源设备与数据监测设备不直接相连时，与数据监测设备直接相连的设备作为目的设备，源设备需 要将镜像报文复制一份至目的设备，然后由目的设备将镜像报文转发至数据监测设备，这种方式实 现的端口镜像称为远程端口镜像。对于远程端口镜像，镜像源和镜像目的分属于不同设备上的不同 镜像组：镜像源所在的镜像组称为远程源镜像组，镜像目的所在的镜像组称为远程目的镜像组，而 位于源设备与目的设备之间的设备则统称为中间设备。 根据源设备与目的设备之间的连接关系，又可将远程端口镜像细分为： • 二层远程端口镜像：源设备与目的设备之间通过二层网络进行连接。 • 三层远程端口镜像：源设备与目的设备之间通过三层网络进行连接。 ","date":"2021-09-29","objectID":"/posts/elk/h3c-mirroring-port/:0:6","tags":["日志收集","H3C"],"title":"H3C-7506E核心交换机数据引流","uri":"/posts/elk/h3c-mirroring-port/"},{"categories":["ElasticStack"],"content":"二层远程端口镜像 二层远程端口镜像的实现方式包括：反射端口方式和出端口方式。 • 反射端口方式： 源设备将进入源端口（或源 CPU）的报文复制一份给反射端口，再由该端口 将镜像报文在远程镜像 VLAN 中广播，最终镜像报文经由中间设备转发至目的设备。目的设 1-3 备收到该报文后判别其 VLAN ID，若与远程镜像 VLAN 的 VLAN ID 相同，就将镜像报文通过 目的端口转发给数据监测设备。过程如下图所示。 出端口方式： 源设备将进入源端口（或源 CPU）的报文复制一份给出端口，该端口将镜像报 文转发给中间设备，再由中间设备在远程镜像 VLAN 中广播，最终到达目的设备。目的设备 收到该报文后判别其 VLAN ID，若与远程镜像 VLAN 的 VLAN ID 相同，就将镜像报文通过目 的端口转发给数据监测设备。过程如下图所示。 ","date":"2021-09-29","objectID":"/posts/elk/h3c-mirroring-port/:0:7","tags":["日志收集","H3C"],"title":"H3C-7506E核心交换机数据引流","uri":"/posts/elk/h3c-mirroring-port/"},{"categories":["ElasticStack"],"content":"配置实战 现有两套态势感知设备需要接入内部网络，需要将办公网、内外网业务流量进行收集汇总后将流量复制一份到与态势感知设备。 对接设备： H3C框式 核心交换机 奇安信 天眼新一代威胁感知系统 深信服 安全感知平台SIP 核心交换机配置 本地端口镜像方式 mirroring-group 1 local mirroring-group 1 mirroring-port g1/1/0/1 to g1/1/0/8 both mirroring-group 1 monitor-port g1/1/0/10 //引g1/1/0/1到8口的流量到 g1/1/0/10 dis mirroring-group all Mirroring group 1: Type: local Status: Active Mirroring port: GigabitEthernet1/1/0/1 Both GigabitEthernet1/1/0/2 Both GigabitEthernet1/1/0/3 Both GigabitEthernet1/1/0/4 Both GigabitEthernet1/1/0/5 Both GigabitEthernet1/1/0/6 Both GigabitEthernet1/1/0/7 Both GigabitEthernet1/1/0/8 Both Monitor-port: GigabitEthernet1/1/0/10 远程端口镜像方式 mirroring-group 2 remote-source mirroring-group 2 mirroring-port g2/2/0/1 to g2/2/0/8 both mirroring-group 2 reflector-port g2/2/0/10 //反射端口 vlan 130 //镜像数据广播VLAN port g2/2/0/11 to g2/2/0/12 //审计设备互联接口 mirroring-group 2 remote-probe vlan 130 dis mirroring-group all Mirroring group 1: Type: Remote source Status: Active Mirroring port: GigabitEthernet2/2/0/1 Both GigabitEthernet2/2/0/2 Both GigabitEthernet2/2/0/3 Both GigabitEthernet2/2/0/4 Both GigabitEthernet2/2/0/5 Both GigabitEthernet2/2/0/6 Both GigabitEthernet2/2/0/7 Both GigabitEthernet2/2/0/8 Both Reflector port: GigabitEthernet1/4/0/10 Remote probe VLAN: 130 设备接口调整 ","date":"2021-09-29","objectID":"/posts/elk/h3c-mirroring-port/:0:8","tags":["日志收集","H3C"],"title":"H3C-7506E核心交换机数据引流","uri":"/posts/elk/h3c-mirroring-port/"},{"categories":["ElasticStack"],"content":"效果验证 数据成功引入 ","date":"2021-09-29","objectID":"/posts/elk/h3c-mirroring-port/:0:9","tags":["日志收集","H3C"],"title":"H3C-7506E核心交换机数据引流","uri":"/posts/elk/h3c-mirroring-port/"},{"categories":["小记"],"content":"Linux命令行提示符默认是白色，很多时候不太方便查看命令和记录，而且默认是显示的完整路径，如果路径过 长，开发和看起来都不方便，所以改变一下终端命令行颜色和格式。 PS（Prompt Sign）命令提示符，PS1是Linux终端用户的一个环境变量，用来定义命令行提示符的参数。 在设定PS1环境变量时，需要用到预设的一些参数来设定PS1。 ","date":"2020-10-04","objectID":"/posts/notes/beautify-terminal/:0:0","tags":["Terminal"],"title":"美化终端命令行显示","uri":"/posts/notes/beautify-terminal/"},{"categories":["小记"],"content":"PS1的常⽤参数以及含义： \\d ：代表日期，格式为weekday month date，例如：“Mon Aug 1” \\H ：完整的主机名称 \\h ：仅取主机名中的第一个名字 \\t ：显示时间为24小时格式，如：HH：MM：SS \\T ：显示时间为12小时格式 \\A ：显示时间为24小时格式：HH：MM \\u ：当前用户的账号名称 \\v ：BASH的版本信息 \\w ：完整的工作目录名称 \\W ：利用basename取得工作目录名称，只显示最后一个目录名 # ：下达的第几个命令 $ ：提示字符，如果是root用户，提示符为 # ，普通用户则为 $ ","date":"2020-10-04","objectID":"/posts/notes/beautify-terminal/:1:0","tags":["Terminal"],"title":"美化终端命令行显示","uri":"/posts/notes/beautify-terminal/"},{"categories":["小记"],"content":"查看当前PS1的设置： echo $PS1 '\\u@\\h:\\w\\$' #含义： #[当前用户名@主机名:完整工作目录 ]$","date":"2020-10-04","objectID":"/posts/notes/beautify-terminal/:2:0","tags":["Terminal"],"title":"美化终端命令行显示","uri":"/posts/notes/beautify-terminal/"},{"categories":["小记"],"content":"设置PS1 样式 当前用户的 .bashrc 环境配置中，在底部添加PS1并赋值： vim ~/.bashrc PS1=\"\\u@\\h:\\W\\$\" export PS1 source ~/.bashrc","date":"2020-10-04","objectID":"/posts/notes/beautify-terminal/:3:0","tags":["Terminal"],"title":"美化终端命令行显示","uri":"/posts/notes/beautify-terminal/"},{"categories":["小记"],"content":"颜色设置参数 \\[\\e[F;Bm\\].....\\[\\e[0m\\] 或者 \\[\\033[F;Bm\\].....\\[\\033[0m\\] 其中“F”为字体颜色，编号为30-37，“B”为背景颜色，编号为40-47，\\[\\e[0m\\] 结束颜色设定。 “B”还可以设置其他格式，例如为1时，将显示加亮加粗的文字，详见下表 F 字体颜⾊ B背景颜⾊ 颜⾊ 其他格式 30 40 ⿊⾊ 代码 含义 31 41 红⾊ 0 OFF 32 42 绿⾊ 1 ⾼亮显⽰ 33 43 黄⾊ 4 underline 34 44 蓝⾊ 5 闪烁 35 45 紫红⾊ 7 反⽩显⽰ 36 46 青蓝⾊ 8 不可⻅ 37 47 ⽩⾊ ","date":"2020-10-04","objectID":"/posts/notes/beautify-terminal/:4:0","tags":["Terminal"],"title":"美化终端命令行显示","uri":"/posts/notes/beautify-terminal/"},{"categories":["小记"],"content":"样式1 export PS1='\\[\\033[01;31m\\]\\u\\[\\033[00m\\]@\\[\\033[01;32m\\]\\h\\[\\033[00m\\][\\[\\033[01;34m\\]\\t\\[\\033[00m\\]]\\[\\033[01;36m\\]\\w\\[\\033[00m\\] \\[\\033[01;33m\\]\\$\\[\\033[00m\\]:' ","date":"2020-10-04","objectID":"/posts/notes/beautify-terminal/:5:0","tags":["Terminal"],"title":"美化终端命令行显示","uri":"/posts/notes/beautify-terminal/"},{"categories":["数据库"],"content":"Redis Sentinel（哨兵） ","date":"2020-08-07","objectID":"/posts/redis/redis-7/:0:0","tags":["Redis"],"title":"Redis Sentinel（哨兵） （七）","uri":"/posts/redis/redis-7/"},{"categories":["数据库"],"content":"简介 主从复制奠定了Redis分布式的基础，但是普通的主从复制并不能达到高可用的状态。在普通的主从复制模式下，如果主服务器宕机，就只能通过运维人员手动切换主服务器，很显然这种方案并不可取。 针对上述情况，Redis官方推出了可抵抗节点故障的高可用方案——Redis Sentinel（哨兵）。Redis Sentinel（哨兵）：由一个或多个Sentinel实例组成的Sentinel系统，它可以监视任意多个主从服务器，当监视的主服务器宕机时，自动下线主服务器，并且择优选取从服务器升级为新的主服务器。 Sentinel 进程是用于监控redis集群中Master主服务器工作的状态，在Master主服务器发生故障的时候，可以实现Master和Slave服务器的切换，保证系统的高可用，其已经被集成在redis2.6+的版本中，Redis的哨兵模式到了2.8版本之后就稳定了下来。一般在生产环境也建议使用Redis的2.8版本的以后版本。 哨兵(Sentinel) 是一个分布式系统，可以在一个架构中运行多个哨兵(sentinel) 进程，这些进程使用流言协议(gossip protocols)来接收关于Master主服务器是否下线的信息，并使用投票协议(Agreement Protocols)来决定是否执行自动故障迁移,以及选择哪个Slave作为新的Master。每个哨兵(Sentinel)进程会向其它哨兵(Sentinel)、Master、Slave定时发送消息，以确认对方是否”活”着，如果发现对方在指定配置时间(可配置的)内未得到回应，则暂时认为对方已掉线，也就是所谓的”主观认为宕机” ，主观是每个成员都具有的独自的而且可能相同也可能不同的意识，英文名称：Subjective Down，简称SDOWN。有主观宕机，肯定就有客观宕机。 当“哨兵群”中的多数Sentinel进程在对Master主服务器做出SDOWN 的判断，并且通过 SENTINEL is-master-down-by-addr 命令互相交流之后，得出的Master Server下线判断，这种方式就是“客观宕机”，客观是不依赖于某种意识而已经实际存在的一切事物，英文名称是：ObjectivelyDown， 简称 ODOWN。通过一定的vote算法，从剩下的slave从服务器节点中，选一台提升为Master服务器节点，然后自动修改相关配置，并开启故障转移（failover）。 Sentinel 机制可以解决master和slave角色的切换问题。 Sentinel 组件 Monitoring : 监控Master节点、Slave节点以及其他Sentine节点是否正常工作。 Automatic failover: 当Master故障时，Sentinel会选择一个Slave节点来替换当前Master， 并配置其他的Slave节点成为新的Master节点的从节点。同时Sentinel会 通知客户端新的Master的地址。 Notification: 当监控的集群节点出故障时，Sentinel可通过执行特定脚本或者订阅来 告知系统管理员或者其他应用程序来通知相应信息。 Configuration provider：如果 从redis 仅仅是断开了 主redis，那么不会删除已经同步过的数据。 Sentinel 流程：当旧Master下线时长超过用户设定的下线时长上限，Sentinel系统就会对旧Master执行故障转移操作，故障转移操作包含三个步骤： 在Slave中选择数据最新的作为新的Master 向其他Slave发送新的复制指令，让其他从服务器成为新的Master的Slave 继续监视旧Master，如果其上线则将旧Master设置为新Master的Slave ","date":"2020-08-07","objectID":"/posts/redis/redis-7/:0:1","tags":["Redis"],"title":"Redis Sentinel（哨兵） （七）","uri":"/posts/redis/redis-7/"},{"categories":["数据库"],"content":"实践操作 实践要点： 准备主从环境 定制 sentinel 配置 配置主从环境 1 bind 开放本机的ip\r2 replicaof 指定主角色\r修改redis.conf 文件中\rrelicaof \u003cmasterip\u003e \u003cmsterport\u003e\rrelicaof 192.168.10.110 6379 配置Sentinel 节点 cp /data/softs/redis-6.2.5/sentinel.conf /data/server/redis/etc/ vim /data/server/redis/etc/sentinel.conf 节点1 bind 192.168.10.110 #注意去掉127的地址发生冲突 port 26379 daemonize yes pidfile /data/server/redis/run/redis-sentinel.pid logfile \"/data/server/redis/log/redis-sentinel.log\" #定制log路径 dir \"/data/server/redis/data\" #定制数据文件路径 sentinel monitor mymaster 192.168.10.110 6379 2 #法定人数限制(quorum)，即有几个slave认为master down了就进行故障转移 sentinel down-after-milliseconds mymaster 3000 #(SDOWN)主观下线的时间 acllog-max-len 128 sentinel parallel-syncs mymaster 1 #发生故障转移时候同时向新master同步数据的slave数量，数字越小总同步时间越长 sentinel failover-timeout mymaster 3000 #所有slaves指向新的master所需的超时时间 sentinel deny-scripts-reconfig yes #禁止修改脚本 SENTINEL resolve-hostnames no SENTINEL announce-hostnames no 节点2 bind 192.168.10.113 port 26379 daemonize yes pidfile \"/data/server/redis/run/redis-sentinel.pid\" logfile \"/data/server/redis/log/redis-sentinel.log\" dir \"/data/server/redis/data\" sentinel deny-scripts-reconfig yes sentinel monitor mymaster 192.168.10.110 6379 2 节点3 bind 192.168.10.114 port 26379 daemonize yes pidfile \"/data/server/redis/run/redis-sentinel.pid\" logfile \"/data/server/redis/log/redis-sentinel.log\" dir \"/data/server/redis/data\" sentinel deny-scripts-reconfig yes sentinel monitor mymaster 192.168.10.110 6379 2启动节点 #启动redis root@redis-master:redis-server /data/server/redis/etc/redis.conf root@redis-slave1：redis-server /data/server/redis/etc/redis.conf root@redis-slave2：redis-server /data/server/redis/etc/redis.conf #启动哨兵 root@redis-master:redis-sentinel /data/server/redis/etc/sentinel.conf root@redis-slave1：redis-sentinel /data/server/redis/etc/sentinel.conf root@redis-slave2：redis-sentinel /data/server/redis/etc/sentinel.conf查看集群状态 节点2 redis-cli -h 192.168.10.113 -p 6379 info Replication # Replication role:master connected_slaves:2 slave0:ip=192.168.10.110,port=6379,state=online,offset=242697,lag=0 slave1:ip=192.168.10.114,port=6379,state=online,offset=242550,lag=1 master_failover_state:no-failover master_replid:d19f968c80d72a23ca373943dc166942a125758a master_replid2:0000000000000000000000000000000000000000 master_repl_offset:242697 second_repl_offset:-1 repl_backlog_active:1 repl_backlog_size:1048576 repl_backlog_first_byte_offset:61752 repl_backlog_histlen:180946 节点3 redis-cli -h 192.168.10.114 -p 6379 info Replication # Replication role:slave master_host:192.168.10.113 master_port:6379 master_link_status:up master_last_io_seconds_ago:0 master_sync_in_progress:0 slave_repl_offset:251426 slave_priority:100 slave_read_only:1 replica_announced:1 connected_slaves:0 master_failover_state:no-failover master_replid:d19f968c80d72a23ca373943dc166942a125758a master_replid2:0000000000000000000000000000000000000000 master_repl_offset:251426 second_repl_offset:-1 repl_backlog_active:1 repl_backlog_size:1048576 repl_backlog_first_byte_offset:174421 repl_backlog_histlen:77006 节点1 redis-cli -h 192.168.10.110 -p 6379 info Replication # Replication role:slave master_host:192.168.10.113 master_port:6379 master_link_status:up master_last_io_seconds_ago:1 master_sync_in_progress:0 slave_repl_offset:254674 slave_priority:100 slave_read_only:1 replica_announced:1 connected_slaves:0 master_failover_state:no-failover master_replid:d19f968c80d72a23ca373943dc166942a125758a master_replid2:0000000000000000000000000000000000000000 master_repl_offset:254674 second_repl_offset:-1 repl_backlog_active:1 repl_backlog_size:1048576 repl_backlog_first_byte_offset:62216 repl_backlog_histlen:192459故障切换效果 redis-cli -h 192.168.10.110 -p 6379 info Replication # Replication role:master connected_slaves:2 #slave 节点 slave0:ip=192.168.10.113,port=6379,state=online,offset=361163,lag=1 slave1:ip=192.168.10.114,port=6379,state=online,offset=361163,lag=1 master_failover_state:no-failover master_replid:c4b564add75141ccb26517b40194e48cc99e92e3 master_replid2:1f7ace","date":"2020-08-07","objectID":"/posts/redis/redis-7/:0:2","tags":["Redis"],"title":"Redis Sentinel（哨兵） （七）","uri":"/posts/redis/redis-7/"},{"categories":["数据库"],"content":"Redis 主从同步 redis 作为一个分布式的数据缓存平台，我们可以借助于redis的多实例机制，让多个实例间的数据， 达成一个同步的效果，这样即使某个实例出现异常，也不影响数据整体的使用。 ","date":"2020-08-06","objectID":"/posts/redis/redis-6/:0:0","tags":["Redis"],"title":"Redis 主从同步 （六）","uri":"/posts/redis/redis-6/"},{"categories":["数据库"],"content":"基础知识 复制特性 redis 如果想要实现主从复制的效果，我们需要为它划分 主角色和从角色，实现数据 由主向从的单向传递。 对于 从redis，一旦发现 主redis 更换了，那么将本地数据清空，从新主上同步数据。 如果 从redis 仅仅是断开了 主redis，那么不会删除已经同步过的数据。 实践要点 把多个主机节点，关联在一起让这些主机节点当中有一个主角色，专门负载数据的写、删除、更新 剩下的两个从节点只用于读数据。因为在大部分场景下都是读多写少。 所以通过从节点做到大范围的读操作，运行三台redis 通过一条命令把从库挂到主库上。 谁是我的老大？ 1 主角色redis 必须开启持久化功能\r2 从角色redis 指定谁是主，以及自己作为从的唯一标识。\rredis4.0之前用 slaveof\rredis4.0之后用 replicaof\r我是谁的副本 谁是我的老大\rrelicaof \u003cmasterip\u003e \u003cmsterport\u003e ","date":"2020-08-06","objectID":"/posts/redis/redis-6/:0:1","tags":["Redis"],"title":"Redis 主从同步 （六）","uri":"/posts/redis/redis-6/"},{"categories":["数据库"],"content":"实践操作 复制命令方式同步 127.0.0.1:6379\u003e help SLAVEOF\rSLAVEOF host port\rsummary: Make the server a replica of another instance, or promote it as\rmaster. Deprecated starting with Redis 5. Use REPLICAOF instead.\rsince: 1.0.0\rgroup: server\r127.0.0.1:6379\u003e help REPLICAOF\rREPLICAOF host port\rsummary: Make the server a replica of another instance, or promote it as\rmaster.\rsince: 5.0.0\rgroup: server\r注意：\r关闭复制关系可以通过 replicaof no one 命令默认情况下，任何一个redis实例启动时候，会自动将自己作为主角色而存在 127.0.0.1:6379\u003e info Replication # Replication role:master connected_slaves:0同步实践 #开启一个redis实例\rredis-server /data/server/redis/etc/redis.conf --port 6666 --daemonize yes\r#连接新实例查看效果\r# redis-cli -h 127.0.0.1 -p 6666\r\u003e info replication\r# Replication\rrole:master\rconnected_slaves:0\r...\r#新实例同步主角色\r# 设置主角色\r127.0.0.1:6666\u003e REPLICAOF 127.0.0.1 6379\rOK\r# 查看状态\r127.0.0.1:6666\u003e info replication\r# Replication\rrole:slave\rmaster_host:127.0.0.1\rmaster_port:6379\rmaster_link_status:up\rmaster_last_io_seconds_ago:3\rmaster_sync_in_progress:0\rslave_repl_offset:0\rslave_priority:100\r...\r# 查看同步效果\r127.0.0.1:6666\u003e KEYS *\r1) \"a1\"\r2) \"a3\"\r3) \"a2\"\r结果显示：\r数据同步成功效果验证 从角色只能查看数据，不能修改数据 127.0.0.1:6666\u003e FLUSHALL (error) READONLY You can't write against a read only replica. # 主角色删除数据 127.0.0.1:6379\u003e keys * 1) \"a2\" 2) \"a1\" 3) \"a3\" 127.0.0.1:6379\u003e DEL a1 (integer) 1 # 从角色查看效果 127.0.0.1:6666\u003e keys * 1) \"a3\" 2) \"a2\" 结果显示： 自动同步成功。 #同步状态查看 # 同步后主角色查看效果 127.0.0.1:6379\u003e info Replication # Replication role:master connected_slaves:1 slave0:ip=127.0.0.1,port=6666,state=online,offset=1038,lag=0 master_failover_state:no-failover master_replid:571bcaecc7eeb590326fc5a9262df569f3623b36 master_replid2:0000000000000000000000000000000000000000 master_repl_offset:1038 # 同步的偏移量 second_repl_offset:-1 repl_backlog_active:1 repl_backlog_size:1048576 repl_backlog_first_byte_offset:1 repl_backlog_histlen:1038 # 同步后从角色查看效果 127.0.0.1:6666\u003e info replication # Replication role:slave master_host:127.0.0.1 master_port:6379 master_link_status:up master_last_io_seconds_ago:10 master_sync_in_progress:0 slave_repl_offset:1038 # 同步的偏移量 slave_priority:100 slave_read_only:1 replica_announced:1 connected_slaves:0 master_failover_state:no-failover master_replid:571bcaecc7eeb590326fc5a9262df569f3623b36 master_replid2:0000000000000000000000000000000000000000 master_repl_offset:1038 second_repl_offset:-1 repl_backlog_active:1 repl_backlog_size:1048576 repl_backlog_first_byte_offset:1 repl_backlog_histlen:1038 配置文件方式主从同步 简介 对于主从同步来说， 主角色不用做任何配置 - 开放自己的怀抱即可 从角色需要做两个方面的配置 1 bind 开放本机的ip 2 replicaof 指定主角色 修改redis.conf 文件中 relicaof \u003cmasterip\u003e \u003cmsterport\u003e relicaof 主 IP 端口 实践 主角色 数据的增删改查 从角色 从主角色主机里获取数据 数据的查看 特点： 如果从角色主机故障，那么主角色主机中的从主机状态会自动消除 如果主角色主机故障，那么整个集群就崩溃了(相对于数据更改来说)","date":"2020-08-06","objectID":"/posts/redis/redis-6/:0:2","tags":["Redis"],"title":"Redis 主从同步 （六）","uri":"/posts/redis/redis-6/"},{"categories":["数据库"],"content":"Redis 持久复制 Redis虽然是一个内存级别的缓存程序，但是其可以将内存的数据按照一定的策略保存到硬盘上，从而实现数据持久保存的目的。 目前，redis支持两种不同方式的数据持久化保存机制： RDB 基于时间，生成某个时间点的快照文件，默认只保留最近的一次快照。 恢复速度非常快，但是可能丢失之前的快照数据，非实时同步。 AOF AppendOnlyFile(日志追加模式),基于Redis协议格式保存信息到指定日志文件的末尾 基于写时复制的机制，每隔x秒将新执行的命令同步到对应的文件中 默认是禁用的，需要开启数据保存全，时间过长导致文件过大，恢复时候速度比RDB慢。 数据保存时有两个命令， 实践效果，先执行一个bgsave把同步过后的文件拷贝到一个临时文件里面，然后再当前环境下做一些操作，把redis 关闭，然后把之前保存的文件拷贝到redis 数据目录，redis在启动的时候会读取db文件，由于这个文件是拷贝回来的所以还原的数据应该是没有执行操作之前的数据。 ","date":"2020-08-05","objectID":"/posts/redis/redis-5/:0:0","tags":["Redis"],"title":"Redis 持久复制 （五）","uri":"/posts/redis/redis-5/"},{"categories":["数据库"],"content":"RDB原理 Redis从master主进程中创建一个子进程，基于写时复制机制，子进程将内存的数据保存到.rdb文件中，数据保存完毕后，再将上次保存的rdb文件覆盖替换掉，最后关闭子进程。 Redis提供了手工的机制，我们可以执行命令实现文件的保存 ","date":"2020-08-05","objectID":"/posts/redis/redis-5/:0:1","tags":["Redis"],"title":"Redis 持久复制 （五）","uri":"/posts/redis/redis-5/"},{"categories":["数据库"],"content":"AOF 原理 AOF 以协议文本的方式，将所有对数据库进行过写入的命令（及其参数）记录到 AOF 文件，以此达到记录数据库状态的目的。 rdb 优势： - 基于数据的快照来进行存储 - 数据完整 - 策略非常灵活 劣势： - 数据量大的时候，快照文件也大 - bgsave的时候，会以覆盖的方式同步数据，有可能导致部分数据丢失 对于此我们可以借助于 定时备份的方式将数据额外保存 aof 优势： - 基于操作命令的方式进行数据的存储 - 容量非常小 劣势： - 对于基础的数据有一定的依赖 使用场景： rdb 做基础数据的备份 aof 做实时数据的备份","date":"2020-08-05","objectID":"/posts/redis/redis-5/:0:2","tags":["Redis"],"title":"Redis 持久复制 （五）","uri":"/posts/redis/redis-5/"},{"categories":["数据库"],"content":"简单实践 RDB 实践 RDB配置解析 自动保存机制 root@python-auto:~# grep -Env '#|^$' /etc/redis/redis.conf ... save '' 关闭该功能 381:save 3600 1 # 3600秒内提交一次数据 382:save 300 100 383:save 60 10000 398:stop-writes-on-bgsave-error yes 404:rdbcompression yes 413:rdbchecksum yes 431:dbfilename dump.rdb #保存文件名 444:rdb-del-sync-files no 454:dir /var/lib/redis #保存路径 RDB持久化命令 # 数据同步操作，执行时候，会导致其他命令无法执行 127.0.0.1:6379\u003e help SAVE SAVE - summary: Synchronously save the dataset to disk since: 1.0.0 group: server # 异步方式后台执行数据的同步，不影响其他命令的执行 127.0.0.1:6379\u003e help BGSAVE BGSAVE [SCHEDULE] summary: Asynchronously save the dataset to disk since: 1.0.0 group: server简单测试 # 执行备份前查看效果 root@python-auto:~# ll -h /var/lib/redis/*.rdb -rw-rw---- 1 redis redis 268 7月 28 18:31 /var/lib/redis/dump.rdb # 执行备份 127.0.0.1:6379\u003e bgsave Background saving started # 执行备份后查看效果 root@python-auto:~# ll -h /var/lib/redis/*.rdb -rw-rw---- 1 redis redis 268 7月 29 09:38 /var/lib/redis/dump.rdb # 备份文件 root@python-auto:/var/lib/redis# cp dump.rdb /tmp # 在做一些操作 127.0.0.1:6379\u003e set xxx xxx ... # 关闭redis systemctl stop redis 查看效果 root@python-auto:~# ll -h /var/lib/redis/*.rdb -rw-rw---- 1 redis redis 488 7月 29 09:41 /var/lib/redis/dump.rdb 还原配置文件 cp /tmp/dump.rdb ./ 启动redis systemctl start redis 查看效果 127.0.0.1:6379\u003e keys *AOF实践 配置解析 root@python-auto:~# grep -Env '#|^$' /etc/redis/redis.conf ... 1252:appendonly no 1256:appendfilename \"appendonly.aof\" 1282:appendfsync everysec 1304:no-appendfsync-on-rewrite no 1323:auto-aof-rewrite-percentage 100 1324:auto-aof-rewrite-min-size 64mb 1348:aof-load-truncated yes 1359:aof-use-rdb-preamble yes ... AOF持久化命令 # 数据同步操作，执行时候，会导致其他命令无法执行 127.0.0.1:6379\u003e help BGREWRITEAOF BGREWRITEAOF - summary: Asynchronously rewrite the append-only file since: 1.0.0 group: server 简单测试 # 检查现状 127.0.0.1:6379\u003e CONFIG GET appendonly 1) \"appendonly\" 2) \"no\" root@python-auto:~# ll -h /var/lib/redis/*.rdb -rw-rw---- 1 redis redis 92 7月 29 10:06 /var/lib/redis/dump.rdb # 修改持久化模式 127.0.0.1:6379\u003e CONFIG SET appendonly yes OK # 确认效果 root@python-auto:~# ll -h /var/lib/redis/ 总用量 16K -rw-rw---- 1 redis redis 92 7月 29 10:09 appendonly.aof -rw-rw---- 1 redis redis 92 7月 29 10:06 dump.rdb # 开始备份 127.0.0.1:6379\u003e MSET a1 v1 a2 v2 a3 v3 OK 127.0.0.1:6379\u003e BGREWRITEAOF Background append only file rewriting started # 检查效果 root@python-auto:~# ll -h /var/lib/redis/*.aof -rw-rw---- 1 redis redis 118 7月 29 10:11 /var/lib/redis/appendonly.aof","date":"2020-08-05","objectID":"/posts/redis/redis-5/:1:0","tags":["Redis"],"title":"Redis 持久复制 （五）","uri":"/posts/redis/redis-5/"},{"categories":["数据库"],"content":"应用Redis python-Web Session实践 我们在后面是准备在python web项目中应用redis，所以我们需要在python虚拟环境中安装redis的模块插件，然后才可以正常的应用。redis-py提供两个类Redis和StrictRedis用于实现Redis的命令，StrictRedis用于实现大部分官方的命令，并使用官方的语法和命令，Redis是StrictRedis的子类，用于向后兼容旧版本的redis-py。 ","date":"2020-08-04","objectID":"/posts/redis/redis-4/:0:0","tags":["Redis"],"title":"Redis 共享Session实践 （四）","uri":"/posts/redis/redis-4/"},{"categories":["数据库"],"content":"环境准备 需要创建一个虚拟环境，为什么要用虚拟环境呢？ 默认情况下在当前操作系统下里面，安装一个版本为3.7的Python 再安装一个版本为3.9的Python 那么就会把之前的版本覆盖掉，那么如果再当前主机中有多个项目时，每个应用都有不同的功能，一个有遗留代码依赖于2.7版本，一个依赖于3.5版本，新的项目要求3.9版本，那么现在如果想要在一个主机把三个app全部运行起来该怎么办？ 那么就需要Python虚拟环境基于目录方式实现多个python版本共存。 软件安装 apt install virtualenv apt install virtualenvwrapper\r定制bash级别的环境变量\rcd vim .bashrc export WORKON_HOME=$HOME/.virtualenvs\rsource /usr/share/virtualenvwrapper/virtualenvwrapper.sh虚拟环境命令 workon 切换到指定的虚拟环境 deactivate 退出虚拟环境 mkvirtualenv 指定python版本创建虚拟环境 rmvirtualenv 删除指定的python版本创建虚拟环境 mkvirtualenv -p /usr/bin/python2.7 python_v2.7 在指定目录下创建对应版本的python site-packages mkvirtualenv -p /usr/bin/python3.8 python_v3.8 mkvirtualenv -p /usr/bin/python3.9 python_v3.9 ","date":"2020-08-04","objectID":"/posts/redis/redis-4/:0:1","tags":["Redis"],"title":"Redis 共享Session实践 （四）","uri":"/posts/redis/redis-4/"},{"categories":["数据库"],"content":"模块安装 pip install redispy\r#python工具集\rpip install ipython\r#查看安装的package\rpip list WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('\u003curllib3.connection.VerifiedHTTPSConnection object at 0x7fe6924bdcd0\u003e: Failed to establish a new connection: [Errno 101] Network is unreachable')': /simple/redispy/出现以上告警，更换pip源为国内源 阿里云 http://mirrors.aliyun.com/pypi/simple/ 中国科技大学 https://pypi.mirrors.ustc.edu.cn/simple/ 豆瓣(douban) http://pypi.douban.com/simple/ 清华大学 https://pypi.tuna.tsinghua.edu.cn/simple/ 中国科学技术大学 http://pypi.mirrors.ustc.edu.cn/simple/ 临时使用： 可以在使用pip的时候在后面加上-i参数，指定pip源 pip install scrapy -i https://pypi.tuna.tsinghua.edu.cn/simple #注：pip/pip.conf” E212: Cannot open file for writing 问题是要先创建 ~/.pip 文件夹。 永久修改： linux: 修改 ~/.pip/pip.conf (没有就创建一个)， 内容如下： #增加配置文件 mkdir ~/.pip vim .pip/pip.conf [global] index-url = https://pypi.tuna.tsinghua.edu.cn/simple #pip版本查询 pip --version pip 20.0.2 from /root/.virtualenvs/python_v3.8/lib/python3.8/site-packages/pip (python 3.8) windows: #直接在user目录中创建一个pip目录，如：C:\\Users\\xx\\pip，在pip 目录下新建文件pip.ini， #或者按照网友的建议：win+R 打开用户目录%HOMEPATH%，在此目录下创建 pip 文件夹，在 pip 目录下创建 pip.ini 文件, 内容如下 [global] timeout = 6000 index-url = https://pypi.tuna.tsinghua.edu.cn/simple trusted-host = pypi.tuna.tsinghua.edu.c 简单操作 - 以String为例 # 导入模块\rimport redis\r# 方法1\rr = redis.Redis(host='127.0.0.1', port=6379, db=2)\r# 方法2\rr = redis.StrictRedis(host='127.0.0.1', port=6379, db=2)\rredis-py使用connection pool来管理对一个redis server的所有连接，避免每次建立、释放连接的开\r销。默认，每个Redis实例都会维护一个自己的连接池。当然，我们还可以直接建立一个连接池，然后作为参\r数Redis，这样就可以实现多个Redis实例共享一个连接池\r# 方法3\rpool = redis.ConnectionPool(host='127.0.0.1', port=6379)\rr = redis.Redis(connection_pool=pool) # 导入模块 import redis # 创建对象 redis_obj = redis.Redis(host='127.0.0.1',port=6379,db=3) #调用设置key值 redis_obj.set('key','value') #获取key值 redis_obj.get('key') #单值实践 r.set('key', 'value', ex=5) 或 r.setex(\"key1\", 5, \"value1\") r.get('key') # 多值实践 r.mset(k1=\"v1\", k2=\"v2\") r.mset({'k3':\"v3\", 'k4':\"v4\"}) r.mget('k1', 'k2') r.mget(['k3', 'k4']) # 自增自减 r.set('num',4) r.get('num') r.incr('num') r.incr('num', 6) r.incrby('num',6) r.decr('num') r.decr('num',3) 注意：没有decrby # 删除操作 r.delete('num') # 判断存在 r.exists('num') # 模糊匹配 r.keys() r.keys('k*') r.keys('*2') # 查询数据量 r.dbsize()","date":"2020-08-04","objectID":"/posts/redis/redis-4/:0:2","tags":["Redis"],"title":"Redis 共享Session实践 （四）","uri":"/posts/redis/redis-4/"},{"categories":["数据库"],"content":"简单实践 对于各种web框架来说，只要涉及到redis，基本上都提供了相关的 属性配置，我们这里以简单的 Flask web框架为例。 安装模块 pip install Flask pip install flask-session 测试运行框架 (python_v3.8) root@elkserver:~# vim python_flask.py #导入模块 from flask import Flask # 创建应用对象 app = Flask(__name__) # 定制路由策略 @app.route('/') def index(): return \"hello-flask app web\" # 启动应用 if __name__ == '__main__': app.run(host='192.168.10.110') (python_v3.8) root@elkserver:~# python python_flask.py * Serving Flask app 'python_flask' (lazy loading) * Environment: production WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Debug mode: off * Running on http://192.168.10.110:5000/ (Press CTRL+C to quitFlask session 官方文档：https://flask-session.readthedocs.io/en/latest/ session常用的方法如下 get：用来从session中获取指定值。\rpop：从session中删除一个值。\rkeys：从session中获取所有的键。\ritems：从session中获取所有的值。\rclear：清除当前这个用户的session数据。\rflush：删除session并且删除在浏览器中存储的session_id，一般在注销的时候用得比较多。\rset_expiry(value)：设置过期时间。代码实现 #导入模块 from flask import Flask, session from flask_session import Session import redis # 创建应用对象 app = Flask(__name__) app.debug = True app.secret_key = 'x123asdaczxdasd' app.config['SESSION_TYPE'] = 'redis' app.config['SESSION_PERMANENT'] = True app.config['SESSION_USE_SIGNER'] = False app.config['SESSION_KEY_PREFIX'] = 'session:' app.config['SESSION_REDIS'] = redis.Redis(host='127.0.0.1', port='6379', db=4) Session(app) # 定制路由策略 @app.route('/') def index(): return \"hello-flask app web\" @app.route('/set') def set_key(): session['user_name'] = \"zhangsan\" return 'ok' @app.route('/get') def get_key(): return session.get('user_name',\"没有设置username key\") @app.route('/pop') def pop_key(): session.pop('user_name') return session.get('user_name','pop key') @app.route('/clean') def clean_key(): session.clear() return session.get('user_name', 'clear key') # 启动应用 if __name__ == '__main__': app.run(host='192.168.10.110')启动flask python flask_redis.py * Serving Flask app \"flask_redis\" (lazy loading) * Environment: production WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Debug mode: on * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit) * Restarting with stat * Debugger is active! * Debugger PIN: 170-146-674 ...浏览器刷新 http://0.0.0.0:5000/index ，查看效果 http://192.168.10.110:5000/set http://192.168.10.110:5000/get 127.0.0.1:6379\u003e select 4 OK 127.0.0.1:6379[4]\u003e KEYS * 1) \"session:8b6dd8af-fca8-4874-b92e-9ef13936287e\" 127.0.0.1:6379[4]\u003e get \"session:8b6dd8af-fca8-4874-b92e-9ef13936287e\" \"\\x80\\x04\\x95*\\x00\\x00\\x00\\x00\\x00\\x00\\x00}\\x94(\\x8c\\n_permanent\\x94\\x88\\x8c\\tuser_name\\x94\\x8c\\bzhangsan\\x94u.\" http://192.168.10.110:5000/pop get \"session:8b6dd8af-fca8-4874-b92e-9ef13936287e\" \"\\x80\\x04\\x95\\x12\\x00\\x00\\x00\\x00\\x00\\x00\\x00}\\x94\\x8c\\n_permanent\\x94\\x88s.\" http://192.168.10.110:5000/clean 127.0.0.1:6379[4]\u003e KEYS * (empty array)","date":"2020-08-04","objectID":"/posts/redis/redis-4/:1:0","tags":["Redis"],"title":"Redis 共享Session实践 （四）","uri":"/posts/redis/redis-4/"},{"categories":["数据库"],"content":"Redis 基础知识 Redis 是 Remote Dictionary Server(远程数据服务)的缩写，由意大利人 antirez(Salvatore Sanfilippo) 开发的一款 内存高速缓存数据库，该软件使用 C 语言编写,它的数据模型为 key-value。 官方介绍： Redis is an open source (BSD licensed), in-memory data structure store, used as a database, cache, and message broker. Redis provides data structures such as strings, hashes, lists, sets, sorted sets with range queries, bitmaps, hyperloglogs, geospatial indexes, and streams. Redis has built-in replication, Lua scripting, LRU eviction, transactions, and different levels of on-disk persistence, and provides high availability via Redis Sentinel and automatic partitioning with Redis Cluster. Redis是一个开源（BSD许可）的内存数据结构存储，被用作数据库、缓存和消息代理。Redis提供的数据结构包括：字符串、哈希值、列表、集合、带范围查询的排序集合、位图、超日志、地理空间索引和流。Redis有内置的复制、Lua脚本、LRU驱逐、事务和不同级别的磁盘持久性，并通过Redis Sentinel和Redis Cluster的自动分区提供高可用性。 关键点： 开源、基于内存的数据结构存储、可以作为数据库、缓存、消息代理 提供了 九种+ 的数据结构。 支持各种功能 - 复制、内部检测、事务操作、数据持久化、高可用功能(高可用、高扩展) 趋势： 资料来源：https://db-engines.com/en/ranking/key-value+store/all ","date":"2020-08-03","objectID":"/posts/redis/redis-3/:0:0","tags":["Redis"],"title":"Redis 环境部署 （三）","uri":"/posts/redis/redis-3/"},{"categories":["数据库"],"content":"应用场景 我们直接从几种数据本身的应用特性来描述一下该软件的应用场景： Sort Set (有序集合) 有序集合在普通集合的基础上做了分数比较的特性，所以主要用来做一些分类排序等功能 比如：排行榜应用，取 top n 操作 List (列表) 列表本身具有排序、切片等特性，因为redis的基于内存的分布式特性，它主要来做一些数据筛选、排序等功能 比如：获得最新 N 个数据 或 某个分类的最新数据等 String (字符串) 字符串的其实就是数据的临时存储，借助于redis的内存特性，主要做一些共享之类的功能。 比如：计数器应用、会话共享、购物车状态数据等 Set (集合) 集合主要是数据的统计，由于数据本身具有权重的特性，所以判断数据是否存在的特性要比list好很多。 比如：获得共同数据、安全防御的ip判断、社交好友等 以数据存储本身的角度来说场景\r有序集合 - 各种排行、topn\rlist - 数据的排布，顺序\rsort集合 - 范围数据列表\rstring - 数据的存储\rhash字典 - 数据分类(子分类)只要你有丰富的想象力，redis你想着么用就怎么用。 ","date":"2020-08-03","objectID":"/posts/redis/redis-3/:1:0","tags":["Redis"],"title":"Redis 环境部署 （三）","uri":"/posts/redis/redis-3/"},{"categories":["数据库"],"content":"Redis 部署 Redis 的安装有两种方式，一个是多主机环境下使用、一个是单台主机部署方便实验测试。 通过软件源安装 # 安装源仓库 add-apt-repository ppa:redislabs/redis apt-get update # 查看软件版本 apt info redis apt info redis-server # 安装软件 apt install redis 注意：会自动安装 redis-server、redis-tools 依赖软件 # 服务管理 systemctl stop redisredis-server systemctl disable redis-server systemctl start redis-server # 默认启动的端口号6379 # 进入redis数据库 redis-cli # 查看信息 info手工安装方式 # 下载软件 mkdir /data/softs \u0026\u0026 cd /data/softs wget https://download.redis.io/releases/redis-6.2.5.tar.gz # 解压文件 tar xf redis-6.2.5.tar.gz cd redis-6.2.5/ # 确认安装效果 grep 'make PREF' -B 2 README.md % make install # 指定安装路径 You can use `make PREFIX=/some/other/directory install` if you wish to use a # 编译安装 make PREFIX=/data/server/redis install # 此命令已经将make编译安装的步骤整合在一起 # 查看效果 # tree /data/server/redis/ /data/server/redis/ └── bin ├── redis-benchmark ├── redis-check-aof -\u003e redis-server ├── redis-check-rdb -\u003e redis-server ├── redis-cli ├── redis-sentinel -\u003e redis-server └── redis-server 1 directory, 6 files # 配置环境变量 echo 'PATH=/data/server/redis/bin:$PATH' \u003e /etc/profile.d/redis.sh source /etc/profile.d/redis.sh # 创建基本目录 mkdir /data/server/redis/{etc,log,data,run} -p # 因为redis service 里面指定这几个路径因此要创建出来用于服务存放数据 cp redis.conf /data/server/redis/etc/启动命令 前台方式启动 redis redis-server /data/server/redis/etc/redis.conf 检查效果 redis在可以基于同一个配置文件启动多个程序 # 启动多个实例 redis-server /data/server/redis/etc/redis.conf --port 6666 redis-server /data/server/redis/etc/redis.conf --port 7777 redis-server /data/server/redis/etc/redis.conf --port 8888 redis-server /data/server/redis/etc/redis.conf --port 9999 # 查看效果 netstat -tnulp | grep redis 后台方式启动 # 定制redis配置文件 root@python-auto:~# vim /data/server/redis/etc/redis.conf # daemonize no 将redis的启动设定为后台启动 daemonize yes bind 10.0.0.12 127.0.0.1 #增加本地IP地址 # 启动redis服务 /data/server/redis/bin/redis-server /data/server/redis/etc/redis.conf # 查看效果 netstat -tnulp | grep redis # 关闭服务 redis-cli shutdown #注意此命令是将本机中的所有实例进程全部关闭 redis-cli -h 127.0.0.1 -p 6666 #访问进程 kill -9 $(lsof -Pti :6379) #启动多实例 redis-server /data/server/redis/etc/redis.conf --port 6380 redis-server /data/server/redis/etc/redis.conf --port 6381 redis-server /data/server/redis/etc/redis.conf --port 6382 redis-server /data/server/redis/etc/redis.conf --port 6383 #伪集群效果","date":"2020-08-03","objectID":"/posts/redis/redis-3/:2:0","tags":["Redis"],"title":"Redis 环境部署 （三）","uri":"/posts/redis/redis-3/"},{"categories":["数据库"],"content":"简单实践 ","date":"2020-08-03","objectID":"/posts/redis/redis-3/:3:0","tags":["Redis"],"title":"Redis 环境部署 （三）","uri":"/posts/redis/redis-3/"},{"categories":["数据库"],"content":"查看配置 root@python-auto:/etc/redis# grep -Env '#|^$' redis.conf 75:bind 127.0.0.1 -::1 绑定地址 94:protected-mode yes 98:port 6379 暴露端口 107:tcp-backlog 511 连接队列 119:timeout 0 136:tcp-keepalive 300 257:daemonize yes 后台启动 275:supervised auto 289:pidfile /run/redis/redis-server.pid 297:loglevel notice 302:logfile /var/log/redis/redis-server.log 327:databases 16 默认16个数据库 336:always-show-logo no 341:set-proc-title yes 358:proc-title-template \"{title} {listen-addr} {server-mode}\" 398:stop-writes-on-bgsave-error yes 404:rdbcompression yes 413:rdbchecksum yes 431:dbfilename dump.rdb 数据文件名称 444:rdb-del-sync-files no 454:dir /var/lib/redis 数据文件所在目录 ... ","date":"2020-08-03","objectID":"/posts/redis/redis-3/:3:1","tags":["Redis"],"title":"Redis 环境部署 （三）","uri":"/posts/redis/redis-3/"},{"categories":["数据库"],"content":"常用操作命令 # 连接数据库 root@python-auto:/etc/redis# redis-cli 精简连接格式 127.0.0.1:6379\u003e root@python-auto:/etc/redis# redis-cli -h localhost -p 6379 标准连接格式 localhost:6379\u003e root@python-auto:~# redis-cli --raw 中文连接格式 127.0.0.1:6379\u003e # 测试效果 127.0.0.1:6379\u003e ping PONG # 退出效果 127.0.0.1:6379[5]\u003e quit查看帮助信息： ​ 127.0.0.1:6379\u003e help redis-cli 6.2.5 To get help about Redis commands type: \"help @\u003cgroup\u003e\" to get a list of commands in \u003cgroup\u003e \"help \u003ccommand\u003e\" for help on \u003ccommand\u003e \"help \u003ctab\u003e\" to get a list of possible help topics \"quit\" to exit To set redis-cli preferences: \":set hints\" enable online hints \":set nohints\" disable online hints Set your preferences in ~/.redisclirc 注意： 历史操作命令在 ~/.rediscli_history 文件中命令组解析 redis将大量的命令进行了简单的分组操作，对于6.2.5来说，他有 15个命令组 @generic 通用的命令组 @string 字符相关命令组 @list 列表相关命令组 @set 集合相关命令组 @sorted_set 有序集合相关命令组 @hash hash相关命令组 @pubsub 发布订阅相关命令组 @transactions 事务相关命令组 @connection 连接相关命令组 @server 服务相关命令组 @scripting 脚本相关命令组 @hyperloglog 超级日志相关命令组 @cluster 集群相关命令组 @geo 基因类数据相关命令组 @stream 流数据相关命令组 —————————————————————————————————— #查看命令组 127.0.0.1:6379\u003e help @generic #查看组中单独命令帮助 127.0.0.1:6379\u003e help ECHO ECHO message summary: Echo the given string since: 1.0.0 group: connection简单实践 # 选择数据库 127.0.0.1:6379\u003e select 5 OK 127.0.0.1:6379[5]\u003e # 查看所有属性信息 127.0.0.1:6379[5]\u003e info ... # 查看部分属性信息 127.0.0.1:6379[5]\u003e info cpu # CPU used_cpu_sys:1.222606 used_cpu_user:0.905046 used_cpu_sys_children:0.000000 used_cpu_user_children:0.000000 used_cpu_sys_main_thread:1.221889 used_cpu_user_main_thread:0.904515 # 获取配置属性 127.0.0.1:6379[5]\u003e CONFIG GET bind 1) \"bind\" 2) \"127.0.0.1 -::1\"Key 相关命令 # 获取所有的key信息 127.0.0.1:6379\u003e help KEYS KEYS pattern summary: Find all keys matching the given pattern since: 1.0.0 group: generic # 判断一个key是否存在 127.0.0.1:6379\u003e help EXISTS EXISTS key [key ...] summary: Determine if a key exists since: 1.0.0 group: generic # 设置一个key 127.0.0.1:6379\u003e help set SET key value [EX seconds|PX milliseconds|EXAT timestamp|PXAT millisecondstimestamp| KEEPTTL] [NX|XX] [GET] summary: Set the string value of a key since: 1.0.0 group: string # 获取一个key 127.0.0.1:6379\u003e help get GET key summary: Get the value of a key since: 1.0.0 group: string # 删除一个key 127.0.0.1:6379\u003e help DEL DEL key [key ...] summary: Delete a key since: 1.0.0 group: generic # 查看key的类型 127.0.0.1:6379\u003e help TYPE TYPE key summary: Determine the type stored at key since: 1.0.0 group: generic # 设置一个有过期期限的key 127.0.0.1:6379\u003e help EXPIRE EXPIRE key seconds summary: Set a key's time to live in seconds since: 1.0.0 group: generic # 查看一个key的有效时间 127.0.0.1:6379\u003e help TTL TTL key summary: Get the time to live for a key since: 1.0.0 group: generic # 删除当前库的所有key 127.0.0.1:6379\u003e help FLUSHDB FLUSHDB [ASYNC|SYNC] summary: Remove all keys from the current database since: 1.0.0 group: server # 删除当前数据库所有的数据 127.0.0.1:6379\u003e help FLUSHALL FLUSHALL [ASYNC|SYNC] summary: Remove all keys from all databases since: 1.0.0 group: server","date":"2020-08-03","objectID":"/posts/redis/redis-3/:3:2","tags":["Redis"],"title":"Redis 环境部署 （三）","uri":"/posts/redis/redis-3/"},{"categories":["数据库"],"content":"String string类型是实战中应用最多的数据类型，可以用于各种其他类型数据的值的存储，非常方便 特点： 其他数据类型的数据表现样式 简单的数据存储 示例： cookie、session、校验码等 简单实践 设定key 设定一个普通的key set key value 设定一个有过期时间的key setex key seconds value 同时设定多个值 mset key1 value1 key2 value2 ... 获取key 获取一个key get key 获取多个key mget key1 key2 ... 删除key 删除一个key del key1 删除多个key del key1 key2 ... ","date":"2020-08-03","objectID":"/posts/redis/redis-3/:3:3","tags":["Redis"],"title":"Redis 环境部署 （三）","uri":"/posts/redis/redis-3/"},{"categories":["数据库"],"content":"List list 是一个string类型的 列表，redis的每个list都可以存储 2^32 -1 个元素，列表的元素方便排 序、获取、统计等相关操作。 各种各样的列表场景都可以 设定key 左侧添加数据 lpush key value1 value2 右侧添加数据 rpush key value1 value2 插入指定元素 linsert key before|after 现有元素 新元素 获取key 获取列表数据 lrange key start stop 注意：start 是从 0开始、stop如果为 -1的话，代表最后一个。 获取key指定位置的值 LINDEX key index 获取key列表的值的数量 LLEN key 根据key获取在当前列表的位置 LPOS key 删除key 从key中删除指定的value lrem key count value 注意： count \u003e 0: 从头往尾移除指定数量个 value count \u003c 0: 从尾往头移除指定数量个 value count = 0: 移除所有的 value 从key的左侧删除指定个数的 value LPOP key [count] 从key的右侧删除指定个数的 value RPOP key [count] 保留范围数据，范围之外的都删除 LTRIM key 起始索引 结束索引 实践： lpush mylist e b e b e b 在左侧插入 ebebebe lrang mylist 0 -1 显示 lrem mylist 3 e 从左侧开始删除3个e lrem mylist 3 b 从左侧开始删除3个b lrem mylist -1 3 从尾部右侧删除1个3 lrem mylist 0 a =0删除所有的a ltrim mylist 2 3 只保留2到3 其他的都删除","date":"2020-08-03","objectID":"/posts/redis/redis-3/:3:4","tags":["Redis"],"title":"Redis 环境部署 （三）","uri":"/posts/redis/redis-3/"},{"categories":["数据库"],"content":"set set 是一个string类型的 集合，redis的每个list都可以存储 2^32 -1 个元素，集合元素无序且不重 复，可以进行各种排序统计场景。 场景： 内容不重复的任何场景都可以 无序集合 设定key 添加数据 SADD key member [member ...] 注意： 因为是无序的，所以查看的时候，没有顺序 如果key中已经存在 member，那么不会重复增加 合并多个key SUNION key [key ...] 将多个key的内容合并在一起，相同的member只会存在一个 获取key 获取set数据 SMEMBERS key 获取set中的member个数 SCARD key 获取多个key相同的内容 -- 取交集 SINTER key [key ...] 获取多个key不相同的内容 -- 取差集 SDIFF key [key ...]删除key 从key中删除指定的member\rSREM key member [member ...]\r从key的随机删除指定个数的 member\rSPOP key [count] ","date":"2020-08-03","objectID":"/posts/redis/redis-3/:3:5","tags":["Redis"],"title":"Redis 环境部署 （三）","uri":"/posts/redis/redis-3/"},{"categories":["数据库"],"content":"Sort set set 是一个string类型的 集合，redis的每个list都可以存储 2^32 -1 个元素，集合元素无序且不重 复，可以进行各种排序统计场景。 有序集合 sortset 场景： 排行榜、topN 设定key 添加数据\rZADD key score member [score member ...]\r注意：\r因为每个member有score，所以查看的时候，有会按照score的值进行排序\r如果key中已经存在 member，那么不会重复增加 获取key 获取有序集合数据\rZRANGE key min max [REV]\r注意：\rmin 是从 0开始、max如果为 -1的话，代表最后一个。\rrev 代表反序\r获取有序集合中的指定分数范围的元素\rZRANGEBYSCORE key min max\r获取有序集合元素的权重\rZSCORE key member\r获取有序集合元素个数\rZCARD key 删除key 从key中删除指定的member\rZREM key member [member ...]\r从key的随机删除指定个数的 member\rZREMRANGEBYSCORE key min maxsortset 实践\r添加数据\rzadd mysortset 89 zhangsan 67 lisi 76 wangwu 91 madong 100 chunpeng\r正序查看\rzrange mysortset 0 -1\r倒序查看\rzrange mysortset 0 -1 rev\r查看有序集合key个数\rzcard mysortset 查看66-89对应范围的人\rzrangebyscore mysortset 66 89\r删除指定 zhangsan lisi\rzrem mysortset lishi zhangsan\r删除指定分数范围的key，删除60分以上的人\rzremrangebyscore mysortset 60 100 ","date":"2020-08-03","objectID":"/posts/redis/redis-3/:3:6","tags":["Redis"],"title":"Redis 环境部署 （三）","uri":"/posts/redis/redis-3/"},{"categories":["数据库"],"content":"Hash hash 是一个string类型的 字段和值 的关联表，redis的每个hash都可以存储 2^32 -1 个键值对，非 常适合于存储对象场景。 使用场景：\r某个对象的特定属性：\rperson: {\rusername: zhangsan,\rpassword: 123456,\raddress: beijign,\rxxx: xxx\r} 设定 key 添加数据\rHSET key field value [field value ...]\r注意：在实践的时候、hset 也可以实现添加多个数据对的效果\r添加多个数据\rHMSET key field value [field value ...] 获取 key 获取所有属性\rHKEYS key\r获取多个属性的值\rHMGET key field [field ...] 删除 key 从key中删除指定的value\rHDEL key field [field ...]\r直接删除key所有的内容\rdel key 实践：\r增加数据\rhset person username zhangsan age 36 weight 140 height 1789 wife hanmeimei\r查看集合\rHKEYS person\rHMGET person username age wife\r晒选删除字段\rhdel person wife","date":"2020-08-03","objectID":"/posts/redis/redis-3/:3:7","tags":["Redis"],"title":"Redis 环境部署 （三）","uri":"/posts/redis/redis-3/"},{"categories":["数据库"],"content":"NoSQL 基础知识 一类新出现的数据库(not only sql) ","date":"2020-08-02","objectID":"/posts/redis/redis-2/:0:0","tags":["Redis"],"title":"Redis 基础-NoSQL （二）","uri":"/posts/redis/redis-2/"},{"categories":["数据库"],"content":"特点 泛指非关系型的数据库 不支持SQL语法 存储结构跟传统关系型数据库中的那种关系表完全不同，nosql中存储的数据都是KV形式 NoSQL的世界中没有一种通用的语言，每种nosql数据库都有自己的api和语法，以及擅长的业务场景 ","date":"2020-08-02","objectID":"/posts/redis/redis-2/:1:0","tags":["Redis"],"title":"Redis 基础-NoSQL （二）","uri":"/posts/redis/redis-2/"},{"categories":["数据库"],"content":"相关软件 Redis 简介：开源的内存结构数据库 官网：https://redis.io/ 最新版本：6.2.5 Mongodb 简介：分布式文档存储数据库，旨在为WEB应用提供可扩展的高性能数据存储解决方案。 官网：https://www.mongodb.com/ 最新版本：4.4 CouchDB 简介：开源的面向文档的数据库管理系统，可以通过 RESTful API 方式访问。 官网：https://couchdb.apache.org/ 版本：3.1.1 ","date":"2020-08-02","objectID":"/posts/redis/redis-2/:2:0","tags":["Redis"],"title":"Redis 基础-NoSQL （二）","uri":"/posts/redis/redis-2/"},{"categories":["数据库"],"content":"NoSQL VS SQL ","date":"2020-08-02","objectID":"/posts/redis/redis-2/:3:0","tags":["Redis"],"title":"Redis 基础-NoSQL （二）","uri":"/posts/redis/redis-2/"},{"categories":["数据库"],"content":"概念 SQL (Structured Query Language) 关系型数据库。 主要代表：SQL Server，Oracle，MySQL(开源)，PostgreSQL(开源)。 NoSQL（Not Only SQL）泛指非关系型数据库。 主要代表：MongoDB，Redis，CouchDB。 ","date":"2020-08-02","objectID":"/posts/redis/redis-2/:3:1","tags":["Redis"],"title":"Redis 基础-NoSQL （二）","uri":"/posts/redis/redis-2/"},{"categories":["数据库"],"content":"存储方式 SQL数据存在特定结构的表中，通常以数据库表形式存储数据。 NoSQL存储方式比较灵活，web场景中，通常以json样式来进行数据的承载。 ​ 数据的存储样式： ​ SQL - 二维表样式 ​ Nosql ​ 本质上都是以 k/v 样式来存储，但是在应用的时候，各有特点 ​ web开发场景以json为主 ​ 自动化测试场景，以 xml 为主(我说的) ","date":"2020-08-02","objectID":"/posts/redis/redis-2/:3:2","tags":["Redis"],"title":"Redis 基础-NoSQL （二）","uri":"/posts/redis/redis-2/"},{"categories":["数据库"],"content":"存储理论 在实际情况下，网站的数据存储不可能在一台主机上实现亿万级用户的访问。在大数据量场景中我们的数据都是以集群的方式进行存储。此时面临一个问题就是集群中的主机之间数据存储如何达到一个可和谐稳定运行状态。因此演变出的各种各样的理论下面主要以ACID、BASE、CAP方面介绍。 ","date":"2020-08-02","objectID":"/posts/redis/redis-2/:4:0","tags":["Redis"],"title":"Redis 基础-NoSQL （二）","uri":"/posts/redis/redis-2/"},{"categories":["数据库"],"content":"ACID 对于一个关系型数据库来说，有一个非常重要的基本属性：ACID ACID，是指数据库管理系统（DBMS）在写入或更新资料的过程中，为保证事务（transaction）是正确可靠的，所必须具备的四个特性. 注意： 事务是由一些列对系统数据进行访问或者更新操作组成的一个程序执行单元，狭义的事务指的是数据库事务，这里主要来说分布式场景的事务 特性 解释 备注 原子性 (atomicity) 一个事务（transaction）中的所有操作，要么全部完成，要么全部不完成，不会结束在中间某个环节。 一致性 (consistency) 在事务开始之前和事务结束以后，数据库的完整性没有被破坏，侧重于整体。 隔离性 (isolation) 数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。 持久性 (durability) 事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。 ","date":"2020-08-02","objectID":"/posts/redis/redis-2/:4:1","tags":["Redis"],"title":"Redis 基础-NoSQL （二）","uri":"/posts/redis/redis-2/"},{"categories":["数据库"],"content":"CAP 对于数据库来说，因为受到主机资源、容量配置等限制，导致我们无法用一个数据库、一台主机来存储所有的数据，所以在实际的工作中，我们的所有信息都是分散的存储在不同的主机上，这就是 – 分布式数据存储。 对于分布式数据存储来说，传统的ACID就不太适合了，所以针对当前环境的特性就梳理出来一种为事务服务的理论 CAP – 它是指在一个分布式系统中，一致性、可用性、容错性三者不可兼得。 特性 解释 一致性 (Consistency) 更新操作成功后，所有节点在同一时间的数据完全一致。 可用性 (Availability) 用户访问数据时，系统是否能在正常响应时间返回结果。 容错性(Partition Tolerance) 分布式系统在遇到某节点或网络分区故障的时候，仍然能够对外提供满足一致性和可用性的服务。 基于CAP三种特性的两两组合，可以将我们之前所说的各种数据库来进行简单的划分归类 CAP理论告诉我们 一个分布式系统不可能同时满足一致性可用性和分区容错性这三个基本需求，最多同时满足. 这三个当中的两项 一般来说：我们都是在一致性和分区容错性之间寻找所谓的平衡 ","date":"2020-08-02","objectID":"/posts/redis/redis-2/:4:2","tags":["Redis"],"title":"Redis 基础-NoSQL （二）","uri":"/posts/redis/redis-2/"},{"categories":["数据库"],"content":"BASE BASE 理论是针对 NoSQL 数据库而言的，它是对 CAP 理论中一致性（C）和可用性（A）进行权衡的结果，源于提出者自己在大规模分布式系统上实践的总结。其核心思想是无法做到强一致性，但每个应用都可以根据 自身的特点，采用适当方式达到最终一致性。 BASE理论是由eBay的架构师提出。 特性 解释 基本可用 (BasicallyAvailable) 分布式系统在出现不可预知故障时，系统允许损失部分可用性，即保证核心功能或者当前最重要功能可用。比如说：访问的网页速度稍微降低一些，用户量大的时候，实时限流等 软状态 (Softstate) 允许系统数据存在中间状态，但不会影响系统的整体可用性，即允许不同节点的副本之间存在暂时的不一致情况。比如：集群系统的多个节点之间运行xx秒是数据同步延迟。 最终一致 (EventuallyConsistent) 这是三个特点中最重要的，它强调的是系统中所有主机的数据副本，在一段时间同步后，最终能够达到一致状态，而不需要实时保证数据副本一致 最终一致性是 BASE 原理的核心，也是 NoSQL 数据库的主要特点，通过弱化一致性，提高系统的可伸缩性、可靠性和可用性。而且对于大多数 Web 应用，其实并不需要强一致性，因此牺牲一致性而换取高可用性，是多数分布式数据库产品的方向。 小结 前提： 一台主机的资源，无法抗住大量用户数据的操作，所以我们需要以集群的方式，来对数据进行管理 问题： 如果保证 集群主机 数据是一致的，对用户来说无所谓 话题： 事务 在一些业务场景中，一个操作需要多个sql才能够完成指定的功能 – 这个整体操作就是一个事务 ACID A 原子性 -- 对于事务操作来说(多条命令)，要么成功，要么一起失败还原\rB 一致性 -- 事务操作前 和后 ，对于数据库本身的数据访问功能没有影响\rC 隔离性 -- 同一个数据集群内部的多个事务操作，彼此间无交叉影响\rD 持久性 -- 数据的落地 CAP 前提： 在分布式集群场景中，无法做到单台主机能够实现的 ACID，那么就做一个缓冲 C -- 集群间所有主机数据是一致的\r-- 数据库集群的同步\rA -- 集群整体提供的服务对用户来说，可用\r-- P -- 集群提供服务的过程中，允许出现一些错误数据，或者过期数据，\r-- 访问课程数据的时候，运行是一个月前的过期数据\rBASE 前提： 我已经确定了，集群环境中，不可能不存异常故障，接下来只能从CAP 里面的 C一致性 和 A可用性 之间来找平衡 BA 基本可用\r-- 无论任何时候，我们的网站服务是正常，虽然效果没有预期的那么好\rS 软状态\r-- 针对的是 集群内部的主机 状态转换的时候 -- 一个中间过渡\rE 最终一致性\r-- 即使集群内部出现故障了，但是最终故障恢复后，要与其他主机数据进行同步\r","date":"2020-08-02","objectID":"/posts/redis/redis-2/:4:3","tags":["Redis"],"title":"Redis 基础-NoSQL （二）","uri":"/posts/redis/redis-2/"},{"categories":["数据库"],"content":"用户访问的过程中会产生各种各样的数据，为了让网站能够正常的运行，并且高效的让用户精准的看到相应的数据，我们就会在不同业务功能场景中采用各种各样的数据类型来进行承载。 数据分类 用户访问的过程中会产生各种各样的数据，为了让网站能够正常的运行，并且高效的让用户精准的看到相应的数据，我们就会在不同业务功能场景中采用各种各样的数据类型来进行承载。 按照我们的项目场景落地的实现方式分为三种类型： 结构化数据 半结构化数据 非结构化数据 ","date":"2020-08-01","objectID":"/posts/redis/redis-1/:0:0","tags":["Redis"],"title":"Redis 基础-数据分类 （一）","uri":"/posts/redis/redis-1/"},{"categories":["数据库"],"content":"结构化数据 所谓的结构化数据，指的是数据的表现样式有一定的(横竖)结构,一般情况下，这种数据是以二维表的方式来实现数据的存储和逻辑关系的表达。 – 数据以行为单位，一行数据表示一个实体的信息，每一行数据的属性是相同的。 这些数据在存储的时候，为了实现数据的统一存储，往往对数据存储的格式和长度规范都进行了一定程度的限制，这些数据的具体存储主要是以关系型数据库软件来实现。 结构化数据，是指由二维表结构来逻辑表达和实现的数据，严格地遵循数据格式与长度规范，主要通过关系型数据库进行存储和管理。 结构化数据的存储和排列是很有规律的，所以这些数据在查询或修改等操作的时候非常方便，但是由于数据在存储的时候，有一定的关联关系，所以在数据扩充属性或者收缩属性的时候不太方便 – 扩展性不好。 数据在存储的过程中本身是有强关联的，在储存数据本身有相应的数据结构来进行限制，两个不同业务场景之中的数据，一旦他们之间有相应关联的话，我们会基于数据存储软件本身的特性将这些数据关联在一起。对于数据完整的整体来说他们之间是强关联的有相应的结构。 结构化数据表现样式如下： ID 姓名 性别 电话 籍贯 1 张三 男 13382261344 山西 2 李四 女 18612388412 山东 对于某一条数据来说，它的内部有相应数据存储格式，对于数据整体来说由大量数据整合一起。 对于行来说是由多个具体的数据组合在一起的具有特殊的含义，对于每一列来说对数据的属性、长度 是否可以为空等等都有相应的限制。可以通过查看表结构查看相应的属性。如 mysql 中 desc user; ","date":"2020-08-01","objectID":"/posts/redis/redis-1/:1:0","tags":["Redis"],"title":"Redis 基础-数据分类 （一）","uri":"/posts/redis/redis-1/"},{"categories":["数据库"],"content":"半结构化数据 所谓的半结构化数据，应用数据使用的时候有一定的关联、层次， 但是这些数据在存储的时候没有像关系型数据有数据属性、长度、是否为空、数据唯一性的限制。但在存储的时候有一定业务关联。 半结构化数据的存储一般是以文件的方式来实现的，比较常见的文件样式有：json、XML等。 在json存储过程中，会构造字典然后按照固定格式进行存储里面的数据自由获取。 Json数据 { \"status\": 200, \"message\": { \"person\": [ { \"id\": 1, \"name\": \"张三\", \"gender\": \"男\", \"address\": { \"Country\": \"中国\", \"Province\": \"北京市\", \"city\": \"北京市\", \"district\": \"丰台区\", \"town\": \"五里店\" }, }, ], } } 数据关系 []中括号代表的是一个数组或列表 {}大括号代表的是一个数据对象 双引号“”表示的是属性值 冒号：代表的是前后之间的关系，冒号前面是属性的名称，后面是属性的值XML 数据 \u003c?xml version=\"1.0\" encoding=\"gb2312\"?\u003e \u003cnamelist\u003e \u003cname1\u003e \u003cID\u003e01\u003c/ID\u003e \u003cname\u003e张三\u003c/name\u003e \u003csex\u003e男\u003c/sex\u003e \u003caddress\u003e北京市市丰台区五里店\u003c/address\u003e \u003c/name1\u003e \u003cnamelist\u003e 数据关系 存储格式是以节点为主,一个节点衍生出另外的子节点 每个节点遵循html的风格，但是里面的标签属性是我们自定义的。XML 主要用于测试，如测试网页功能 将成功的数据和不成功的数据全部罗列出来，以XML的样式单独实现，在测试的时候基于对应的测试软件框架加载定制好的测试数据以自动化的方式，把所有的功能全部测试出来。 测试的数据仅仅是为了功能测试时使用的，没有必要存储下来，所以我们就用简单的Html场景当中的数据单独存储xml格式来进行存储。 数据本身存储没有强关联，但是应用的时候有一定的关联结构。这种数据被称之为半结构化数据。 ","date":"2020-08-01","objectID":"/posts/redis/redis-1/:2:0","tags":["Redis"],"title":"Redis 基础-数据分类 （一）","uri":"/posts/redis/redis-1/"},{"categories":["数据库"],"content":"非结构化数据 所谓非结构化数据，在存储的时候数据和数据之间没有所谓的强关联，应用场景也没有所谓的整体一起使用。 用的时候直接调用就可以了。 非结构化数据，其实就是没有固定结构的数据 – 即结构化数据之外的一切数据。它们常以 图片、视频、音频等 样式存在。对于这类数据，我们一般直接整体进行存储，而且一般存储为二进制的数据格式。 非结构化数据一般有两种生成方式： 人为手工生成 - 文本文件、图片、视频、音频、业务应用程序等。 机器自动生成 - 卫星图形、科学数据、数据监控、传感数据等 一般情况下，非结构化数据存储在非关系数据库中，并使用NoSQL进行查询。工作生活，非结构化数据是越来越多，占比远远的超出结构化数据。 资料来源：https://db-engines.com/en/ranking 数据软件的应用 目前 结构化软件还是占据绝对的位置 非结构化数据软件，在特定场景中，占据一席地位 目前所有的软件，都有一个趋同的发展 “我是结构化数据软件，但是能做对方的事情“ 小结 结构化数据 1 数据存储本身是有意义的 – 强关联和存储约束 2 数据存储的整体，在业务场景中也有关联 – 一对多、多对一、一对一等 半结构化数据 1 开发场景： 页面的展示 和 展示的数据 分开 数据本身没有意义，但是组合在一起能够使用 - json 2 测试场景； 自动化测试 – 构造大量的测试数据单独保存 - xml 非结构化数据 数据本身存储和业务场景存储没有所谓的关联 – 但是我要的时候，必须给我 kv 注意： 这里的结构 不是 数据结构与算法里面 数据存储应用到的结构 指的是 业务场景中的数据关联存储 ","date":"2020-08-01","objectID":"/posts/redis/redis-1/:3:0","tags":["Redis"],"title":"Redis 基础-数据分类 （一）","uri":"/posts/redis/redis-1/"},{"categories":null,"content":"关于我 👨🏻‍💻 一名云原生工程师 🍟爱吃好吃的饭,玩好玩的游戏，看好看的剧 🛸 AI依赖患者 @Ryan's Recent activity\rhttps://github.com/ryanxin7\r2020 年毕业后为了“混口饭吃”便开启了运维开发的升级之路，在升级的过程中，我发现我很喜欢“折腾”🤔。 发现了自己对技术的兴趣和热爱，也喜欢不断地尝试新的东西和探索未知的领域。 我希望能够与他人交流和分享自己的经验和心得🙈。 在这个过程中，我相信自己可以不断地成长和进步，实现自己的梦想和目标。 为捋清脑袋中无序的片段，于是我开始在此进行记录📝。\r","date":"2020-07-28","objectID":"/about/:1:0","tags":null,"title":"关于","uri":"/about/"},{"categories":["Linux"],"content":"LNMP网站架构实战 [[toc]] ","date":"2019-11-28","objectID":"/posts/linux-basic/lnmp/:0:0","tags":["Linux 学习之旅","LNMP"],"title":"实战案例-LNMP网站架构","uri":"/posts/linux-basic/lnmp/"},{"categories":["Linux"],"content":"1. 实施步骤说明 :::tip 服务实施步骤 了解LNMP架构的组成作用 架构的部署 架构通讯原理 LNMP服务之间如何建立管理 运维人员代码上线 NFS服务器和Web服务器建立联系 数据库、存储远端迁移 ::: ","date":"2019-11-28","objectID":"/posts/linux-basic/lnmp/:1:0","tags":["Linux 学习之旅","LNMP"],"title":"实战案例-LNMP网站架构","uri":"/posts/linux-basic/lnmp/"},{"categories":["Linux"],"content":"1.1 Nginx 模块回顾 Nginx 服务的企业应用 (nginx模块) 实现网站页面目录索引功能 (yum仓库搭建) 实现网站访问别名功能 server_name 实现网站页面用户访问监控 keepalived_timeout 65s HTTP请求报文: 请求头—connection: keepalived…/closed 短连接 HTTP响应报文: 响应头—connection: closed 短连接 VPN—无法访问外网/xshell无法远程连接 实现网站服务日志功能配置 错误日志: 错误日志级别 访问日志: 日志的格式信息 自动化分析日志(ELK 三个软件) 根据用户访问uri进行匹配处理 location = xxx 精确匹配 优先级01 location ^~ xxx 优先匹配 优先级02 location ~ 区分大小写匹配 优先级03 ​ location ~* 不区分大小写 优先级03 ​ location uri 根据uri进行匹配 优先级03 ​ location / 默认匹配 优先级最低 ​ ","date":"2019-11-28","objectID":"/posts/linux-basic/lnmp/:1:1","tags":["Linux 学习之旅","LNMP"],"title":"实战案例-LNMP网站架构","uri":"/posts/linux-basic/lnmp/"},{"categories":["Linux"],"content":"1.2 企业应用: 网站 location应用案例 例如在活动时期打折促销，网站的页面信息和平常不一样。我们不可能把平常的页面为活动直接进行修改，这时我们就需要调用一个专门在活动时上线的页面作为主站广告。 有两个站点目录: 平常网站的站点目录 /html/jd-normal 节日网站的站点目录 /html/jd-teshu ​ location / { root /html/jd-normal } location / 特殊 { root /html/jd-teshu } ","date":"2019-11-28","objectID":"/posts/linux-basic/lnmp/:1:2","tags":["Linux 学习之旅","LNMP"],"title":"实战案例-LNMP网站架构","uri":"/posts/linux-basic/lnmp/"},{"categories":["Linux"],"content":"1.3 网站页面跳转功能 ​ 第一种方法： server { location / { rewrite ^/(.*)$ https://www.ryanxin.com/$1 permanent; } } 第二种方法： server { location / { return 301 https://www.ryanxin.com; } } ","date":"2019-11-28","objectID":"/posts/linux-basic/lnmp/:1:3","tags":["Linux 学习之旅","LNMP"],"title":"实战案例-LNMP网站架构","uri":"/posts/linux-basic/lnmp/"},{"categories":["Linux"],"content":"2. LNMP 架构介绍 L 代表 Linux系统 注意: selinux必须关闭 防火墙关闭 /tmp 1777 mysql 服务无法启动 N 代表 Nginx服务 作用: 处理用户的静态请求 html jpg txt mp4/avi P 代表 php 服务 作用: 处理动态的页面请求 负责和数据库建立关系 M 代表 mysql服务部署 作用: 存储用户的字符串数据信息 ","date":"2019-11-28","objectID":"/posts/linux-basic/lnmp/:2:0","tags":["Linux 学习之旅","LNMP"],"title":"实战案例-LNMP网站架构","uri":"/posts/linux-basic/lnmp/"},{"categories":["Linux"],"content":"3. 网站的 LNMP 架构部署 ","date":"2019-11-28","objectID":"/posts/linux-basic/lnmp/:3:0","tags":["Linux 学习之旅","LNMP"],"title":"实战案例-LNMP网站架构","uri":"/posts/linux-basic/lnmp/"},{"categories":["Linux"],"content":"3.1创建虚拟用户和根目录 创建用户 useradd -M -s /sbin/nologin www -u 1002 id www systemctl restart nginx 创建战点根目录 mkdir -p /html/bbs/ chown -R www.www /html/bbs/ ","date":"2019-11-28","objectID":"/posts/linux-basic/lnmp/:3:1","tags":["Linux 学习之旅","LNMP"],"title":"实战案例-LNMP网站架构","uri":"/posts/linux-basic/lnmp/"},{"categories":["Linux"],"content":"3.2 编写 Nginx 配置文件 vim /etc/nginx/nginx.conf server { server_name ryanxin.com; rewrite ^/(.*) http://www.ryanxin.com/$1 permanent; } server { listen 80; server_name www.ryanxin.com; error_page 500 502 503 504 /50x.html; location / { root /html/bbs; index index.html; } } ","date":"2019-11-28","objectID":"/posts/linux-basic/lnmp/:3:2","tags":["Linux 学习之旅","LNMP"],"title":"实战案例-LNMP网站架构","uri":"/posts/linux-basic/lnmp/"},{"categories":["Linux"],"content":"3.3 安装数据库软件 yum install mariadb-server mariadb -y #补充: 数据库初始化过程 mysql_install_db --basedir=path The path to the MariaDB installation directory. #指定mysql程序目录 --datadir=path The path to the MariaDB data directory. #指定数据信息保存的目录 --user=mysql #让mysql管理数据目录 700 创建数据库的密码信息: /application/mysql/bin/mysqladmin -u root password 'new-password' # 给本地数据库设置密码 /application/mysql/bin/mysqladmin -u root -h web01 password 'new-password' # 给远程数据库设置密码 mysqladmin -u root password 'xin123' --- 设置密码 mysql -u root -pxin123 启动数据库服务 systemctl start mariadb.service systemctl enable mariadb.service ","date":"2019-11-28","objectID":"/posts/linux-basic/lnmp/:3:3","tags":["Linux 学习之旅","LNMP"],"title":"实战案例-LNMP网站架构","uri":"/posts/linux-basic/lnmp/"},{"categories":["Linux"],"content":"4. PHP服务部署流程 ","date":"2019-11-28","objectID":"/posts/linux-basic/lnmp/:4:0","tags":["Linux 学习之旅","LNMP"],"title":"实战案例-LNMP网站架构","uri":"/posts/linux-basic/lnmp/"},{"categories":["Linux"],"content":"4.1 更新 源卸载系统自带的PHP软件 ​ yum remove php-mysql php php-fpm php-common rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm rpm -Uvh https://mirror.webtatic.com/yum/el7/webtatic-release.rpm ","date":"2019-11-28","objectID":"/posts/linux-basic/lnmp/:4:1","tags":["Linux 学习之旅","LNMP"],"title":"实战案例-LNMP网站架构","uri":"/posts/linux-basic/lnmp/"},{"categories":["Linux"],"content":"4.2 安装 PHP yum install -y php71w php71w-cli php71w-common php71w-devel php71w-embedded php71w-gd php71w-mcrypt php71w-mbstring php71w-pdo php71w-xml php71w-fpm php71w-mysqlnd php71w-opcache php71w-pecl-memcached php71w-pecl-redis php71w-pecl-mongodb ","date":"2019-11-28","objectID":"/posts/linux-basic/lnmp/:4:2","tags":["Linux 学习之旅","LNMP"],"title":"实战案例-LNMP网站架构","uri":"/posts/linux-basic/lnmp/"},{"categories":["Linux"],"content":"4.3 编写配置文件 vim /etc/php-fpm.d/www.conf user = www group = www #保证nginx进程的管理用户和php服务进程的管理用户保持一致 systemctl start php-fpm LNMP 服务间调用图 Nginx 通过location匹配以PHP结尾的文件，调用Fastcgi接口执行fastcgi_pass 命令发送给PHP的php-fam进程接收，wrapper进程进行处理，处理后在原路返回给nginx最后客户端可以看到动态页面，如果需要调用数据库在通过php解析器解析成sql语句与数据库进行调用数据操作。最后生成动态页面返回给nginx客户端可以看到页面。 调用流程 用户访问网站 — \u003e nginx(fastcgi_pass) – FastCGI–\u003e (php-fpm – wrapper) php (php解析器) —\u003e mysql(读取或写入) ","date":"2019-11-28","objectID":"/posts/linux-basic/lnmp/:4:3","tags":["Linux 学习之旅","LNMP"],"title":"实战案例-LNMP网站架构","uri":"/posts/linux-basic/lnmp/"},{"categories":["Linux"],"content":"5. 实现 LNMP 架构服务之间建立关系 Nginx无法直接 和 数据库建立联系，因此要先和PHP 建立关系。 ","date":"2019-11-28","objectID":"/posts/linux-basic/lnmp/:5:0","tags":["Linux 学习之旅","LNMP"],"title":"实战案例-LNMP网站架构","uri":"/posts/linux-basic/lnmp/"},{"categories":["Linux"],"content":"5.1 实现 Nginx与 PHP 建立关系 编写 nginx 配置文件 location ~ \\.php$ { root /www; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; fastcgi_pass 127.0.0.1:9000; include fastcgi_params; #变量配置文件 } systemctl nginx restart #重启服务 server { server_name ryanxin.com rewrite ^/(.*) http://www.ryanxin.com/$1 permanent; } server { listen 80; server_name www.ryanxin.com; error_page 500 502 503 504 /50x.html; location /{ root /html/bbs; index index.html; } location ~ \\.php$ { root /bbs; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; fastcgi_pass 127.0.0.1:9000; include fastcgi_params; } } ","date":"2019-11-28","objectID":"/posts/linux-basic/lnmp/:5:1","tags":["Linux 学习之旅","LNMP"],"title":"实战案例-LNMP网站架构","uri":"/posts/linux-basic/lnmp/"},{"categories":["Linux"],"content":"5.2 编写动态资源文件测试页面 vim /html/blog/test_php.php \u003c?php phpinfo(); ?\u003e 浏览器输入地址进行访问测试 http://www.ryanxin.com/index.php 注意 ： 要在本地Host文件手动添加域名地址，才能实现本地域名解析访问。 ","date":"2019-11-28","objectID":"/posts/linux-basic/lnmp/:5:2","tags":["Linux 学习之旅","LNMP"],"title":"实战案例-LNMP网站架构","uri":"/posts/linux-basic/lnmp/"},{"categories":["Linux"],"content":"6. 实现 PHP 与数据库建立关系 ","date":"2019-11-28","objectID":"/posts/linux-basic/lnmp/:6:0","tags":["Linux 学习之旅","LNMP"],"title":"实战案例-LNMP网站架构","uri":"/posts/linux-basic/lnmp/"},{"categories":["Linux"],"content":"6.1 编写 PHP 连接数据库测试文件 # vim test_mysql.php \u003c?php $servername = \"localhost\"; $username = \"root\"; $password = \"oldboy123\"; //$link_id=mysql_connect('主机名','用户','密码'); //mysql -u用户 -p密码 -h 主机 $conn = mysqli_connect($servername, $username, $password); if($conn) { echo \"mysql successful by root !\\n\"; }else{ die(\"Connection failed: \" . mysqli_connect_error()); } ?\u003e ","date":"2019-11-28","objectID":"/posts/linux-basic/lnmp/:6:1","tags":["Linux 学习之旅","LNMP"],"title":"实战案例-LNMP网站架构","uri":"/posts/linux-basic/lnmp/"},{"categories":["Linux"],"content":"6.2 访问测试 http://www.ryanxin.com/test_mysql.php 如果连接成功 ：提示 Mysql successful by root！ 失败则提示报错信息 ","date":"2019-11-28","objectID":"/posts/linux-basic/lnmp/:6:2","tags":["Linux 学习之旅","LNMP"],"title":"实战案例-LNMP网站架构","uri":"/posts/linux-basic/lnmp/"},{"categories":["Linux"],"content":"7. 部署搭建网站页面 代码上线 ","date":"2019-11-28","objectID":"/posts/linux-basic/lnmp/:7:0","tags":["Linux 学习之旅","LNMP"],"title":"实战案例-LNMP网站架构","uri":"/posts/linux-basic/lnmp/"},{"categories":["Linux"],"content":"7.1 常用的开源源码网站 ​ 主站网站页面: http://www.dedecms.com/ ​ 论坛网站页面: http://www.discuz.net/forum.php ​ 博客网站页面: https://cn.wordpress.org/ ​ 知乎类型网站页面: http://www.wecenter.com/?copyright ","date":"2019-11-28","objectID":"/posts/linux-basic/lnmp/:7:1","tags":["Linux 学习之旅","LNMP"],"title":"实战案例-LNMP网站架构","uri":"/posts/linux-basic/lnmp/"},{"categories":["Linux"],"content":"7.3 将源码解压后放入到站点目录中 这里演示的是 Wordpress 博客系统 tar xf wordpress-5.2.1.tar.gz mv ./* /html/bbs 修改站点目录权限 chown -R www.www blog ","date":"2019-11-28","objectID":"/posts/linux-basic/lnmp/:7:2","tags":["Linux 学习之旅","LNMP"],"title":"实战案例-LNMP网站架构","uri":"/posts/linux-basic/lnmp/"},{"categories":["Linux"],"content":"7.4 进行网站页面初始化操作 ","date":"2019-11-28","objectID":"/posts/linux-basic/lnmp/:7:3","tags":["Linux 学习之旅","LNMP"],"title":"实战案例-LNMP网站架构","uri":"/posts/linux-basic/lnmp/"},{"categories":["Linux"],"content":"7.5 对数据库服务进行配置 --创建数据库 create databases wordpress; --检查 show databases; --创建数据库管理用户: grant all on wordpress.* to 'wordpress'@'localhost' identified by 'xin123'; --检查 select user,host from mysql.user --优化: 删除无用的用户信息 delete from mysql.user where user=\"\" and host=\"localhost\"; delete from mysql.user where user=\"\" and host=\"web01\"; flush privileges; --刷新 利用blog网站发布博文 ","date":"2019-11-28","objectID":"/posts/linux-basic/lnmp/:7:4","tags":["Linux 学习之旅","LNMP"],"title":"实战案例-LNMP网站架构","uri":"/posts/linux-basic/lnmp/"},{"categories":["Linux"],"content":"8. 常见问题解决 上传wordpress主题,报413错误,如何解决? 1.修改nginx配置文件 vim blog.conf server { client_max_body_size 50m; #指定用户上传数据的大小限制(默认1M) } 修改php.ini配置文件 upload_max_filesize = 50M #使PHP接收用户上传的更大的数据(默认2M) ","date":"2019-11-28","objectID":"/posts/linux-basic/lnmp/:8:0","tags":["Linux 学习之旅","LNMP"],"title":"实战案例-LNMP网站架构","uri":"/posts/linux-basic/lnmp/"},{"categories":["Linux"],"content":" Keepalived 高可用服务部署 ​ Keepalived 软件最早是配合 LVS 负载均衡软件而设计的，用来管理并监控LVS集群系统中各个服务节点的状态，后来又加入了VRRP 协议可以实现高可用的功能。 软件主要是通过 VRRP 协议实现高可用功能的,VRRP 是Virtual Router Redundancy Protocol（虚拟路由器冗余协议）的缩写，VRRP出现的目的就是为了解决静态路由单点故障问题的，它能够保证当个别节点宕机时，整个网络可以不间断地运行 ","date":"2019-11-25","objectID":"/posts/linux-basic/linux3/:0:0","tags":["Linux 学习之旅","Keepalived"],"title":"Keepalived 高可用服务部署","uri":"/posts/linux-basic/linux3/"},{"categories":["Linux"],"content":"一、Keepalived 软件工作原理 ​ 启初 VRRP 的出现是为了解决静态路由的单点故障。VRRP 是用过IP多播的方式实现高可用对之间通信的。工作时主服务器节点发包，备服务器节点接包，当备服务器节点接收不到主服务器节点发的数据包的时候，就启动接管程序接管主服务器节点的资源。备服务器节点可以有多个，通过优先级竞选。优先级数值越大优先级越大。 ","date":"2019-11-25","objectID":"/posts/linux-basic/linux3/:1:0","tags":["Linux 学习之旅","Keepalived"],"title":"Keepalived 高可用服务部署","uri":"/posts/linux-basic/linux3/"},{"categories":["Linux"],"content":"二、Keepalived 高可用服务部署 ","date":"2019-11-25","objectID":"/posts/linux-basic/linux3/:2:0","tags":["Linux 学习之旅","Keepalived"],"title":"Keepalived 高可用服务部署","uri":"/posts/linux-basic/linux3/"},{"categories":["Linux"],"content":"1.确认反向代理服务是否工作正常 在kl1和kl02服务器上测试web服务器是否可以正常（最好有3台反向代理功能的Web服务器） curl -H host:www.rxinxin.org 192.168.10.10/webserver.html curl -H host:www.rxinxin.org 192.168.10.11/webserver.html curl -H host:www.rxinxin.org 192.168.10.11/webserver.html systemctl enable mariadb","date":"2019-11-25","objectID":"/posts/linux-basic/linux3/:2:1","tags":["Linux 学习之旅","Keepalived"],"title":"Keepalived 高可用服务部署","uri":"/posts/linux-basic/linux3/"},{"categories":["Linux"],"content":"2.在浏览器上测试访问kl1和kl2 域名 解析hosts文件，将域名解析为192.168.10.20，进行测试访问 解析hosts文件，将域名解析为192.168.10.30，进行测试访问 测试前同步kl1和kl2的 Nginx 配置文件 scp -rp /app/nginx/conf/nginx.conf 192.168.10.30:/app/nginx/conf/ ","date":"2019-11-25","objectID":"/posts/linux-basic/linux3/:2:2","tags":["Linux 学习之旅","Keepalived"],"title":"Keepalived 高可用服务部署","uri":"/posts/linux-basic/linux3/"},{"categories":["Linux"],"content":"三、安装 Keepalived 服务软件 第一步：安装软件 yum install -y keepalived第二步：编写keepalived配置文件 vim /etc/keepalived/keepalived.conf\rman keepalived.conf //查看文件说明信息配置文件结构： GLOBAL CONFIGURATION --- 全局配置\rVRRPD CONFIGURATION --- vrrp配置\rLVS CONFIGURATION --- LVS服务相关配置 （可以删掉不用） kl1主 负载均衡器配置 global_defs { //全局配置 router_id kl1 //定义路由标识信息，相同局域网唯一 } vrrp_instance klg1 { //Vrrp 配置 state MASTER //定义实例中主备状态的角色（MASTER/BACKUP） interface eth0 //设置主备服务器虚拟IP地址放置网卡位置 virtual_router_id 31 //虚拟路由ID标识，不同实例不同，主备相同 priority 150 //设置抢占优先级，数值越大越优先 advert_int 1 //主备间通讯时间间隔 authentication { //主备间通过认证建立连接 auth_type PASS auth_pass 1111 } virtual_ipaddress { 定义主备服务器之间使用的虚拟IP地址信息 192.168.10.60/24 dev eth0 label eth0:1 } } /etc/init.d/keepalived reload //平滑重启 Keeplived kl2备 负载均衡器配置 global_defs { router_id kl2 } vrrp_instance klg1 { state BACKUP interface eth0 virtual_router_id 31 priority 100 advert_int 1 authentication { auth_type PASS auth_pass 1111 } virtual_ipaddress { 192.168.10.60/24 dev eth0 label eth0:1 } } /etc/init.d/keepalived reload","date":"2019-11-25","objectID":"/posts/linux-basic/linux3/:3:0","tags":["Linux 学习之旅","Keepalived"],"title":"Keepalived 高可用服务部署","uri":"/posts/linux-basic/linux3/"},{"categories":["Linux"],"content":"四、部署高可用服务时遇到的问题 同时在keepalived高可用集群中，出现了两个虚拟IP地址信息，这种情况就称为脑裂 脑裂情况出现原因： 1. 心跳线出现问题\r网卡配置有问题\r交换设备有问题\r线缆连接有问题\r2. 有防火墙软件阻止问题\r3. virtual_router_id\r配置数值不正确 总之：只要备服务器收不到主服务器发出的组播包，就会成为主服务器，而主服务器资源没有释放，备服务器要篡位就会出现脑裂。 ","date":"2019-11-25","objectID":"/posts/linux-basic/linux3/:4:0","tags":["Linux 学习之旅","Keepalived"],"title":"Keepalived 高可用服务部署","uri":"/posts/linux-basic/linux3/"},{"categories":["Linux"],"content":"五、利用shell脚本实现监控管理 备设备有 VIP 就是表示不正常\r真正实现主备切换 2. 出现脑裂情况了 #!/bin/bash\rcheck_info=$(ip a|grep -c 192.168.10.60) //定义一个参数为VIP地址 .60\rif [ $check_info -ne 0 ] //如果等于 0 then\recho \"keepalived server error!!!\" //打印告警提示 keepalived 服务出现错误\rfi","date":"2019-11-25","objectID":"/posts/linux-basic/linux3/:5:0","tags":["Linux 学习之旅","Keepalived"],"title":"Keepalived 高可用服务部署","uri":"/posts/linux-basic/linux3/"},{"categories":["Linux"],"content":"六、Nginx反向代理监听虚拟IP地址 编写nginx反向代理配置 server {\rlisten 192.168.10.60:80;\rserver_name www.rxinxin.org;\rroot html;\rindex index.html index.htm;\rlocation / {\rproxy_pass http://xinxin;\rproxy_set_header host $host;\rproxy_set_header X-Forwarded-For $remote_addr;\r}\r}\rserver {\rlisten 192.168.10.60:80;\rserver_name bbs.rxinxin.org;\rroot html;\rindex index.html index.htm;\rlocation / {\rproxy_pass http://xinxin;\rproxy_set_header host $host;\rproxy_set_header X-Forwarded-For $remote_addr;\r}\r}\r/application/nginx/sbin/nginx -s stop //Nginx 更改ip 一定要重启\r/application/nginx/sbin/nginx\rnetstat -lntup|grep nginx //查看端口\rtcp 0 0 192.168.10.60:80 0.0.0.0:* LISTEN 53334/nginx 虚拟IP地址 实现监听本地网卡上没有的IP地址 echo 'net.ipv4.ip_nonlocal_bind = 1' \u003e\u003e/etc/sysctl.conf 更改内核 sysctl -p ","date":"2019-11-25","objectID":"/posts/linux-basic/linux3/:6:0","tags":["Linux 学习之旅","Keepalived"],"title":"Keepalived 高可用服务部署","uri":"/posts/linux-basic/linux3/"},{"categories":["Linux"],"content":"七、将高可用和反向代理服务建立联系 因为Nginx 反向代理服务处于异常状态下，keepalived服务并没有从主服务器切换到备服务器，所以客户访问网站时反向代理服务一直处于挂了的异常状态导致网站无法正常访问。 实现的目的：Nginx反向代理服务停止，Keepalived服务也停止 编写脚本 #!/bin/bash\rweb_info=$(ps -ef|grep [n]ginx|wc -l) //当Nginx进程小于2时\rif [ $web_info -lt 2 ]\rthen\r/etc/init.d/keepalived stop //关闭keepalived 服务\rfi2.运行脚本，实现监控nginx服务 编辑keepalived服务配置文件 #定义一个监控脚本，脚本必须有执行权限\rscript \"/server/scripts/check_web.sh\" #指定脚本间隔时间\rinterval 2 #脚本执行完成，让优先级值和权重值进行运算，从而实现主备切换 weight 2 }\rtrack_script {\rcheck_web\r} chmod +x check_kls.sh 给予脚本可执行权限 ","date":"2019-11-25","objectID":"/posts/linux-basic/linux3/:7:0","tags":["Linux 学习之旅","Keepalived"],"title":"Keepalived 高可用服务部署","uri":"/posts/linux-basic/linux3/"},{"categories":["Linux"],"content":"八、实现高可用集群架构中双主配置 互为主备配置配置 由于企业实际环境，很少等主服务器挂掉才调用备服务器，所以会将Web服务分配给两节点或多个集群并行使用节约成本。 kl1\rvrrp_instance klg1 {\rstate MASTER\rinterface eth0\rvirtual_router_id 31\rpriority 150\radvert_int 1\rauthentication {\rauth_type PASS\rauth_pass 1111\r}\rvirtual_ipaddress {\r192.168.10.60/24 dev eth0 label eth0:1\r}\r}\rvrrp_instance klg2 {\rstate BACKUP\rinterface eth0\rvirtual_router_id 32\rpriority 100\radvert_int 1\rauthentication {\rauth_type PASS\rauth_pass 1111\r}\rvirtual_ipaddress {\r192.168.10.80/24 dev eth0 label eth0:1\r}\r} kl2 vrrp_instance klg1 { state BACKUP interface eth0 virtual_router_id 31 priority 100 advert_int 1 authentication { auth_type PASS auth_pass 1111 } virtual_ipaddress { 192.168.10.60/24 dev eth0 label eth0:1 } } vrrp_instance klg2 { state MASTER interface eth0 virtual_router_id 32 priority 150 advert_int 1 authentication { auth_type PASS auth_pass 1111 } virtual_ipaddress { 192.168.10.80/24 dev eth0 label eth0:1 } } 最后修改 Nginx 反向代理服务配置文件的监听IP地址信息 完成对 Keepalived 互为主备节点的配置部署。 ","date":"2019-11-25","objectID":"/posts/linux-basic/linux3/:8:0","tags":["Linux 学习之旅","Keepalived"],"title":"Keepalived 高可用服务部署","uri":"/posts/linux-basic/linux3/"},{"categories":["Linux"],"content":"远程管理服务知识介绍 ","date":"2019-11-22","objectID":"/posts/linux-basic/linux2/:0:0","tags":["Linux 学习之旅","SSH"],"title":"远程管理服务知识介绍","uri":"/posts/linux-basic/linux2/"},{"categories":["Linux"],"content":"一、SSH远程登录服务介绍说明 SSH是Secure Shell Protocol的简写，由 IETF 网络工作小组（Network Working Group）制定；在进行数据传输之前，SSH先对联机数据包通过加密技术进行加密处理，加密后在进行数据传输。确保了传递的数据安全。 ​ SSH是专为远程登录会话和其他网络服务提供的安全性协议。 利用SSH协议可以有效的防止远程管理过程中的信息泄露问题，在当前的生产环境运维工作中，绝大多数企业普遍采用SSH协议服务来代替传统的不安全的远程联机服务软件，如telnet(23端口，非加密的)等。 在默认状态下，SSH服务主要提供两个服务功能： 一是提供类似telnet远程联机服务器的服务，即上面提到的SSH服务； 另一个是类似FTP服务的sftp-server，借助SSH协议来传输数据的，提供更安全的SFTP服务(vsftp,proftp)。 SSH远程登录服务排错思路 检查链路是否通畅—ping(icmp)/tracert/traceroute 检查链路是否阻断—将防火墙功能关闭 检查服务是否开启—ss/netstat -lntup（服务端检查） /telnet/nmap/nc（客户端检查） [D:\\~]$ telnet 10.0.0.41 22 Connecting to 10.0.0.41:22 ... Connection established. To escape to local shell, press 'Ctrl+Alt+]'. SSH-2.0-OpenSSH_5.3 ","date":"2019-11-22","objectID":"/posts/linux-basic/linux2/:1:0","tags":["Linux 学习之旅","SSH"],"title":"远程管理服务知识介绍","uri":"/posts/linux-basic/linux2/"},{"categories":["Linux"],"content":"二、SSH远程登录服务特点说明 SSH服务端口号为22 SSH服务采用密文方式传输数据 SSH服务默认支持root用户进行远程登录 [root@backup ~]# netstat -lntp Active Internet connections (only servers) Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 1227/sshd ","date":"2019-11-22","objectID":"/posts/linux-basic/linux2/:2:0","tags":["Linux 学习之旅","SSH"],"title":"远程管理服务知识介绍","uri":"/posts/linux-basic/linux2/"},{"categories":["Linux"],"content":"三、Telnet远程登录服务功能作用 SSH服务端口号为23 SSH服务采用明文方式传输数据 SSH服务默认不支持root用户进行远程登录 Telnet需要单独安装 yum install -y telnet-server telnet # xinetd相当于一个超级守护进程服务，用xinetd服务为那些小进程，小服务提供管理 # 首先小进程telnet要允许被管理，即disable=no,默认是yes [root@backup ~]# vim /etc/xinetd.d/telnet # default: on # description: The telnet server serves telnet sessions; it uses \\ # unencrypted username/password pairs for authentication. service telnet { flags = REUSE socket_type = stream wait = no user = root server = /usr/sbin/**in**.telnetd log_on_failure += USERID disable = no } # 重启一下，xinetd（因为之前没有启动过，所以用的start） [root@backup ~]# netstat -lntup|grep 23 tcp 0 0 :::23 :::* LISTEN 1340/xinetd # 已经有了 #现在可以测试了，使用telnet连接服务器（刚才是在backup主机上安装的） [D:\\~]$ telnet 10.0.0.41 connecting to 10.0.0.41:23... Connection established. To escape to local shell, press 'Ctrl+Alt+]'. CentOS release 6.9 (Final) Kernel 2.6.32-696.el6.x86_64 on an x86_64 backup login: root Password: Login incorrect backup login: # 因为telnet默认不支持root backup login: xinxin Password: [xinxin**@backup** ~]$ # 连上了 # 对于没有安装 telnet的主机，就连不上了 [D:\\~]$ telnet 10.0.0.31 Connecting to 10.0.0.31:23... Could not connect to '10.0.0.31' (port 23): Connection failed. # 使用ssh连接 [D:\\~]$ ssh 10.0.0.41 Connecting to 10.0.0.41:22... Connection established. To escape to local shell, press 'Ctrl+Alt+]'. WARNING! The remote SSH server rejected X11 forwarding request. [root@backup ~]# 用wireshark 抓包查看发现 一个是明文，一个是密文 ","date":"2019-11-22","objectID":"/posts/linux-basic/linux2/:3:0","tags":["Linux 学习之旅","SSH"],"title":"远程管理服务知识介绍","uri":"/posts/linux-basic/linux2/"},{"categories":["Linux"],"content":"四、远程管理服务概念详解 SSH远程管理服务加密技术 采用公钥和私钥进行算法加密，口令登录 ssh连接登录过程 ssh客户端发出连接请求 \\\u003e/root/.ssh/known_hosts 清空文件 ssh服务端会发出确认信息，询问客户端你是否真的要连接我 ssh 10.0.0.41 ssh客户端输入完成yes，会等到一个公钥信息 cat /root/.ssh/known_hosts ssh服务端将公钥信息发送给ssh客户端 ssh客户端利用密码进行登录 [root@web01 ~]# cat .ssh/known_hosts 10.0.0.41 ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEA47PsJW7rLbHv7mREm2 juOVznW7dJamrWQSSHyprcvfQvMQjg6f7P9XLEfLfqx67kQvpUdZ65S52DMp cDlEmiiX8hseiM/8zIMBw3XH/F3BHPxlP54067QaRFRFPsZ4nFH4LCF 6u4uST1YBJfIvyKKpQb6s+ZplDIMwfZxeWTK9QSREKijQzf7CvLN+Ekt 9rYWqKFHrhv/Ae5Lxro0jKasgF3tIsvnq75cqYN57or+mZux5v jnrs/PkRsLLNnAkIJfDMbUoG3WfAjJD18UPL3glmyzuMNfj5YHT4Vj QKC9Eq+WmOsDKl/Q501xpCDXx/dunCXDy+MRXgs/hpRtInjw== # 上面时客户端私钥 [root@web01 ~]# cd /etc/ssh/ [root@web01 ssh]# ll\\ [root@web01 ssh]# ll total 160 -rw-------. 1 root root 125811 Mar 22 2017 moduli -rw-r--r--. 1 root root 2047 Mar 22 2017 ssh_config -rw------ 1 root root 3876 Feb 22 19:36 sshd_config -rw------ 1 root root 3876 Feb 22 17:22 sshd_config.bak -rw-------. 1 root root 672 Jan 10 14:54 ssh_host_dsa_key -rw-r--r--. 1 root root 590 Jan 10 14:54 ssh_host_dsa_key.pub -rw-------. 1 root root 963 Jan 10 14:54 ssh_host_key -rw-r--r--. 1 root root 627 Jan 10 14:54 ssh_host_key.pub -rw-------. 1 root root 1675 Jan 10 14:54 ssh_host_rsa_key -rw-r--r--. 1 root root 382 Jan 10 14:54 ssh_host_rsa_key.pub # ssh [root@backup ssh]# cat ssh_host_rsa_key.pub ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEA47PsJW7rLbHv7mREm2juOVznW7dJamr WQSSHyprcvfQvMQjg6f7P9XLEfLfqx67kQvpUdZ65S52DMpcDlEmiiX8hseiM/8zIMB w3XH/F3BHPxlP54067QaRFRFPsZ4nFH4LCF6u4uST1YBJfIvyKKpQb6s+ZplDIMwfZx eWTK9QSREKijQzf7CvLN+Ekt9rYWqKFHrhv/Ae5Lxro0jKasgF3tIsvnq75cqYN57o r+mZux5vjnrs/PkRsLLNnAkIJfDMbUoG3WfAjJD18UPL3glmyzuMNfj5YHT4VjQKC9E q+WmOsDKl/Q501xpCDXx/dunCXDy+MRXgs/hpRtInjw== # 上面是服务端私钥 # 加密也是利用秘钥 # 客户端有锁头，服务端有钥匙和锁头，每次客户端要连接的时候就带着锁头， # 如果在服务端能用钥匙打开这个锁，就连接成功 SSH远程管理服务认证类型 加密技术分为v1和v2两个版本 sshv1版本不会经常更换锁头和钥匙，因此会有安全隐患 sshv2版本会经常更换锁头和钥匙，因此提高了远程连接安全性 基于密钥方式实现远程登录 ssh管理服务器上创建密钥对信息（公钥 私钥） ssh管理服务器上将公钥发送给被管理服务器 ssh管理服务器向被管理服务器发送连接请求 ssh被管理服务器向管理服务器发送公钥质询 ssh管理服务器处理公钥质询请求，将公钥质询结果发送给被管理主机 ssh被管理服务器接收公钥质询响应信息，从而确认认证成功 ssh管理服务端可以和被管理服务端建立基于密钥连接登录 ","date":"2019-11-22","objectID":"/posts/linux-basic/linux2/:4:0","tags":["Linux 学习之旅","SSH"],"title":"远程管理服务知识介绍","uri":"/posts/linux-basic/linux2/"},{"categories":["Linux"],"content":"五、基于密钥登录方式部署流程 （1）在管理主机上创建密钥对信息 - ssh-keygen -t dsa #\u003c- 创建密钥对命令 -t dsa表示指定密钥对加密类型 - Generating public/private dsa key pair. - Enter file in which to save the key (/root/.ssh/id_dsa): #\u003c- 确认私钥文件所保存的路径 - Created directory '/root/.ssh'. - Enter passphrase (empty for no passphrase): #\u003c- 确认是否给私钥设置密码信息（一般为空） - Enter same passphrase again: - Your identification has been saved in /root/.ssh/id_dsa. - Your public key has been saved in /root/.ssh/id_dsa.pub. - The key fingerprint is: 95:12:71:f9:ea:34:18:e8:a4:fa:0b:0b:f5:92:5c:1b root@m01 - The key's randomart image is: +--[ DSA 1024]----+ | o... | | o.. | | .. o. | | o .o . | | . E+ So . | | o +.o. . + | |. =.o o . | | ..+ . | | ..o. | +-----------------+ [root@m01 ~]# ll /root/.ssh/ - total 8 -rw------ 1 root root 668 Sep 21 17:23 id_dsa -rw-r--r- 1 root root 598 Sep 21 17:23 id_dsa.pub （2）将管理主机上公钥信息发送给被管理主机 - ssh-copy-id -i /root/.ssh/id_dsa.pub 172.16.1.31 - root@172.16.1.31's password: - Now try logging into the machine, with \"ssh '172.16.1.31'\", **and** check **in**: .ssh/authorized_keys - to make sure we haven't added extra keys that you weren't expecting. 管理服务器端 被管理服务器端 # 保存在了远程主机中用户的家目录下的.ssh目录下 [root**@nfs01** ~]# cd /root/.ssh/ [root**@nfs01** .ssh]# ll - total 4 -rw------ 1 root root 598 Feb 25 11:09 authorized_keys ssh-keygen -R 172.16.152.209 清空公钥记录（3）进行远程管理测试（基于密钥的方式进行远程管理） ssh 172.16.1.31 \u003c- 可以不用输入密码信息，就能登陆成功 ssh 172.16.1.31 uptime \u003c- 可以不用登陆到远程主机，就可以直接查看远程主机信息 ","date":"2019-11-22","objectID":"/posts/linux-basic/linux2/:5:0","tags":["Linux 学习之旅","SSH"],"title":"远程管理服务知识介绍","uri":"/posts/linux-basic/linux2/"},{"categories":["Linux"],"content":"六、SSH服务端配置文件信息说明 配置文件路径： /etc/ssh/sshd_config 修改SSH服务端端口号 为52113 配置监听地址 客户端测试： 监听地址说明图： SSH服务端配置文件信息说明 /etc/ssh/sshd_config Port 52113 \u003c- 修改ssh服务端口号信息 ListenAddress 0.0.0.0 \u003c- 主要作用提升网络连接安全性 PS：监听地址只能配置为服务器网卡上拥有的地址 PermitRootLogin no \u003c- 是否允许root用户远程登录 PermitEmptyPasswords no \u003c- 是否允许空密码 UseDNS no \u003c- 是否进行DNS反向解析（提升ssh远程连接效率） GSSAPIAuthentication no \u003c- 是否进行远程GSSAPI认证（提升ssh远程连接效率） ","date":"2019-11-22","objectID":"/posts/linux-basic/linux2/:6:0","tags":["Linux 学习之旅","SSH"],"title":"远程管理服务知识介绍","uri":"/posts/linux-basic/linux2/"},{"categories":["Linux"],"content":"七、SSH远程管理服务入侵防范 利用密钥登录提高安全性 安全设备策略阻止访问，只放开少量服务端口 开启SSH监听地址功能，只监听内网网卡地址 利用服务器不配置外网IP提高安全性 利用授权与系统安装最小化提高安全性 利用指纹信息对系统重要文件进行加密处理 利用系统重要文件加锁功能提高安全性 ","date":"2019-11-22","objectID":"/posts/linux-basic/linux2/:7:0","tags":["Linux 学习之旅","SSH"],"title":"远程管理服务知识介绍","uri":"/posts/linux-basic/linux2/"},{"categories":["Linux"],"content":"八、SFTP 常用操作命令总结 命令 帮助信息 操作说明 bye Quit sftp 表示退出 sftp 传输模式 cd path Change remote directory to path 改变远程目录信息 pwd Display remote working directory 显示远程主机当前目录信息 lcd path Change local directory to ‘path’ 改变本地目录路径信息 lpwd Print local working directory 输出本地目录路径信息 get [-P] remote-path [local-path] Download file 下载文件命令 put [-P] local-path [remote-path] Upload file 上传文件命令 ","date":"2019-11-22","objectID":"/posts/linux-basic/linux2/:8:0","tags":["Linux 学习之旅","SSH"],"title":"远程管理服务知识介绍","uri":"/posts/linux-basic/linux2/"},{"categories":["Linux"],"content":"Linux的目录结构特点 ","date":"2019-11-21","objectID":"/posts/linux-basic/linux-basics/:0:0","tags":["Linux 学习之旅"],"title":"Linux的目录结构特点","uri":"/posts/linux-basic/linux-basics/"},{"categories":["Linux"],"content":"目录结构特点 一切从根root 开始 Linux中每个设备可以挂在任何目录上面 Linux下面设备没有挂载无法使用 linux中一切皆文件 ","date":"2019-11-21","objectID":"/posts/linux-basic/linux-basics/:1:0","tags":["Linux 学习之旅"],"title":"Linux的目录结构特点","uri":"/posts/linux-basic/linux-basics/"},{"categories":["Linux"],"content":"什么是挂载？ 举例：在 linux 系统下面使用光盘 把光盘放入光驱中 查看光盘 ll /dev/cdrom 使用光盘 cat 乱码 把光盘挂载 mount /dev/cdrom /mnt/ read-only 只读 df-h 查看系统空间谁挂载路径 进入目录使用 cd /mnt 实际上进入了光盘 可以看到光盘内容 如果硬盘设备不挂载是没有访问入口的,就像房子没有大门，挂载就像给设备找了一个入口。 mount /dev/cdrom /mnt 设备 挂载点(mount point) ","date":"2019-11-21","objectID":"/posts/linux-basic/linux-basics/:2:0","tags":["Linux 学习之旅"],"title":"Linux的目录结构特点","uri":"/posts/linux-basic/linux-basics/"},{"categories":["Linux"],"content":"Linux 目录结构详解 目录名称 目录介绍 /etc 配置文件 /home 用户的家目录，每一个用户的家目录通常默认为/home/USERNAME /root 管理员的家目录 /lib 库文件 静态库：单在程序中的库，其他程序不能使用该库文件动态库：在内存中，任何用到该库的程序都可以使用 /lib/modules 内核模块文件 /media 挂载点目录，移动设备。在windows中，插入一张光盘，系统会自动读取光盘，用户可以直接执行，但在linux中，插入光盘后需要在挂载点挂载这个设备之后才可以使用这个设备 /mnt 挂载点目录，额外的临时文件系统 /opt 可选目录，第三方程序的安装目录 /proc 伪文件系统，内核映射文件 /sys 伪文件系统，跟硬件设备相关的属性映射文件 /tmp 临时文件 /var 可变化的文件，经常发生变化的文件 /bin 可执行文件，用户命令；其中用到的库文件可能在/lib，配置文件可能在/etc /sbin 可执行文件，管理命令；其中用到的库文件可能在/lib，配置文件可能在/etc /usr 只读文件，shared read-only /usr/local 第三方软件 /boot 系统启动相关的文件，如内核、initrd，以及grub（BootLoader） ​ 详细原文 ","date":"2019-11-21","objectID":"/posts/linux-basic/linux-basics/:3:0","tags":["Linux 学习之旅"],"title":"Linux的目录结构特点","uri":"/posts/linux-basic/linux-basics/"},{"categories":["Linux"],"content":"Linux 基础 Linux，全称GNU/Linux，是一套免费使用和自由传播的类UNIX操作系统，其内核由林纳斯·本纳第克特·托瓦兹于1991年第一次释出，它主要受到Minix和Unix思想的启发，是一个基于POSIX和Unix的多用户、多任务、支持多线程和多CPU的操作系统。它能运行主要的Unix工具软件、应用程序和网络协议。它支持32位和64位硬件。Linux继承了Unix以网络为核心的设计思想，是一个性能稳定的多用户网络操作系统。Linux有上百种不同的发行版，如基于社区开发的debian、archlinux，和基于商业开发的[Red Hat Enterprise Linux](https://baike.baidu.com/item/Red Hat Enterprise Linux/10770503)、SUSE、[oracle linux](https://baike.baidu.com/item/oracle linux/6876458)等。 ","date":"2019-11-20","objectID":"/posts/linux-basic/linux1/:0:0","tags":["Linux 学习之旅"],"title":"Linux 基础知识","uri":"/posts/linux-basic/linux1/"},{"categories":["Linux"],"content":"一、操作系统的基本知识 一般而言，现代计算机计算机系统是一个复杂的系统，故若程序员需要掌握该系统的每一个细节例如如何通过代码去调用音响等这些事情，那可能不再编写代码了，这种情况会严重影响程序员的开发效率。 并且管理这些部件并加以优化使用，是一件极富挑战性的工作，于是，计算安装了一层软件（系统软件），称为操作系统。它的任务就是为用户程序提供一个更好、更简单、更清晰的计算机模型，并管理刚才提到的所有设备。 ","date":"2019-11-20","objectID":"/posts/linux-basic/linux1/:1:0","tags":["Linux 学习之旅"],"title":"Linux 基础知识","uri":"/posts/linux-basic/linux1/"},{"categories":["Linux"],"content":"1. linux 介绍 Linux 是一个多用户多任务的操作系统，也是一款自由软件，拥有良好的用户界面，支持多种处理器架构，移植方便。严格的来讲，Linux 并不算是一个操作系统，只是一个 Linux 系统中的内核，即计算机软件与硬件通讯之间的平台。 1.1 GUN协议 Linux的全称是GNU/Linux，这才算是一个真正意义上的Linux系统。 设计原则： 1）所有的东西都是文件，所以管理简单\r2）所有操作系统配置数据都存储在正文文件中\r3）每个操作系统命令或应用程序很小，只完成单一功能\r4）避免使用俘获用户的接口，很少交互命令，应用程序由vi编辑器等完成交互\r5）多个程序串接在一起完成复杂任务\r二、Linux 常用命令 1、pwd 返回当前工作目录，直接输入 pwd 即可，后面不带参数。 2、ls 即英文单词list的缩写，列出指定目录的所有文件名或者文件夹名（默认为当前工作目录下）， 其选项如下： 参数 含义 -a 显示指定目录下所有子目录与文件，包括隐藏文件 -i 以列表方式显示文件的详细信息 -h 配合 -i 以人性化方式显示文件大小 3、cd 切换工作目录； 在使用Unix/Linux的时候，经常需要更换工作目录。cd命令可以帮助用户切换工作目录。Linux所有的目录和文件名大小写敏感cd后面可跟绝对路径，也可以跟相对路径。如果省略目录，则默认切换到当前用户的主目录。 命令 含义 cd 切换到当前用户的主目录，用户登录的使用，默认的目录就是用户的主目录。 cd ~ 切换到当前用户的主目录 cd . 切换到当前目录 ![](C:\\Users\\xinxi\\Desktop\\期末考试 2020春\\1328034-20180704191623328-304845393.png) ","date":"2019-11-20","objectID":"/posts/linux-basic/linux1/:1:1","tags":["Linux 学习之旅"],"title":"Linux 基础知识","uri":"/posts/linux-basic/linux1/"},{"categories":["Linux"],"content":"三、文件权限 在Linux操作系统上，有些文件很重要，这些文件只有系统或经过授权的用户才能使用，这样才能保护系统的安全。因为有一些文件是只有部分指定的人才能存取，以免不小心被他人删除或修改，因此文件的安全管理是非常重要的。为了防止未授权用户访问你的 文件，可以在文件和目录上设置权限位。还可以设定文件在创建时所具有的缺省权限。 ","date":"2019-11-20","objectID":"/posts/linux-basic/linux1/:2:0","tags":["Linux 学习之旅"],"title":"Linux 基础知识","uri":"/posts/linux-basic/linux1/"},{"categories":["Linux"],"content":"网卡文件配置文件详解 文件路径： //etc/sysconfig/network-scripts/ifcfg-eth0 为网卡文件路径 配置参数 参数详解 DEVICE=eth0 网卡名字 HWADDR=00:0c:29:bc:1f:24 Hardware address 硬件地址 TYPE=Ethernet 网络类型 因特网 UUID=1c340c4c-e0ec-4672-81e7-a5f4110dd1f9 UUID系统中唯一的标识 ONBOOT=yes boot on 在重启时候是否开启网卡 自动运行 NM_CONTROLLED=yes 是否通过network软件进行管理 BOOTPROTO=none 网卡获取IP地址的方式 static ip地址固 dhcp 自动获取 IPADDR=10.0.0.200 IP地址 NETMASK=255.255.255.0 子网掩码 GATEWAY=10.0.0.2 网关 默认的出口 USERCTL=no 是否允许普通用户管理网卡 开 关 重启 PEERDNS=yes DNS 优先性 IPV6INIT=no IPv6 DNS1=223.5.5.5 DNS地址 DNS2=223.6.6.6 DNS地址 ","date":"2019-11-20","objectID":"/posts/linux-basic/linux-network/:0:0","tags":["Linux 学习之旅"],"title":"網卡文件配置文件詳解","uri":"/posts/linux-basic/linux-network/"},{"categories":["Linux"],"content":"基于 Linux 系统的服务课程笔记 ​ 互联网运维是一个融合多学科（网络、系统、开发、安全、应用架构、存储等）的综合性技术岗位，给运维工程师提供了一个很好的个人能力与技术的发展空间。运维工作的相关经验将会变得非常重要，而且也将成为个人的核心竞争力，优秀的运维工程师具备很好的各层面问题的解决能力及方案提供、全局思考的能力等。由于运维岗位所接触的知识面非常广阔，更容易培养或发挥出个人某些方面的特长或爱好，如内核、网络、开发、数据库等方面，可以做得非常深入精通、成为这方面的专家。 ","date":"2019-11-11","objectID":"/posts/linux-basic/linux-technology-stack/:0:0","tags":["Linux 学习之旅"],"title":"Linux 系统的服务课程笔记","uri":"/posts/linux-basic/linux-technology-stack/"},{"categories":["Linux"],"content":"Linux 运维工程师技术图谱 运维架构技术类型 主要技术关键词 脚本编程 AWK、Sed、Grep、Shell、Python Web服务 Apache、Nginx、Tomcat、JBoss、Resin 数据传输 Rsync、Scp、Inodify/Sersync 性能分析 top、free、df、iftop、iostat、vmstat、dstat、sar、sysdig 进程管理 Supervisor 网络服务 vsftp、nfs、samba、bind、dhcp、postfifix 数据库 MySQL、MariaDB、PostgreSQL，Oracle NoSQL Redis、MongoDB 消息中间件 RabbitMQ、ActiveMQ 版本管理 SVN、Git 静态缓存 Squid、Varnish、Nginx 负载均衡 LVS、HAProxy、Nginx 高可用软件 Keepalived、Heartbeat、DRBD、corosync+pacemaker 集中管理工具 Ansible、Saltstack、Chef、Puppet 虚拟化 KVM、Xen、Openstack、Cloudstack 容器化 Docker、Kubernetes、Rancher、Openshift 自动装机 Kickstart、Cobbler 抓包分析 Tcpdump、Wireshark 持续集成 Jenkins、Gitlab MySQL代理 Altas、Cobar、Mycat 压力测试 ab、fifio、sysbench、mysqlslap、Jemter 日志收集 ELK Stack、Graylog 监控系统 Zabbix、Prometheus、Open-falcon 分布式文件系统 Ceph、GlusterFS、FastDFS :::tip 课程知识指南 Linux 初始基础阶段需要熟悉 Linux 操作系统安装，目录结构、文件权限、网卡文件等配置、系统启动流程等。 系统管理 主要学习Linux系统，掌握常用的几十个基本管理命令，包括用户管理、磁盘分区、软件包管理、文件权限、文本处理、进程管理、性能分析工具等。 网络基础 OSI和TCP/IP模型一定要熟悉。基本的交换机、路由器概念及实现原理要知道。 Shell脚本编程基础 掌握Shell基本语法结构，能编写简单的脚本即可。 网络服务 常见的网络服务要会部署，比如vsftp、nfs、samba、bind、dhcp等。 代码版本管理系统少不了，可以学习下主流的GIT，能部署和简单使用就可以了。 经常在服务器之间传输数据，所以要会使用：rsync和scp。 数据同步：inotify/sersync。 重复性完成一些工作，可写成脚本定时去运行，所以得会配置Linux下的定时任务服务crond。 Web服务 每个公司基本都会有网站，能让网站跑起来，就需要搭建Web服务平台了。 如果是用PHP语言开发的，通常搭建LNMP网站平台，这是一个技术名词组合的拼写，分开讲就是得会部署Nginx、MySQL和PHP。 如果是JAVA语言开发的，通常使用Tomcat运行项目，为了提高访问速度，可以使用Nginx反向代理Tomcat，Nginx处理静态页面，Tomcat处理动态页面，实现动静分离。 不是会部署这么简单，还要知道HTTP协议工作原理、简单的性能调优。 负载均衡器 单台服务器终究资源有限，抵抗高访问量肯定是无法支撑的，解决此问题最关键的技术就是采用负载均衡器，水平扩展多台Web服务器，同时对外提供服务，这样就成倍扩展性能了。负载均衡器主流开源技术有LVS、HAProxy和Nginx。一定要熟悉一两个！ 数据库 数据库选择MySQL，它是世界上使用最为广泛的开源数据库。学它准没错！ 也要会一些简单的SQL语句、用户管理、常用存储引擎、数据库备份与恢复。 想要深入点，必须会主从复制、性能优化、主流集群方案：MHA、MGR等。 NoSQL这么流行当然也少不了，学下Redis、MongoDB这两个就好了。 监控系统 监控必不可少，是及时发现问题和追溯问题的救命稻草。可以选择学习主流的Zabbix、Prometheus开源监控系统，功能丰富，能满足企业级监控需求。监控点包括服务器硬件、服务器性能、API、业务、PV/UV、日志等方面。 也可以弄个仪表盘展示几个实时关键的数据，比如Grafana，会非常炫酷。 日志分析系统 日志也很重要，定期的分析，可发现潜在隐患，提炼出有价值的东西。 主流日志系统：ELK Stack 学会部署使用，能分析日志并可视化，方便故障排查。 安全防范 安全很重要，不要等到系统被攻击了，再做安全策略，此时已晚！所以，当一台服务器上线后应马上做安全访问控制策略，比如使用iptables限制只允许信任源IP访问，关闭一些无用的服务和端口等。 一些常见的攻击类型一定得知道啊，否则怎么对症下药呢！比如CC、DDOS、ARP等。 Shell脚本编程进阶 Shell脚本是Linux自动完成工作的利器，必须得熟练编写，所以得进一步学习函数、数组、信号、发邮件等。 文本处理三剑客（grep、sed、awk）得玩6啊，Linux下文本处理就指望它们了。 Python/Go 开发基础 Shell脚本只能完成一些基本的任务，想要完成更复杂些的任务，比如调用API、多进程等。就需要学高级语言了。Python是运维领域使用最多的语言，简单易用，学它准没错！此阶段掌握基础就可以了，例如基本语法结构、文件对象操作、函数、迭代对象、异常处理、发邮件、数据库编程等。 ::: ::: warning 版权声明 本站文章来源于互联网与个人学习笔记总结，仅用于技术分享交流使用。未经允许不得转载！ ::: ","date":"2019-11-11","objectID":"/posts/linux-basic/linux-technology-stack/:1:0","tags":["Linux 学习之旅"],"title":"Linux 系统的服务课程笔记","uri":"/posts/linux-basic/linux-technology-stack/"},{"categories":["Linux"],"content":"nfs共享存储 ","date":"2019-09-13","objectID":"/posts/linux-basic/nfs/:0:0","tags":["Linux 学习之旅","NFS"],"title":"NFS-网络共享存储服务","uri":"/posts/linux-basic/nfs/"},{"categories":["Linux"],"content":"1.NFS基本概述 ","date":"2019-09-13","objectID":"/posts/linux-basic/nfs/:1:0","tags":["Linux 学习之旅","NFS"],"title":"NFS-网络共享存储服务","uri":"/posts/linux-basic/nfs/"},{"categories":["Linux"],"content":"1.1 什么是NFS? NFS是Network File System的缩写及网络文件系统。[ 通常我们称NFS为共享存储] ","date":"2019-09-13","objectID":"/posts/linux-basic/nfs/:1:1","tags":["Linux 学习之旅","NFS"],"title":"NFS-网络共享存储服务","uri":"/posts/linux-basic/nfs/"},{"categories":["Linux"],"content":"1.2 NFS能干什么? NFS的主要功能是通过局域网络让不同主机系统之间可以共享目录。 ","date":"2019-09-13","objectID":"/posts/linux-basic/nfs/:1:2","tags":["Linux 学习之旅","NFS"],"title":"NFS-网络共享存储服务","uri":"/posts/linux-basic/nfs/"},{"categories":["Linux"],"content":"1.3 为什么要使用NFS? 在网站集群架构中如果没有共享存储的情况如下: A用户上传图片经过负载均衡，负载均衡将上传请求调度至WEB1服务器上。 B用户访问A用户上传的图片，此时B用户被负载均衡调度至WEB2_上，因为WEB2_ 上没有这张图片，所以B用户无法看到A用户传的图片。 在网站集群架构中如果有共享存储的情况如下: A用户上传图片无论被负载均衡调度至WEB1还是WEB2,最终数据都被写入至共享存储 B用户访问A用户上传图片时，无论调度至WEB1还是WEB2，最终都会上共享存储访问对应的文件，这样就可以访问到资源了 ","date":"2019-09-13","objectID":"/posts/linux-basic/nfs/:1:3","tags":["Linux 学习之旅","NFS"],"title":"NFS-网络共享存储服务","uri":"/posts/linux-basic/nfs/"},{"categories":["Linux"],"content":"1.4 使用NFS共享存储能解决集群架构的什么问题? 解决多台web静态资源的共享(所有客户端都挂载服务端，看到的数据都- -样) 解决多台web静态资源-致性(如果客户端A删除NFS服务上的test文件，客户端B. 上也会看不见test文件) 解决多台web磁盘空间的浪费 ","date":"2019-09-13","objectID":"/posts/linux-basic/nfs/:1:4","tags":["Linux 学习之旅","NFS"],"title":"NFS-网络共享存储服务","uri":"/posts/linux-basic/nfs/"},{"categories":["Linux"],"content":"1.5 企业使用NFS注意事项 由于用户请求静态资源每次都需要web连接NFS服务获取，那么势必会带来-定的网络开销、以及网络延时、所以增加NFS服务并不能给网站带来访问速度的提升。 如果希望对上传的图片、附件等静资源进行加速，建议将静态资源统-存放至NFS服务端。这样便于后期统一推送至CDN, 以此来实现资源的加速。 ","date":"2019-09-13","objectID":"/posts/linux-basic/nfs/:1:5","tags":["Linux 学习之旅","NFS"],"title":"NFS-网络共享存储服务","uri":"/posts/linux-basic/nfs/"},{"categories":["Linux"],"content":"2.NFS原理 ","date":"2019-09-13","objectID":"/posts/linux-basic/nfs/:2:0","tags":["Linux 学习之旅","NFS"],"title":"NFS-网络共享存储服务","uri":"/posts/linux-basic/nfs/"},{"categories":["Linux"],"content":"2.1 本地文件操作方式 当用户执行mkdir命令, BashShell无法完成该命令操作，会将其翻译给内核。 Kernel内核解析完成后会驱动对应的磁盘设备，完成创建目录的操作。 ","date":"2019-09-13","objectID":"/posts/linux-basic/nfs/:2:1","tags":["Linux 学习之旅","NFS"],"title":"NFS-网络共享存储服务","uri":"/posts/linux-basic/nfs/"},{"categories":["Linux"],"content":"NFS实现原理 需要先了解 程序|进程|线程 NFS客户端执行增、删等操作，客户端会使用不同的函数对该操作进行封装。 NFS客户端会通过TCP/IP的方式传递给NFS服务端。 NFS服务端接收到请求后，会先 调用portmap进程进行端C映射。 nfsd进程用于判断NFS客户端是否拥有权限连接NFS服务端。 Rpc.mount进程判断客户端是否有对应的权限进行验证。 idmap进程实现用户映射和压缩。 最后NFS服务端会将客户端的函数转换为本地能执行的命令，然后将命令传递至内核，由内核 驱动硬件 注意:rpc是 一个远程过程调用，那么使用nfs必须有rpc服务 ","date":"2019-09-13","objectID":"/posts/linux-basic/nfs/:2:2","tags":["Linux 学习之旅","NFS"],"title":"NFS-网络共享存储服务","uri":"/posts/linux-basic/nfs/"},{"categories":["Linux"],"content":"3.NFS服务安装 服务器系统 角色 外网IP 内网IP CentOS 7.6 NFS服务端 eth0:10.0.0.31 eth1:172.16.1.31 CentOS 7.6 NFS客户端 eth0:10.0.0.41 eth1:172.16.1.41 ","date":"2019-09-13","objectID":"/posts/linux-basic/nfs/:3:0","tags":["Linux 学习之旅","NFS"],"title":"NFS-网络共享存储服务","uri":"/posts/linux-basic/nfs/"},{"categories":["Linux"],"content":"3.1 环境配置 #关闭Firewalld防火墙 [root@nfs ~]# systemctL disable firewalld [root@nfs ~]# systemctl stop firewalld #关闭selinux [root@nfs ~]# sed -ri ' #^SEL INUX=#cSEL INUX=Disabled' /etc/selinux/config [root@nfs ~]# setenforce 0 #安装nfs-server服务 yum -y install nfs-utils ","date":"2019-09-13","objectID":"/posts/linux-basic/nfs/:3:1","tags":["Linux 学习之旅","NFS"],"title":"NFS-网络共享存储服务","uri":"/posts/linux-basic/nfs/"},{"categories":["Linux"],"content":"3.2配置nfs服务 nfs服务程序的配置文件为/etc/exports，需要严格按照共享目录的路径允许访问的NFS客户端(共享权限参数)格式书写， 定义要共享的目录与相应的权限，具体书写方式如下图所示。 ","date":"2019-09-13","objectID":"/posts/linux-basic/nfs/:3:2","tags":["Linux 学习之旅","NFS"],"title":"NFS-网络共享存储服务","uri":"/posts/linux-basic/nfs/"},{"categories":["Linux"],"content":"3.3.使用场景 将nfs服务端的 /data 目录共享给 172.16.1.0/24 网段内的所有主机 所有客户端主机都拥有读写权限 在将数据写入到NFS服务器的硬盘中后才会结束操作，最大限度保证数据不丢失 将所有用户映射为本地的匿名用户(nfsnobody) #NFS客户端地址与权限之间没有空格 [root@nfs ~]# vim /etc/exports /data 172.16.1.0/24(rw, sync , all_ squash) #在NFS服务器上建立用于NFS文件共享的目录，并设置对应权限 [root@nfs ~]# mkdir /data [root@nfs ~]# chown -R nfsnobody. nfsnobody /data #NFS共享目录会记录至/var/lib/nfs/etab,如果该目录不存在共享信息，请检查/etc/exports是否配置错误 ","date":"2019-09-13","objectID":"/posts/linux-basic/nfs/:3:3","tags":["Linux 学习之旅","NFS"],"title":"NFS-网络共享存储服务","uri":"/posts/linux-basic/nfs/"},{"categories":["Linux"],"content":"3.4 安装RPC 在使用NFS服务进行文件共享之前，需要使用RPC (Remote Procedure Call 远程过程调用服务 将NFS服务器的IP地址和端口号信息发送给客户端。因此，在启动NFS服务之前，需要先重启并. 启用rpcbind服务程序,同时都加入开机自启动 #加入开机自启 [root@nfs ~]# systemctL enable rpcbind nfs -server #启动服务 [root@nfs ~]# systemctl restart rpcbind nfs-server ","date":"2019-09-13","objectID":"/posts/linux-basic/nfs/:3:4","tags":["Linux 学习之旅","NFS"],"title":"NFS-网络共享存储服务","uri":"/posts/linux-basic/nfs/"},{"categories":["Linux"],"content":"4.NFS客户端挂载卸载 ","date":"2019-09-13","objectID":"/posts/linux-basic/nfs/:4:0","tags":["Linux 学习之旅","NFS"],"title":"NFS-网络共享存储服务","uri":"/posts/linux-basic/nfs/"},{"categories":["Linux"],"content":"4.1 查看远程共享目录 NFS客户端的配置步骤也+分简单。先使用showmount命令，查询NFS服务器的远程共享信息，其输出格式为“共享的目录名称允许使用客户端地址”。 #安装客户端工具，安装nfs-utils即可， 会自动启动rpcbind服务。 [root@nfs-client ~]# yum -y install nfs-utils #客户端使用showmount -e查看远程服务器rpc提供的可挂载nfs信息. [root@nfs-client ~]# showmount -e 172.16.1.31 Export list for 172.16.1.31: /data 172.16.1.0/24 ","date":"2019-09-13","objectID":"/posts/linux-basic/nfs/:4:1","tags":["Linux 学习之旅","NFS"],"title":"NFS-网络共享存储服务","uri":"/posts/linux-basic/nfs/"},{"categories":["Linux"],"content":"4.2 挂载远程共享目录 在NFS客户端创建一个挂载目录,使用mount命令并结合-t参数,指定要挂载的文件系统的类型,并在命令后面写上服务器的IP地址,以及服务器上的共享目录,最后需要写上要挂载到本地系统(客户端)的目录。 [root@nfs-client ~]# mkdir /nfsdir [root@nfs-client ~]# mount -t nfs 172. 16.1.31:/data /nfsdir #查看挂载信息(mount也可以查看) [root@nfs-client ~]# df -h Filesystem Size Used Avail Use% Mounted on /dev/sda3 62G 845M 58G 2% /tmpfs 244M 0 244M 0% /dev/shm /dev/sda1 190M 26M 155M 14% /boot 172. 16.1.31:/data 62G 880M 58G 2% /nfsdir ","date":"2019-09-13","objectID":"/posts/linux-basic/nfs/:4:2","tags":["Linux 学习之旅","NFS"],"title":"NFS-网络共享存储服务","uri":"/posts/linux-basic/nfs/"},{"categories":["Linux"],"content":"4.3 客户端远程共享目录操作 挂载成功后可以进行增删改操作 #使用客户端往nfs存储写入 [root@nfs-client ~]# echo \"nfs-client\" \u003e\u003e /mnt/test. txt #检查nfs服务端是否存在客户端创建的新文件 [root@nfs-client ~]# cat /data/test. txt nfs-client #如果希望NFS文件共享服务能一直有效，则需要将其写入到fstab文件中 [root@nfs-client ~]# vim /etc/fstab 172.16.1.31: /data /nfsdir nfs defaults 0 0 ","date":"2019-09-13","objectID":"/posts/linux-basic/nfs/:4:3","tags":["Linux 学习之旅","NFS"],"title":"NFS-网络共享存储服务","uri":"/posts/linux-basic/nfs/"},{"categories":["Linux"],"content":"4.4 客户端卸载 如果不希望使用NFS共享,可进行卸载 umount /nfsdir #注意:卸载的时候如果提示”umount. nfs: /nfsdir: device is busy” #切换至其他目录，然后在进行卸载。 #NFS Server宕机，强制卸载 umount -lf /nfsdir ","date":"2019-09-13","objectID":"/posts/linux-basic/nfs/:4:4","tags":["Linux 学习之旅","NFS"],"title":"NFS-网络共享存储服务","uri":"/posts/linux-basic/nfs/"},{"categories":["Linux"],"content":"4.5 客户端安全参数 在企业工作场景，通常情况NFS服务器共享的只是普通静态数据(图片、附件、视频)，不需 要执行suid、exec 等权限，挂载的这个文件系统只能作为数据存取之用，无法执行程序，对于 客户端来讲增加了安全性。 例如:很多木马篡改站点文件都是由上传入口上传的程序到存储目录。然后执行的。 #通过mount -o指定挂载参数，禁止使用suid, exec, 增加安全性能 [root@nfs-client ~]# mount -t nfs -。 nosuid, noexec, nodev 172. 16. 1.31:/data /mnt有时也需要考虑性能相关参数[可选] #通过mount -o指定挂载参数，禁止更新目录及文件时间戳挂载 [root@nfs-client ~]# mount -t nfs -0 noatime, nodiratime 172. 16.1.31:/data /mnt ","date":"2019-09-13","objectID":"/posts/linux-basic/nfs/:4:5","tags":["Linux 学习之旅","NFS"],"title":"NFS-网络共享存储服务","uri":"/posts/linux-basic/nfs/"},{"categories":["Linux"],"content":"5.NFS配置详解 nfs共享参数 参数作用 rw* 读写权限 ro 只读权限 root_ squash 当NFS客户端以root管理员访问时，映射为NFS服务器的匿名用户(不常用) no_ root_ squash 当NFS客户端以root管理员访问时，映射为NFS服务器的root管理员(不常用) all squash 无论NFS客户端使用什么账户访问，均映射为NFS服务器的匿名用户(常用) no_ all squash 无论NFS客户端使用什么账户访问，都不进行压缩 sync* 同时将数据写入到内存与硬盘中，保证不丢失数据 async 优先将数据保存到内存，然后再写入硬盘; 这样效率更高，但可能会丢失数据 anonuid* 配置all_ squash使用,指定NFS的用户UID,必须存在系统 anongid* 配置all_ squash使用，指定NFS的用户UID,必须存在系统 ","date":"2019-09-13","objectID":"/posts/linux-basic/nfs/:5:0","tags":["Linux 学习之旅","NFS"],"title":"NFS-网络共享存储服务","uri":"/posts/linux-basic/nfs/"},{"categories":["Linux"],"content":"6.NFS权限实践 ","date":"2019-09-13","objectID":"/posts/linux-basic/nfs/:6:0","tags":["Linux 学习之旅","NFS"],"title":"NFS-网络共享存储服务","uri":"/posts/linux-basic/nfs/"},{"categories":["Linux"],"content":"6.1 验证ro权限实践 #服务端修改rw为ro参数 [root@nfs ~]# cat /etc/exports /data 172.16.1. 0/24(ro, sync ,all_ squash) [root@nfs ~]# systemctl restart nfs -server #客户端验证 [root@nfs-client ~]# mount -t nfs 172. 16.1.31:/data /mnt [root@nfs-client ~]# df -h Filesystem Size Used Avail Use% Mounted on 172.16.1.31: /data 98G 1.7G 97G 2% /mnt #发现无法正常写入文件 [root@backup mnt]# touch file touch: cannot touch 'file' : Read-only file system","date":"2019-09-13","objectID":"/posts/linux-basic/nfs/:6:1","tags":["Linux 学习之旅","NFS"],"title":"NFS-网络共享存储服务","uri":"/posts/linux-basic/nfs/"},{"categories":["Linux"],"content":"6.2 验证all squash、anonuid、 anongid权限 #NFS服务端配置 [root@nfs ~]# cat /etc/exports /data 172.16.1.0/24(rw,sync,all_squash,anonuid=666,anongid=666) #服务端需要创建对应的用户 [root@nfs ~]# groupadd -g 666 ww [root@nfs ~]# useradd -u 666 -g 666 www [root@nfs ~]# id www uid=666(www) gid=666(www) groups=666(www) #重载nfs-server [root@nfs ~]# systemctl restart nfs -server [root@nfs ~]# cat /var/lib/nfs/etab /data 172.16.1.0/ 24(rw, sync,wdelay,hide,nocrossmnt,secure,root_squash,all_squash, no_subtree_check,secure_locks,acl,no_ pnfs,anonuid=666,anongid=666,sec=sys,secure,ro ot_squash,all_squash) #授权共享目录为www [root@nfs ~]# chown -R www. www /data/ [root@nfs ~]# ll -d /data/ drwxr-xr-x 3 wwW WWW 53 Sep 3 02:08 /data/ #客户端验证 [root@backup ~]# umount /mnt/ [root@backup ~]# mount -t nfs 172. 16.1.31:/data /mnt #客户端查看到的文件，身份是666 [root@backup ~]# Ll /mnt/ drwxr-xr-x 2 666 666 6 Sep3 02:08 rsync_ dir-rw-r--r-- 1 666 666 0 Sep 3 02:08 rsync_ file #客户端依旧能往/mnt目录下写文件 [root@backup mnt]# touch fff [root@backup mnt]# mkdir 111 [root@backup mnt]# ll drwxr-xr-x 2 666 666 6 Sep3 03:05 111 -rw-r--r-- 1 666 666 0 Sep3 03:05 fff #建议:将客户端也创建一个uid为666, gid为666， 统一身份,避免后续出现权限不足的情况 [root@backup mnt]# groupadd -g 666 Www [root@backup mnt]# useradd -g 666 -u 666 Www [root@backup mnt]# id www uid=666(www) gid=666(www) groups=666( www) #最后检查文件的身份 [root@backup mnt]# ll /mnt/ total 4 drwxr-xr-x 2 wwW WWW 6 Sep 3 03:05 111 -rw-r--r-- 1 WwW wwW 0 Sep 3 03:05 fff ","date":"2019-09-13","objectID":"/posts/linux-basic/nfs/:6:2","tags":["Linux 学习之旅","NFS"],"title":"NFS-网络共享存储服务","uri":"/posts/linux-basic/nfs/"},{"categories":["Linux"],"content":"7.NFS存储总结 NFS存储优点 NFS简单易用、方便部署数据可靠、服务稳定、满足中小企业需求。 NFS的数据都在文件系统之上，所有数据都是能看得见。 NFS存储局限 存在单点故障,如果构建高可用维护麻烦web- \u003enfs( )- \u003ebackup NFS数据都是明文，并不对数据 做任何校验，也没有密码验证(强烈建议内网使用)。 NFS应用建议 生产场景应将静态数据(ipg\\png\\mp4\\av\\cssjs)尽可能放置CDN场景进行环境以此来减少后端存储压力 如果没有缓存或架构、代码等,本身历史遗留问题太大，在多存储也没意义 ","date":"2019-09-13","objectID":"/posts/linux-basic/nfs/:7:0","tags":["Linux 学习之旅","NFS"],"title":"NFS-网络共享存储服务","uri":"/posts/linux-basic/nfs/"},{"categories":null,"content":"1.下载Elasticserch安装包 https://www.elastic.co/cn/downloads/elasticsearch 当前最新版本为 8.10.4 ","date":"0001-01-01","objectID":"/posts/elk/elasticsearch-8.10.4%E5%AE%89%E8%A3%85/:1:0","tags":null,"title":"","uri":"/posts/elk/elasticsearch-8.10.4%E5%AE%89%E8%A3%85/"},{"categories":null,"content":"集群规划 服务器 角色 192.168.10.107,eslg01 master/data 192.168.10.108,eslg02 master/data 192.168.10.109,eslg03 master/data 各节点创建普通用户 ES不能使用root用户来启动，否则会报错，使用普通用户来安装启动。创建一个普通用户以及定义一些常规目录用于存放我们的数据文件以及安装包等 root@LogServer02:/softws/elasticsearch-8.10.4/jdk# useradd es root@LogServer02:/softws/elasticsearch-8.10.4/jdk# passwd es New password: Retype new password: passwd: password updated successfully echo \"$user:$password\" | chpasswdvim /etc/hosts 192.168.10.107 eslg01 192.168.10.108 eslg02 192.168.10.109 eslg03","date":"0001-01-01","objectID":"/posts/elk/elasticsearch-8.10.4%E5%AE%89%E8%A3%85/:2:0","tags":null,"title":"","uri":"/posts/elk/elasticsearch-8.10.4%E5%AE%89%E8%A3%85/"},{"categories":null,"content":"调整操作系统参数 elasticsearch要求进程最大打开数量为最低65536 vim /etc/security/limits.conf修改/etc/sysctl.conf文件，增加配置vm.max_map_count=262144 vim /etc/sysctl.conf vm.max_map_count=262144 # 退出执行 sysctl -pvm.max_map_count用于限制单个进程的VMA（虚拟内存区域）数量。默认值为65530，对于绝大多数应用程序来说已经足够。如果应用程序因为内存消耗过大而报错，请增大本参数的值 https://docs.nebula-graph.com.cn/2.5.0/5.configurations-and-logs/1.configurations/6.kernel-config/ ","date":"0001-01-01","objectID":"/posts/elk/elasticsearch-8.10.4%E5%AE%89%E8%A3%85/:3:0","tags":null,"title":"","uri":"/posts/elk/elasticsearch-8.10.4%E5%AE%89%E8%A3%85/"},{"categories":null,"content":"安装Elasticsearch tar -xvf 配置环境变量 vim /etc/profile.d/elasticsearch.sh export JAVA_HOME=/softws/elasticsearch-8.10.4/jdk export ES_HOME=/softws/elasticsearch-8.10.4 export PATH=$ES_HOME/bin:$PATH source /etc/profile.d/elasticsearch.sh","date":"0001-01-01","objectID":"/posts/elk/elasticsearch-8.10.4%E5%AE%89%E8%A3%85/:4:0","tags":null,"title":"","uri":"/posts/elk/elasticsearch-8.10.4%E5%AE%89%E8%A3%85/"},{"categories":null,"content":"配置es 创建数据文件，证书目录, 并修改 Elasticsearch 文件拥有者 # 创建数据文件目录 mkdir /softws/elasticsearch-8.10.4/data # 创建证书目录 mkdir /softws/elasticsearch-8.10.4/config/certs # 修改文件拥有者 chown -R es:es /softws/elasticsearch-8.10.4 # 分发到其他节点,并chown scp -r /opt/elasticsearch-8.3.2 lsyk02:/opt scp -r /opt/elasticsearch-8.3.2 lsyk03:/opt ssh lsyk02 chown -R es:es /opt/elasticsearch-8.3.2 ssh lsyk03 chown -R es:es /opt/elasticsearch-8.3.2签发 ca 证书，过程中需按两次回车键,生成目录：es的home:/opt/elasticsearch-8.3.2/ $ ./elasticsearch-certutil ca warning: ignoring JAVA_HOME=/softws/elasticsearch-8.10.4/jdk; using bundled JDK This tool assists you in the generation of X.509 certificates and certificate signing requests for use with SSL/TLS in the Elastic stack. The 'ca' mode generates a new 'certificate authority' This will create a new X.509 certificate and private key that can be used to sign certificate when running in 'cert' mode. Use the 'ca-dn' option if you wish to configure the 'distinguished name' of the certificate authority By default the 'ca' mode produces a single PKCS#12 output file which holds: * The CA certificate * The CA's private key If you elect to generate PEM format certificates (the -pem option), then the output will be a zip file containing individual files for the CA certificate and private key Please enter the desired output file [elastic-stack-ca.p12]: Enter password for elastic-stack-ca.p12 :用 ca 证书签发节点证书，过程中需按三次回车键,生成目录：es的home:/opt/elasticsearch-8.3.2/ $ ./elasticsearch-certutil cert --ca /softws/elasticsearch-8.10.4/elastic-stack-ca.p12 warning: ignoring JAVA_HOME=/softws/elasticsearch-8.10.4/jdk; using bundled JDK This tool assists you in the generation of X.509 certificates and certificate signing requests for use with SSL/TLS in the Elastic stack. The 'cert' mode generates X.509 certificate and private keys. * By default, this generates a single certificate and key for use on a single instance. * The '-multiple' option will prompt you to enter details for multiple instances and will generate a certificate and key for each one * The '-in' option allows for the certificate generation to be automated by describing the details of each instance in a YAML file * An instance is any piece of the Elastic Stack that requires an SSL certificate. Depending on your configuration, Elasticsearch, Logstash, Kibana, and Beats may all require a certificate and private key. * The minimum required value for each instance is a name. This can simply be the hostname, which will be used as the Common Name of the certificate. A full distinguished name may also be used. * A filename value may be required for each instance. This is necessary when the name would result in an invalid file or directory name. The name provided here is used as the directory name (within the zip) and the prefix for the key and certificate files. The filename is required if you are prompted and the name is not displayed in the prompt. * IP addresses and DNS names are optional. Multiple values can be specified as a comma separated string. If no IP addresses or DNS names are provided, you may disable hostname verification in your SSL configuration. * All certificates generated by this tool will be signed by a certificate authority (CA) unless the --self-signed command line option is specified. The tool can automatically generate a new CA for you, or you can provide your own with the --ca or --ca-cert command line options. By default the 'cert' mode produces a single PKCS#12 output file which holds: * The instance certificate * The private key for the instance certificate * The CA certificate If you specify any of the following options: * -pem (PEM formatted output) * -multiple (generate multiple certificates) * -in (generate certificates from an input file) then the output will be be a zip file containing individual certificate/key files Enter password for CA (/softws/elasticsearch-8.10.4/elastic-stack-ca.p12) : Please enter the desired output file [elastic-certificates.p12]: Enter password for elas","date":"0001-01-01","objectID":"/posts/elk/elasticsearch-8.10.4%E5%AE%89%E8%A3%85/:4:1","tags":null,"title":"","uri":"/posts/elk/elasticsearch-8.10.4%E5%AE%89%E8%A3%85/"},{"categories":null,"content":"Cert-manager + ZeroSSL 的一些配置的研究 ","date":"0001-01-01","objectID":"/posts/kubernetes/primary/cert-manager-+-zerossl-%E7%9A%84%E4%B8%80%E4%BA%9B%E9%85%8D%E7%BD%AE%E7%9A%84%E7%A0%94%E7%A9%B6/:0:0","tags":null,"title":"","uri":"/posts/kubernetes/primary/cert-manager-+-zerossl-%E7%9A%84%E4%B8%80%E4%BA%9B%E9%85%8D%E7%BD%AE%E7%9A%84%E7%A0%94%E7%A9%B6/"},{"categories":null,"content":"安装cert-manager helm install \\ cert-manager ./cert-manager-v1.8.2.tgz \\ --namespace cert-manager \\ --create-namespace root@k8s-master01:/helm/cert-manager# helm upgrade cert-manager ./cert-manager-v1.8.2.tgz --namespace cert-manager --create-namespace Release \"cert-manager\" has been upgraded. Happy Helming! NAME: cert-manager LAST DEPLOYED: Fri Oct 13 15:30:56 2023 NAMESPACE: cert-manager STATUS: deployed REVISION: 2 TEST SUITE: None NOTES: cert-manager v1.8.2 has been deployed successfully! In order to begin issuing certificates, you will need to set up a ClusterIssuer or Issuer resource (for example, by creating a 'letsencrypt-staging' issuer). More information on the different types of issuers and how to configure them can be found in our documentation: https://cert-manager.io/docs/configuration/ For information on how to configure cert-manager to automatically provision Certificates for Ingress resources, take a look at the `ingress-shim` documentation: https://cert-manager.io/docs/usage/ingress/","date":"0001-01-01","objectID":"/posts/kubernetes/primary/cert-manager-+-zerossl-%E7%9A%84%E4%B8%80%E4%BA%9B%E9%85%8D%E7%BD%AE%E7%9A%84%E7%A0%94%E7%A9%B6/:1:0","tags":null,"title":"","uri":"/posts/kubernetes/primary/cert-manager-+-zerossl-%E7%9A%84%E4%B8%80%E4%BA%9B%E9%85%8D%E7%BD%AE%E7%9A%84%E7%A0%94%E7%A9%B6/"},{"categories":null,"content":"Values File ingressShim: defaultIssuerName: \"zerossl-production\" defaultIssuerKind: \"ClusterIssuer\" installCRDs: trueroot@k8s-master01:/helm/cert-manager# helm upgrade cert-manager ./cert-manager-v1.8.2.tgz --namespace cert-manager --create-namespace -f values.yaml Release \"cert-manager\" has been upgraded. Happy Helming! NAME: cert-manager LAST DEPLOYED: Fri Oct 13 15:32:30 2023 NAMESPACE: cert-manager STATUS: deployed REVISION: 3 TEST SUITE: None NOTES: cert-manager v1.8.2 has been deployed successfully! In order to begin issuing certificates, you will need to set up a ClusterIssuer or Issuer resource (for example, by creating a 'letsencrypt-staging' issuer). More information on the different types of issuers and how to configure them can be found in our documentation: https://cert-manager.io/docs/configuration/ For information on how to configure cert-manager to automatically provision Certificates for Ingress resources, take a look at the `ingress-shim` documentation: https://cert-manager.io/docs/usage/ingress/","date":"0001-01-01","objectID":"/posts/kubernetes/primary/cert-manager-+-zerossl-%E7%9A%84%E4%B8%80%E4%BA%9B%E9%85%8D%E7%BD%AE%E7%9A%84%E7%A0%94%E7%A9%B6/:2:0","tags":null,"title":"","uri":"/posts/kubernetes/primary/cert-manager-+-zerossl-%E7%9A%84%E4%B8%80%E4%BA%9B%E9%85%8D%E7%BD%AE%E7%9A%84%E7%A0%94%E7%A9%B6/"},{"categories":null,"content":"EAB Credentials https://app.zerossl.com/developer echo -n \"sbp1S7OUASDD12Kci8233vITEWlw7_cCI23XWo3ngD-L8z94gvSDStYLMICXhYjp-CZXC4DDNFLwaz8iTVI0g\" | base64 -w 0kubectl apply -f zero-ssl-eabsecret.yaml -n cert-manager","date":"0001-01-01","objectID":"/posts/kubernetes/primary/cert-manager-+-zerossl-%E7%9A%84%E4%B8%80%E4%BA%9B%E9%85%8D%E7%BD%AE%E7%9A%84%E7%A0%94%E7%A9%B6/:3:0","tags":null,"title":"","uri":"/posts/kubernetes/primary/cert-manager-+-zerossl-%E7%9A%84%E4%B8%80%E4%BA%9B%E9%85%8D%E7%BD%AE%E7%9A%84%E7%A0%94%E7%A9%B6/"},{"categories":null,"content":"Cluster issuer apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: zerossl-production spec: acme: # ZeroSSL ACME server server: https://acme.zerossl.com/v2/DV90 email: xx9z@outlook.com # name of a secret used to store the ACME account private key privateKeySecretRef: name: zerossl-prod # for each cert-manager new EAB credencials are required externalAccountBinding: keyID: Bkxv--iXeFgLY61uePdMQg keySecretRef: name: zero-ssl-eabsecret key: secret keyAlgorithm: HS256 # ACME DNS-01 provider configurations to verify domain solvers: - selector: {} dns01: route53: region: us-west-2 # optional if ambient credentials are available; see ambient credentials documentation # see Route53 for \u003e0 issue \"letsencrypt.org\" and change to \u003e0 issue \"sectigo.com\" accessKeyID: ACCESS_KEY_ID secretAccessKeySecretRef: name: route53-credentials-secret key: secret-access-keycs https://www.bsmithio.com/post/cert-manager-zerossl/ ","date":"0001-01-01","objectID":"/posts/kubernetes/primary/cert-manager-+-zerossl-%E7%9A%84%E4%B8%80%E4%BA%9B%E9%85%8D%E7%BD%AE%E7%9A%84%E7%A0%94%E7%A9%B6/:4:0","tags":null,"title":"","uri":"/posts/kubernetes/primary/cert-manager-+-zerossl-%E7%9A%84%E4%B8%80%E4%BA%9B%E9%85%8D%E7%BD%AE%E7%9A%84%E7%A0%94%E7%A9%B6/"}]